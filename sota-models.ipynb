{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd048f22",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-04-22T12:11:43.998036Z",
     "iopub.status.busy": "2024-04-22T12:11:43.997456Z",
     "iopub.status.idle": "2024-04-22T12:12:09.190326Z",
     "shell.execute_reply": "2024-04-22T12:12:09.188819Z"
    },
    "papermill": {
     "duration": 25.203485,
     "end_time": "2024-04-22T12:12:09.193060",
     "exception": false,
     "start_time": "2024-04-22T12:11:43.989575",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-22 12:11:47.062650: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-22 12:11:47.062833: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-22 12:11:47.216398: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOTA model implementation, unofficial model zoo\n",
      "+=+=+=+=+=+=+=+=+=+=+=+=+=\n",
      "\n",
      "Requirements loaded, keras : v3.2.1, Tensorflow : v2.15.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "seed = 2024\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ML tools \n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras_nlp\n",
    "import keras_cv\n",
    "from keras import ops\n",
    "\n",
    "keras.utils.set_random_seed(seed)\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow_decision_forests as tfdf\n",
    "\n",
    "from keras import Input, Model\n",
    "from keras.models import load_model\n",
    "\n",
    "from keras.layers import Conv2D, DepthwiseConv2D, Dense, Activation, BatchNormalization, LayerNormalization, MultiHeadAttention, Embedding, Subtract, Add, Multiply, GlobalAveragePooling2D, GlobalAveragePooling1D, LayerNormalization\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.applications import *\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "print(\"SOTA model implementation, unofficial model zoo\\n+=+=+=+=+=+=+=+=+=+=+=+=+=\\n\")\n",
    "print(f\"Requirements loaded, keras : v{keras.__version__}, Tensorflow : v{tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfe5a52",
   "metadata": {
    "papermill": {
     "duration": 0.003854,
     "end_time": "2024-04-22T12:12:09.201880",
     "exception": false,
     "start_time": "2024-04-22T12:12:09.198026",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Base helper functions(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5c9fec5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-22T12:12:09.213194Z",
     "iopub.status.busy": "2024-04-22T12:12:09.211916Z",
     "iopub.status.idle": "2024-04-22T12:12:09.239277Z",
     "shell.execute_reply": "2024-04-22T12:12:09.237399Z"
    },
    "papermill": {
     "duration": 0.03674,
     "end_time": "2024-04-22T12:12:09.242637",
     "exception": false,
     "start_time": "2024-04-22T12:12:09.205897",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AttentionPooling(keras.layers.Layer):\n",
    "    def __init__(self, attention_heads, attention_dims = None, bias = False, scale = None, \n",
    "                 dropout_rate = 0.05, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.n_heads = attention_heads\n",
    "        self.n_dims = attention_dims\n",
    "        self.bias = bias\n",
    "        self.scale = scale\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # query, key\n",
    "        query_dims = input_shape[0][-1]\n",
    "        key_length = input_shape[1][1]\n",
    "        \n",
    "        if self.n_dims == None:\n",
    "            embed_dims = query_dims\n",
    "        else:\n",
    "            embed_dims = self.n_dims\n",
    "        self.embed_dims = embed_dims\n",
    "        self.per_head_dims = embed_dims//self.n_heads\n",
    "        \n",
    "        self.scale = self.scale if self.scale != None else embed_dims**-0.5\n",
    "        \n",
    "        self.query_embed_fn = keras.layers.Dense(units = embed_dims, use_bias = self.bias,\n",
    "                                               name = \"Q_Embedding_Dense_layer\")\n",
    "        self.key_embed_fn = keras.layers.Dense(units = embed_dims, use_bias = self.bias,\n",
    "                                               name = \"K_Embedding_Dense_layer\")\n",
    "        self.value_embed_fn = keras.layers.Dense(units = embed_dims, use_bias = self.bias,\n",
    "                                               name = \"V_Embedding_Dense_layer\")\n",
    "        \n",
    "        self.softmax = keras.layers.Activation(\"softmax\", name = \"AttentionWeightSoftmax\")\n",
    "        self.proj = keras.layers.Dense(units = query_dims, use_bias = self.bias, \n",
    "                                      name = \"ProjectToOriginalDimension\")\n",
    "        self.att_dropout = keras.layers.Dropout(self.dropout_rate)\n",
    "        self.proj_dropout = keras.layers.Dropout(self.dropout_rate)\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, inputs, **kwargs):\n",
    "        if len(inputs) == 2:\n",
    "            q, k = inputs\n",
    "            value_ = False\n",
    "        elif len(inputs) == 3:\n",
    "            q, k, v = inputs\n",
    "            value_ = True\n",
    "            \n",
    "        if len(ops.shape(q)) == 2:\n",
    "            q = q[:, tf.newaxis, :]\n",
    "        if len(ops.shape(k)) == 4:\n",
    "            b_, w_, h_, dims_ = ops.shape(k)\n",
    "            k = ops.reshape(k, [b_, w_*h_, dims_])\n",
    "        if (value_) and (len(ops.shape(v))) == 4:\n",
    "            b_, w_, h_, dims_ = ops.shape(v)\n",
    "            v = ops.reshape(v, [b_, w_*h_, dims_])\n",
    "            \n",
    "        batch_size, query_length, q_dims = ops.shape(q)\n",
    "        _, key_length, k_dms = ops.shape(k)\n",
    "        \n",
    "        query = self.query_embed_fn(q) * self.scale #batch, 1(or, query length), q_dims\n",
    "        query = ops.reshape(query, [batch_size, query_length, self.n_heads, self.per_head_dims])\n",
    "        \n",
    "        key = self.key_embed_fn(k)\n",
    "        key = ops.reshape(key, [batch_size, key_length, self.n_heads, self.per_head_dims])\n",
    "        \n",
    "        if value_:\n",
    "            value = self.value_embed_fn(v)#각각 batch, token_length, heads, per_head_dims\n",
    "        else:\n",
    "            value = self.value_embed_fn(k)\n",
    "        value = ops.reshape(value, [batch_size, key_length, self.n_heads, self.per_head_dims])\n",
    "        attention_score = keras.ops.einsum(\"abhd, achd -> ahbc\",\n",
    "                                          query, key) #b = query length, c = key length\n",
    "        attention_weight = self.softmax(attention_score)\n",
    "        attention_weight = self.att_dropout(attention_weight)\n",
    "        self.attention_weight = attention_weight \n",
    "        attended_output = keras.ops.einsum(\"ahbc, achd -> abhd\",\n",
    "                                          attention_weight, value)\n",
    "        attended_output = keras.ops.reshape(attended_output, \n",
    "                                           [batch_size, query_length, self.n_heads*self.per_head_dims]\n",
    "                                           )\n",
    "        attended_output = self.proj(attended_output)\n",
    "        attended_output = self.proj_dropout(attended_output)\n",
    "        if keras.ops.shape(attended_output)[1] == 1:\n",
    "            attended_output = keras.ops.squeeze(attended_output, axis = 1)\n",
    "        return attended_output, attention_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1711fc",
   "metadata": {
    "papermill": {
     "duration": 0.004722,
     "end_time": "2024-04-22T12:12:09.252562",
     "exception": false,
     "start_time": "2024-04-22T12:12:09.247840",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# GC-ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c945fc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-22T12:12:09.264156Z",
     "iopub.status.busy": "2024-04-22T12:12:09.263706Z",
     "iopub.status.idle": "2024-04-22T12:12:09.272514Z",
     "shell.execute_reply": "2024-04-22T12:12:09.271007Z"
    },
    "papermill": {
     "duration": 0.017517,
     "end_time": "2024-04-22T12:12:09.275277",
     "exception": false,
     "start_time": "2024-04-22T12:12:09.257760",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PatchEncoder(keras.layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super().__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = keras.layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = keras.layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = ops.expand_dims(\n",
    "            ops.arange(start=0, stop=self.num_patches, step=1), axis=0\n",
    "        )\n",
    "        projected_patches = self.projection(patch)\n",
    "        encoded = projected_patches + self.position_embedding(positions)\n",
    "        return encoded\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"num_patches\": self.num_patches})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6a28f94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-22T12:12:09.286074Z",
     "iopub.status.busy": "2024-04-22T12:12:09.285637Z",
     "iopub.status.idle": "2024-04-22T12:12:09.375345Z",
     "shell.execute_reply": "2024-04-22T12:12:09.373257Z"
    },
    "papermill": {
     "duration": 0.098771,
     "end_time": "2024-04-22T12:12:09.378159",
     "exception": false,
     "start_time": "2024-04-22T12:12:09.279388",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SE(keras.layers.Layer):\n",
    "    def __init__(self, output_dim = None, squeeze_rate = 0.25, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.output_dim = output_dim\n",
    "        self.rate = squeeze_rate\n",
    "    def build(self, input_shape) : #batch_size, h, w, dims\n",
    "        if self.output_dim == None:\n",
    "            self.output_dim = input_shape[-1]\n",
    "        else:\n",
    "            pass\n",
    "        self.avg_pool = keras.layers.GlobalAveragePooling2D(keepdims = True, name = \"AvgPooling\")\n",
    "        self.mlps = keras.Sequential([keras.layers.Dense(units = int(self.rate * self.output_dim),\n",
    "                                                        use_bias = False, name = \"Dense1\"),\n",
    "                                      keras.layers.Activation(\"gelu\", name = \"GeluAct\"),\n",
    "                                      keras.layers.Dense(units = self.output_dim, use_bias = False, name = \"Dense2\"),\n",
    "                                      keras.layers.Activation(\"sigmoid\", name = \"Excitation_Sigmoid\")\n",
    "                                     ])\n",
    "        #super().build(input_shape)\n",
    "    def call(self, inputs, **kwargs):\n",
    "        pooled = self.avg_pool(inputs)\n",
    "        weights = self.mlps(pooled)\n",
    "        return inputs * weights\n",
    "    \n",
    "class DownSampler(keras.layers.Layer):\n",
    "    def __init__(self, keepdims = False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.keepdims = keepdims\n",
    "    def build(self, input_shape):\n",
    "        embed_dims = input_shape[-1]\n",
    "        out_dim = embed_dims if self.keepdims else 2*embed_dims\n",
    "        self.fused_mbconv = keras.Sequential([keras.layers.DepthwiseConv2D(kernel_size = 3, padding = 'same', use_bias = False, name = \"DWConv\"),\n",
    "                                             keras.layers.Activation(\"gelu\", name = 'GeluAct'),\n",
    "                                             SE(name = \"SqueezeAndExcitation2D\"),\n",
    "                                             keras.layers.Conv2D(filters = embed_dims, kernel_size = 1, padding = 'same', use_bias = False, name = \"PointWiseConv\")],\n",
    "                                            name = \"Fused_MBConvLayer\")\n",
    "        self.down_conv = keras.layers.Conv2D(filters = out_dim, kernel_size = 3, strides = 2, padding = 'same', use_bias = False, name = \"DownConvolution\")\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon = 1e-5, name = 'LayerNorm1')\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon = 1e-5, name = 'LayerNorm2')\n",
    "    def call(self, inputs, **kwargs):\n",
    "        x = self.layernorm1(inputs)\n",
    "        x += self.fused_mbconv(inputs)\n",
    "        x = self.down_conv(x)\n",
    "        return self.layernorm2(x)\n",
    "    \n",
    "class MLP(keras.layers.Layer):\n",
    "    def __init__(self, middle_dim = None, output_dim = None,\n",
    "                activation = 'gelu', dropout = 0.2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.middle_dim = middle_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.activation = activation\n",
    "        self.dropout_rate = dropout\n",
    "    def build(self, input_shape):\n",
    "        self.input_dims = input_shape[-1]\n",
    "        self.middle_dim = int(1.5*self.input_dims) if self.middle_dim == None else self.middle_dim\n",
    "        self.output_dim = self.input_dims if self.output_dim == None else self.output_dim\n",
    "        self.mlp1 = keras.layers.Dense(units = self.middle_dim, name = \"FirstMLP\")\n",
    "        self.act = keras.layers.Activation(self.activation, name = \"MiddleActivation\")\n",
    "        self.mlp2 = keras.layers.Dense(units = self.output_dim, name = \"SecondMLP\")\n",
    "        self.drop1 = keras.layers.Dropout(self.dropout_rate, name = \"Dropout1\")\n",
    "        self.drop2 = keras.layers.Dropout(self.dropout_rate, name = \"Dropout2\")\n",
    "    def call(self, inputs, **kwargs):\n",
    "        x = self.mlp1(inputs)\n",
    "        x = self.act(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.mlp2(x)\n",
    "        x = self.drop2(x)\n",
    "        return x\n",
    "    \n",
    "class PatchEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, patching_type = \"conv\", #conv or tokenlearner\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.patching_type = patching_type\n",
    "    def build(self, input_shape):\n",
    "        if (self.patching_type == \"tokenlearner\") or (self.patching_type == \"token_learner\"):\n",
    "            self.proj = keras.layers.Conv2D(self.embed_dim, kernel_size = 3, strides = 2, padding = 'same', name = \"projection_conv\") #Overlapping patches\n",
    "            #token learner implementation\n",
    "            batch_size, w, h, filters = input_shape\n",
    "            n_tokens = (w//4) * (h//4) ; self.resized_w, self.resized_h = int(w//4), int(h/4)\n",
    "            self.input_seq_flatten = keras.layers.Reshape([1, -1, self.embed_dim], name = \"image_to_sequence_reshape\")\n",
    "            self.layer_norm = keras.layers.LayerNormalization(epsilon = 1e-5)\n",
    "            self.attention_ops = keras.Sequential([keras.layers.Conv2D(n_tokens, kernel_size = 3, activation = \"gelu\", use_bias = False, padding = 'same'),\n",
    "                                                  keras.layers.Conv2D(n_tokens, kernel_size = 3, activation = \"gelu\", use_bias = False, padding = 'same'),\n",
    "                                                  keras.layers.Conv2D(n_tokens, kernel_size = 3, activation = \"sigmoid\", use_bias = False, padding = 'same'),\n",
    "                                                  keras.layers.Reshape([-1, n_tokens]), #batch_size, HW, n_tokens\n",
    "                                                  keras.layers.Permute([2,1])], #batch_size, n_tokens, HW\n",
    "                                                 name = \"Conv_for_attention_weight\")\n",
    "        else:\n",
    "            self.proj = keras.layers.Conv2D(self.embed_dim, kernel_size = 3, strides = 2, padding = 'same') #Overlapping patches\n",
    "            self.down_sample = DownSampler(keepdims = True, name = \"DownSampler_after_projection\")\n",
    "    def call(self, inputs, **kwargs):\n",
    "        if (self.patching_type == \"tokenlearner\") or (self.patching_type == \"token_learner\"):\n",
    "            #token learner implementation\n",
    "            norm_input = self.layer_norm(inputs)\n",
    "            proj_inputs = self.proj(norm_input)\n",
    "            seq_inputs = self.input_seq_flatten(proj_inputs) #batch, 1, HW, embed_dims\n",
    "            att_weights = self.attention_ops(proj_inputs) #batch, n_tokens, HW\n",
    "            att_weights = ops.expand_dims(att_weights, axis = -1) #batch, n_tokens, HW, 1\n",
    "            attended = att_weights * seq_inputs #batch, n_tokens, HW, embed_dims\n",
    "            attended = ops.mean(attended, axis = 2) #batch, n_tokens, embed_dims\n",
    "            #reshape to 2D array\n",
    "            attended = ops.reshape(attended, [-1, self.resized_w, self.resized_h, self.embed_dim]\n",
    "                                  )\n",
    "            return attended\n",
    "        else:\n",
    "            x = self.proj(inputs)\n",
    "            x = self.down_sample(x)\n",
    "            return x\n",
    "        \n",
    "class FeatExtract(keras.layers.Layer):\n",
    "    def __init__(self, keepdims = False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.keepdims = keepdims\n",
    "    def build(self, input_shape):\n",
    "        batch_size, H, W, embed_dims = input_shape\n",
    "        self.fused_mbconv = keras.Sequential([keras.layers.DepthwiseConv2D(kernel_size = 3, padding = 'same', use_bias = False, name = \"DWConv\"),\n",
    "                                             keras.layers.Activation(\"gelu\", name = 'GeluAct'),\n",
    "                                             SE(name = \"SqueezeAndExcitation2D\"),\n",
    "                                             keras.layers.Conv2D(filters = embed_dims, kernel_size = 1, padding = 'same', use_bias = False, name = \"PointWiseConv\")],\n",
    "                                            name = \"Fused_MBConvLayer\")\n",
    "        if self.keepdims == False:\n",
    "            self.pool = keras.layers.MaxPooling2D(name = \"FeatExtractMaxPool2D\")\n",
    "    def call(self, inputs):\n",
    "        x = inputs + self.fused_mbconv(inputs)\n",
    "        if self.keepdims == False:\n",
    "            return self.pool(x)\n",
    "        return x\n",
    "    \n",
    "class GlobalQueryGenerator(keras.layers.Layer):\n",
    "    def __init__(self, keepdims = False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.keepdims = keepdims #Keepdims는 여기서 0과 1로 이루어진 list도 될 수 있다 -> FeatExtract layer를 keepdims의 원소 갯수만큼 repeat!\n",
    "    def build(self, input_shape):\n",
    "        self.q_generator = keras.Sequential([FeatExtract(keepdims = keepdim, name = f\"FeatureExtraction_{idx+1}\") for idx, keepdim in enumerate(self.keepdims)])\n",
    "    def call(self, inputs):\n",
    "        return self.q_generator(inputs)\n",
    "    \n",
    "class WindowAttention(keras.layers.Layer):\n",
    "    def __init__(self, window_size, n_heads, global_query, #제공된다면 global, 아니라면 local mHSA -> 0 or 1\n",
    "                qkv_bias = True, qk_scale = None,\n",
    "                dropout_rate = 0.05, return_attention_weights = False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.window_size = (window_size, window_size)\n",
    "        self.n_heads = n_heads\n",
    "        self.global_query = global_query\n",
    "        self.bias = qkv_bias\n",
    "        self.scale = qk_scale\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.return_attention_weights = return_attention_weights\n",
    "    def build(self, input_shape) :\n",
    "        #input = [query, key, value]\n",
    "        embed_dims = input_shape[0][-1]\n",
    "        head_dims = embed_dims//self.n_heads\n",
    "        self.scale = self.scale if self.scale != None else embed_dims**-0.5\n",
    "        self.qkv_size = 3-int(self.global_query)\n",
    "        self.qkv_embed_fn = keras.layers.Dense(units = embed_dims * self.qkv_size, use_bias = self.bias,\n",
    "                                 name = \"QKV_Embedding_Dense_layer\")\n",
    "        self.softmax = keras.layers.Activation(\"softmax\", name = \"AttentionWeightSoftmax\") #for attention weight computation\n",
    "        self.proj = keras.layers.Dense(units = embed_dims, use_bias = self.bias, name = \"Projection\")\n",
    "        self.attention_dropout = keras.layers.Dropout(self.dropout_rate)\n",
    "        self.projection_dropout = keras.layers.Dropout(self.dropout_rate)\n",
    "        self.relative_position_bias_table = self.add_weight(\n",
    "            name=\"relative_position_bias_table\",\n",
    "            shape=[\n",
    "                (2 * self.window_size[0] - 1) * (2 * self.window_size[1] - 1),\n",
    "                self.n_heads,\n",
    "            ],\n",
    "            initializer=keras.initializers.TruncatedNormal(stddev=0.02),\n",
    "            trainable=True,\n",
    "            dtype=self.dtype,\n",
    "        ) #<- learnable weight of relational position. 위 window size = 4의 예시에서, 임의의 두 지점 간 거리의 경우의 수는 총 49개 -> 이에 해당하는 weight tensor를 만듬.\n",
    "        super().build(input_shape)\n",
    "    def get_relative_position_index(self): #<- window 내 2 지점 간 거리의 index matrix.\n",
    "        coords_h = ops.arange(self.window_size[0])\n",
    "        coords_w = ops.arange(self.window_size[1])\n",
    "        coords = ops.stack(ops.meshgrid(coords_h, coords_w, indexing=\"ij\"), axis=0)\n",
    "        coords_flatten = ops.reshape(coords, [2, -1])\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
    "        relative_coords = ops.transpose(relative_coords, axes=[1, 2, 0])\n",
    "        relative_coords_xx = relative_coords[:, :, 0] + self.window_size[0] - 1\n",
    "        relative_coords_yy = relative_coords[:, :, 1] + self.window_size[1] - 1\n",
    "        relative_coords_xx = relative_coords_xx * (2 * self.window_size[1] - 1)\n",
    "        relative_position_index = relative_coords_xx + relative_coords_yy\n",
    "        return relative_position_index\n",
    "    def call(self, inputs, **kwargs):\n",
    "        #input : key(=value), global query OR key only\n",
    "        # input component shape : batch*n_windows, h_window*w_window, embed_dims -> level/block 설계 시 repeat 처리 후 attention에 feed\n",
    "        if self.global_query :\n",
    "            inputs, q_global = inputs\n",
    "            batch_size = ops.shape(q_global)[0]\n",
    "        else:\n",
    "            inputs = inputs[0]\n",
    "        batch_, token_length, embed_dims = ops.shape(inputs) #global query는 query generator에 의해 token_length 개 만큼의 token으로 전체 이미지/feature map을 압축한 상태\n",
    "        \n",
    "        qkv = self.qkv_embed_fn(inputs) #batch*n_windows, h_w * w_w, qkv_size * embed_dims\n",
    "        \n",
    "        qkv = ops.reshape(qkv, [-1, token_length, self.qkv_size, self.n_heads, embed_dims//self.n_heads])\n",
    "        qkv = ops.transpose(qkv, [2, 0, 3, 1, 4]) #qkv_size, batch_, n_heads, token_length, C\n",
    "        \n",
    "        #QKV 분리\n",
    "        if self.global_query:\n",
    "            k, v = ops.split(qkv, 2, axis = 0) #각각 batch_, n_heads, token_length, C\n",
    "            #repeat the global query tensor\n",
    "            # batch_size, n_query_tokens, dims -> batch_(=batch * n_windows), n_query_tokens, dims\n",
    "            q_global = ops.repeat(q_global, batch_//batch_size, axis = 0) #->batch_, n_query_tokens, dims\n",
    "            q = ops.reshape(q_global, [batch_, token_length, self.n_heads, embed_dims//self.n_heads])\n",
    "            q = ops.transpose(q, [0, 2, 1, 3]\n",
    "                             )\n",
    "        else:\n",
    "            q, k, v = ops.split(qkv, 3, axis = 0)\n",
    "            q = ops.squeeze(q, axis = 0)\n",
    "        k = ops.squeeze(k, axis = 0)\n",
    "        v = ops.squeeze(v, axis = 0)\n",
    "        \n",
    "        q *= self.scale #batch_, n_heads, token_length, dimension_per_heads(=C)\n",
    "        attention_score = q@ops.transpose(k, [0, 1, 3, 2]) #batch_, n_heads, token_length, token_length\n",
    "        \n",
    "        #positional encoding(bias) 계산 -> attention score에 더해 주기\n",
    "        # Code from original keras homepage\n",
    "        relative_position_bias = ops.take(\n",
    "            self.relative_position_bias_table,\n",
    "            ops.reshape(self.get_relative_position_index(), [-1]),\n",
    "        )\n",
    "        relative_position_bias = ops.reshape(\n",
    "            relative_position_bias,\n",
    "            [\n",
    "                self.window_size[0] * self.window_size[1],\n",
    "                self.window_size[0] * self.window_size[1],\n",
    "                -1,\n",
    "            ],\n",
    "        )\n",
    "        relative_position_bias = ops.transpose(relative_position_bias, axes=[2, 0, 1])\n",
    "        attention_score += relative_position_bias[None,]\n",
    "        attention_weight = self.softmax(attention_score)\n",
    "        attention_weight = self.attention_dropout(attention_weight) #batch_, n_heads, token_length, token_length\n",
    "        #value tensor shape : batch_, n_heads, token_length, dimension_per_heads(=C)\n",
    "        attended_output = attention_weight@v\n",
    "        attended_output = ops.transpose(attended_output, [0, 2, 1, 3])\n",
    "        attended_output = ops.reshape(attended_output, [batch_, token_length, embed_dims])\n",
    "        attended_output = self.projection_dropout(self.proj(attended_output))\n",
    "        self.attention_weight = attention_weight\n",
    "        if self.return_attention_weights:\n",
    "            return attended_output, attention_weight\n",
    "        else:\n",
    "            return attended_output\n",
    "        \n",
    "        \n",
    "class Block(keras.layers.Layer):\n",
    "    def __init__(self, #이하는 Window Attention configurations\n",
    "                 window_size, num_heads, global_query, \n",
    "                 qkv_bias = True, qk_scale = None, dropout_rate = 0.05, \n",
    "                 # 이하는 MLP module의 configuration\n",
    "                 mlp_ratio = 4.0, layer_scale = None, return_attention_weights = False,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.window_size = window_size\n",
    "        self.n_heads = num_heads\n",
    "        self.global_query = global_query\n",
    "        self.qkv_bias = qkv_bias\n",
    "        self.qk_scale = qk_scale\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        self.layer_scale = layer_scale\n",
    "        self.return_attention_weights = return_attention_weights\n",
    "    def build(self, input_shape):\n",
    "        #input tensor : list of key/query or key only\n",
    "        # each tensor is batch_size, w, h, channel dims shape tensor\n",
    "        batch_size, H, W, dims = input_shape[0]\n",
    "        self.norm1 = keras.layers.LayerNormalization(epsilon = 1e-5)\n",
    "        self.norm2 = keras.layers.LayerNormalization(epsilon = 1e-5)\n",
    "        self.window_attention = WindowAttention(window_size = self.window_size,\n",
    "                                               n_heads = self.n_heads,\n",
    "                                               global_query = self.global_query,\n",
    "                                               qkv_bias = self.qkv_bias,\n",
    "                                               qk_scale = self.qk_scale,\n",
    "                                               dropout_rate = self.dropout_rate,\n",
    "                                                return_attention_weights = self.return_attention_weights)\n",
    "        self.mlps = MLP(middle_dim = int(self.mlp_ratio * dims), dropout = self.dropout_rate)\n",
    "        if self.layer_scale != None:\n",
    "            self.gamma1 = self.add_weight(shape = [dims], name = \"Gamma1\", trainable = True,\n",
    "                                         initializer = keras.initializer.Constant(self.layer_scale), dtype = self.dtype)\n",
    "            self.gamma2 = self.add_weight(shape = [dims], name = \"Gamma2\", trainable = True,\n",
    "                                         initializer = keras.initializer.Constant(self.layer_scale), dtype = self.dtype)\n",
    "        else:\n",
    "            self.gamma1, self.gamma2 = 1.0, 1.0\n",
    "        self.n_windows = int(H//self.window_size) * int(W//self.window_size)\n",
    "        \n",
    "    #input feature map을 일정 크기의 window로 partition을 만들어주는 함수 및\n",
    "    # 그 partition을 받아 원래의 feature map으로 돌려주는 함수를 만들자\n",
    "    def window_partition(self, inputs): #feature map -> multiple windows\n",
    "        batch_size, H, W, dims = ops.shape(inputs)\n",
    "        h, w = H//self.window_size, W//self.window_size\n",
    "        inputs = ops.reshape(inputs, [batch_size, \n",
    "                                      h, self.window_size,\n",
    "                                     w, self.window_size, \n",
    "                                     dims])\n",
    "        inputs = ops.transpose(inputs, [0,#batch_size\n",
    "                                        1,3, #h, w\n",
    "                                        2,4, #winsize, winsize\n",
    "                                        5])\n",
    "        return ops.reshape(inputs, [-1, self.window_size, self.window_size, dims]) #batch_size*n_windows, window_size, window_size, dims\n",
    "        \n",
    "    def window_reverse(self, inputs, H, W, dims): #window partition -> original feature map\n",
    "        x = ops.reshape(inputs, [-1, H//self.window_size, W//self.window_size, self.window_size, self.window_size, dims])\n",
    "        x = ops.transpose(x, [0, 1, 3, 2, 4, 5])\n",
    "        return ops.reshape(x, [-1, H, W, dims])\n",
    "    \n",
    "    def call(self, inputs, **kwargs):\n",
    "        if self.global_query:\n",
    "            inputs, global_query = inputs\n",
    "        else:\n",
    "            inputs = inputs[0]\n",
    "        batch_size, H, W, dims = ops.shape(inputs)\n",
    "        x = self.norm1(inputs)\n",
    "        x = self.window_partition(x) \n",
    "        x = ops.reshape(x, [-1, self.window_size*self.window_size, dims])\n",
    "        if self.global_query:\n",
    "            outputs_ = self.window_attention([x, global_query]\n",
    "                                     )\n",
    "        else:\n",
    "            outputs_ = self.window_attention([x])\n",
    "        if self.return_attention_weights:\n",
    "            x, attention_weight = outputs_\n",
    "        else:\n",
    "            x = outputs_\n",
    "        x = self.window_reverse(x, H, W, dims)\n",
    "        x = inputs + self.gamma1*x\n",
    "        x += self.gamma2*(self.mlps(self.norm2(x)))\n",
    "        if self.return_attention_weights:\n",
    "            return x, attention_weight\n",
    "        else:\n",
    "            return x\n",
    "    \n",
    "class Level(keras.layers.Layer):\n",
    "    def __init__(self, \n",
    "                depth, #<- Block repetition depth\n",
    "                num_heads, window_size, keepdims, #downsampler 및 block의 hyperparameter\n",
    "                downsample = True, mlp_ratio = 4.0,\n",
    "                qkv_bias = True, qk_scale = None,\n",
    "                dropout = 0.05, layer_scale = None, return_attention_weights = True,\n",
    "                **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.depth = depth\n",
    "        self.n_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.keepdims = keepdims\n",
    "        self.downsample = downsample\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        self.qkv_bias = qkv_bias\n",
    "        self.qk_scale = qk_scale\n",
    "        self.dropout_rate = dropout\n",
    "        self.layer_scale = layer_scale\n",
    "        self.return_attention_weights = return_attention_weights\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        #input tensor : feature map / patches\n",
    "        batch_size, H, W, dims = input_shape\n",
    "        self.blocks = [Block(window_size = self.window_size, num_heads = self.n_heads, global_query = bool(idx%2), \n",
    "                             qkv_bias = self.qkv_bias, qk_scale = self.qk_scale, dropout_rate = self.dropout_rate, \n",
    "                             mlp_ratio = self.mlp_ratio, layer_scale = self.layer_scale, return_attention_weights = self.return_attention_weights,\n",
    "                             name = f\"GCViTBlock{idx+1}\") for idx in range(self.depth)]\n",
    "        self.downsampler = DownSampler(name = \"Downsampler\")\n",
    "        self.query_generator = GlobalQueryGenerator(keepdims = self.keepdims, name = \"GlobalQueryGenerator\")\n",
    "        \n",
    "    def call(self, inputs, **kwargs):\n",
    "        patches = inputs\n",
    "        global_query = self.query_generator(inputs)\n",
    "        for idx, block in enumerate(self.blocks):\n",
    "            if idx % 2 :\n",
    "                outputs_ = block([patches, global_query])\n",
    "            else:\n",
    "                outputs_ = block([patches])\n",
    "            if self.return_attention_weights:\n",
    "                patches, attention_weights = outputs_\n",
    "            else:\n",
    "                patches = outputs_\n",
    "        if self.downsample == False:\n",
    "            return patches\n",
    "        else:\n",
    "            return self.downsampler(patches)\n",
    "def get_gcvit_configs(res, initial_embedding_dims, name = None):\n",
    "    return {'res' : res,\n",
    "            'embed_dims' : initial_embedding_dims,\n",
    "            \"patch_embedding_type\" : \"conv\", #conv or tokenlearner\n",
    "            \"level_depth\" : [2,4,6,8],\n",
    "            \"level_heads\" : [2,4,8,16],\n",
    "            \"level_keepdims\" : [[0,0,0],\n",
    "                                   [0,0],\n",
    "                                   [1], \n",
    "                                    [1]\n",
    "                                   ], #3번째 level부터는 window attention == global attention\n",
    "            \"level_window_size\" : [res//32, res//32, res//16, res//32],\n",
    "            \"model_name\" : f\"GCViT_res{res}\" if name == None else name\n",
    "                }\n",
    "def get_gcvit(configs):\n",
    "    res = configs[\"res\"]\n",
    "    inputs = Input([res,res,3], name = \"ImageInput\")\n",
    "    patcher = PatchEmbedding(embed_dim = configs['embed_dims'], patching_type = configs[\"patch_embedding_type\"],\n",
    "                             name = \"PatchEmbedding\")\n",
    "    patches = patcher(inputs)\n",
    "    \n",
    "    for idx, (depth, heads, keepdims, window_size) in enumerate(zip(configs[\"level_depth\"], configs[\"level_heads\"], configs[\"level_keepdims\"], configs[\"level_window_size\"])):\n",
    "        if idx == len(configs['level_depth'])-1:\n",
    "            downsample = False\n",
    "        else:\n",
    "            downsample = True\n",
    "        level = Level(depth = depth, num_heads = heads, window_size = window_size, keepdims = keepdims, downsample = downsample,\n",
    "                      name = f\"GCViT_Lv{idx+1}_downsample_{downsample}\")\n",
    "        patches = level(patches)\n",
    "    model = keras.Model(inputs, patches,\n",
    "                       name = configs[\"model_name\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c4be17",
   "metadata": {
    "papermill": {
     "duration": 0.004562,
     "end_time": "2024-04-22T12:12:09.386875",
     "exception": false,
     "start_time": "2024-04-22T12:12:09.382313",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Wrap to End-to-End model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1488689",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-22T12:12:09.398804Z",
     "iopub.status.busy": "2024-04-22T12:12:09.398236Z",
     "iopub.status.idle": "2024-04-22T12:12:09.425334Z",
     "shell.execute_reply": "2024-04-22T12:12:09.423679Z"
    },
    "papermill": {
     "duration": 0.036515,
     "end_time": "2024-04-22T12:12:09.428269",
     "exception": false,
     "start_time": "2024-04-22T12:12:09.391754",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_feature_extractor(conv_base, #if None, Vanilla ViT\n",
    "                         embed_dims, res, pe_type = \"rotary\",\n",
    "                          patch_size = 16,\n",
    "                        att_depth = 4, att_heads = 16) : \n",
    "    inputs = Input([res,res,3], name = \"Input_images\")\n",
    "    batch_size = ops.shape(inputs)[0]\n",
    "    if conv_base is None : #Vanilla Vision Transformer\n",
    "        scaled_inputs = keras.layers.LayerNormalization(name = \"InitialLN\")(inputs)\n",
    "        patches = Conv2D(filters = embed_dims, activation = \"gelu\", kernel_size = patch_size, strides = patch_size, padding = 'SAME', name = \"PatchingStem\")(scaled_inputs)\n",
    "        _, w, h, dims = ops.shape(patches)\n",
    "        patches = ops.reshape(patches, [-1, w*h, dims])\n",
    "        cls_token = ops.expand_dims(keras.layers.GlobalAveragePooling1D(name = 'GAPforClsToken')(patches),\n",
    "                                    axis = 1)\n",
    "        patches = ops.concatenate([cls_token, patches], axis = 1)\n",
    "        \n",
    "        if pe_type in ['rotary', 'rotation', 'rotatory', 'roformer']:\n",
    "            patches = keras_nlp.layers.RotaryEmbedding(name = \"RotaryPositionalEmbedding\")(patches)\n",
    "        elif pe_type in [\"learnable\", 'absolute']:\n",
    "            patches = PatchEncoder(num_patches = 1+w*h, projection_dim = embed_dims)(patches)\n",
    "        elif pe_type == None:\n",
    "            pass\n",
    "            \n",
    "        for idx in range(att_depth):\n",
    "            x0 = LayerNormalization(name = f\"PreLN{idx+1}\")(patches)\n",
    "            x1, attention_score = AttentionPooling(att_heads, embed_dims, name = f\"MHA{idx+1}\")([x0, x0])\n",
    "            x2 = keras.layers.Add(name = f\"PreAdd{idx+1}\")([patches, x1])\n",
    "            x3 = LayerNormalization(name = f\"PostLN{idx+1}\")(x2)\n",
    "            x4 = Dense(units = embed_dims, activation = 'gelu', name = f\"TokenMixMLP{idx+1}\")(x3)\n",
    "            patches = keras.layers.Add(name = \"Encoded_Patches\" if idx == att_depth-1 else f\"PostAdd{idx+1}\")([x4, x2])\n",
    "        learned_token = keras.layers.Identity(name = \"feature_vector\")(patches[:, 0, :])\n",
    "        attention_score = keras.layers.Identity(name = \"attention_weight\")(attention_score[..., 1:])\n",
    "        model_name = f\"ViT_depth{att_depth}_dims{embed_dims}_heads{att_heads}_patch{patch_size}\"\n",
    "    else:\n",
    "        feature_map = conv_base(inputs)\n",
    "        _, w, h, dims = ops.shape(feature_map)\n",
    "        dims = ops.shape(feature_map)[-1] ; batch_size = ops.shape(feature_map)[0]\n",
    "        feature_map = ops.reshape(feature_map, [-1, w*h, dims])\n",
    "        learned_token = keras.layers.GlobalAveragePooling1D(name = 'GAPforRepVec')(feature_map)\n",
    "        feature_map = ops.concatenate([learned_token[:, tf.newaxis, :],\n",
    "                                      feature_map], axis = 1)\n",
    "        \n",
    "        if pe_type in ['rotary', 'rotation', 'rotatory', 'roformer']:\n",
    "            feature_map = keras_nlp.layers.RotaryEmbedding(name = \"RotaryPositionalEmbedding\")(feature_map)\n",
    "        elif pe_type in [\"learnable\", 'absolute']:\n",
    "            feature_map = PatchEncoder(num_patches = 1+w*h, projection_dim = embed_dims)(feature_map)\n",
    "        elif pe_type == None:\n",
    "            pass\n",
    "        for idx in range(att_depth):\n",
    "            feature_map, attention_score = AttentionPooling(att_heads, embed_dims, name = f\"MHA_after_Conv_{idx+1}\")([feature_map, feature_map])\n",
    "        learned_token = keras.layers.Identity(name = \"feature_vector\")(feature_map[:, 0, :])\n",
    "        attention_score = keras.layers.Identity(name = \"attention_weight\")(attention_score[..., 1:])\n",
    "        \n",
    "        model_name = f\"{conv_base.name}_depth{att_depth}_dims{embed_dims}_heads{att_heads}\"\n",
    "    model = Model(inputs, [learned_token, attention_score],\n",
    "                  name = model_name)\n",
    "    return model\n",
    "\n",
    "def get_full_model(conv_base_name, res, embed_dims = 1280, patch_size = 16, pe_type = 'rotary',\n",
    "                   att_depth = 4, att_heads = 8,\n",
    "                  extra_configs = None):\n",
    "    if conv_base_name in [\"effnet\", 'EfficientNet']:\n",
    "        conv_base = keras.applications.EfficientNetV2B1(input_shape = [res,res,3],\n",
    "                                                       include_top = False)\n",
    "    elif conv_base_name in [\"effnet_small\", \"EfficientNetSmall\"]:\n",
    "        conv_base = keras.applications.EfficientNetV2S(input_shape = [res,res,3],\n",
    "                                                       include_top = False)\n",
    "    elif conv_base_name in [\"effnet_base\", \"EfficientNetBase\"]:\n",
    "        conv_base = keras.applications.EfficientNetV2M(input_shape = [res,res,3],\n",
    "                                                       include_top = False)\n",
    "    elif conv_base_name in [\"convnext\", 'ConvNeXt']:\n",
    "        conv_base = keras.applications.ConvNeXtTiny(input_shape = [res,res,3],\n",
    "                                                       include_top = False)\n",
    "    elif conv_base_name in [\"convnext_small\", 'ConvNeXtSmall']:\n",
    "        conv_base = keras.applications.ConvNeXtSmall(input_shape = [res,res,3],\n",
    "                                                       include_top = False)\n",
    "    elif conv_base_name in [\"convnext_base\", 'ConvNeXtBase']:\n",
    "        conv_base = keras.applications.ConvNeXtBase(input_shape = [res,res,3],\n",
    "                                                       include_top = False)\n",
    "    elif isinstance(conv_base_name, dict):\n",
    "        conv_base = get_gcvit(conv_base_name)\n",
    "    else:\n",
    "        conv_base = None\n",
    "    return get_feature_extractor(conv_base, pe_type = pe_type, res = res, patch_size = patch_size, embed_dims = embed_dims, att_depth = att_depth, att_heads = att_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf535e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-22T12:00:52.600880Z",
     "iopub.status.busy": "2024-04-22T12:00:52.600401Z",
     "iopub.status.idle": "2024-04-22T12:00:52.675347Z",
     "shell.execute_reply": "2024-04-22T12:00:52.674121Z",
     "shell.execute_reply.started": "2024-04-22T12:00:52.600850Z"
    },
    "papermill": {
     "duration": 0.004935,
     "end_time": "2024-04-22T12:12:09.437509",
     "exception": false,
     "start_time": "2024-04-22T12:12:09.432574",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30702,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 31.173008,
   "end_time": "2024-04-22T12:12:12.149753",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-22T12:11:40.976745",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
