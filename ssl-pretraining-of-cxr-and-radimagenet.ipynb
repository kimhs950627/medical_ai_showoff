{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":10892142,"datasetId":6173119,"databundleVersionId":11260142},{"sourceType":"datasetVersion","sourceId":8471595,"datasetId":5051531,"databundleVersionId":8611166},{"sourceType":"datasetVersion","sourceId":8556597,"datasetId":5112865,"databundleVersionId":8701143},{"sourceType":"datasetVersion","sourceId":10855900,"datasetId":5323611,"databundleVersionId":11219920},{"sourceType":"kernelVersion","sourceId":169421886},{"sourceType":"kernelVersion","sourceId":219820776}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os, sys\nimport random\nimport pydicom\nimport sklearn\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import AgglomerativeClustering, KMeans\nfrom sklearn.preprocessing import StandardScaler\ntry:\n    import umap\nexcept:\n    !pip install umap\n    import umap\n    \nimport re\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nseed = 42\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# ML tools \nsys.path.append(\"/kaggle/input/kimm-keras-image-model-repository\"\n               )\n\nimport tensorflow as tf\nimport keras# ; keras.config.set_dtype_policy(\"mixed_float16\")\nfrom keras import ops, layers, models, losses, optimizers, metrics\nimport keras_hub\nimport kimm\nimport keras_cv\nimport keras_nlp\n\nimport cv2\nfrom skimage.io import imread\nkeras.utils.set_random_seed(seed)\nimport tensorflow_io as tfio\nfrom kaggle_datasets import KaggleDatasets\nimport tensorflow_datasets as tfds\nimport tensorflow_probability as tfp\nimport tensorflow_decision_forests as tfdf\n\nprint(f\"Tensorflow version : {tf.__version__}\")\ntry:\n    print(f\"Keras version : {keras.__version__}\")\nexcept:\n    pass\n\nfrom keras import Input, Model, ops\nfrom keras.models import load_model\n\nfrom keras.layers import Conv2D, DepthwiseConv2D, Dense, Activation, BatchNormalization, LayerNormalization, MultiHeadAttention, Embedding, Subtract, Add, Multiply, GlobalAveragePooling2D, GlobalAveragePooling1D, LayerNormalization\nfrom keras.utils import load_img, img_to_array\nfrom keras.applications import *\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nfrom sklearn.model_selection import train_test_split\nfrom keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\nfrom tqdm.notebook import tqdm\nimport wandb\n#from wandb.keras import WandbCallback, WandbModelCheckpoint, WandbMetricsLogger\ndef wandb_config():\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    try:\n        secret_value_0 = user_secrets.get_secret(\"__gcloud_sdk_auth__\")\n        secret_value_1 = user_secrets.get_secret(\"huggingface_key\")\n        secret_value_2 = user_secrets.get_secret(\"wandb_key\")\n        !wandb login $secret_value_2\n    except:\n        secret_value_0 = user_secrets.get_secret(\"huggingface_key\")\n        secret_value_1 = user_secrets.get_secret(\"wandb_key\")\n        !wandb login $secret_value_1\n    \ndef auto_select_accelerator():\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n        print(\"Running on TPU:\", tpu.master())\n    except ValueError:\n        tpu = False\n        strategy = tf.distribute.MirroredStrategy() # for GPU or multi-GPU machines\n    print(f\"Running on {strategy.num_replicas_in_sync} replicas\")\n    \n    return tpu, strategy\n\ntpu, strategy = auto_select_accelerator()\nimport ssl_module\nfrom ssl_module import feature_visualize, get_masking_fn, get_map_fn, get_gcvit_configs, get_flops, att_visualize, get_full_model, BarlowModel, VICRegModel, Moco, SimSiam, CLIP, SigLIP\nimport nas_ftp_module\nfrom nas_ftp_module import upload_file, download_file\nimport PIL\nfrom PIL import Image as PILImage\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-03-03T16:48:20.231708Z","iopub.execute_input":"2025-03-03T16:48:20.232000Z"}},"outputs":[{"name":"stdout","text":"Collecting umap\n  Downloading umap-0.1.1.tar.gz (3.2 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nBuilding wheels for collected packages: umap\n  Building wheel for umap (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for umap: filename=umap-0.1.1-py3-none-any.whl size=3542 sha256=815dc3c9dbfe62b224520e0c7a576c9650d3f4af55c0816f05ea6dbb3655d1b3\n  Stored in directory: /root/.cache/pip/wheels/15/f1/28/53dcf7a309118ed35d810a5f9cb995217800f3f269ab5771cb\nSuccessfully built umap\nInstalling collected packages: umap\nSuccessfully installed umap-0.1.1\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"# 실험 계획\n- token mixer : gMLP vs gaMLP vs Attention\n- ConvNeXt vs pure-metaformer\n     - if convnext, FE 후 token mixer의 갯수에 따른 변화\n     - if convnext, ImageNet weight vs randomly initialized","metadata":{}},{"cell_type":"markdown","source":"# HybridViT\n\n - from [here](https://www.kaggle.com/code/khs224025/hybridvit)","metadata":{}},{"cell_type":"code","source":"class TransformerEncoderLayer(layers.Layer):\n    def __init__(self, embed_dims, num_heads, ff_dim, return_attention_scores, \n                 orthogonal_factor = 0.01,\n                 **kwargs):\n        super().__init__(**kwargs)\n        self.attn = keras.layers.MultiHeadAttention(num_heads, embed_dims//num_heads,\n                                             kernel_regularizer=keras.regularizers.OrthogonalRegularizer(factor = orthogonal_factor),\n                                             bias_regularizer=keras.regularizers.OrthogonalRegularizer(factor = orthogonal_factor)\n                                               )\n        self.ffn = models.Sequential([\n            layers.Dense(ff_dim, use_bias = False), \n            layers.Dense(embed_dims, use_bias = False)\n        ])\n        self.norm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.norm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.return_attention_scores = return_attention_scores\n        \n    def call(self, inputs):\n        if self.return_attention_scores :\n            attn_out, attn_weights = self.attn(inputs, inputs, return_attention_scores=self.return_attention_scores)\n        else:\n            attn_out = self.attn(inputs, inputs, return_attention_scores=self.return_attention_scores)\n        x = self.norm1(inputs + attn_out)\n        ffn_out = self.ffn(x)\n        output = self.norm2(x + ffn_out)\n        if self.return_attention_scores:\n            return self.norm2(x+ffn_out), attn_weights\n        else:\n            return self.norm2(x+ffn_out)\n            \nclass RotaryEmbedding2D(keras.layers.Layer):\n    def __init__(\n        self,\n        max_wavelength=10000,\n        scaling_factor=1.0,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.max_wavelength = max_wavelength\n        self.scaling_factor = scaling_factor\n        # 가로, 세로 방향에 대한 각각의 RotaryEmbedding 레이어 생성\n        self.horizontal_rope = keras_hub.layers.RotaryEmbedding(\n            max_wavelength=max_wavelength,\n            scaling_factor=scaling_factor,\n            sequence_axis=2,  # 가로 방향 축\n            feature_axis=-1\n        )\n        self.vertical_rope = keras_hub.layers.RotaryEmbedding(\n            max_wavelength=max_wavelength,\n            scaling_factor=scaling_factor,\n            sequence_axis=1,  # 세로 방향 축\n            feature_axis=-1\n        )\n        \n    def call(self, inputs):\n        # 입력 형태: [batch_size, height, width, channels]\n        \n        # 특성 차원을 두 부분으로 분할\n        channels = ops.shape(inputs)[-1]\n        half_channels = channels // 2\n        \n        # 특성 차원 분할\n        x_h, x_v = ops.split(inputs, 2, axis=-1)\n        \n        # 수평 방향(width) rotary embedding 적용\n        x_h = self.horizontal_rope(x_h)\n        \n        # 수직 방향(height) rotary embedding 적용\n        x_v = self.vertical_rope(x_v)\n        \n        # 결과 합치기\n        x = tf.concat([x_h, x_v], axis=-1)\n        return x\n        \nclass HybridViT(models.Model):\n    def __init__(self, image_size, patch_size=16, num_layers=12,num_heads=12, embed_dims=768, \n                 proj_dims = 128, split_layer=4, n_register = 0,\n                 q_size = 2**15, t = 0.07,\n                 **kwargs):\n        super().__init__(**kwargs)\n        self.n_reg = n_register\n        self.image_size = image_size\n        self.res = image_size #for convenient\n        self.heads = num_heads\n        self.patch_size = patch_size\n        self.embed_dims = embed_dims\n        self.split_layer = split_layer\n\n        # 1. Learnable CLS token and positional embedding\n        self.cls_token = self.add_weight(\n            shape=(1, 1, embed_dims),\n            initializer=keras.initializers.RandomNormal(),\n            name=\"cls_token\"\n        )\n        if self.n_reg > 0:\n            self.reg_tokens = self.add_weight(\n                shape=(1, self.n_reg, embed_dims),\n                initializer=keras.initializers.RandomNormal(),\n                name=\"cls_token\"\n            )\n        \n        # 2. Patch embedding and RoPE\n        self.patch_embed = layers.Conv2D(embed_dims//2, patch_size, strides=patch_size, name = \"PatchConvolution\")\n        self.middle_conv = layers.Conv2D(embed_dims, 5, padding = 'same', name = \"mid_sep_conv2d\")\n        self.rope = RotaryEmbedding2D(name = \"RoPE_2Dims\")\n\n        # 3. Transformer layers\n        self.early_layers = keras.Sequential([TransformerEncoderLayer(embed_dims, num_heads, 2*embed_dims, False, orthogonal_factor = 0.01)\n                            for _ in range(split_layer)],\n                                            name = \"EarlyTREncoder\")\n        self.late_layers = keras.Sequential([keras.layers.MultiHeadAttention(num_heads, embed_dims//num_heads,\n                                             kernel_regularizer=keras.regularizers.OrthogonalRegularizer(factor = 0.01),\n                                             bias_regularizer=keras.regularizers.OrthogonalRegularizer(factor = 0.01))\n                           for _ in range(num_layers - split_layer)], name = \"LateTREncoder\"\n                                           )\n        # 4. Projection heads\n        self.mim_head = layers.Dense(patch_size**2, activation = \"sigmoid\", name = \"MIM_Regressor\")\n        self.nnclr_proj = layers.Dense(proj_dims, name = \"NCLR_Projector\")\n\n        # 5. Feature Q for NNCLR/SNCLR\n        self.q_size = q_size\n        self.feature_q = keras.Variable(\n            keras.utils.normalize(\n                keras.random.normal(shape=(self.q_size, proj_dims)), #q_size, embed_dims shape matrix : FIFO로 update해야 함\n                axis=1,\n                order=2,\n            ),\n            trainable=False, dtype = \"float32\"\n        )\n        self.t = t\n        # 6. Image augmenter for Contrastive learning\n        # Loss : CL + MIM\n        # CL : SNCLR; [original vs whole with downsampling] // + DenseCL [DenseCLView 1 vs DenseCLView 2] <- learn the whole feature\n        # MIM : Sim MIM [Original image vs masked image] <- learn the high frequency feature (partial view learning)\n        \n        self.whole_view_augmenter = keras.Sequential([\n                            keras.layers.RandomFlip(),\n                            keras.layers.RandomRotation(0.2),\n                            keras.layers.RandomCrop(height = int(0.8 * image_size), width = int(0.8 * image_size)),\n                            keras.layers.Resizing(288,288)\n                        ],\n                                         name = \"WholeViewDownsampler\")\n\n        self.possible_sizes = [(192, 192), (192, 384), (384, 192), \n                              (384, 256),(256,384),\n                               (192, 256),(256,192), (256,256)]\n        \n    #### FOR SNCLR ###\n    def get_neighbor(self, projections):  #for NNCLR/SNCLR ; finding positive match\n        p = projections\n        cos_sim = ops.matmul(p, ops.transpose(self.feature_q)\n                            ) # batch, q_size\n        w = ops.softmax(cos_sim/self.t) # batch, q size\n        support_f = ops.matmul(w, self.feature_q) ; del w #batch, embed_dims\n        return p + ops.stop_gradient(support_f - p)\n\n    def compute_nclr_loss(self, projections, training = True): #SimCLR-like loss shape\n        # NCLR\n        f_original, f_aug = projections[0], projections[1]\n        f_original, f_aug = ops.normalize(f_original), ops.normalize(f_aug)\n        # 각각 batch, embed_dims shape tensor\n        f_original_n, f_aug_n = self.get_neighbor(f_original), self.get_neighbor(f_aug)\n        sim_matrix_1_1 = ops.matmul(f_original, ops.transpose(f_aug_n)) / self.t\n        sim_matrix_1_2 = ops.matmul(f_original_n, ops.transpose(f_aug)) / self.t\n        batch_size = ops.shape(f_original)[0]\n        pseudo_label = ops.arange(ops.shape(f_original)[0])\n        \n        loss_1 = (keras.losses.sparse_categorical_crossentropy(pseudo_label, sim_matrix_1_1, from_logits = True) + \n                 keras.losses.sparse_categorical_crossentropy(pseudo_label, ops.transpose(sim_matrix_1_1), from_logits = True) + \n                 keras.losses.sparse_categorical_crossentropy(pseudo_label, sim_matrix_1_2, from_logits = True) + \n                 keras.losses.sparse_categorical_crossentropy(pseudo_label, ops.transpose(sim_matrix_1_2), from_logits = True))\n        \n        if training:\n            self.feature_q.assign(\n                ops.concatenate([f_original, \n                                 self.feature_q[:-batch_size ]\n                                ], axis=0)\n            )\n            \n        nclr_loss = ops.mean(loss_1)\n        return nclr_loss\n    ### for DenseCL ###\n    def compute_densecl_loss(self, f1, f2):\n        #f1, f2 : feature maps of 2 different views\n        f1, f2 = ops.normalize(f1), ops.normalize(f2)\n        sim = ops.einsum(\"bld, bnd -> bln\", f1, f2)\n        sim = ops.exp(sim)\n        max_val1, max_val2 = ops.max(sim, axis = -1), ops.max(sim, axis = 1)\n        sum_val1, sum_val2 = ops.sum(sim, axis = -1), ops.sum(sim, axis = 1)\n        loss1, loss2 = -ops.log(1e-5 + max_val1/sum_val1), -ops.log(max_val2/sum_val2)\n        return ops.mean(loss1)+ops.mean(loss2)\n    ### For MIM ###\n\n    def _dynamic_masking(self, images, base_ratio=0.7):\n        #중요한 영역을 우선적으로 마스킹하는 함수 (Masked Image Modeling용)\n        patch_size = self.patch_size\n        images = ops.cast(images, \"float32\")\n        batch_size = ops.shape(images)[0]\n        image_size = ops.shape(images)[1]  # 정사각형 이미지 가정\n        P = patch_size\n        c = ops.shape(images)[-1]\n        num_patches_h = image_size // P\n        num_patches_w = image_size // P\n        \n        # 1. 지역적 콘트라스트 계산\n        if c == 3:  # RGB 이미지인 경우\n            # RGB → 그레이스케일 변환 (가중치 적용)\n            gray = ops.sum(images * ops.convert_to_tensor([[[[0.299, 0.587, 0.114]]]]), axis=-1, keepdims=True)\n        else:  # 이미 그레이스케일이거나 단일 채널\n            gray = images\n        \n        # Sobel 필터 적용 (TF 사용)\n        sobel = tf.image.sobel_edges(gray)\n        sobel_magnitude = ops.sqrt(ops.sum(ops.square(ops.convert_to_tensor(sobel)), axis=-1))\n        \n        # 패치 단위로 콘트라스트 맵 다운샘플링\n        contrast_patches = ops.image.extract_patches(\n            images=sobel_magnitude,\n            size=(P, P),\n            strides=(P, P),\n            dilation_rate=1,\n            padding=\"valid\"\n        )\n        \n        # 패치별 평균 콘트라스트 계산\n        contrast_map = ops.mean(contrast_patches, axis=-1)\n        contrast_map = ops.reshape(contrast_map, [batch_size, num_patches_h, num_patches_w, 1])\n        \n        # 2. 중요도 기반 마스크 비율 생성 (높은 콘트라스트 = 중요한 영역)\n        # 콘트라스트 맵 정규화 (0-1 범위로)\n        contrast_min = ops.min(contrast_map, axis=[1, 2, 3], keepdims=True)\n        contrast_max = ops.max(contrast_map, axis=[1, 2, 3], keepdims=True)\n        normalized_contrast = (contrast_map - contrast_min) / (contrast_max - contrast_min + 1e-8)\n        \n        # 중요한 영역(높은 콘트라스트)에 더 높은 마스킹 확률 부여\n        # Sigmoid 함수: 1/(1+exp(-x))\n        importance_score = normalized_contrast  # 높을수록 중요한 영역\n        mask_ratio = base_ratio * (0.5 + importance_score)  # 중요한 영역은 더 높은 마스킹 확률\n        \n        # 3. 확률적 마스크 생성 (1: 마스킹됨, 0: 유지됨)\n        random_values = np.random.uniform(size=ops.shape(mask_ratio))\n        mask = ops.cast(random_values < mask_ratio, \"float32\")\n        \n        # 최소 마스킹 비율 보장 (전체 패치의 최소 50%는 마스킹)\n        current_mask_ratio = ops.mean(mask)\n        mask = tf.cond(\n                        current_mask_ratio < 0.5,\n                        lambda: ops.maximum(mask, \n                                            ops.cast(np.random.uniform(size=ops.shape(mask_ratio)) < (0.5 - current_mask_ratio), \"float32\")),\n                        lambda: mask\n                    )\n        \n        \n        # 4. 마스크 업샘플링 및 적용\n        mask_up = ops.repeat(mask, P, axis=1)\n        mask_up = ops.repeat(mask_up, P, axis=2)\n        mask_up = ops.reshape(mask_up, [batch_size, image_size, image_size, 1])\n        \n        # 마스킹된 이미지 생성 (마스크=1인 위치는 0으로 설정)\n        masked_images = images * (1.0 - mask_up)\n        \n        # 5. 원본 이미지에서 패치 추출\n        patches = ops.image.extract_patches(\n            images=images,\n            size=(P, P),\n            strides=(P, P),\n            dilation_rate=1,\n            padding=\"valid\"\n        )\n        patches = ops.reshape(patches, [batch_size, num_patches_h * num_patches_w, P*P*c])\n        \n        # 6. 마스크 평탄화 (loss 계산용)\n        mask_flat = ops.reshape(mask, [batch_size, -1])\n        \n        return masked_images, mask_flat, patches\n\n    def _compute_mim_loss(self, gt, pred, mask):\n        mask = ops.expand_dims(ops.cast(mask, \"float32\"), -1)\n        mse = ops.square(gt - pred) * mask\n        return ops.sum(mse) / (ops.sum(mask) + 1e-8)\n    def compute_headwise_sim(self, attn_weights):\n        # attn_weights shape: [batch, heads, 1, n_patches]\n        batch = ops.shape(attn_weights)[0]\n        num_heads = ops.shape(attn_weights)[1]\n        \n        # 1. 공간 차원을 평탄화: [B, H, N] (N = n_patches)\n        flat_attn = ops.reshape(attn_weights, (batch, num_heads, -1))\n        \n        # 2. 각 head마다 L2 정규화 (수치 안전성을 위해 epsilon 추가)\n        norm_attn = ops.normalize(flat_attn, axis=-1)  + 1e-8\n        \n        # 3. head간 코사인 유사도 계산: einsum을 통해 [B, H, H] 크기의 유사도 행렬 생성\n        cosine_sim = ops.einsum(\"bhi,bji->bhj\", norm_attn, norm_attn)\n        \n        # 4. 대각선이 아닌 상삼각 행렬만 선택 (중복 비교 방지)\n        # num_heads가 static일 경우 이용 가능\n        n_heads = norm_attn.shape[1]\n        identity = ops.eye(n_heads)\n        mask = 1-identity\n        # k=1: 대각선 위쪽만 True인 마스크 생성\n        \n        # 5. boolean_mask로 상삼각 값만 추출 후 제곱하여 평균 계산\n        sim_values = mask[tf.newaxis, ...] * cosine_sim\n        \n        sims = ops.mean(sim_values)\n        \n        return sims\n\n    def call(self, inputs):\n        \n        # 1. Patch embedding\n        inputs = ops.cast(inputs, 'float32')\n        patches = self.patch_embed(inputs)\n        patches = self.middle_conv(patches)\n        _, w, h, dims_ = patches.shape ; n_patches = w*h\n        \n        # 2. Apply RoPE to patches\n        patches = self.rope(patches)\n        patches = ops.reshape(patches, (ops.shape(patches)[0], n_patches, self.embed_dims))\n        \n        # 3. Add CLS token with positional embedding\n        cls_token = ops.repeat(self.cls_token, ops.shape(inputs)[0], axis=0)\n        if self.n_reg > 0:\n            register_tokens = ops.repeat(self.reg_tokens, ops.shape(inputs)[0], axis=0)\n        #x = ops.concatenate([cls_tokens, patches], axis=1)\n        #if self.n_reg > 0:\n        #    x = ops.concatenate([x, register_tokens], axis = 1)\n        \n        # 4. Early layers (MIM)\n        encoded_patches = self.early_layers(patches)\n        print(\"Encoded patches shape after Early layer encoding : \", ops.shape(encoded_patches))\n        #encoded_patches = x[:, 1:(n_patches + 1)]\n        #cls_token = x[:, 0:1]\n        \n        # 5. Late layers (NNCLR)\n        for layer in self.late_layers.layers:\n            cls_token, attn_weights = layer(query = cls_token, key = encoded_patches, value = encoded_patches, \n                                            return_attention_scores = True)\n        \n        return cls_token[:,0,:], encoded_patches, attn_weights\n\n    def train_step(self, data):\n        images = data\n        batch_size = ops.shape(images)[0] ; channels = ops.shape(images)[-1]\n        thumbnail = self.whole_view_augmenter(images)\n\n        crop_size = random.choice(self.possible_sizes)\n        crop_h, crop_w = crop_size[0], crop_size[1]\n        \n        # 4. 크롭 적용 (배치 차원 보존)\n        dense_view1 = tf.image.random_crop(\n            images,\n            size=tf.concat(  # 모든 차원에 대해 명시적 크기 지정\n                [\n                    [batch_size],  # 배치 차원 유지 (있을 경우)\n                    [crop_h], \n                    [crop_w], \n                    [channels]\n                ], \n                axis=0\n            )\n        )\n        dense_view1 = ops.reshape(dense_view1, [batch_size, crop_h, crop_w, channels])\n        ############################\n        crop_size = random.choice(self.possible_sizes)\n        crop_h, crop_w = crop_size[0], crop_size[1]\n        \n        # 4. 크롭 적용 (배치 차원 보존)\n        dense_view2 = tf.image.random_crop(\n            images,\n            size=tf.concat(  # 모든 차원에 대해 명시적 크기 지정\n                [\n                    [batch_size],  # 배치 차원 유지 (있을 경우)\n                    [crop_h], \n                    [crop_w], \n                    [channels]\n                ], \n                axis=0\n            )\n        )\n        dense_view2 = ops.reshape(dense_view2, [batch_size, crop_h, crop_w, channels])\n        \n        masked_images, mask_flat, gt_patches = self._dynamic_masking(images)\n        gt_patches /= 255\n\n        if np.random.randint(0, 10) <= 5:\n            thumbnail = 255 - thumbnail\n                \n        with tf.GradientTape(watch_accessed_variables=True) as tape:\n            #1. SimMIM\n            _, encoded_patches, _ = self(masked_images)\n            pred_patches = self.mim_head(encoded_patches)\n            mim_loss = self._compute_mim_loss(gt_patches, pred_patches, mask_flat)\n            \n            #2. NCLR b/w thumbnail and original images\n            cls_token, _, attn_weights = self(images)\n            aug_token, _, _ = self(thumbnail)\n            proj_features = self.nnclr_proj(cls_token)\n            proj_aug_features = self.nnclr_proj(aug_token)\n            nnclr_loss = self.compute_nclr_loss([proj_features, proj_aug_features])\n\n            #3. DenseCL b/w 2 small feature maps\n            print(ops.shape(dense_view1))\n            _, f1, _ = self(dense_view1)\n            _, f2, _ = self(dense_view2)\n            dense_cl_loss = self.compute_densecl_loss(f1, f2)\n            \n            headwise_att_sim = self.compute_headwise_sim(attn_weights)\n            loss = 0.2*mim_loss + nnclr_loss + dense_cl_loss\n        # 3. 그래디언트 병합 및 적용\n        grads = tape.gradient(\n            loss, \n            self.trainable_weights\n        )\n        \n        self.optimizer.apply_gradients(zip(\n            grads,\n            self.trainable_weights\n        ))\n        \n        return {\"mim_loss\": mim_loss, \"nnclr_loss\": nnclr_loss, 'DenseCL_loss' : dense_cl_loss,\n                \"HeadwiseAttn_w_Cos_sim\" : headwise_att_sim,\n                'total_loss' : loss}\n\n    def mimmax_scale(self, data):\n        data = np.array(data)\n        return (data - np.min(data)) / (np.max(data) - np.min(data) + 1e-4)\n    def get_segmentation_maps(self, feature_map):\n        n_clusters = 20\n        cluster_cmap = \"tab20\"\n        feature_map = np.array(feature_map)\n        if len(ops.shape(feature_map)) == 3:\n            batch_size, seq_len, embed_dims = feature_map.shape\n            res_ = ops.sqrt(ops.cast(seq_len, \"float32\"))\n            res_ = ops.cast(res_, 'int32')\n            w, h = res_, res_\n            flatten_map = feature_map.reshape((batch_size*seq_len, embed_dims))\n        elif len(ops.shape(feature_map)) == 4:\n            batch_size, w, h, embed_dims = feature_map.shape\n            flatten_map = feature_map.reshape((batch_size*w*h, embed_dims))\n        result_maps = {}\n        scaler = StandardScaler()\n        scaled_features = scaler.fit_transform(flatten_map)\n        \n        #1. PCA\n        pca = PCA(n_components=3)\n        pca_result = pca.fit_transform(scaled_features)\n        pca_result_norm = self.mimmax_scale(pca_result)\n        result_maps[\"pca\"] = pca_result_norm.reshape((batch_size, w, h, 3))\n        \n        #2. t-SNE\n        tsne = TSNE(n_components=3, random_state=42, perplexity=min(30, (w*h)//5))\n        tsne_result = tsne.fit_transform(scaled_features)\n        tsne_result_norm = self.mimmax_scale(tsne_result)\n        result_maps[\"tsne\"] = tsne_result_norm.reshape((batch_size, w, h, 3))\n\n        #3. UMAP\n        #reducer = umap.UMAP(n_components=3, random_state=42)\n        #umap_result = reducer.fit_transform(scaled_features)\n        #umap_result_norm = self.mimmax_scale(umap_result)\n        #result_maps[\"umap\"] = umap_result_norm.reshape((batch_size, w, h, 3))\n\n        #4. Agglomerative clustering\n        agg_clustering = AgglomerativeClustering(n_clusters=n_clusters)\n        agg_labels = agg_clustering.fit_predict(scaled_features)\n        \n        # Agglomerative Clustering 결과에 colormap 적용\n        cmap = plt.cm.get_cmap(cluster_cmap, n_clusters)\n        agg_colors = cmap(agg_labels / (n_clusters - 1 + 1e-5))[..., :3]\n        result_maps['agglomerative'] = agg_colors.reshape((batch_size, w, h, 3))\n\n        #5. K-means\n        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init='auto')\n        kmeans_labels = kmeans.fit_predict(scaled_features)\n        \n        cmap = plt.cm.get_cmap(cluster_cmap, n_clusters)\n        kmeans_colors = cmap(kmeans_labels / (n_clusters - 1 + 1e-5))[..., :3]\n        \n        result_maps['kmeans'] = kmeans_colors.reshape((batch_size, w, h, 3))\n        return result_maps\n        \n    def infer(self, images):\n        #output : attention map, merged map.\n        batch_size, res, _, __ = ops.shape(images)\n        print(\"Heatmap calculation...\", '\\n')\n        feature_vector, encoded_patches, attn_weights = self(images)\n        \n        _, patch_len_, __ = encoded_patches.shape\n        n_patches = patch_len_  ; small_res = res // self.patch_size\n\n        attn_weights = attn_weights[:, :, 0, :]\n        attn_weights = ops.reshape(attn_weights, [-1, small_res * small_res])\n        attn_weights = (attn_weights - ops.min(attn_weights, axis = -1, keepdims = True)) / (ops.max(attn_weights, axis = -1, keepdims = True) - ops.min(attn_weights, axis = -1, keepdims = True))\n        attn_weights *= 255.0\n        threshold = ops.median(attn_weights, axis = -1, keepdims = True)\n        \n        attn_weights = ops.where(attn_weights < threshold, 0.0, attn_weights)\n        heatmap = ops.reshape(attn_weights, [batch_size*self.heads, small_res, small_res])\n        heatmap = np.array(heatmap).astype(\"int32\")\n        cmap = mpl.colormaps[\"jet\"]\n        # Use RGB values of the colormap\n        _colors = cmap(np.arange(256))[:, :3]\n        heatmap = _colors[heatmap]\n        heatmap = [keras.utils.array_to_img(h) for h in heatmap]\n        heatmap = [h.resize((res,res)) for h in heatmap]\n        heatmap = [keras.utils.img_to_array(h) for h in heatmap]\n        heatmap = np.array(heatmap)\n        heatmap = ops.reshape(heatmap, [batch_size, self.heads, res, res, 3])\n        mean_heatmap = ops.mean(heatmap, axis = 1)\n        mean_merged = ops.cast(images, 'float32') * 0.5 + ops.cast(mean_heatmap, 'float32') * 0.5\n        \n        images = images[:, tf.newaxis, ...]\n        merged = ops.cast(images, 'float32') * 0.5 + ops.cast(heatmap, 'float32') * 0.5\n        \n        merged = ops.cast(merged, 'uint8')\n        mean_merged = ops.cast(mean_merged, 'uint8')\n        heatmap = ops.cast(heatmap, 'uint8')\n        mean_heatmap = ops.cast(mean_heatmap, 'uint8')\n        print(\"Segmentation...\")\n        seg_maps = self.get_segmentation_maps(encoded_patches)\n        result = {\"attn_map\" : heatmap, \"merged_original\" : merged,\n               \"attn_map_head_merged\" : mean_heatmap,\n               \"head_merged_original\": mean_merged,\n                \"encoded_patches\" : encoded_patches,\n               }\n        result.update(seg_maps)\n        \n        return result\n","metadata":{"trusted":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Setting hyperparameters","metadata":{}},{"cell_type":"code","source":"batch_size = 8\nbatch_size = strategy.num_replicas_in_sync * batch_size\nprint('batch size', batch_size)\n\nres = int(3.0*256)\nsmall_res = 64\n\nn_multicrop = 2\nrandaug =keras_cv.layers.RandAugment(\n    value_range=(0, 255), magnitude=0.1, magnitude_stddev=0.1, geometric = False\n)\n\ngrayscale = False # False if using pretrained model, True if from scratch\npatch_size = 12\nheads = 8\natt_dims = 64\nembed_dims = 512\n\nc = 1 if grayscale else 3\nif grayscale:\n    pretrained_encoder = None\n    depth = 2\n    registers = 2\n    pretrained_note = \"gray_metaformer\"\nelse:\n    depth = 3\n    registers = 0\n    pretrained_encoder = keras.applications.ConvNeXtTiny(input_shape = [res,res,3], \n                                                         include_top = False,\n                                                        #weights = None,\n                                                        ); patch_size = 32\n    \n    pretrained_vit = kimm.models.VisionTransformerTiny32(input_shape = [res,res,3], include_top = False)\n    #pretrained_regnet = kimm.models.RegNetY040(input_shape = [res,res,3], include_top = False); patch_size = 32\n    #pretrained_regnet = keras.Model(inputs = pretrained_regnet.input, outputs = pretrained_regnet.get_layer(\"s4_b1_conv1\").output,\n    #                    name = f\"{pretrained_regnet.name}_upsample\")\n    #pretrained_vit = kimm.models.VisionTransformerBase16(input_shape = [res,res,3], include_top = False) ; patch_size = 16\n    #pretrained_vit = kimm.models.VisionTransformerLarge16(input_shape = [res,res,3], include_top = False) ; patch_size = 16\n    \n    for layer in pretrained_encoder.layers:\n        layer.dtype_policy = keras.mixed_precision.Policy('mixed_float16')\n    for layer in pretrained_vit.layers:\n        layer.dtype_policy = keras.mixed_precision.Policy('mixed_float16')\n    #for layer in pretrained_regnet.layers:\n    #    layer.dtype_policy = keras.mixed_precision.Policy('mixed_float16')\n    pretrained_note = f\"ConvWithMetaEncoder_ImageNet_TM{depth}\"\n    #depth = 6\n    #registers = 0\n    #pretrained_encoder = None\n    #pretrained_note = f\"RGB_metaformer_TM{depth}\"\n    #patch_size = 24","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_dual_encoder():\n    input_tensor = Input([res,res,3], name = \"DualEncoderInputImg\")\n    # Step 1: Load pretrained lightweight networks\n    # Extract feature maps from the pretrained models\n    f1 = pretrained_encoder\n    f2 = pretrained_vit\n\n    m1 = f1(input_tensor)  # Feature map from MobileNetV2\n    m2 = f2(input_tensor)[:, 1:, :]  # Feature map from ViT\n    _, w, h, dims = ops.shape(m1)\n    m1 = ops.reshape(m1, [-1, w*h, dims])\n    combined_features = keras.layers.Identity(name = \"MergedFeatureMap\")(ops.concatenate([m1, m2], axis = -1))\n\n    return Model(input_tensor, combined_features,\n                name = f\"{f1.name}With{f2.name}_dualencoder\")\n\n# Example usage\n#dual_model = get_dual_encoder()\n#dual_model.summary()","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- radimagenet tfrecord key : image, label\n- nih cxr tfrecord key : image_raw, label","metadata":{}},{"cell_type":"markdown","source":"# RadImageNet decoding","metadata":{}},{"cell_type":"code","source":"def _parse_tfrecord(res = res):\n    def parse_tfrecord(tfrecord):\n        features = {'image': tf.io.FixedLenFeature([], tf.string),\n                    'label': tf.io.FixedLenFeature([], tf.int64),\n                    }\n        x = tf.io.parse_single_example(tfrecord, features)\n        image_train = tf.image.decode_jpeg(x['image'], channels=1)\n        image_train = _transform_images(res = res)(image_train)\n        label = tf.cast(x[\"label\"], tf.int32)\n        return (image_train, label)\n    \n    return parse_tfrecord\n\n\ndef _transform_images(res = res):\n    def transform_images(x_train):\n        x_train = tf.image.resize_with_pad(x_train, res, res, antialias = True)\n        x_train = tf.cast(x_train, tf.uint8)\n        return x_train\n    return transform_images\n\ndef load_tfrecord_dataset(tfrecord_name, res = res, batch_size = batch_size, shuffle=True, buffer_size=10240):\n    \"\"\"load dataset from tfrecord\"\"\"\n    raw_dataset = tf.data.TFRecordDataset(tfrecord_name, compression_type = \"GZIP\")\n    raw_dataset = raw_dataset.repeat()\n    if shuffle:\n        raw_dataset = raw_dataset.shuffle(buffer_size=buffer_size)\n    dataset = raw_dataset.map(\n        _parse_tfrecord(),\n        num_parallel_calls=tf.data.AUTOTUNE\n    )\n    dataset = dataset.batch(batch_size, drop_remainder = True)\n    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n    return dataset\n\ntrain_radimagenet_ds = load_tfrecord_dataset(\"/kaggle/input/radimagenet-and-nih-cxr-dataset-tfrecord/RagImageNet_Train_GZIP.tfrecord\")\nval_ds = load_tfrecord_dataset(\"/kaggle/input/radimagenet-and-nih-cxr-dataset-tfrecord/RagImageNet_Test_GZIP.tfrecord\")","metadata":{"_kg_hide-input":true,"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# NIH CXR decoding","metadata":{}},{"cell_type":"code","source":"def _parse_tfrecord(res = res):\n    def parse_tfrecord(tfrecord):\n        features = {'image_raw': tf.io.FixedLenFeature([], tf.string),\n                    'label': tf.io.FixedLenFeature([], tf.int64),\n                    }\n        x = tf.io.parse_single_example(tfrecord, features)\n        image_train = tf.image.decode_jpeg(x['image_raw'], channels=1)\n        image_train = _transform_images(res = res)(image_train)\n        label = tf.cast(x[\"label\"], tf.int32)\n        return (image_train, label)\n    \n    return parse_tfrecord\n\n\ndef _transform_images(res = res):\n    def transform_images(x_train):\n        x_train = tf.image.resize_with_pad(x_train, res, res, antialias = True)\n        x_train = tf.cast(x_train, tf.uint8)\n        return x_train\n    return transform_images\n\ndef load_tfrecord_dataset(tfrecord_name, res = res, batch_size = batch_size, shuffle=True, buffer_size=10240):\n    \"\"\"load dataset from tfrecord\"\"\"\n    raw_dataset = tf.data.TFRecordDataset(tfrecord_name)\n    raw_dataset = raw_dataset.repeat()\n    if shuffle:\n        raw_dataset = raw_dataset.shuffle(buffer_size=buffer_size)\n    dataset = raw_dataset.map(\n        _parse_tfrecord(),\n        num_parallel_calls=tf.data.AUTOTUNE\n    )\n    if batch_size:\n        dataset = dataset.batch(batch_size, drop_remainder = True)\n    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n    return dataset\n\nnih_cxr_ds = load_tfrecord_dataset(\"/kaggle/input/radimagenet-and-nih-cxr-dataset-tfrecord/nih_cxr_images.tfrecords\")","metadata":{"_kg_hide-input":true,"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Merging 2 datasets","metadata":{}},{"cell_type":"code","source":"train_ds = tf.data.Dataset.sample_from_datasets([train_radimagenet_ds.unbatch(), nih_cxr_ds.unbatch()], weights = [0.75, 0.25]).batch(batch_size, drop_remainder = True).repeat().prefetch(tf.data.AUTOTUNE)\nval_ds_ = tf.data.Dataset.sample_from_datasets([train_radimagenet_ds.unbatch(), nih_cxr_ds.unbatch()], weights = [0.75, 0.25]).batch(6, drop_remainder = True).prefetch(tf.data.AUTOTUNE)\n#train ds output : ([batch_size, res, res, 1], [batch_size,])\n# train data curation\nfor images, labels in val_ds_.take(1):\n    sample_img = images\n    labels = labels\n    \ndel val_ds_","metadata":{"_kg_hide-input":true,"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_sobel_fn():\n    def sobel_merge(image, label):\n        image = image[tf.newaxis, ...]\n        rand_num = keras.random.randint(shape = (), minval = 1, maxval = 10)\n        if rand_num > 5:\n            image = ops.cast(image, 'float32')\n            ed = tf.image.sobel_edges(image)[..., 0, :]\n            ed_norm = 255.0 * (ed - ops.min(ed)) / (ops.max(ed) - ops.min(ed)) ; del ed\n            ed_norm = ops.cast(ed_norm, \"uint8\")\n            image =ops.concatenate([image, ed_norm],\n                                  axis = -1)\n            image = ops.cast(image, \"uint8\")\n        else:\n            try:\n                image = tf.image.grayscale_to_rgb(image)\n            except:\n                pass\n        image = image[0]\n        return image, label\n    return sobel_merge\nsobel_merge = get_sobel_fn()","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Convert supervised dataset into SSL dataset","metadata":{}},{"cell_type":"code","source":"multiview_fn = get_map_fn(res = res, input_type = \"supervised\", output_type = \"ssl\",\n                         n_view = n_multicrop, grayscale = grayscale)\n\ntrain_ds_multiview = train_ds.map(multiview_fn, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\nval_ds_multiview = val_ds.map(multiview_fn, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n\nmask_map_fn_ = get_masking_fn(grayscale = grayscale, masking_rate = 0.5, patch_size = patch_size)\n\ntrain_edge_ds = train_ds.unbatch().map(sobel_merge, num_parallel_calls=tf.data.AUTOTUNE).batch(batch_size, drop_remainder = True).prefetch(tf.data.AUTOTUNE)\n\ndef masking_function(image, label):\n    return mask_map_fn_(image)\n\ntrain_ds_masked = train_ds.map(masking_function, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE).repeat()\n#train_ds_edge_masked = train_edge_ds.unbatch().map(masking_function, num_parallel_calls=tf.data.AUTOTUNE).batch(batch_size, drop_remainder = True).prefetch(tf.data.AUTOTUNE).repeat()\n#train_ds_edge_multiview = train_edge_ds.map(multiview_fn, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n#train_ds_edge_simple_multiview = train_edge_ds.map(simple_aug_fn, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"----------\n# Experiment - helper functions","metadata":{}},{"cell_type":"code","source":"df_train_rad = pd.read_csv(\"/kaggle/input/radimagenet-and-nih-cxr-dataset-tfrecord/RadImgNet_train.csv\")\ndf_train_nih = pd.read_csv(\"/kaggle/input/radimagenet-and-nih-cxr-dataset-tfrecord/nih_trainval_split.csv\"\n                          )\ndf_val_rad = pd.read_csv(\"/kaggle/input/radimagenet-and-nih-cxr-dataset-tfrecord/RadImgNet_test.csv\")\ndf_val_nih = pd.read_csv(\"/kaggle/input/radimagenet-and-nih-cxr-dataset-tfrecord/nih_test_split.csv\")\n\n\ntrain_cases = len(df_train_rad) + len(df_train_nih) + len(df_val_nih)\nval_cases = len(df_val_rad)\n\ntrain_steps = train_cases//batch_size\nval_steps = val_cases//batch_size\nprint(f\"Total train cases : {train_cases}, validation cases : {val_cases}\")","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class GCAdamW(keras.optimizers.AdamW):\n    def get_gradients(self, loss, params):\n        # We here just provide a modified get_gradients() function since we are\n        # trying to just compute the centralized gradients.\n\n        grads = []\n        gradients = super().get_gradients()\n        for grad in gradients:\n            grad_len = len(grad.shape)\n            if grad_len > 1:\n                axis = list(range(grad_len - 1))\n                grad -= ops.mean(grad, axis=axis, keep_dims=True)\n            grads.append(grad)\n\n        return grads\n    \nclass GCAdam(keras.optimizers.Adam):\n    def get_gradients(self, loss, params):\n        # We here just provide a modified get_gradients() function since we are\n        # trying to just compute the centralized gradients.\n\n        grads = []\n        gradients = super().get_gradients()\n        for grad in gradients:\n            grad_len = len(grad.shape)\n            if grad_len > 1:\n                axis = list(range(grad_len - 1))\n                grad -= ops.mean(grad, axis=axis, keep_dims=True)\n            grads.append(grad)\n\n        return grads","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ModelSaveCallback(keras.callbacks.Callback):\n    def __init__(self, exp_name, message = None, **kwargs):\n        super().__init__(**kwargs)\n        self.exp_name = exp_name\n        self.message = message if message is not None else \" \"\n    def on_epoch_end(self, epoch, logs=None):\n        feature_ext_name = self.model.feature_extractor.name\n        save_dir = \"/kaggle/working/\" ; target_dir = '/kaggle/working/model_save'\n        os.makedirs(target_dir, exist_ok = True)\n        if (epoch % 1 == 0):\n            try:\n                print(\"\\nModel Saving to local notebook...\")\n                file_name = f\"{feature_ext_name}_FE{self.exp_name}_Epoch{epoch}_{self.message}.keras\"\n                filepath = os.path.join(target_dir, file_name)\n                saved_dir = self.model.feature_extractor.save(filepath, overwrite=True)\n                print(\"\\nModel Uploading to NAS...\")\n                upload_file(file_name, filepath)\n                print(\"\\nModel Saved to Local NAS\")\n            except Exception as e: \n                print('Model Saving Error:\\n', e)\n    def on_train_batch_end(self, batch, logs=None):\n        feature_ext_name = self.model.feature_extractor.name\n        save_dir = \"/kaggle/working/\" ; target_dir = '/kaggle/working/model_save'\n        os.makedirs(target_dir, exist_ok = True)\n        if (batch % 20000 == 0) and (batch != 0): \n            print(\"\\nModel Saving to local notebook...\")\n            file_name = f\"{feature_ext_name}_FE{self.exp_name}_Batch{batch}_{self.message}.keras\"\n            filepath = os.path.join(target_dir, file_name)\n            saved_dir = self.model.feature_extractor.save(filepath, overwrite=True)\n            print(\"\\nModel Uploading to NAS...\")\n            upload_file(file_name, filepath)\n            print(\"\\nModel Saved to Local NAS\")\n                \nclass TemperatureScheduler(keras.callbacks.Callback):\n    def __init__(self, initial_t = 0.5, decay_rate = 0.99):\n        super().__init__()\n        self.initial_t = initial_t\n        self.decay_rate = decay_rate\n\n    def on_train_batch_begin(self, batch, logs=None):\n        if not hasattr(self.model, 't'):\n            self.model.t = ops.convert_to_tensor(self.initial_t, dtype='float32')\n        else:\n            if (batch > 0) and (batch % 5000 == 0):\n                self.model.t = self.model.t * self.decay_rate","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SimpleModelSaveCallback(keras.callbacks.Callback):\n    def __init__(self, exp_name, message = None, **kwargs):\n        super().__init__(**kwargs)\n        self.exp_name = exp_name\n        self.message = message if message is not None else \" \"\n    \n    def on_train_batch_end(self, batch, logs=None):\n        feature_ext_name = self.model.name\n        save_dir = \"/kaggle/working/\" ; target_dir = '/kaggle/working/model_save'\n        os.makedirs(target_dir, exist_ok = True)\n        if (batch % 20000 == 0) and (batch != 0): \n            print(\"\\nModel Saving to local notebook...\")\n            file_name = f\"{feature_ext_name}_FE{self.exp_name}_Batch{batch}_{self.message}.keras\"\n            filepath = os.path.join(target_dir, file_name)\n            saved_dir = self.model.save(filepath, overwrite=True)\n            print(\"\\nModel Uploading to NAS...\")\n            upload_file(file_name, filepath)\n            print(\"\\nModel Saved to Local NAS\")\n                ","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TrainingViz(keras.callbacks.Callback):\n    def __init__(self, run):\n        super().__init__()\n        self.run = run\n    def on_epoch_end(self, epoch, logs=None):\n        try:\n            configs = self.model.get_env_config() ; method = configs[\"SSL_method\"]\n            if method in [\"CLIP\" , \"SigLIP\", \"SPARC\"]:\n                feature_extractor = self.model\n            else:\n                try:\n                    feature_extractor = self.model.feature_extractor\n                except:\n                    feature_extractor = self.model.get_full_model(res = res)\n            viz_weights, merged_weights = ssl_module.att_visualize(feature_extractor, sample_img, res,\n                                                  thresholding = 0)\n            viz_weights = np.array(viz_weights) #batch, heads, res, res, 3\n            merged_weights = np.array(merged_weights)\n            heads = viz_weights.shape[1]\n            origin = [\"Original Image\"]\n            col = [f\"Head{idx + 1}\" for idx in range(heads)]\n            col = origin + [\"Merged\"] + col\n\n            visualize_data = []\n            for idx, weights in enumerate(viz_weights):\n                origin_img = [wandb.Image(sample_img[idx])]\n                merged_tmp = [wandb.Image(merged_weights[idx])]\n                tmp = [wandb.Image(weights[idx]) for idx in range(heads)]\n                tmp = origin_img + merged_tmp + tmp\n                visualize_data.append(tmp)\n                del tmp, origin_img, merged_tmp\n            tbl = wandb.Table(columns = col, data = visualize_data)\n            wandb.log({f\"Epoch{epoch+1}_{method}_result\": tbl})\n            del feature_extractor, tbl\n            tf.keras.backend.clear_session()\n            \n            # feature vector visualization\n            embed_v = feature_visualize(self.model, sample_img)\n            data = [[x, y] for (x, y) in zip(embed_v[..., 0], embed_v[..., 1])]\n            table = wandb.Table(data=data, columns = [\"x\", \"y\"])\n            wandb.log({f\"Epoch{epoch+1}_{method}_FeatureViz\" : wandb.plot.scatter(table, \"x\", \"y\", title=\"TSNE Scatter Plot\")})\n            tf.keras.backend.clear_session()\n\n            \n        except Exception as e: \n                print('Model Saving Error:\\n', e)\n        \n    def on_train_batch_end(self, batch, logs=None):\n        if (batch % (10000) == 0) : \n            try:\n                configs = self.model.get_env_config() ; method = configs[\"SSL_method\"]\n                if method in [\"CLIP\" , \"SigLIP\", \"SPARC\"]:\n                    feature_extractor = self.model\n                else:\n                    try:\n                        feature_extractor = self.model.feature_extractor\n                    except:\n                        feature_extractor = self.model.get_full_model(res = res)\n                viz_weights, merged_weights = ssl_module.att_visualize(feature_extractor, sample_img, res,\n                                                      thresholding = False)\n                _, rollout_merged_image = ssl_module.att_visualize_merged(feature_extractor, sample_img, res)\n                viz_weights = np.array(viz_weights) #batch, heads, res, res, 3\n                merged_weights = np.array(merged_weights)\n                heads = viz_weights.shape[1]\n                origin = [\"Original Image\"]\n                col = [f\"Head{idx + 1}\" for idx in range(heads)]\n                col = origin + [\"MergedMap\"] + ['Top-K head merging map'] + col\n                \n                visualize_data = []\n                for idx, weights in enumerate(viz_weights): #heads, res, res, 3\n                    origin_img = [wandb.Image(sample_img[idx])]\n                    merged_map = [wandb.Image(merged_weights[idx])]\n                    merged_map_rollout = [wandb.Image(rollout_merged_image[idx])]\n                    \n                    tmp = [wandb.Image(weights[idx]) for idx in range(heads)]\n                    tmp = origin_img + merged_map + merged_map_rollout + tmp\n                    visualize_data.append(tmp)\n                    del tmp, origin_img, merged_map, merged_map_rollout\n                tbl = wandb.Table(columns = col, data = visualize_data)\n                if batch == 0:\n                    wandb.log({f\"ZeroBatch_{method}_result\": tbl})\n                else:\n                    wandb.log({f\"MidEpoch_{method}_result\": tbl})\n                del feature_extractor, tbl\n                tf.keras.backend.clear_session()\n                       \n                embed_v = feature_visualize(self.model, sample_img)\n                data = [[x, y] for (x, y) in zip(embed_v[..., 0], embed_v[..., 1])]\n                table = wandb.Table(data=data, columns = [\"x\", \"y\"])\n                wandb.log({f\"Batch{batch}_{method}_FeatureViz\" : wandb.plot.scatter(table, \"x\", \"y\", title=\"TSNE Scatter Plot\")})\n                tf.keras.backend.clear_session()\n                       \n            except Exception as e:\n                print(\"Error code in callback : \", e)\n           \n        else:\n            pass","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"> Real world evaluation and Segmentation callback","metadata":{}},{"cell_type":"code","source":"real_world_dir = \"/kaggle/input/real-world-medical-image-dataset-for-evaluation/radiopaedia_example\" ; filenames_ = os.listdir(real_world_dir)\nfilenames_.sort()\nlabels_ = [name.split('.')[0] for name in filenames_]\nreal_world_files = [os.path.join(real_world_dir, paths) for paths in filenames_]\ndef get_img_tensor(path, res = res) :\n    file = tf.io.read_file(path)\n    c =1 if grayscale else 3\n    image = tf.io.decode_image(file, channels=c)\n    image = tf.image.resize_with_pad(image, res, res, antialias = True)\n    image = ops.cast(image, \"uint8\")\n    return image\nreal_world_images = tf.stack([get_img_tensor(f) for f in real_world_files],\n                             axis = 0)\n","metadata":{"_kg_hide-input":true,"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"> medpix external evaluation","metadata":{}},{"cell_type":"code","source":"def _parse_function_medpix(proto):\n    feature_description = {\n        'image': tf.io.FixedLenFeature([], tf.string),\n        'caption': tf.io.FixedLenFeature([], tf.string)\n    }\n    parsed_features = tf.io.parse_single_example(proto, feature_description)\n\n    # image 데이터를 디코딩 (채널 수 1: grayscale 이미지로 가정)\n    image = tf.image.decode_image(parsed_features['image'], channels=1)\n    image = ops.cast(image, \"float32\")\n    image = (image - ops.min(image)) / (ops.max(image) - ops.min(image) + 1e-4)\n    image = 255.0*image\n    image = ops.cast(image, \"uint8\")\n\n    # caption은 bytes 타입이므로 그대로 반환 (나중에 decode 처리)\n    caption = parsed_features['caption']\n    return image, caption\n\n\n\n# TFRecord 파일을 GZIP 압축 옵션과 함께 읽어들임\ntfrecord_file = '/kaggle/input/real-world-medical-image-dataset-for-evaluation/medpix_val.tfrecord'\nraw_dataset = tf.data.TFRecordDataset(tfrecord_file, compression_type='GZIP')\nmedpix_val = raw_dataset.map(_parse_function_medpix)\nmedpix_images = []\nmedpix_captions = []\n\nfor i, c in medpix_val.take(60):\n    i = tf.image.resize_with_pad(i, 512,512, antialias = True)\n    c = c.numpy().decode('utf-8')\n    medpix_images.append(i)\n    medpix_captions.append(c)\nmedpix_images = tf.stack(medpix_images, axis = 0)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class RealWorldViz(keras.callbacks.Callback):\n    def __init__(self, run):\n        super().__init__()\n        self.run = run\n    \n    def on_train_batch_end(self, batch, logs=None):\n        if (batch % (5000) == 0) : \n            try:\n                configs = self.model.get_env_config() ; method = configs[\"SSL_method\"]\n                if method in [\"CLIP\" , \"SigLIP\", \"SPARC\"]:\n                    feature_extractor = self.model\n                else:\n                    try:\n                        feature_extractor = self.model.feature_extractor\n                    except:\n                        feature_extractor = self.model.get_full_model(res = res)\n                viz_weights, merged_weights = ssl_module.att_visualize(feature_extractor, real_world_images, res,\n                                                      thresholding = False)\n                _, rollout_merged_image = ssl_module.att_visualize_merged(feature_extractor, \n                                                                          real_world_images, res)\n                viz_weights = np.array(viz_weights) #batch, heads, res, res, 3\n                merged_weights = np.array(merged_weights)\n                \n                heads = viz_weights.shape[1]\n                origin = [\"Original Image\"]\n                col = [f\"Head{idx + 1}\" for idx in range(heads)]\n                col = origin + [\"Original Label\"] + [\"MergedMap\"] + ['Top-K head merging map'] + col\n                visualize_data = []\n                for idx, weights in enumerate(viz_weights):\n                    origin_img = [wandb.Image(real_world_images[idx])]\n                    lab = [labels_[idx]]\n                    \n                    merged_map = [wandb.Image(merged_weights[idx])]\n                    merged_map_rollout = [wandb.Image(rollout_merged_image[idx])]\n                    \n                    tmp = [wandb.Image(weights[idx]) for idx in range(heads)]\n                    tmp = origin_img +lab +  merged_map + merged_map_rollout + tmp\n                    visualize_data.append(tmp)\n                    del tmp, origin_img, merged_map, merged_map_rollout\n                tbl = wandb.Table(columns = col, data = visualize_data)\n                if batch == 0:\n                    wandb.log({f\"RW_ZeroBatch_{method}_result\": tbl})\n                else:\n                    wandb.log({f\"RW_Batch{batch}_{method}_result\": tbl})\n                del tbl\n                tf.keras.backend.clear_session()\n                \n                embed_v = feature_visualize(self.model, real_world_images)\n                data = [[x, y] for (x, y) in zip(embed_v[..., 0], embed_v[..., 1])]\n                table = wandb.Table(data=data, columns = [\"x\", \"y\"])\n                wandb.log({f\"RW_Batch{batch}_{method}_FeatureViz\" : wandb.plot.scatter(table, \"x\", \"y\", \n                                                                                       title=f\"RW_Batch{batch}_TSNE\")})\n                #########Hierarchical clustering##########\n                feature_map = feature_extractor(real_world_images)[1]\n                n_patch = feature_map.shape[1] ; w_ = ops.sqrt(ops.cast(n_patch, \"float32\")\n                                                              )\n                w_ = ops.cast(w_, \"int32\")\n                embed_dims = feature_map.shape[-1]\n                clustering_output = ssl_module.H_clustering(n_clusters = 200)(feature_map)\n                clustering_output = ops.reshape(clustering_output, [-1, w_, w_,1])\n                data = []\n                for i, sample_image in enumerate(real_world_images):\n                    cluster_plot = tf.convert_to_tensor(clustering_output[i])\n                    cluster_plot = (cluster_plot - ops.min(cluster_plot)) / (ops.max(cluster_plot) - ops.min(cluster_plot)) \n                    cluster_plot *= 255 ; cluster_plot = tf.image.grayscale_to_rgb(ops.cast(cluster_plot, \"uint8\"))\n                    cluster_plot = np.array(cluster_plot)\n                    cluster_plot = PILImage.fromarray(cluster_plot, mode=\"RGB\")\n                    cluster_plot = wandb.Image(cluster_plot)\n                    rw_image = wandb.Image(sample_image)\n                    tmp = [rw_image, cluster_plot]\n                    data.append(tmp) ; del tmp, rw_image\n                 \n                table = wandb.Table(data=data, columns = [\"Original_image\", \"AgglomerativeCluster\"])\n                wandb.log({f\"Cluster_RW_Batch{batch}_{method}_result\": table})\n                tf.keras.backend.clear_session()\n                del feature_extractor\n            except Exception as e:\n                print(\"Error code in callback : \", e)\n        else:\n            pass\n        ","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class AttVizForHybridViT(keras.callbacks.Callback):\n    def __init__(self, run):\n        super().__init__()\n        self.run = run\n    \n    def on_train_batch_end(self, batch, logs=None):\n        if (batch % (5000) == 0) : \n            print(\"Callback for visualization...\", \"\\n\")\n            print(\"val dataset infer...\\n\")\n            infer_output_val = self.model.infer(sample_img)\n            print(\"MedPix dataset infer...\\n\")\n            infer_output_medpix = self.model.infer(medpix_images)\n            \n            heads = ops.shape(infer_output_val['merged_original'])[1]\n            batch_size_val = ops.shape(infer_output_val['merged_original'])[0]\n            batch_size_medpix = ops.shape(infer_output_medpix['merged_original'])[0]\n            \n            feature_name_ = ['pca', 'tsne',  'agglomerative', 'kmeans']\n            \n            col_features_ = [\"PCA map\", \"TSNE map\", \"Agglomerative c. map\", \"Kmeans map\"]\n            \n            viz_val, viz_medpix = [], []\n            # val ds loop\n            col_medpix = ['Original Image', 'Captions', 'Headwise merged att map'] + col_features_ + [f\"Head{k}\" for k in range(heads)]\n            print(\"MedPix dataset uploading...\\n\")\n            for idx in range(batch_size_medpix):\n                origin = [wandb.Image(medpix_images[idx])]\n                lab = [medpix_captions[idx]]\n                \n                head_merged = [wandb.Image(infer_output_medpix[\"head_merged_original\"][idx])]\n                tmp = [wandb.Image(infer_output_medpix[\"merged_original\"][idx, h]) for h in range(heads)]\n                feature_tmp = [wandb.Image(infer_output_medpix[N][idx]) for N in feature_name_]\n                \n                data_ = origin + lab + head_merged + feature_tmp + tmp\n                viz_medpix.append(data_)\n            tbl = wandb.Table(columns = col_medpix, data = viz_medpix)\n            wandb.log({f\"MedPix_data_viz_{batch}batch\": tbl})\n\n            col_val = ['Original Image', 'Headwise merged att map'] + col_features_ + [f\"Head{k}\" for k in range(heads)]\n            print(\"Val dataset uploading...\\n\")\n            for idx in range(batch_size_val):\n                origin = [wandb.Image(sample_img[idx])]\n                head_merged = [wandb.Image(infer_output_val[\"head_merged_original\"][idx])]\n                tmp = [wandb.Image(infer_output_val[\"merged_original\"][idx, h]) for h in range(heads)]\n                feature_tmp = [wandb.Image(infer_output_val[N][idx]) for N in feature_name_]\n                data_ = origin + head_merged + feature_tmp + tmp\n                viz_val.append(data_)\n            tbl = wandb.Table(columns = col_val, data = viz_val)\n            wandb.log({f\"val_data_viz_{batch}batch\": tbl})\n            \n            tf.keras.backend.clear_session()\n            print(\"Done!\\n\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"-------------\n- Special callback for QNCLR","metadata":{}},{"cell_type":"code","source":"class QRealWorldViz(keras.callbacks.Callback):\n    def __init__(self, run):\n        super().__init__()\n        self.run = run\n    \n    def on_train_batch_end(self, batch, logs=None):\n        if (batch % (5000) == 0) : \n            if True:\n                feature_extractor = self.model.feature_extractor\n                real_world_images = ops.cast(real_world_images, \"float32\")\n                q_attention_weights, q_batch_merged = ssl_module.q_visualize(feature_extractor, real_world_images, res,\n                                                      thresholding = False)\n                q_attention_weights = np.array(q_attention_weights) #batch, N_Q, res, res, 3\n                q_batch_merged = np.array(q_batch_merged) #batch, res, res, 3\n                \n                n_queries = q_attention_weights.shape[1]\n                origin = [\"Original Image\"]\n                col = [f\"LearnableQuery{idx + 1}\" for idx in range(n_queries)]\n                \n                col = origin + [\"Original Label\"] + [\"MergedMap\"] + col\n                print(col)\n                \n                visualize_data = []\n                for idx, weights in enumerate(q_attention_weights):\n                    origin_img = [wandb.Image(real_world_images[idx])]\n                    lab = [labels_[idx]]\n                    \n                    merged_map = [wandb.Image(q_batch_merged[idx])]\n                    each_query_map = [wandb.Image(weights[idx]) for idx in range(n_queries)]\n                    \n                    tmp = origin_img +lab + merged_map + each_query_map\n                    visualize_data.append(tmp)\n                    print(len(tmp))\n                    del tmp, origin_img, lab, merged_map, each_query_map\n                    \n                tbl = wandb.Table(columns = col, data = visualize_data)\n                if batch == 0:\n                    wandb.log({f\"RW_ZeroBatch_{method}_result\": tbl})\n                else:\n                    wandb.log({f\"RW_Batch{batch}_{method}_result\": tbl})\n                del feature_extractor, tbl\n                tf.keras.backend.clear_session()\n\n            \n        else:\n            pass","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class RealWorldPatchViz(keras.callbacks.Callback):\n    def __init__(self, run):\n        super().__init__()\n        self.run = run\n    \n    def on_train_batch_end(self, batch, logs=None):\n        if (batch % (5000) == 0) : \n            if True:\n                #real_world_images = ops.cast(real_world_images, \"uint8\")\n                feature_extractor = self.model.feature_extractor\n                patch_heatmap, patch_merged_images = ssl_module.pca_patch_viz(feature_extractor, real_world_images)\n                patch_heatmap = np.array(patch_heatmap)\n                patch_merged_images = np.array(patch_merged_images)\n                \n                col = [\"Original image\"] + [\"Original Label\"] + [\"Merged image\"] + [\"Encoded Patches\"]\n                print(col)\n                \n                visualize_data = []\n                for idx, m_img in enumerate(patch_merged_images):\n                    origin_img = [wandb.Image(real_world_images[idx])]\n                    lab = [labels_[idx]]\n                    \n                    merged_image = [wandb.Image(m_img)]\n                    e_patches = [wandb.Image(patch_heatmap[idx])]\n                    \n                    tmp = origin_img +lab + merged_image + e_patches\n                    visualize_data.append(tmp)\n                    \n                    del tmp, origin_img, lab, merged_image, e_patches\n                    \n                tbl = wandb.Table(columns = col, data = visualize_data)\n                if batch == 0:\n                    wandb.log({f\"RW_ZeroBatch_Patch_result\": tbl})\n                else:\n                    wandb.log({f\"RW_Batch{batch}_Patch_result\": tbl})\n                del feature_extractor, tbl\n                tf.keras.backend.clear_session()\n\n            \n        else:\n            pass","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SegViz(keras.callbacks.Callback):\n    def __init__(self, run, images, labels = None):\n        super().__init__()\n        self.run = run\n        self.images = images\n        self.labels = labels\n    def on_train_batch_end(self, batch, logs=None):\n        configs = self.model.get_env_config() ; method = configs[\"SSL_method\"]\n        if (batch % (10000) == 0) and  (method in [\"UnsupSeg\", \"MixedUnsupSeg\"]): \n            try:\n                heatmap, superimposed_images = self.model.get_segments(self.images)\n                origin = [\"Original Image\"]\n                col = origin + [\"Original Label\"] + [\"Segmentation Result\"]\n                visualize_data = []\n                for idx, sup_img in enumerate(superimposed_images):\n                    origin_img = [wandb.Image(self.images[idx])]\n                    if self.labels is None:\n                        lab = [\"Label not provided.\"]\n                    else:\n                        lab = [self.labels[idx]] \n                    tmp = [wandb.Image(sup_img)]\n                    tmp = origin_img + lab + tmp\n                    visualize_data.append(tmp)\n                    del tmp, origin_img\n                tbl = wandb.Table(columns = col, data = visualize_data)\n                if batch == 0:\n                    wandb.log({f\"Seg_ZeroBatch_{method}_result\": tbl})\n                else:\n                    wandb.log({f\"Seg_MidEpoch_{method}_result\": tbl})\n                \n                tf.keras.backend.clear_session()\n            except Exception as e:\n                print(\"Error code in Segmentation callback : \", e)\n        else:\n            pass","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def run_exp(model, train_ds = train_ds, val_ds = val_ds, epochs = 10, note= None, exp_name = None):\n    try:\n        wandb.finish()\n    except:\n        pass\n    \n    if True :\n        wandb_config()\n        configs = model.get_env_config()\n        method = configs[\"SSL_method\"]\n        try:\n            feature_extractor = model.feature_extractor\n        except:\n            feature_extractor = model.get_full_model(res = res)\n        \n        if method in ['CLIP', \"SigLIP\", \"SPARC\"]:\n            _ = model((example_images[:2], example_reports[:2]))\n        elif method in [\"SimMIM\", \"MixedMIM\",\"DistilMIM\", \"MixedUnsupSeg\", \n                        \"NCLR_nnclr_without_momentum\", 'NCLR_snclr_without_momentum']:\n            pass\n        else:\n            pass\n        try:\n            feature_extractor_flops = get_flops(feature_extractor, [tf.random.normal([1,res,res,c])])\n        except:\n            feature_extractor_flops = \"Uncheck\"\n        del feature_extractor\n        \n        env_config = {\"batch_size\" : batch_size, \"Patch size\": patch_size,\n                      \"original resolution\" : res, \"local view resolution\" : small_res,\n                     \"Training steps\" : train_steps,\n                     \"Val steps\" : val_steps,\n                     \"train cases\" : train_cases,\n                     \"val cases\" : val_cases,\n                     \"embed_dims\" : embed_dims,\n                     \"Image resolution\" : res,\n                     \"(Image) Encoder Flops(G)\" : feature_extractor_flops,\n                     \"dtype\" : keras.mixed_precision.dtype_policy(),\n                      \"Optimizer configs\" : model.optimizer.get_config(),\n                      \"Multicrop N\" : n_multicrop, \"metaencoder depth\" : depth, 'embedding dims' : embed_dims,\n                     }\n        configs.update(env_config)\n        \n        wd = \"/kaggle/working/\"\n        file_name = os.path.join(wd, f\"{method}_radimgnet_mini.keras\")\n        print(configs, \"\\n\\n\")\n        \n        run = wandb.init(project=\"RadImageNet\", \n                         entity=\"gongbungkim\", config = configs, notes = note,\n                        name = exp_name)\n        wandb.run.log_code(\".\")\n        pass_error = keras.callbacks.TerminateOnNaN()\n        wb_callback = wandb.keras.WandbMetricsLogger(log_freq = 100)\n        if isinstance(model, ssl_module.QNCLR):\n            callbacks = [pass_error, wb_callback, ModelSaveCallback(f\"RI_SSL_{method}\", note), \n                        QRealWorldViz(run),\n                        TemperatureScheduler()]\n        elif isinstance(model, HybridViT):\n            callbacks = [pass_error, wb_callback, AttVizForHybridViT()]\n        else:\n            callbacks = [pass_error, wb_callback, ModelSaveCallback(f\"RI_SSL_{method}\", note), \n                         TrainingViz(run),\n                        RealWorldViz(run), RealWorldPatchViz(run),\n                        SegViz(run, images = sample_img),\n                        SegViz(run, images = real_world_images, labels = labels_),\n                        TemperatureScheduler()]\n        if val_ds is not None:\n            hist = model.fit(train_ds, \n                             steps_per_epoch = train_steps, \n                             epochs = epochs, \n                             validation_data = val_ds, \n                             validation_steps = val_steps, \n                             verbose = 1,\n                             callbacks = callbacks)\n        else:\n            hist = model.fit(train_ds, \n                         steps_per_epoch = train_steps, \n                         epochs = epochs, \n                         verbose = 1,\n                         callbacks = callbacks)\n    return hist","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cosine_decay = keras.optimizers.schedules.CosineDecay(\n    initial_learning_rate = 1e-6,\n    decay_steps = train_steps - 10000,\n    alpha=1e-5,\n    name='CosineDecay',\n    warmup_target=2e-4,\n    warmup_steps=10000\n)\nsimple_cos_decay = keras.optimizers.schedules.CosineDecay(\n    initial_learning_rate = 3e-5,\n    decay_steps = train_steps,\n    alpha=1e-5,\n    name='SimpleCosineDecay',\n    #warmup_target=1e-3,\n    #warmup_steps=train_steps - int(0.3*train_steps)\n)\nlr_schedule = keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate = 2e-4,\n    decay_steps=20000,\n    decay_rate=0.75,\n    staircase=True)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def ssl_train(module, feature_extractor, learning_rate = lr_schedule,\n              embed_dims = embed_dims, multiview = True, gradient_accumulation = None, use_ema = False,\n             note = \"\", name = \"\",\n             apply_barlow = False, apply_simclr = False):\n    try:\n        ssl_trainer = module(feature_extractor, embed_dims = embed_dims, multiview = multiview,\n                            apply_barlow = apply_barlow, apply_simclr = apply_simclr)\n    except Exception as e:\n        print(\"Error : \",e)\n        ssl_trainer = module(feature_extractor, embed_dims = embed_dims, multiview = multiview)\n    ssl_trainer.compile(optimizer = keras.optimizers.Adam(learning_rate = learning_rate,\n                                                         clipnorm = 0.5,\n                                                         #amsgrad = True,\n                                                           gradient_accumulation_steps=gradient_accumulation,\n                                                         use_ema = use_ema),\n                        jit_compile = False\n                      )\n    \n    run_exp(ssl_trainer, train_ds_multiview, None, epochs = 100,\n       note = note, exp_name = name)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Hybrid ViT training","metadata":{}},{"cell_type":"code","source":"def simple_train_map(image, lab):\n    #image = tf.image.grayscale_to_rgb(image)\n    return image\ntrain_ds_vit = train_ds.map(simple_train_map).prefetch(tf.data.AUTOTUNE)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with strategy.scope():\n    reg_token = 0\n    vit_layers = 8\n    cutoff_layer = 4\n    embed_dims_vit = 768\n    heads = 12\n    model = HybridViT(image_size = res, \n                      num_layers = vit_layers, \n                      split_layer = cutoff_layer, \n                      num_heads = heads,\n                     embed_dims = embed_dims_vit,\n                     patch_size = 32,\n                     n_register = reg_token)\n    \n    _, encoded_patches, w = model(tf.image.rgb_to_grayscale(real_world_images)) ; print(encoded_patches.shape, w.shape)\n    dummy_input = tf.zeros((1, embed_dims_vit))\n    _ = model.mim_head(dummy_input)\n    _ = model.nnclr_proj(dummy_input)\n    \n    opt = keras.optimizers.SGD(learning_rate = simple_cos_decay, \n                                                   #clipnorm = 1.0, \n                                                   #use_ema = True\n                                                        )\n    \n    model_summary = []\n    model.summary(print_fn=lambda x: model_summary.append(x))\n    summary_str = \"\\n\".join(model_summary)\n    \n    model.compile(optimizer = opt)\n    \n    wandb_config()\n    env_config = {\"batch_size\" : batch_size, \"Patch size\": patch_size,\n                         \"Training steps\" : train_steps,\n                         \"Val steps\" : val_steps,\n                         \"train cases\" : train_cases,\n                         \"val cases\" : val_cases,\n                         \"embed_dims\" : embed_dims_vit,\n                         \"Image resolution\" : res,\n                         \"dtype\" : keras.mixed_precision.dtype_policy(),\n                          \"Optimizer configs\" : model.optimizer.get_config(),\n                  \"metaencoder depth\" : vit_layers, \n                  'embedding dims' : embed_dims_vit,\n                         }\n    run = wandb.init(project=\"RadImageNet\", \n                             entity=\"gongbungkim\", config = env_config,\n                            name = f'HybridVit_depth{vit_layers}_{heads}heads_cutoff{cutoff_layer}',\n                    notes = f\"{reg_token}reg, Dynamic masking, Variable Resolution with denseCL, gray input, res{res}\")\n    run.summary[\"model_summary\"] = summary_str\n    pass_error = keras.callbacks.TerminateOnNaN()\n    wb_callback = wandb.keras.WandbMetricsLogger(log_freq = 100)\n    callbacks = [pass_error, wb_callback, AttVizForHybridViT(run),\n                SimpleModelSaveCallback(f\"HybridVit_depth{vit_layers}_{heads}heads_cutoff{cutoff_layer}\")]\n    \n    model.fit(train_ds_vit, epochs = 1, steps_per_epoch = train_steps, \n             verbose = 1, callbacks = callbacks)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Else","metadata":{}},{"cell_type":"code","source":"ibot = 0\nother = 0\nmim = 0\nqnclr = 0\nsobel_other = 0","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if mim:\n    model_ = 'attention'\n    vanilla_model = ssl_module.get_metaformer(model_, res = res, embed_dims = embed_dims, \n                                              att_depth = depth, att_heads = heads,\n                                              att_dims = att_dims,\n                                              grayscale = grayscale, patch_size = patch_size, \n                                              register_tokens = registers,\n                                             pretrained_encoder = pretrained_encoder,\n                                             return_patches = True)\n    vanilla_model.summary()\n    ssl_trainer = ssl_module.MixedMIM(vanilla_model, grayscale = grayscale, patch_size = patch_size)\n    ssl_trainer.compile(optimizer = keras.optimizers.AdamW(learning_rate = cosine_decay, \n                                                           clipnorm = 1.0,\n                                                           #gradient_accumulation_steps=64,\n                                                           use_ema = True\n                                                          ),\n                        jit_compile = False\n                      )\n    configs = ssl_trainer.get_env_config()\n    method = configs[\"SSL_method\"]\n    run_exp(ssl_trainer, train_ds_edge_masked, None, \n           note = pretrained_note+\"_\"+model_, exp_name = f\"SobelMerging_Patch{patch_size}_{method}_{model_}\")","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if qnclr:\n    vanilla_model = ssl_module.get_encdec_model(pretrained_encoder,\n                                               res = res,\n                                               att_dims = embed_dims,\n                                               q_size = 8,\n                                               encoder_trainable = True)\n    ssl_trainer = ssl_module.QNCLR(vanilla_model, embed_dims = embed_dims, t = 0.05)\n    ssl_trainer.use_mim = True\n    ssl_trainer.compile(optimizer = keras.optimizers.AdamW(learning_rate = 5e-5, \n                                                               clipnorm = 1.0,\n                                                               gradient_accumulation_steps=32,\n                                                               #use_ema = True\n                                                              ),\n                            jit_compile = False,\n                            \n                          )\n    method = \"Q_NNCLR\"\n\n    run_exp(ssl_trainer, train_ds_masked, None, \n               note = pretrained_note, exp_name = f\"{method}_StrongAug\")","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if sobel_other:\n    if True:\n        model_ = 'attention'\n        vanilla_model = ssl_module.get_metaformer(model_, res = res, embed_dims = embed_dims, \n                                                  att_depth = depth, att_heads = heads,\n                                                  att_dims = att_dims,\n                                                  grayscale = grayscale, patch_size = patch_size, \n                                                  register_tokens = registers,\n                                                 pretrained_encoder = pretrained_encoder,\n                                                 # pretrained_encoder = pretrained_regnet,\n                                                  #pretrained_encoder = pretrained_vit,pretrained_vit = True,\n                                                 return_patches = True)\n        #dual_model = ssl_module.get_metaformer(model_, res = res, embed_dims = embed_dims, \n        #                                          att_depth = depth, att_heads = heads,\n        #                                          att_dims = att_dims,\n        #                                          grayscale = grayscale, patch_size = patch_size, \n        #                                          register_tokens = registers,\n        #                                         pretrained_encoder = get_dual_encoder(),\n        #                                         # pretrained_encoder = pretrained_regnet,\n        #                                          #pretrained_encoder = pretrained_vit,pretrained_vit = True,\n        #                                         return_patches = True)\n        #dual_model.summary()\n        \n        feature_map = vanilla_model(real_world_images)[1]\n        n_patch = feature_map.shape[1] ; w_ = ops.sqrt(ops.cast(n_patch, \"float32\")\n                                                                      )\n        w_ = ops.cast(w_, \"int32\")\n        embed_dims = feature_map.shape[-1]\n        #pretrained_note = f\"ViT{depth}\"\n        #ssl_trainer = ssl_module.DINO_MIM(vanilla_model, vanilla_model)\n        #ssl_trainer = ssl_module.NCLR(vanilla_model, embed_dims = embed_dims, subtype = \"nnclr\", use_mim = True, patch_size = patch_size)\n        ssl_trainer = ssl_module.SNCLR_SwAV(vanilla_model, swav_weight = 10, snclr_weight = 1,\n                                           q_size = 2**15)\n        #ssl_trainer = ssl_module.Moco(dual_model, use_dino = True)\n        ssl_trainer.compile(optimizer = keras.optimizers.SGD(learning_rate = 1e-4,\n                                                            momentum = 0.9,\n                                                            weight_decay = 0.0001,\n                                                            ),\n                            jit_compile = False,\n                            \n                          )\n        configs = ssl_trainer.get_env_config()\n        method = configs[\"SSL_method\"]\n\n        run_exp(ssl_trainer, train_ds_masked, None, \n               note = pretrained_note+\"_\"+model_, exp_name = f\"{method}_{model_}_StrongAug_SimplerNCLR\")\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if ibot:\n    ssl_trainer = ssl_module.iBOT(att_depth = depth, att_dims = att_dims, att_heads = heads,\n                                  embed_dims = 2048, patch_size = patch_size,\n\n                                  multiview = True, apply_simclr = False,\n                                  grayscale = True\n                                 )\n    ssl_trainer.compile(optimizer = keras.optimizers.AdamW(learning_rate = lr_schedule,\n                                                         clipnorm = 1.0, use_ema = True),\n                       jit_compile = False)\n    run_exp(ssl_trainer, train_ds_multiview, None, epochs = 100,\n           note = \"+ NEW aug, New Patching\", exp_name = \"iBOT_VanillaViT\")","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if other:\n    model_ = 'gMLP'\n    if pretrained_encoder is None:\n        note = \"From Scratch\"\n        assert grayscale is True, \"If building from scratch, make sure [grayscale = True]\"\n    else:\n        note = f\"With pretrained {pretrained_encoder.name}\"\n        assert grayscale is False, \"If using pretrained network, make sure [grayscale = False]\"\n    vanilla_model = ssl_module.get_metaformer(model_, res = res, embed_dims = embed_dims, \n                                              att_depth = depth, att_heads = heads,att_dims = att_dims,\n                                              grayscale = grayscale, patch_size = patch_size, \n                                              register_tokens = 4,\n                                             pretrained_encoder = pretrained_encoder)\n    ssl_train(ssl_module.DINO, vanilla_model, \n             note = note + \" / 2-view\",\n             name = f\"DINO_{model_}_reg\",\n             learning_rate = lr_schedule,\n             multiview = False,\n             gradient_accumulation = 32)","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null}]}