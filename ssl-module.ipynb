{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8471595,"sourceType":"datasetVersion","datasetId":5051531}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport random\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nseed = 1\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# ML tools \n\nimport tensorflow as tf\nimport keras #; keras.config.set_dtype_policy(\"mixed_float16\")\nimport keras_nlp\nimport keras_cv\nfrom keras import ops\n\nkeras.utils.set_random_seed(seed)\n\nimport cv2\nimport tensorflow_io as tfio\nfrom kaggle_datasets import KaggleDatasets\nimport tensorflow_datasets as tfds\n\nfrom keras import Input, Model\nfrom keras.models import load_model\n\nfrom keras.layers import Conv2D, DepthwiseConv2D, Dense, Activation, BatchNormalization, LayerNormalization, MultiHeadAttention, Embedding, Subtract, Add, Multiply, GlobalAveragePooling2D, GlobalAveragePooling1D, LayerNormalization\nfrom keras.preprocessing.image import load_img, img_to_array\nfrom keras.applications import *\nimport os, sys\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nfrom sklearn.model_selection import train_test_split\nfrom keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\nfrom tqdm.notebook import tqdm\nprint(f\"Requirements loaded, keras : v{keras.__version__}, Tensorflow : v{tf.__version__}\")\n\n#import external packages\nsys.path.append(\"/kaggle/input/kimm-keras-image-model-repository\")\n#sys.setrecursionlimit(10**7)\nimport kimm","metadata":{"execution":{"iopub.status.busy":"2024-06-04T07:54:04.211368Z","iopub.execute_input":"2024-06-04T07:54:04.212267Z","iopub.status.idle":"2024-06-04T07:54:31.807216Z","shell.execute_reply.started":"2024-06-04T07:54:04.212222Z","shell.execute_reply":"2024-06-04T07:54:31.805836Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-06-04 07:54:07.126614: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-04 07:54:07.126814: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-04 07:54:07.316872: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Requirements loaded, keras : v3.3.3, Tensorflow : v2.15.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Dataloader setting\n- original dataset(input as tf.ds) : image only or image/label paired dataset\n- target dataset(output as tf.ds) : (image1, image2), (image1, image2, image3), (image1, image2, label)\n    - Also, implement the mixing function : merges 2 homogenous dataset","metadata":{}},{"cell_type":"code","source":"kimm_list = kimm.list_models()\ndef available_models():\n    return {\"models_from_kimm\": kimm_list,\n           'models_from_keras' : [\"effnet\", \"effnet_small\", \"effnet_base\",\n                                 \"convnext\", \"convnext_small\", \"convnext_base\",\n                                 \"mlpmixer_patch_depth_dims\", \"convmixer_patch_depth_dims\"]\n           }","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-06-04T07:54:31.809468Z","iopub.execute_input":"2024-06-04T07:54:31.810377Z","iopub.status.idle":"2024-06-04T07:54:31.818230Z","shell.execute_reply.started":"2024-06-04T07:54:31.810332Z","shell.execute_reply":"2024-06-04T07:54:31.816790Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"basic_aug = keras.Sequential([keras.layers.RandomFlip(), keras.layers.RandomRotation(factor = 0.25),\n                             keras.layers.GaussianDropout(0.1)])\naug_layers = keras_cv.layers.RandAugment.get_standard_policy(\n    value_range=(0, 255), magnitude=0.1, magnitude_stddev=0.1\n)\naug_layers = aug_layers + [keras_cv.layers.GridMask()]\naug_layers.pop(0); aug_layers.pop(0) ; aug_layers.pop(0)\nprint(\"RandAug Component in this SSL module : \",[layer.name for layer in aug_layers])\nrandaug = keras_cv.layers.RandomAugmentationPipeline(\n    layers=aug_layers, augmentations_per_image=2, seed = seed, auto_vectorize = True\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-04T07:54:31.820077Z","iopub.execute_input":"2024-06-04T07:54:31.820575Z","iopub.status.idle":"2024-06-04T07:54:31.946157Z","shell.execute_reply.started":"2024-06-04T07:54:31.820530Z","shell.execute_reply":"2024-06-04T07:54:31.944765Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"RandAug Component in this SSL module :  ['random_color_degeneration', 'random_contrast', 'random_brightness', 'random_shear', 'random_shear_1', 'random_translation', 'random_translation_1', 'grid_mask']\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_other_augs_(res):\n    \n    \n    crop_resize_global = keras.Sequential([keras.layers.RandomCrop(int(0.8*res), int(0.8*res)),\n                                        keras.layers.Resizing(res, res)\n                                       ])\n    crop_resize_local = keras_cv.layers.JitteredResize(\n                                            target_size = (res, res),\n                                            scale_factor = (0.8, 1.25),\n                                            crop_size=(int(0.5*res), int(0.5*res)),\n                                            bounding_box_format=None,\n                                            interpolation=\"bilinear\",\n                                            seed=seed)\n    crop_resize_medium = keras_cv.layers.JitteredResize(\n                                            target_size = (res, res),\n                                            scale_factor = (0.8, 1.25),\n                                            crop_size=(int(0.6*res), int(0.6*res)),\n                                            bounding_box_format=None,\n                                            interpolation=\"bilinear\",\n                                            seed=seed)\n\n    return crop_resize_global, crop_resize_medium, crop_resize_local","metadata":{"execution":{"iopub.status.busy":"2024-06-04T07:54:31.950632Z","iopub.execute_input":"2024-06-04T07:54:31.951045Z","iopub.status.idle":"2024-06-04T07:54:31.961320Z","shell.execute_reply.started":"2024-06-04T07:54:31.951012Z","shell.execute_reply":"2024-06-04T07:54:31.959860Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def get_map_fn(res, input_type = None, output_type = None, n_view = 2):\n    # input_type as \"supervised\", \"with_label\", \"with label\" / OR / \"unsupervised\", \"without label\", \"without_label\", \"image_only\", \"image\"\n    # output_ytpe as \"ssl\", \"ssl_with_label\" / if ssl, image output : image1, image2, ..., image_n_view\n    assert n_view >= 2, \"Augmented View number must be >= 2\"\n    assert input_type in [\"supervised\", \"with_label\", \"with label\", \"unsupervised\", \"without label\", \"without_label\", \"image_only\", \"image\"], 'Pick one of input_type : \"supervised\", \"with_label\", \"with label\",\\n \"unsupervised\", \"without label\", \"without_label\", \"image_only\", \"image\"'\n    assert output_type in [\"ssl\", \"ssl_with_label\"], 'Pick one of output_type : \"ssl\", \"ssl_with_label\"'\n    crop_resize_global, crop_resize_medium, crop_resize_local = get_other_augs_(res)\n    def map_fn(image, label = None):\n        if input_type in [\"supervised\", \"with_label\", \"with label\"]:\n            image, label = image, label\n        elif input_type in [\"unsupervised\", \"without label\", \"without_label\", \"image_only\", \"image\"]:\n            image = image\n            label = None\n        batch_size = ops.shape(image)[0]\n        \n        aug_ = basic_aug(image)\n        aug_ = randaug(aug_)\n        if tf.random.normal([1]) > 0.67455:\n            aug_ = ops.max(aug_) - aug_\n        else:\n            pass\n        global_image = crop_resize_global(aug_)\n        \n        if n_view == 2:\n            if output_type == \"ssl\":\n                return (image, global_image)\n            elif output_type == \"ssl_with_label\":\n                return (image, global_image, label)\n        elif n_view == 3:\n            medium_image = crop_resize_medium(aug_)\n            if output_type == \"ssl\":\n                return (image, global_image, medium_image)\n            elif output_type == \"ssl_with_label\":\n                return (image, global_image, medium_image, label)\n        elif n_view > 3:\n            medium_image = crop_resize_medium(aug_)\n            local_images = [crop_resize_local(image) for _ in range(n_view - 3)]\n            local_images = tuple(local_images)\n            outputs = (image, global_image, medium_image) + local_images\n            if output_type == \"ssl\":\n                return outputs\n            elif output_type == \"ssl_with_label\":\n                return outputs+(label)\n    return map_fn\n","metadata":{"execution":{"iopub.status.busy":"2024-06-04T07:54:31.964122Z","iopub.execute_input":"2024-06-04T07:54:31.964684Z","iopub.status.idle":"2024-06-04T07:54:31.982547Z","shell.execute_reply.started":"2024-06-04T07:54:31.964629Z","shell.execute_reply":"2024-06-04T07:54:31.981274Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Helper functions","metadata":{}},{"cell_type":"code","source":"def get_flops(model, model_inputs) -> float:\n        \"\"\"\n        Calculate FLOPS [GFLOPs] for a tf.keras.Model or tf.keras.Sequential model\n        in inference mode. It uses tf.compat.v1.profiler under the hood.\n        Code reference : https://github.com/tensorflow/tensorflow/issues/32809\n        \"\"\"\n        # if not hasattr(model, \"model\"):\n        #     raise wandb.Error(\"self.model must be set before using this method.\")\n        \n        if not isinstance(\n            model, (tf.keras.models.Sequential, tf.keras.models.Model, keras.models.Model, keras.Sequential, keras.Model)\n        ):\n            raise ValueError(\n                \"Calculating FLOPS is only supported for \"\n                \"`tf.keras.Model` and `tf.keras.Sequential` instances.\"\n            )\n\n        from tensorflow.python.framework.convert_to_constants import (\n            convert_variables_to_constants_v2_as_graph,\n        )\n\n        # Compute FLOPs for one sample\n        batch_size = 1\n        inputs = [\n            tf.TensorSpec([batch_size] + inp.shape[1:], inp.dtype)\n            for inp in model_inputs\n        ]\n\n        # convert tf.keras model into frozen graph to count FLOPs about operations used at inference\n        real_model = tf.function(model).get_concrete_function(inputs)\n        frozen_func, _ = convert_variables_to_constants_v2_as_graph(real_model)\n\n        # Calculate FLOPs with tf.profiler\n        run_meta = tf.compat.v1.RunMetadata()\n        opts = (\n            tf.compat.v1.profiler.ProfileOptionBuilder(\n                tf.compat.v1.profiler.ProfileOptionBuilder().float_operation()\n            )\n            .with_empty_output()\n            .build()\n        )\n\n        flops = tf.compat.v1.profiler.profile(\n            graph=frozen_func.graph, run_meta=run_meta, cmd=\"scope\", options=opts\n        )\n\n        tf.compat.v1.reset_default_graph()\n\n        # convert to GFLOPs\n        return (flops.total_float_ops / 1e9)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-06-04T07:54:31.984844Z","iopub.execute_input":"2024-06-04T07:54:31.985374Z","iopub.status.idle":"2024-06-04T07:54:32.008704Z","shell.execute_reply.started":"2024-06-04T07:54:31.985328Z","shell.execute_reply":"2024-06-04T07:54:32.006056Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# preliminary for MIM\ndef generate_mask(patches, masking_rate, update_value):\n    batch_size, length, dims = ops.shape(patches)\n    batch_size = int(batch_size)\n    num_ones = int(masking_rate * length)\n    if num_ones > length:\n        raise ValueError(\"num_ones cannot be greater than the length of the sequence\")\n\n    # Initialize the sequence with zeros\n    sequence = ops.zeros([batch_size, length], dtype=tf.int32)\n\n    # Generate random indices for the ones\n    indices = [tf.random.shuffle(tf.range(length))[:num_ones] for _ in range(batch_size)]\n\n    # Update the sequence with ones at the specified indices\n    for i in range(batch_size):\n        updates = ops.ones([num_ones], dtype=tf.int32)\n        scatter_indices = ops.stack([tf.ones([num_ones], dtype=tf.int32) * i, indices[i]], axis=1)\n        sequence = tf.tensor_scatter_nd_update(sequence, scatter_indices, updates)\n    mask_indices = tf.where(sequence ==1)\n    if len(ops.shape(update_value)) == 2:\n        update_value = ops.squeeze(update_value, axis = 0)\n        \n    masked_patches = tf.tensor_scatter_nd_update(patches, \n                                                 mask_indices, \n                                                 tf.repeat([update_value], tf.shape(mask_indices)[0], axis=0))\n    \n    return ops.cast(sequence, \"float32\"), masked_patches\n\n# Example usage\n#patches = tf.random.normal([4, 64, 256]) \n#mask, masked_patches = generate_mask(patches, 32, 10*tf.ones([1,256]))","metadata":{"execution":{"iopub.status.busy":"2024-06-04T07:54:32.011127Z","iopub.execute_input":"2024-06-04T07:54:32.011591Z","iopub.status.idle":"2024-06-04T07:54:32.032539Z","shell.execute_reply.started":"2024-06-04T07:54:32.011557Z","shell.execute_reply":"2024-06-04T07:54:32.031072Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"> custom losses\n- keras_cv에서 simclr loss 지원\n- Barlow Twins가 배경 vs 전체 사물의 구별에 효과적(empirical) -> 다른 SSL과 같이 쓰자\n    - awesome code reference in [keras.io](https://keras.io/examples/vision/barlow_twins/)","metadata":{}},{"cell_type":"code","source":"#helper functions\n# For Barlow Twins\ndef get_cor_matrix(z1, z2, e = 1e-5):\n    #z1, z2 = [batch, embed_dims] shape tensor\n    # 각각 Normalize한 뒤 내적 -> dims by dims correlation matrix\n    z1_mean, z1_std = ops.mean(z1, axis = 0), ops.std(z1, axis = 0)\n    z2_mean, z2_std = ops.mean(z2, axis = 0), ops.std(z2, axis = 0)\n    z1 = (z1 - z1_mean) / (z1_std + e)\n    z2 = (z2 - z2_mean) / (z2_std + e)\n    bs = tf.cast(ops.shape(z1)[0], tf.float32)\n    matrix = (ops.transpose(z1)@z2) / bs\n    matrix = tf.cast(matrix, tf.float32)\n    return matrix\n\n# For VICreg\ndef invariance_loss(za, zb): #invariance\n    l2_distances = keras.losses.MeanSquaredError(reduction = None)(za, zb)\n    return ops.cast(l2_distances, \"float32\")\n\ndef variance(za, e = 1e-4, gamma = 5.0):\n    mu, var = tf.nn.moments(za, axes = 0)\n    mu = tf.cast(mu, tf.float32)\n    var = tf.cast(var, tf.float32)\n    var += tf.cast(e, tf.float32)\n    \n    s_xe_hinge = gamma - tf.math.sqrt(var)\n    s_val = tf.math.maximum(0.0, s_xe_hinge)\n    return ops.cast(s_val, \"float32\")\n\ndef covariance(za, e = 1e-4, testing = False):\n    mu, var = tf.nn.moments(za, axes = 0) ; bs = tf.cast(ops.shape(za)[0], tf.float32) ; embed_dims = tf.cast(ops.shape(za)[-1], tf.float32)\n    mu = tf.cast(mu, tf.float32)\n    var = tf.cast(var, tf.float32)\n    div_ = ops.maximum(bs-1, 1)\n    \n    za -= mu #batch, dims\n    cov_mat = tf.transpose(za)@za/div_\n    lower_tri = cov_mat - tf.linalg.band_part(cov_mat, 0, -1)\n    upper_tri = cov_mat - tf.linalg.band_part(cov_mat, -1, 0)\n    off_diag = lower_tri + upper_tri\n    off_diag = ops.sum(tf.math.square(off_diag))/embed_dims\n    off_diag = ops.cast(off_diag, \"float32\")\n    if testing : \n        print(f\"Covariacne matrix shape : {ops.shape(cov_mat)}\")\n    return off_diag","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-06-04T07:54:32.034168Z","iopub.execute_input":"2024-06-04T07:54:32.034658Z","iopub.status.idle":"2024-06-04T07:54:32.055967Z","shell.execute_reply.started":"2024-06-04T07:54:32.034616Z","shell.execute_reply":"2024-06-04T07:54:32.054714Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class BarlowLoss(keras.losses.Loss):\n    def __init__(self, diag = 0.6, off_diag = 0.4):\n        super().__init__()\n        self.diag = diag\n        self.off_diag = off_diag\n        \n    def compute_loss(self, correlation_matrix):\n        diag_component = tf.linalg.diag_part(correlation_matrix)\n        zero_diag = ops.zeros(correlation_matrix.shape[-1])\n        off_diag_matrix = tf.linalg.set_diag(correlation_matrix, zero_diag)\n        \n        diag_loss = tf.pow(diag_component-1, 2) * self.diag\n        off_diag_loss = tf.pow(off_diag_matrix, 2) * self.off_diag\n        \n        loss = ops.mean(diag_loss) + ops.mean(off_diag_loss)\n        loss = tf.cast(loss, tf.float32)\n        return loss\n    def call(self, z_a, z_b):\n        cor_matrix = get_cor_matrix(z_a, z_b)\n        return self.compute_loss(cor_matrix)","metadata":{"execution":{"iopub.status.busy":"2024-06-04T07:54:32.057574Z","iopub.execute_input":"2024-06-04T07:54:32.058094Z","iopub.status.idle":"2024-06-04T07:54:32.078689Z","shell.execute_reply.started":"2024-06-04T07:54:32.058050Z","shell.execute_reply":"2024-06-04T07:54:32.077155Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class TRBlock(keras.layers.Layer): #ViT Transformer block with register token, attention weight return\n    def __init__(self, att_depth, att_heads, att_dims,\n                 return_att_weight = True,\n                n_register_token = 3, dropout_rate = 0.25,\n                 **kwargs):\n        super().__init__(**kwargs)\n        self.att_depth = att_depth\n        self.att_dims = att_dims\n        self.return_weight = return_att_weight\n        self.n_reg = n_register_token\n        \n        self.ln_before_set = [keras.layers.LayerNormalization(name = f'LN_before_{i+1}') for i in range(self.att_depth)]\n        self.ln_after_set = [keras.layers.LayerNormalization(name = f'LN_after_{i+1}') for i in range(self.att_depth)]\n        self.mha_set = [keras.layers.MultiHeadAttention(att_heads, att_dims, name = f\"MHA_{i+1}\") for i in range(self.att_depth)]\n        self.att_dropout = keras.layers.Dropout(dropout_rate)\n        self.proj_dropout = keras.layers.Dropout(dropout_rate)\n    def build(self, input_shape):\n        #query, key, value\n        if len(input_shape) == 3:\n            query_shape, key_shape, value_shape = input_shape[0], input_shape[1], input_shape[2]\n        elif len(input_shape) == 2:\n            query_shape, key_shape = input_shape[0], input_shape[1]\n            value_shape = key_shape\n        batch_size = query_shape[0]\n        self.query_length = query_shape[1]\n        self.embed_dims = query_shape[2]\n        self.embed_dim = self.embed_dims\n        self.dense_set = [Dense(units = self.embed_dims, name = f\"Dense_{i+1}\", activation = \"gelu\") for i in range(self.att_depth)]\n        \n\n    def call(self, inputs):\n        if len(inputs) == 2:\n            query = inputs[0]\n            key = inputs[1]\n            value = key\n        elif len(inputs) == 3:\n            query, key, value = inputs\n        tok_ = keras.layers.GlobalAveragePooling1D()(query)\n        tok_ = tok_[:, tf.newaxis, :]\n        if self.n_reg > 0:\n            self.register_tokens = ops.cast(ops.ones_like(tok_), \"float32\")\n            self.register_tokens = ops.tile(self.register_tokens, [1, self.n_reg, 1])\n        else:\n            self.register_tokens = None\n        self.cls_token = ops.cast(ops.ones_like(tok_), \"float32\")\n        \n        \n        if self.register_tokens is not None:\n            encoded_patches = ops.concatenate([query, \n                                              self.cls_token,\n                                              self.register_tokens], axis = 1)\n        else:\n            encoded_patches = ops.concatenate([query, \n                                              self.cls_token], axis = 1)\n        for idx in range(self.att_depth):\n            x0 = self.ln_before_set[idx](encoded_patches)\n            x1_ = self.mha_set[idx](query = encoded_patches, key = key, value = value,\n                                    return_attention_scores = self.return_weight)\n\n            if self.return_weight:\n                x1, att_weights = x1_\n                del x1_\n            else:\n                x1 = x1_\n                del x1_\n            x1 = self.att_dropout(x1)\n            x2 = x0 + x1\n            x3 = self.ln_after_set[idx](x2)\n            x4 = self.dense_set[idx](x3)\n            x4 = self.proj_dropout(x4)\n            encoded_patches = x2+x4\n        patches, cls_token = encoded_patches[:, :self.query_length, :], encoded_patches[:, self.query_length, :]\n        del encoded_patches\n        if self.return_weight:\n            return cls_token, patches, att_weights[:, :, self.query_length:self.query_length+1, :self.query_length]\n        else:\n            return cls_token, patches","metadata":{"execution":{"iopub.status.busy":"2024-06-04T07:54:32.083332Z","iopub.execute_input":"2024-06-04T07:54:32.083756Z","iopub.status.idle":"2024-06-04T07:54:32.109469Z","shell.execute_reply.started":"2024-06-04T07:54:32.083713Z","shell.execute_reply":"2024-06-04T07:54:32.108266Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class AttentionPooling(keras.layers.Layer):\n    def __init__(self, attention_heads, attention_dims = None, bias = False, scale = None, \n                 dropout_rate = 0.05, **kwargs):\n        super().__init__(**kwargs)\n        self.n_heads = attention_heads\n        self.n_dims = attention_dims\n        self.bias = bias\n        self.scale = scale\n        self.dropout_rate = dropout_rate\n        \n    def build(self, input_shape):\n        # query, key\n        query_dims = input_shape[0][-1]\n        key_length = input_shape[1][1]\n        \n        if self.n_dims == None:\n            embed_dims = query_dims\n        else:\n            embed_dims = self.n_dims\n        self.embed_dims = embed_dims\n        self.per_head_dims = embed_dims//self.n_heads\n        \n        self.scale = self.scale if self.scale != None else embed_dims**-0.5\n        \n        self.query_embed_fn = keras.layers.Dense(units = embed_dims, use_bias = self.bias,\n                                               name = \"Q_Embedding_Dense_layer\")\n        self.key_embed_fn = keras.layers.Dense(units = embed_dims, use_bias = self.bias,\n                                               name = \"K_Embedding_Dense_layer\")\n        self.value_embed_fn = keras.layers.Dense(units = embed_dims, use_bias = self.bias,\n                                               name = \"V_Embedding_Dense_layer\")\n        \n        self.softmax = keras.layers.Activation(\"softmax\", name = \"AttentionWeightSoftmax\")\n        self.proj = keras.layers.Dense(units = query_dims, use_bias = self.bias, \n                                      name = \"ProjectToOriginalDimension\")\n        self.att_dropout = keras.layers.Dropout(self.dropout_rate)\n        self.proj_dropout = keras.layers.Dropout(self.dropout_rate)\n        super().build(input_shape)\n        \n    def call(self, inputs, **kwargs):\n        if len(inputs) == 2:\n            q, k = inputs\n            value_ = False\n        elif len(inputs) == 3:\n            q, k, v = inputs\n            value_ = True\n            \n        if len(ops.shape(q)) == 2:\n            q = q[:, tf.newaxis, :]\n        if len(ops.shape(k)) == 4:\n            b_, w_, h_, dims_ = ops.shape(k)\n            k = ops.reshape(k, [b_, w_*h_, dims_])\n        if (value_) and (len(ops.shape(v))) == 4:\n            b_, w_, h_, dims_ = ops.shape(v)\n            v = ops.reshape(v, [b_, w_*h_, dims_])\n            \n        batch_size, query_length, q_dims = ops.shape(q)\n        _, key_length, k_dms = ops.shape(k)\n        \n        query = self.query_embed_fn(q) * self.scale #batch, 1(or, query length), q_dims\n        query = ops.reshape(query, [batch_size, query_length, self.n_heads, self.per_head_dims])\n        \n        key = self.key_embed_fn(k)\n        key = ops.reshape(key, [batch_size, key_length, self.n_heads, self.per_head_dims])\n        \n        if value_:\n            value = self.value_embed_fn(v)#각각 batch, token_length, heads, per_head_dims\n        else:\n            value = self.value_embed_fn(k)\n        value = ops.reshape(value, [batch_size, key_length, self.n_heads, self.per_head_dims])\n        attention_score = keras.ops.einsum(\"abhd, achd -> ahbc\",\n                                          query, key) #b = query length, c = key length\n        attention_weight = self.softmax(attention_score)\n        attention_weight = self.att_dropout(attention_weight)\n        self.attention_weight = attention_weight \n        attended_output = keras.ops.einsum(\"ahbc, achd -> abhd\",\n                                          attention_weight, value)\n        attended_output = keras.ops.reshape(attended_output, \n                                           [batch_size, query_length, self.n_heads*self.per_head_dims]\n                                           )\n        attended_output = self.proj(attended_output)\n        attended_output = self.proj_dropout(attended_output)\n        return attended_output, attention_weight","metadata":{"execution":{"iopub.status.busy":"2024-06-04T07:54:32.111474Z","iopub.execute_input":"2024-06-04T07:54:32.111938Z","iopub.status.idle":"2024-06-04T07:54:32.137776Z","shell.execute_reply.started":"2024-06-04T07:54:32.111898Z","shell.execute_reply":"2024-06-04T07:54:32.136459Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class ImagePatchEmbedding(keras.layers.Layer):\n    def __init__(self, patch_size, embed_dim, **kwargs):\n        super().__init__(**kwargs)\n        self.patch_size = patch_size\n        self.embed_dim = embed_dim\n        self.proj = Dense(units = embed_dim, activation = \"gelu\")\n        self.layernorm = keras.layers.LayerNormalization()\n    def build(self, input_shape): #image input\n        batch_size, h, w, c = input_shape\n        self.n_patches = (h//self.patch_size) * (w//self.patch_size)\n        self.pe_coefficient = c**-0.5\n        self.position_embedding= keras.layers.Embedding(\n            input_dim=self.n_patches, output_dim=self.embed_dim\n        )\n    def call(self, image):\n        \n        positions = ops.expand_dims(\n            ops.arange(start=0, stop=self.n_patches, step=1), axis=0\n        )\n        \n        \n        image = self.layernorm(image)\n        image = ops.image.extract_patches(image, size= self.patch_size, padding = 'same')\n        image = self.proj(image)\n        batch, w, h, dims = ops.shape(image)\n        image = ops.reshape(image, [-1, w*h, dims])\n        image += self.pe_coefficient * (self.position_embedding(positions))\n        return image","metadata":{"execution":{"iopub.status.busy":"2024-06-04T07:54:32.139417Z","iopub.execute_input":"2024-06-04T07:54:32.139811Z","iopub.status.idle":"2024-06-04T07:54:32.157618Z","shell.execute_reply.started":"2024-06-04T07:54:32.139775Z","shell.execute_reply":"2024-06-04T07:54:32.156298Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"class PatchEncoder(keras.layers.Layer):\n    def __init__(self, num_patches, projection_dim):\n        super().__init__()\n        self.num_patches = num_patches\n        self.projection = keras.layers.Dense(units=projection_dim)\n        self.position_embedding = keras.layers.Embedding(\n            input_dim=num_patches, output_dim=projection_dim\n        )\n\n    def call(self, patch):\n        positions = ops.expand_dims(\n            ops.arange(start=0, stop=self.num_patches, step=1), axis=0\n        )\n        projected_patches = self.projection(patch)\n        encoded = projected_patches + self.position_embedding(positions)\n        return encoded\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\"num_patches\": self.num_patches})\n        return config","metadata":{"execution":{"iopub.status.busy":"2024-06-04T07:54:32.161948Z","iopub.execute_input":"2024-06-04T07:54:32.162409Z","iopub.status.idle":"2024-06-04T07:54:32.178565Z","shell.execute_reply.started":"2024-06-04T07:54:32.162367Z","shell.execute_reply":"2024-06-04T07:54:32.176953Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# General Context Vision Transformer implementation\n- Reference : [another kaggle notebook, identical implementation](https://www.kaggle.com/code/hskimjjys/general-context-vit-script/).","metadata":{}},{"cell_type":"code","source":"class SE(keras.layers.Layer):\n    def __init__(self, output_dim = None, squeeze_rate = 0.25, **kwargs):\n        super().__init__(**kwargs)\n        self.output_dim = output_dim\n        self.rate = squeeze_rate\n    def build(self, input_shape) : #batch_size, h, w, dims\n        if self.output_dim == None:\n            self.output_dim = input_shape[-1]\n        else:\n            pass\n        self.avg_pool = keras.layers.GlobalAveragePooling2D(keepdims = True, name = \"AvgPooling\")\n        self.mlps = keras.Sequential([keras.layers.Dense(units = int(self.rate * self.output_dim),\n                                                        use_bias = False, name = \"Dense1\"),\n                                      keras.layers.Activation(\"gelu\", name = \"GeluAct\"),\n                                      keras.layers.Dense(units = self.output_dim, use_bias = False, name = \"Dense2\"),\n                                      keras.layers.Activation(\"sigmoid\", name = \"Excitation_Sigmoid\")\n                                     ])\n        #super().build(input_shape)\n    def call(self, inputs, **kwargs):\n        pooled = self.avg_pool(inputs)\n        weights = self.mlps(pooled)\n        return inputs * weights\n    \nclass DownSampler(keras.layers.Layer):\n    def __init__(self, keepdims = False, **kwargs):\n        super().__init__(**kwargs)\n        self.keepdims = keepdims\n    def build(self, input_shape):\n        embed_dims = input_shape[-1]\n        out_dim = embed_dims if self.keepdims else 2*embed_dims\n        self.fused_mbconv = keras.Sequential([keras.layers.DepthwiseConv2D(kernel_size = 3, padding = 'same', use_bias = False, name = \"DWConv\"),\n                                             keras.layers.Activation(\"gelu\", name = 'GeluAct'),\n                                             SE(name = \"SqueezeAndExcitation2D\"),\n                                             keras.layers.Conv2D(filters = embed_dims, kernel_size = 1, padding = 'same', use_bias = False, name = \"PointWiseConv\")],\n                                            name = \"Fused_MBConvLayer\")\n        self.down_conv = keras.layers.Conv2D(filters = out_dim, kernel_size = 3, strides = 2, padding = 'same', use_bias = False, name = \"DownConvolution\")\n        self.layernorm1 = keras.layers.LayerNormalization(epsilon = 1e-5, name = 'LayerNorm1')\n        self.layernorm2 = keras.layers.LayerNormalization(epsilon = 1e-5, name = 'LayerNorm2')\n    def call(self, inputs, **kwargs):\n        x = self.layernorm1(inputs)\n        x += self.fused_mbconv(inputs)\n        x = self.down_conv(x)\n        return self.layernorm2(x)\n    \nclass MLP(keras.layers.Layer):\n    def __init__(self, middle_dim = None, output_dim = None,\n                activation = 'gelu', dropout = 0.2, **kwargs):\n        super().__init__(**kwargs)\n        self.middle_dim = middle_dim\n        self.output_dim = output_dim\n        self.activation = activation\n        self.dropout_rate = dropout\n    def build(self, input_shape):\n        self.input_dims = input_shape[-1]\n        self.middle_dim = int(1.5*self.input_dims) if self.middle_dim == None else self.middle_dim\n        self.output_dim = self.input_dims if self.output_dim == None else self.output_dim\n        self.mlp1 = keras.layers.Dense(units = self.middle_dim, name = \"FirstMLP\")\n        self.act = keras.layers.Activation(self.activation, name = \"MiddleActivation\")\n        self.mlp2 = keras.layers.Dense(units = self.output_dim, name = \"SecondMLP\")\n        self.drop1 = keras.layers.Dropout(self.dropout_rate, name = \"Dropout1\")\n        self.drop2 = keras.layers.Dropout(self.dropout_rate, name = \"Dropout2\")\n    def call(self, inputs, **kwargs):\n        x = self.mlp1(inputs)\n        x = self.act(x)\n        x = self.drop1(x)\n        x = self.mlp2(x)\n        x = self.drop2(x)\n        return x\n    \nclass PatchEmbedding(keras.layers.Layer):\n    def __init__(self, embed_dim, patching_type = \"conv\", #conv or tokenlearner\n                 **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.patching_type = patching_type\n    def build(self, input_shape):\n        if (self.patching_type == \"tokenlearner\") or (self.patching_type == \"token_learner\"):\n            self.proj = keras.layers.Conv2D(self.embed_dim, kernel_size = 3, strides = 2, padding = 'same', name = \"projection_conv\") #Overlapping patches\n            #token learner implementation\n            batch_size, w, h, filters = input_shape\n            n_tokens = (w//4) * (h//4) ; self.resized_w, self.resized_h = int(w//4), int(h/4)\n            self.input_seq_flatten = keras.layers.Reshape([1, -1, self.embed_dim], name = \"image_to_sequence_reshape\")\n            self.layer_norm = keras.layers.LayerNormalization(epsilon = 1e-5)\n            self.attention_ops = keras.Sequential([keras.layers.Conv2D(n_tokens, kernel_size = 3, activation = \"gelu\", use_bias = False, padding = 'same'),\n                                                  keras.layers.Conv2D(n_tokens, kernel_size = 3, activation = \"gelu\", use_bias = False, padding = 'same'),\n                                                  keras.layers.Conv2D(n_tokens, kernel_size = 3, activation = \"sigmoid\", use_bias = False, padding = 'same'),\n                                                  keras.layers.Reshape([-1, n_tokens]), #batch_size, HW, n_tokens\n                                                  keras.layers.Permute([2,1])], #batch_size, n_tokens, HW\n                                                 name = \"Conv_for_attention_weight\")\n        else:\n            self.proj = keras.layers.Conv2D(self.embed_dim, kernel_size = 3, strides = 2, padding = 'same') #Overlapping patches\n            self.down_sample = DownSampler(keepdims = True, name = \"DownSampler_after_projection\")\n    def call(self, inputs, **kwargs):\n        if (self.patching_type == \"tokenlearner\") or (self.patching_type == \"token_learner\"):\n            #token learner implementation\n            norm_input = self.layer_norm(inputs)\n            proj_inputs = self.proj(norm_input)\n            seq_inputs = self.input_seq_flatten(proj_inputs) #batch, 1, HW, embed_dims\n            att_weights = self.attention_ops(proj_inputs) #batch, n_tokens, HW\n            att_weights = ops.expand_dims(att_weights, axis = -1) #batch, n_tokens, HW, 1\n            attended = att_weights * seq_inputs #batch, n_tokens, HW, embed_dims\n            attended = ops.mean(attended, axis = 2) #batch, n_tokens, embed_dims\n            #reshape to 2D array\n            attended = ops.reshape(attended, [-1, self.resized_w, self.resized_h, self.embed_dim]\n                                  )\n            return attended\n        else:\n            x = self.proj(inputs)\n            x = self.down_sample(x)\n            return x\n        \nclass FeatExtract(keras.layers.Layer):\n    def __init__(self, keepdims = False, **kwargs):\n        super().__init__(**kwargs)\n        self.keepdims = keepdims\n    def build(self, input_shape):\n        batch_size, H, W, embed_dims = input_shape\n        self.fused_mbconv = keras.Sequential([keras.layers.DepthwiseConv2D(kernel_size = 3, padding = 'same', use_bias = False, name = \"DWConv\"),\n                                             keras.layers.Activation(\"gelu\", name = 'GeluAct'),\n                                             SE(name = \"SqueezeAndExcitation2D\"),\n                                             keras.layers.Conv2D(filters = embed_dims, kernel_size = 1, padding = 'same', use_bias = False, name = \"PointWiseConv\")],\n                                            name = \"Fused_MBConvLayer\")\n        if self.keepdims == False:\n            self.pool = keras.layers.MaxPooling2D(name = \"FeatExtractMaxPool2D\")\n    def call(self, inputs):\n        x = inputs + self.fused_mbconv(inputs)\n        if self.keepdims == False:\n            return self.pool(x)\n        return x\n    \nclass GlobalQueryGenerator(keras.layers.Layer):\n    def __init__(self, keepdims = False, **kwargs):\n        super().__init__(**kwargs)\n        self.keepdims = keepdims #Keepdims는 여기서 0과 1로 이루어진 list도 될 수 있다 -> FeatExtract layer를 keepdims의 원소 갯수만큼 repeat!\n    def build(self, input_shape):\n        self.q_generator = keras.Sequential([FeatExtract(keepdims = keepdim, name = f\"FeatureExtraction_{idx+1}\") for idx, keepdim in enumerate(self.keepdims)])\n    def call(self, inputs):\n        return self.q_generator(inputs)\n    \nclass WindowAttention(keras.layers.Layer):\n    def __init__(self, window_size, n_heads, global_query, #제공된다면 global, 아니라면 local mHSA -> 0 or 1\n                qkv_bias = True, qk_scale = None,\n                dropout_rate = 0.05, return_attention_weights = False, **kwargs):\n        super().__init__(**kwargs)\n        self.window_size = (window_size, window_size)\n        self.n_heads = n_heads\n        self.global_query = global_query\n        self.bias = qkv_bias\n        self.scale = qk_scale\n        self.dropout_rate = dropout_rate\n        self.return_attention_weights = return_attention_weights\n    def build(self, input_shape) :\n        #input = [query, key, value]\n        embed_dims = input_shape[0][-1]\n        head_dims = embed_dims//self.n_heads\n        self.scale = self.scale if self.scale != None else embed_dims**-0.5\n        self.qkv_size = 3-int(self.global_query)\n        self.qkv_embed_fn = keras.layers.Dense(units = embed_dims * self.qkv_size, use_bias = self.bias,\n                                 name = \"QKV_Embedding_Dense_layer\")\n        self.softmax = keras.layers.Activation(\"softmax\", name = \"AttentionWeightSoftmax\") #for attention weight computation\n        self.proj = keras.layers.Dense(units = embed_dims, use_bias = self.bias, name = \"Projection\")\n        self.attention_dropout = keras.layers.Dropout(self.dropout_rate)\n        self.projection_dropout = keras.layers.Dropout(self.dropout_rate)\n        self.relative_position_bias_table = self.add_weight(\n            name=\"relative_position_bias_table\",\n            shape=[\n                (2 * self.window_size[0] - 1) * (2 * self.window_size[1] - 1),\n                self.n_heads,\n            ],\n            initializer=keras.initializers.TruncatedNormal(stddev=0.02),\n            trainable=True,\n            dtype=self.dtype,\n        ) #<- learnable weight of relational position. 위 window size = 4의 예시에서, 임의의 두 지점 간 거리의 경우의 수는 총 49개 -> 이에 해당하는 weight tensor를 만듬.\n        super().build(input_shape)\n    def get_relative_position_index(self): #<- window 내 2 지점 간 거리의 index matrix.\n        coords_h = ops.arange(self.window_size[0])\n        coords_w = ops.arange(self.window_size[1])\n        coords = ops.stack(ops.meshgrid(coords_h, coords_w, indexing=\"ij\"), axis=0)\n        coords_flatten = ops.reshape(coords, [2, -1])\n        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n        relative_coords = ops.transpose(relative_coords, axes=[1, 2, 0])\n        relative_coords_xx = relative_coords[:, :, 0] + self.window_size[0] - 1\n        relative_coords_yy = relative_coords[:, :, 1] + self.window_size[1] - 1\n        relative_coords_xx = relative_coords_xx * (2 * self.window_size[1] - 1)\n        relative_position_index = relative_coords_xx + relative_coords_yy\n        return relative_position_index\n    def call(self, inputs, **kwargs):\n        #input : key(=value), global query OR key only\n        # input component shape : batch*n_windows, h_window*w_window, embed_dims -> level/block 설계 시 repeat 처리 후 attention에 feed\n        if self.global_query :\n            inputs, q_global = inputs\n            batch_size = ops.shape(q_global)[0]\n        else:\n            inputs = inputs[0]\n        batch_, token_length, embed_dims = ops.shape(inputs) #global query는 query generator에 의해 token_length 개 만큼의 token으로 전체 이미지/feature map을 압축한 상태\n        \n        qkv = self.qkv_embed_fn(inputs) #batch*n_windows, h_w * w_w, qkv_size * embed_dims\n        \n        qkv = ops.reshape(qkv, [-1, token_length, self.qkv_size, self.n_heads, embed_dims//self.n_heads])\n        qkv = ops.transpose(qkv, [2, 0, 3, 1, 4]) #qkv_size, batch_, n_heads, token_length, C\n        \n        #QKV 분리\n        if self.global_query:\n            k, v = ops.split(qkv, 2, axis = 0) #각각 batch_, n_heads, token_length, C\n            #repeat the global query tensor\n            # batch_size, n_query_tokens, dims -> batch_(=batch * n_windows), n_query_tokens, dims\n            q_global = ops.repeat(q_global, batch_//batch_size, axis = 0) #->batch_, n_query_tokens, dims\n            q = ops.reshape(q_global, [batch_, token_length, self.n_heads, embed_dims//self.n_heads])\n            q = ops.transpose(q, [0, 2, 1, 3]\n                             )\n        else:\n            q, k, v = ops.split(qkv, 3, axis = 0)\n            q = ops.squeeze(q, axis = 0)\n        k = ops.squeeze(k, axis = 0)\n        v = ops.squeeze(v, axis = 0)\n        \n        q *= self.scale #batch_, n_heads, token_length, dimension_per_heads(=C)\n        attention_score = q@ops.transpose(k, [0, 1, 3, 2]) #batch_, n_heads, token_length, token_length\n        \n        #positional encoding(bias) 계산 -> attention score에 더해 주기\n        # Code from original keras homepage\n        relative_position_bias = ops.take(\n            self.relative_position_bias_table,\n            ops.reshape(self.get_relative_position_index(), [-1]),\n        )\n        relative_position_bias = ops.reshape(\n            relative_position_bias,\n            [\n                self.window_size[0] * self.window_size[1],\n                self.window_size[0] * self.window_size[1],\n                -1,\n            ],\n        )\n        relative_position_bias = ops.transpose(relative_position_bias, axes=[2, 0, 1])\n        attention_score += relative_position_bias[None,]\n        attention_weight = self.softmax(attention_score)\n        attention_weight = self.attention_dropout(attention_weight) #batch_, n_heads, token_length, token_length\n        #value tensor shape : batch_, n_heads, token_length, dimension_per_heads(=C)\n        attended_output = attention_weight@v\n        attended_output = ops.transpose(attended_output, [0, 2, 1, 3])\n        attended_output = ops.reshape(attended_output, [batch_, token_length, embed_dims])\n        attended_output = self.projection_dropout(self.proj(attended_output))\n        self.attention_weight = attention_weight\n        if self.return_attention_weights:\n            return attended_output, attention_weight\n        else:\n            return attended_output\n        \n        \nclass Block(keras.layers.Layer):\n    def __init__(self, #이하는 Window Attention configurations\n                 window_size, num_heads, global_query, \n                 qkv_bias = True, qk_scale = None, dropout_rate = 0.05, \n                 # 이하는 MLP module의 configuration\n                 mlp_ratio = 4.0, layer_scale = None, return_attention_weights = False,\n                 **kwargs):\n        super().__init__(**kwargs)\n        self.window_size = window_size\n        self.n_heads = num_heads\n        self.global_query = global_query\n        self.qkv_bias = qkv_bias\n        self.qk_scale = qk_scale\n        self.dropout_rate = dropout_rate\n        \n        self.mlp_ratio = mlp_ratio\n        self.layer_scale = layer_scale\n        self.return_attention_weights = return_attention_weights\n    def build(self, input_shape):\n        #input tensor : list of key/query or key only\n        # each tensor is batch_size, w, h, channel dims shape tensor\n        batch_size, H, W, dims = input_shape[0]\n        self.norm1 = keras.layers.LayerNormalization(epsilon = 1e-5)\n        self.norm2 = keras.layers.LayerNormalization(epsilon = 1e-5)\n        self.window_attention = WindowAttention(window_size = self.window_size,\n                                               n_heads = self.n_heads,\n                                               global_query = self.global_query,\n                                               qkv_bias = self.qkv_bias,\n                                               qk_scale = self.qk_scale,\n                                               dropout_rate = self.dropout_rate,\n                                                return_attention_weights = self.return_attention_weights)\n        self.mlps = MLP(middle_dim = int(self.mlp_ratio * dims), dropout = self.dropout_rate)\n        if self.layer_scale != None:\n            self.gamma1 = self.add_weight(shape = [dims], name = \"Gamma1\", trainable = True,\n                                         initializer = keras.initializer.Constant(self.layer_scale), dtype = self.dtype)\n            self.gamma2 = self.add_weight(shape = [dims], name = \"Gamma2\", trainable = True,\n                                         initializer = keras.initializer.Constant(self.layer_scale), dtype = self.dtype)\n        else:\n            self.gamma1, self.gamma2 = 1.0, 1.0\n        self.n_windows = int(H//self.window_size) * int(W//self.window_size)\n        \n    #input feature map을 일정 크기의 window로 partition을 만들어주는 함수 및\n    # 그 partition을 받아 원래의 feature map으로 돌려주는 함수를 만들자\n    def window_partition(self, inputs): #feature map -> multiple windows\n        batch_size, H, W, dims = ops.shape(inputs)\n        h, w = H//self.window_size, W//self.window_size\n        inputs = ops.reshape(inputs, [batch_size, \n                                      h, self.window_size,\n                                     w, self.window_size, \n                                     dims])\n        inputs = ops.transpose(inputs, [0,#batch_size\n                                        1,3, #h, w\n                                        2,4, #winsize, winsize\n                                        5])\n        return ops.reshape(inputs, [-1, self.window_size, self.window_size, dims]) #batch_size*n_windows, window_size, window_size, dims\n        \n    def window_reverse(self, inputs, H, W, dims): #window partition -> original feature map\n        x = ops.reshape(inputs, [-1, H//self.window_size, W//self.window_size, self.window_size, self.window_size, dims])\n        x = ops.transpose(x, [0, 1, 3, 2, 4, 5])\n        return ops.reshape(x, [-1, H, W, dims])\n    \n    def call(self, inputs, **kwargs):\n        if self.global_query:\n            inputs, global_query = inputs\n        else:\n            inputs = inputs[0]\n        batch_size, H, W, dims = ops.shape(inputs)\n        x = self.norm1(inputs)\n        x = self.window_partition(x) \n        x = ops.reshape(x, [-1, self.window_size*self.window_size, dims])\n        if self.global_query:\n            outputs_ = self.window_attention([x, global_query]\n                                     )\n        else:\n            outputs_ = self.window_attention([x])\n        if self.return_attention_weights:\n            x, attention_weight = outputs_\n        else:\n            x = outputs_\n        x = self.window_reverse(x, H, W, dims)\n        x = inputs + self.gamma1*x\n        x += self.gamma2*(self.mlps(self.norm2(x)))\n        if self.return_attention_weights:\n            return x, attention_weight\n        else:\n            return x\n    \nclass Level(keras.layers.Layer):\n    def __init__(self, \n                depth, #<- Block repetition depth\n                num_heads, window_size, keepdims, #downsampler 및 block의 hyperparameter\n                downsample = True, mlp_ratio = 4.0,\n                qkv_bias = True, qk_scale = None,\n                dropout = 0.05, layer_scale = None, return_attention_weights = True,\n                **kwargs):\n        super().__init__(**kwargs)\n        self.depth = depth\n        self.n_heads = num_heads\n        self.window_size = window_size\n        self.keepdims = keepdims\n        self.downsample = downsample\n        self.mlp_ratio = mlp_ratio\n        self.qkv_bias = qkv_bias\n        self.qk_scale = qk_scale\n        self.dropout_rate = dropout\n        self.layer_scale = layer_scale\n        self.return_attention_weights = return_attention_weights\n        \n    def build(self, input_shape):\n        #input tensor : feature map / patches\n        batch_size, H, W, dims = input_shape\n        self.blocks = [Block(window_size = self.window_size, num_heads = self.n_heads, global_query = bool(idx%2), \n                             qkv_bias = self.qkv_bias, qk_scale = self.qk_scale, dropout_rate = self.dropout_rate, \n                             mlp_ratio = self.mlp_ratio, layer_scale = self.layer_scale, return_attention_weights = self.return_attention_weights,\n                             name = f\"GCViTBlock{idx+1}\") for idx in range(self.depth)]\n        self.downsampler = DownSampler(name = \"Downsampler\")\n        self.query_generator = GlobalQueryGenerator(keepdims = self.keepdims, name = \"GlobalQueryGenerator\")\n        \n    def call(self, inputs, **kwargs):\n        patches = inputs\n        global_query = self.query_generator(inputs)\n        for idx, block in enumerate(self.blocks):\n            if idx % 2 :\n                outputs_ = block([patches, global_query])\n            else:\n                outputs_ = block([patches])\n            if self.return_attention_weights:\n                patches, attention_weights = outputs_\n            else:\n                patches = outputs_\n        if self.downsample == False:\n            return patches\n        else:\n            return self.downsampler(patches)\ndef get_gcvit_configs(res, initial_embedding_dims, name = None):\n    return {'res' : res,\n            'embed_dims' : initial_embedding_dims,\n            \"patch_embedding_type\" : \"conv\", #conv or tokenlearner\n            \"level_depth\" : [2,4,6,8],\n            \"level_heads\" : [2,4,8,16],\n            \"level_keepdims\" : [[0,0,0],\n                                   [0,0],\n                                   [1], \n                                    [1]\n                                   ], #3번째 level부터는 window attention == global attention\n            \"level_window_size\" : [res//32, res//32, res//16, res//32],\n            \"model_name\" : f\"GCViT_res{res}\" if name == None else name\n                }\ndef get_gcvit(configs):\n    res = configs[\"res\"]\n    inputs = Input([res,res,3], name = \"ImageInput\")\n    patcher = PatchEmbedding(embed_dim = configs['embed_dims'], patching_type = configs[\"patch_embedding_type\"],\n                             name = \"PatchEmbedding\")\n    patches = patcher(inputs)\n    \n    for idx, (depth, heads, keepdims, window_size) in enumerate(zip(configs[\"level_depth\"], configs[\"level_heads\"], configs[\"level_keepdims\"], configs[\"level_window_size\"])):\n        if idx == len(configs['level_depth'])-1:\n            downsample = False\n        else:\n            downsample = True\n        level = Level(depth = depth, num_heads = heads, window_size = window_size, keepdims = keepdims, downsample = downsample,\n                      name = f\"GCViT_Lv{idx+1}_downsample_{downsample}\")\n        patches = level(patches)\n    model = keras.Model(inputs, patches,\n                       name = configs[\"model_name\"])\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-06-04T07:54:32.180563Z","iopub.execute_input":"2024-06-04T07:54:32.180965Z","iopub.status.idle":"2024-06-04T07:54:32.294485Z","shell.execute_reply.started":"2024-06-04T07:54:32.180926Z","shell.execute_reply":"2024-06-04T07:54:32.293126Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"--------------","metadata":{}},{"cell_type":"markdown","source":"# MLP-mixer\n- for lower resource demand!\n- Code Reference : [Keras.io](https://keras.io/examples/vision/mlp_image_classification/#the-mlpmixer-model)\n\n![](https://velog.velcdn.com/images/minkyu4506/post/0237ee55-74eb-4836-952c-6bd33694aecc/image.png)","metadata":{}},{"cell_type":"code","source":"class NaivePatchesExtraction(keras.layers.Layer): #untrainable layer\n    def __init__(self, patch_size, output_type = \"seq\", **kwargs):\n        super().__init__(**kwargs)\n        self.patch_size = patch_size\n        self.output_type = output_type\n        assert output_type in [\"seq\", \"sequence\", \"Sequence\", \"3D\", \"conv\", \"feature_map\", \"2D\"]\n    def call(self, x):\n        if len(ops.shape(x)) == 3:\n            x = ops.expand_dims(x, 0)\n        patches = keras.ops.image.extract_patches(x, self.patch_size)\n        batch_size = keras.ops.shape(patches)[0]\n        num_patches = keras.ops.shape(patches)[1] * keras.ops.shape(patches)[2]\n        patch_dim = keras.ops.shape(patches)[3]\n        if self.output_type in [\"seq\", \"sequence\", \"Sequence\", \"3D\"] : \n            out = keras.ops.reshape(patches, (batch_size, num_patches, patch_dim))\n        elif self.output_type in [\"conv\", \"feature_map\", \"2D\"]:\n            out = keras.ops.reshape(patches, (batch_size, keras.ops.shape(patches)[1], keras.ops.shape(patches)[2],\n                                             patch_dim))\n        return out","metadata":{"execution":{"iopub.status.busy":"2024-06-04T07:54:32.296018Z","iopub.execute_input":"2024-06-04T07:54:32.296481Z","iopub.status.idle":"2024-06-04T07:54:32.316473Z","shell.execute_reply.started":"2024-06-04T07:54:32.296442Z","shell.execute_reply":"2024-06-04T07:54:32.315290Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"class MLPMixer(keras.layers.Layer): #untrainable layer\n    def __init__(self, units = None, dropout_rate = 0.2, **kwargs):\n        super().__init__(**kwargs)\n        self.layernorm1 = keras.layers.LayerNormalization()\n        self.layernorm2 = keras.layers.LayerNormalization()\n        self.drop_rate = dropout_rate\n        self.units = units\n        \n        \n    def build(self, input_shape):\n        #batch, seq_len, dims\n        batch_size = input_shape[0]\n        n_tokens = input_shape[1]\n        channels = input_shape[2]\n        embed_dim_middle = channels if self.units is None else self.units\n        \n        self.mlp1 = keras.Sequential([keras.layers.Dense(units = n_tokens, use_bias = False),\n                                     keras.layers.Activation(\"gelu\"),\n                                     keras.layers.Dropout(self.drop_rate),\n                                     keras.layers.Dense(units = n_tokens, use_bias = False),],\n                                    name = \"MLP1_TokenWiseMixing\") #output : batch, embedding_dims(channels), n_tokens\n        \n        self.mlp2 = keras.Sequential([keras.layers.Dense(units = embed_dim_middle, use_bias = False),\n                                     keras.layers.Activation(\"gelu\"),\n                                     keras.layers.Dropout(self.drop_rate),\n                                     keras.layers.Dense(units = channels, use_bias = False),],\n                                    name = \"MLP2_ChannelWiseMixing\")\n    def enable_lora(self, rank):\n        print(\"Use only in pretrained model!\\n\\n\")\n        if rank == 0:\n            print(\"lora disabled\\n\\n\")\n            pass\n        else:\n            self.layernorm1.trainable = False\n            self.layernorm2.trainable = False\n            for layer in self.mlp1.layers:\n                if isinstance(layer, keras.layers.Dense):\n                    layer.enable_lora(rank)\n                else:\n                    layer.trainable = False\n            for layer in self.mlp2.layers:\n                if isinstance(layer, keras.layers.Dense):\n                    layer.enable_lora(rank)\n                else:\n                    layer.trainable = False\n    def call(self, x):\n        # x : patches, [batch, n_patches, embed_dims]\n        normed_patch = self.layernorm1(x)\n        normed_patch = ops.transpose(normed_patch, [0,2,1]) #batch, embed_dims, n_patches\n        normed_patch = self.mlp1(normed_patch)\n        normed_patch = ops.transpose(normed_patch, [0,2,1]) #batch, n_patches, embed_dims\n        normed_patch += x\n        normed_patch2 = self.layernorm2(normed_patch)\n        normed_patch2 = self.mlp2(normed_patch2)\n        normed_patch2 += normed_patch\n        return normed_patch2","metadata":{"execution":{"iopub.status.busy":"2024-06-04T07:54:32.318002Z","iopub.execute_input":"2024-06-04T07:54:32.318473Z","iopub.status.idle":"2024-06-04T07:54:32.336646Z","shell.execute_reply.started":"2024-06-04T07:54:32.318430Z","shell.execute_reply":"2024-06-04T07:54:32.335292Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def get_mlp_mixer(res, name = \"mlpmixer_16_4_768\", mask = False):\n    _, patch_size, depth, embed_dims = name.split(\"_\")\n    patch_size = int(patch_size)\n    depth = int(depth)\n    embed_dims = int(embed_dims)\n    \n    model_name = f\"Res{res}_{name}\"\n    inputs = Input([res,res,3], name = \"ImageInput\")\n    scaled_input = inputs/255.0\n    patches = NaivePatchesExtraction(patch_size, name = \"PatchExtraction\")(scaled_input)\n    patches_embedding = PatchEncoder(num_patches = ops.shape(patches)[1], projection_dim = embed_dims)(patches)\n    if mask:\n        patches_embedding *= mask\n    for idx in range(depth):\n        patches_embedding = MLPMixer(name = f\"MLPMixer_{idx+1}\")(patches_embedding)\n    model = Model(inputs, patches_embedding,\n                 name = model_name)\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-06-04T07:54:32.338235Z","iopub.execute_input":"2024-06-04T07:54:32.338697Z","iopub.status.idle":"2024-06-04T07:54:32.357740Z","shell.execute_reply.started":"2024-06-04T07:54:32.338652Z","shell.execute_reply":"2024-06-04T07:54:32.356471Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"# ConvMixer\n- Dense layer inside MLPmixer --> Conv2D\n- Code Reference : [Keras.io](https://keras.io/examples/vision/convmixer/)\n\n![](https://i.imgur.com/yF8actg.png)","metadata":{}},{"cell_type":"code","source":"class ConvMixer(keras.layers.Layer): #untrainable layer\n    def __init__(self, filters = None, kernel_size = 5, dropout_rate = 0.2, output_type = \"2D\", **kwargs):\n        super().__init__(**kwargs)\n        self.layernorm1 = keras.layers.LayerNormalization()\n        self.layernorm2 = keras.layers.LayerNormalization()\n        self.drop_rate = dropout_rate ; self.drop = keras.layers.Dropout(self.drop_rate)\n        \n        self.filters = filters\n        self.kernel_size = kernel_size\n        self.output_type = output_type\n        assert output_type in [\"seq\", \"sequence\", \"Sequence\", \"3D\", \"conv\", \"feature_map\", \"2D\"]\n        \n    def build(self, input_shape):\n        #batch, seq_len, dims\n        batch_size = input_shape[0]\n        h = input_shape[1]\n        w = input_shape[2]\n        channels = input_shape[3]\n        c = channels if self.filters is None else self.filters\n        \n        self.depthconv = keras.layers.DepthwiseConv2D(kernel_size = self.kernel_size, padding = \"SAME\", use_bias = False)\n        self.pointconv = keras.layers.Conv2D(filters = c, kernel_size = 1, padding = 'SAME', use_bias = False)\n        self.gelu = keras.layers.Activation('gelu')\n\n    def call(self, x):\n        # x : feature_map, [batch, h,w, embed_dims]\n        fmap = self.depthconv(x)\n        fmap = self.gelu(fmap)\n        fmap = self.layernorm1(fmap) + x\n        fmap = self.pointconv(fmap)\n        fmap = self.gelu(fmap)\n        fmap = self.layernorm2(fmap)\n        \n        batch_size = ops.shape(fmap)[0]\n        h = ops.shape(fmap)[1]\n        w = ops.shape(fmap)[2]\n        dims_ = ops.shape(fmap)[3]\n        \n        if self.output_type in [\"seq\", \"sequence\", \"Sequence\", \"3D\"] : \n            fmap = keras.ops.reshape(fmap, (batch_size, h*w, dims_))\n            return fmap\n        elif self.output_type in [\"conv\", \"feature_map\", \"2D\"]:\n            return fmap","metadata":{"execution":{"iopub.status.busy":"2024-06-04T07:54:32.359430Z","iopub.execute_input":"2024-06-04T07:54:32.360331Z","iopub.status.idle":"2024-06-04T07:54:32.376534Z","shell.execute_reply.started":"2024-06-04T07:54:32.360287Z","shell.execute_reply":"2024-06-04T07:54:32.375264Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def get_conv_mixer(res, name = \"convmixer_16_4_768\", mask = False):\n    _, patch_size, depth, embed_dims = name.split(\"_\")\n    patch_size = int(patch_size)\n    depth = int(depth)\n    embed_dims = int(embed_dims)\n    \n    model_name = f\"Res{res}_{name}\"\n    inputs = Input([res,res,3], name = \"ImageInput\")\n    scaled_input = inputs/255.0\n    \n    patches = NaivePatchesExtraction(patch_size, name = \"PatchExtraction\")(scaled_input)\n    patches_embedding = PatchEncoder(num_patches = ops.shape(patches)[1], projection_dim = embed_dims)(patches)\n    batch, n_patches, dims = ops.shape(patches_embedding) ; res_ = int(ops.sqrt(ops.cast(n_patches, \"float32\")))\n    patches_embedding = ops.reshape(patches_embedding, (-1, res_, res_, dims))\n    if mask:\n        patches_embedding *= mask\n    for idx in range(depth):\n        if idx == depth-1:\n            types = \"seq\"\n        else:\n            types = \"2D\"\n        patches_embedding = ConvMixer(name = f\"ConvMixer_{idx+1}\", output_type = types)(patches_embedding)\n    model = Model(inputs, patches_embedding,\n                 name = model_name)\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-06-04T07:54:32.378207Z","iopub.execute_input":"2024-06-04T07:54:32.378570Z","iopub.status.idle":"2024-06-04T07:54:32.397802Z","shell.execute_reply.started":"2024-06-04T07:54:32.378539Z","shell.execute_reply":"2024-06-04T07:54:32.396481Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"-----------","metadata":{}},{"cell_type":"markdown","source":"# Get final Feature extractor\n- according to [recent study](https://arxiv.org/abs/2309.16588), add cls token and register tokens \"after\" the patches!","metadata":{}},{"cell_type":"code","source":"def get_feature_extractor(conv_base, #if None, Vanilla ViT\n                         embed_dims, res, pe_type = \"rotary\",\n                          patch_size = 16,\n                        att_depth = 4, att_heads = 16, mask = False, \n                          return_patches = False) : \n    inputs = Input([res,res,3], name = \"Input_images\")\n    batch_size = ops.shape(inputs)[0]\n    if conv_base in [None, 'vit', \"ViT\"] : #Vanilla Vision Transformer\n        scaled_inputs = keras.layers.LayerNormalization(name = \"InitialLN\")(inputs)\n        patches = ops.image.extract_patches(scaled_inputs,\n                                           size = patch_size,\n                                           padding = 'same')\n        \n        _, w, h, dims = ops.shape(patches) ; n_patches = w*h \n        \n        patches = ops.reshape(patches, [-1, w*h, dims])\n        if pe_type in ['rotary', 'rotation', 'rotatory', 'roformer']:\n            patches = keras_nlp.layers.RotaryEmbedding(name = \"RotaryPositionalEmbedding\")(patches)\n            patches = Dense(units = embed_dims, activation = \"gelu\", name = \"EmbeddingAfterRPE\")(patches)\n        elif pe_type in [\"learnable\", 'absolute']:\n            patches = PatchEncoder(num_patches = w*h, projection_dim = embed_dims)(patches)\n        elif pe_type == None:\n            pass\n        \n        if mask:\n            patches *= mask\n        \n        learned_token, patches, attention_score = TRBlock(att_depth = att_depth, \n                                                          att_dims = embed_dims, \n                                                          att_heads = att_heads)([patches, patches])\n        \n        learned_token = keras.layers.Identity(name = \"feature_vector\")(learned_token)\n        patches = keras.layers.Identity(name = \"encoded_patches\")(patches)\n        attention_score = keras.layers.Identity(name = \"attention_weight\")(attention_score) \n        model_name = f\"ViT_depth{att_depth}_dims{embed_dims}_heads{att_heads}_patch{patch_size}\"\n    else:\n        feature_map = conv_base(inputs)\n        dims = ops.shape(feature_map)[-1] ; batch_size = ops.shape(feature_map)[0]\n        if len(ops.shape(feature_map)) == 4:\n            _, w, h, dims = ops.shape(feature_map)\n            feature_map = ops.reshape(feature_map, [-1, w*h, dims])\n        n_patches = ops.shape(feature_map)[1]\n        \n        if pe_type in ['rotary', 'rotation', 'rotatory', 'roformer']:\n            feature_map = keras_nlp.layers.RotaryEmbedding(name = \"RotaryPositionalEmbedding\")(feature_map)\n            feature_map = Dense(units = embed_dims, name = \"FeatureEmbedding\", activation = \"gelu\")(feature_map)\n        elif pe_type in [\"learnable\", 'absolute']:\n            feature_map = PatchEncoder(num_patches = n_patches, projection_dim = embed_dims)(feature_map)\n        elif pe_type == None:\n            feature_map = Dense(units = embed_dims, name = \"FeatureEmbedding\", activation = \"gelu\")(feature_map)\n        \n        learned_token, patches, attention_score = TRBlock(att_depth = att_depth, att_dims = embed_dims,\n                                                          att_heads = att_heads)([feature_map, feature_map])    \n        \n        learned_token = keras.layers.Identity(name = \"feature_vector\")(learned_token)\n        patches = keras.layers.Identity(name = \"encoded_patches\")(patches)\n        attention_score = keras.layers.Identity(name = \"attention_weight\")(attention_score) \n        \n        model_name = f\"{conv_base.name}_depth{att_depth}_dims{embed_dims}_heads{att_heads}\"\n    if return_patches:\n        \n        model = Model(inputs,\n                     [learned_token, patches, \n                      attention_score],\n                     name = model_name + \"_withPatches\")\n    else:\n        model = Model(inputs, [learned_token, attention_score],\n                      name = model_name)\n    return model\n\ndef get_full_model(conv_base_name, res, embed_dims = 1280, patch_size = 16, pe_type = 'rotary',\n                   att_depth = 4, att_heads = 8,\n                  extra_configs = None, mask = False,\n                   return_patches = False):\n    if isinstance(conv_base_name, keras.Model):\n        conv_base = conv_base_name\n        #pe_type = \"learnable\"\n    elif conv_base_name in [\"effnet\", 'EfficientNet']:\n        conv_base = keras.applications.EfficientNetV2B1(input_shape = [res,res,3],\n                                                       include_top = False)\n    elif conv_base_name in [\"effnet_small\", \"EfficientNetSmall\"]:\n        conv_base = keras.applications.EfficientNetV2S(input_shape = [res,res,3],\n                                                       include_top = False)\n    elif conv_base_name in [\"effnet_base\", \"EfficientNetBase\"]:\n        conv_base = keras.applications.EfficientNetV2M(input_shape = [res,res,3],\n                                                       include_top = False)\n    elif conv_base_name in [\"convnext\", 'ConvNeXt']:\n        conv_base = keras.applications.ConvNeXtTiny(input_shape = [res,res,3],\n                                                       include_top = False)\n    elif conv_base_name in [\"convnext_small\", 'ConvNeXtSmall']:\n        conv_base = keras.applications.ConvNeXtSmall(input_shape = [res,res,3],\n                                                       include_top = False)\n    elif conv_base_name in [\"convnext_base\", 'ConvNeXtBase']:\n        conv_base = keras.applications.ConvNeXtBase(input_shape = [res,res,3],\n                                                       include_top = False)\n    elif isinstance(conv_base_name, dict):\n        conv_base = get_gcvit(conv_base_name)\n    elif conv_base_name.split(\"_\")[0] in [\"MLPMixer\", \"mlpmixer\", \"MLP\", \"mlp\"]:\n        conv_base = get_mlp_mixer(res = res, name = conv_base_name, mask = mask)\n        pe_type = None\n    elif conv_base_name.split(\"_\")[0] in [\"ConvMixer\", \"convmixer\", \"Conv\", \"conv\"]:\n        conv_base = get_conv_mixer(res = res, name = conv_base_name, mask = mask)\n        pe_type = None\n    else:\n        conv_base = None\n    return get_feature_extractor(conv_base, pe_type = pe_type, res = res, patch_size = patch_size, embed_dims = embed_dims, att_depth = att_depth, att_heads = att_heads,\n                                return_patches = return_patches, mask = mask)","metadata":{"execution":{"iopub.status.busy":"2024-06-04T07:54:32.399746Z","iopub.execute_input":"2024-06-04T07:54:32.400126Z","iopub.status.idle":"2024-06-04T07:54:32.433635Z","shell.execute_reply.started":"2024-06-04T07:54:32.400095Z","shell.execute_reply":"2024-06-04T07:54:32.432286Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"---------\n# Information-maximization method","metadata":{}},{"cell_type":"code","source":"class BarlowModel(keras.Model):\n    def __init__(self, feature_extractor, embed_dims, probe = False, multiview = False,\n                 probe_heads = None, probe_activation = \"sigmoid\",\n                 diag = 0.6, off_diag = 0.4, \n                 **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dims = embed_dims\n        self.multiview = multiview\n        self.feature_extractor = feature_extractor #weight sharing for VICreg, Barlow Twins\n        self.projector_a = self.get_projector()\n        self.projector_b = self.get_projector()\n        self.diag = diag ; self.off_diag = off_diag\n        self.loss_tracker = tf.keras.metrics.Mean(\"Barlow_loss_tracker\")\n        self.linear_probing = probe\n        if probe:\n            self.probe_loss_tracker = tf.keras.metrics.Mean(\"Probe_loss_tracker\")\n            self.probe_metrics = [\n                                  keras.metrics.F1Score(average=\"micro\", threshold=None, name=\"probe_micro_f1_score\"),\n                                 keras.metrics.TruePositives(name = \"probe_TP\"),\n                                 keras.metrics.TrueNegatives(name = \"probe_TN\"),\n                                 keras.metrics.FalsePositives(name = \"probe_FP\"),\n                                 keras.metrics.FalseNegatives(name = \"probe_FN\")]\n            \n            self.probe_categories = probe_heads\n            self.probe_act = probe_activation\n            self.linear_probe = keras.layers.Dense(units = probe_heads, activation = self.probe_act, dtype = \"float32\")\n            if self.probe_act == \"sigmoid\":\n                self.probe_loss_fn = keras.losses.BinaryCrossentropy(reduction = None, label_smoothing = 0.1)\n                self.probe_metrics.append(keras.metrics.BinaryAccuracy(name=\"probe_accuracy\", threshold=0.5))\n            elif self.probe_act == \"softmax\":\n                self.probe_loss_fn = keras.losses.CategoricalCrossentropy(reduction = None, label_smoothing = 0.1)\n                self.probe_metrics.append(keras.metrics.CategoricalAccuracy(name=\"probe_accuracy\", threshold=0.5))\n    def compile(self, optimizer, probe_optimizer = None, **kwargs):\n        super().compile(**kwargs)\n        self.optimizer = optimizer\n        if self.linear_probing:\n            self.probe_optimizer = probe_optimizer\n        \n    def get_config(self):\n        return {\"feature_extractor_name\" : self.feature_extractor.name,\n               \"embed_dims\" : self.embed_dims,\n                \"Multiview(>2)\" : self.multiview,\n               'diag_part_coefficient' : self.diag,\n               'off_diag_coefficient' : self.off_diag,\n               \"SSL_method\" : \"Barlow_Twins\",\n               \"Linear Probe\" : self.linear_probing,\n               \"N_Categories\" : self.probe_categories if self.linear_probing else 0,\n               \"Probe Activation\" : self.probe_act if self.linear_probing else \"NA\"}\n    \n    def get_projector(self):\n        \n        model = keras.Sequential([Dense(units = self.embed_dims),\n                                 Dense(units = self.embed_dims),\n                                 Dense(units = self.embed_dims, dtype = \"float32\")]\n                                )\n        return model\n    \n    def compute_loss(self, correlation_matrix):\n        diag_component = tf.linalg.diag_part(correlation_matrix)\n        zero_diag = tf.zeros(correlation_matrix.shape[-1])\n        off_diag_matrix = tf.linalg.set_diag(correlation_matrix, zero_diag)\n        \n        diag_loss = tf.pow(diag_component-1, 2) * self.diag\n        off_diag_loss = tf.pow(off_diag_matrix, 2) * self.off_diag\n        loss = tf.reduce_mean(diag_loss) + tf.reduce_mean(off_diag_loss)\n        loss = tf.cast(loss, tf.float32)\n        \n        return loss\n        \n    def train_step(self, dataset):\n        \n        if self.linear_probing:\n            data, aug_data, labels = dataset\n        else:\n            if self.multiview:\n                data = dataset[0]\n                aug_data = dataset[1:] ; n_augs = len(aug_data)\n                aug_data = ops.concatenate(aug_data, axis = 0)\n            else:\n                data, aug_data = dataset\n        ## 1. SSL encoder loss ##\n        with tf.GradientTape() as tape:\n            if len(self.feature_extractor.outputs) == 2 :\n                feature_seq, weights = self.feature_extractor(data)\n                feature_seq_aug, weights_aug = self.feature_extractor(aug_data)\n            elif len(self.feature_extractor.outputs) == 1:\n                feature_seq = self.feature_extractor(data)\n                feature_seq_aug = self.feature_extractor(aug_data)\n                \n            if len(ops.shape(feature_seq)) == 3: \n                rep_vector = keras.layers.GlobalAveragePooling1D()(feature_seq)\n                rep_vector_aug = keras.layers.GlobalAveragePooling1D()(feature_seq_aug)\n            elif len(ops.shape(feature_seq)) == 4:\n                rep_vector = keras.layers.GlobalAveragePooling2D()(feature_seq)\n                rep_vector_aug = keras.layers.GlobalAveragePooling2D()(feature_seq_aug)\n            elif len(ops.shape(feature_seq)) == 2:\n                rep_vector = feature_seq\n                rep_vector_aug = feature_seq_aug\n            original_rep_vector = rep_vector #for Linear probing\n            \n            rep_vector = self.projector_a(rep_vector)\n            rep_vector_aug = self.projector_b(rep_vector_aug)\n            if self.multiview:\n                rep_vector_aug_set = ops.split(rep_vector_aug, n_augs, 0)\n                loss = []\n                for idx in range(n_augs):\n                    correlation_matrix = get_cor_matrix(rep_vector, rep_vector_aug_set[idx])\n                    loss.append(self.compute_loss(correlation_matrix)\n                               )\n                loss = ops.mean(loss)\n            else:\n                correlation_matrix = get_cor_matrix(rep_vector, rep_vector_aug)\n                loss = self.compute_loss(correlation_matrix)\n    \n\n        gradients = tape.gradient(loss, \n                                 (self.feature_extractor.trainable_variables + self.projector_a.trainable_variables + self.projector_b.trainable_variables))\n        self.optimizer.apply_gradients(zip(gradients, \n                                          (self.feature_extractor.trainable_variables + self.projector_a.trainable_variables + self.projector_b.trainable_variables)\n                                          ))\n        self.loss_tracker.update_state(loss)\n        output_dict = {'Barlow_loss' : self.loss_tracker.result()}\n        if self.linear_probing:\n            ##2. linear probing and calculate metrics##\n            \n            with tf.GradientTape() as tape:\n                if len(self.feature_extractor.outputs) == 2 :\n                    feature_seq, weights = self.feature_extractor(data, training = False)\n                elif len(self.feature_extractor.outputs) == 1:\n                    feature_seq = self.feature_extractor(data, training = False)\n                if len(ops.shape(feature_seq)) == 3: \n                    original_rep_vector = keras.layers.GlobalAveragePooling1D()(feature_seq)\n                elif len(ops.shape(feature_seq)) == 4:\n                    original_rep_vector = keras.layers.GlobalAveragePooling2D()(feature_seq)\n                elif len(ops.shape(feature_seq)) == 2:\n                    rep_vector = feature_seq\n        \n                class_logits = self.linear_probe(original_rep_vector, training = True)\n                class_probe_loss = self.probe_loss_fn(y_true = labels, y_pred = class_logits)\n                class_probe_loss = ops.mean(class_probe_loss)\n            gradients = tape.gradient(class_probe_loss, \n                                            self.linear_probe.trainable_weights)\n            self.probe_optimizer.apply_gradients(zip(gradients, self.linear_probe.trainable_weights))\n            self.probe_loss_tracker.update_state(class_probe_loss)\n            output_dict.update({\"probe_classification_loss\" : self.probe_loss_tracker.result()})\n            #metrics\n            for comp in self.probe_metrics:\n                comp.update_state(labels, class_logits)\n                \n            probe_metric_dict = {comp.name :  comp.result() for comp in self.probe_metrics}\n            output_dict.update(probe_metric_dict)\n        else:\n            pass\n        del tape\n        return output_dict\n    def test_step(self, dataset):\n        if self.linear_probing:\n            data, aug_data, labels = dataset\n        else:\n            if self.multiview:\n                data = dataset[0]\n                aug_data = dataset[1:] ; n_augs = len(aug_data)\n                aug_data = ops.concatenate(aug_data, axis = 0)\n            else:\n                data, aug_data = dataset\n            \n        if len(self.feature_extractor.outputs) == 2 :\n            feature_seq, weights = self.feature_extractor(data)\n            feature_seq_aug, weights_aug = self.feature_extractor(aug_data)\n        elif len(self.feature_extractor.outputs) == 1:\n            feature_seq = self.feature_extractor(data)\n            feature_seq_aug = self.feature_extractor(aug_data)\n        if len(ops.shape(feature_seq)) == 3: \n            rep_vector = keras.layers.GlobalAveragePooling1D()(feature_seq)\n            rep_vector_aug = keras.layers.GlobalAveragePooling1D()(feature_seq_aug)\n        elif len(ops.shape(feature_seq)) == 4:\n            rep_vector = keras.layers.GlobalAveragePooling2D()(feature_seq)\n            rep_vector_aug = keras.layers.GlobalAveragePooling2D()(feature_seq_aug)\n        elif len(ops.shape(feature_seq)) == 2:\n            rep_vector = feature_seq\n            rep_vector_aug = feature_seq_aug\n        \n        rep_vector = self.projector_a(rep_vector)\n        rep_vector_aug = self.projector_b(rep_vector_aug)\n        \n        if self.multiview:\n            rep_vector_aug_set = ops.split(rep_vector_aug, n_augs, 0)\n            loss = []\n            for idx in range(n_augs):\n                correlation_matrix = get_cor_matrix(rep_vector, rep_vector_aug_set[idx])\n                loss.append(self.compute_loss(correlation_matrix)\n                            )\n            loss = ops.mean(loss)\n        else:\n            correlation_matrix = get_cor_matrix(rep_vector, rep_vector_aug)\n            loss = self.compute_loss(correlation_matrix)\n        \n        self.loss_tracker.update_state(loss)\n        output_dict = {'Barlow_loss' : self.loss_tracker.result()}\n        \n        if self.linear_probing:\n            ##2. linear probing and calculate metrics##\n            if len(self.feature_extractor.outputs) == 2 :\n                feature_seq, weights = self.feature_extractor(data, training = False)\n            elif len(self.feature_extractor.outputs) == 1:\n                feature_seq = self.feature_extractor(data, training = False)\n            if len(ops.shape(feature_seq)) == 3: \n                original_rep_vector = keras.layers.GlobalAveragePooling1D()(feature_seq)\n            elif len(ops.shape(feature_seq)) == 4:\n                original_rep_vector = keras.layers.GlobalAveragePooling2D()(feature_seq)\n            elif len(ops.shape(feature_seq)) == 2:\n                original_rep_vector = feature_seq\n\n            class_logits = self.linear_probe(original_rep_vector, training = False)\n            class_probe_loss = self.probe_loss_fn(y_true = labels, y_pred = class_logits)\n            class_probe_loss = ops.mean(class_probe_loss)\n            self.probe_loss_tracker.update_state(class_probe_loss)\n            output_dict.update({\"probe_classification_loss\" : self.probe_loss_tracker.result()})\n            #metrics\n            for comp in self.probe_metrics:\n                comp.update_state(labels, class_logits)\n                \n            probe_metric_dict = {comp.name :  comp.result() for comp in self.probe_metrics}\n            output_dict.update(probe_metric_dict)\n        else:\n            pass\n        return output_dict\n    def get_classifier_model(self):\n        self.feature_extractor.trainable = True\n        if self.linear_probing == False:\n            return self.feature_extractor\n        self.linear_probe.trainable = True\n        inputs = self.feature_extractor.input\n        outputs = self.feature_extractor.outputs\n        if len(self.feature_extractor.outputs) == 2 :\n            feature_map, att_weights = outputs\n        elif len(self.feature_extractor.outputs) == 1:\n            feature_map = outputs[0]\n        \n        if len(ops.shape(feature_map)) == 3: \n            rep_vector = keras.layers.GlobalAveragePooling1D()(feature_map)\n        elif len(ops.shape(feature_map)) == 4:\n            rep_vector = keras.layers.GlobalAveragePooling2D()(feature_map)\n        elif len(ops.shape(feature_map)) == 2:\n            rep_vector = feature_map\n            \n        class_logits = self.linear_probe(rep_vector)\n        classifier_model = keras.Model(inputs, class_logits,\n                                      name = f\"Barlow_pretrained_{self.feature_extractor.name}\")\n        return classifier_model\n    def call(self, dataset):\n        return self.test_step(dataset)","metadata":{"execution":{"iopub.status.busy":"2024-06-04T07:54:32.435490Z","iopub.execute_input":"2024-06-04T07:54:32.435894Z","iopub.status.idle":"2024-06-04T07:54:32.500604Z","shell.execute_reply.started":"2024-06-04T07:54:32.435861Z","shell.execute_reply":"2024-06-04T07:54:32.499382Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"class VICRegModel(keras.Model):\n    def __init__(self, feature_extractor, embed_dims, multiview = False,\n                 probe = False, \n                 probe_heads = None, probe_activation = \"sigmoid\",\n                 variance_coeff = 20, invariance_coeff = 20, covariance_coeff = 1, \n                 variance_gamma = 5.0,\n                 **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dims = embed_dims\n        self.multiview = multiview\n        self.feature_extractor = feature_extractor #weight sharing for VICreg, Barlow Twins\n        self.projector_a = self.get_projector()\n        self.projector_b = self.get_projector()\n        self.var_coef = variance_coeff ; self.invar_coef = invariance_coeff ; self.cov_coef = covariance_coeff\n        self.gamma = variance_gamma\n        \n        self.loss_tracker = tf.keras.metrics.Mean(\"VIC_loss_tracker\")\n        self.invar_loss_tracker = tf.keras.metrics.Mean(\"Invariance_loss_tracker\")\n        self.var_loss_tracker = tf.keras.metrics.Mean(\"Variance_loss_tracker\")\n        self.covar_loss_tracker = tf.keras.metrics.Mean(\"Covariance_loss_tracker\")\n        self.linear_probing = probe\n        if probe:\n            self.probe_loss_tracker = tf.keras.metrics.Mean(\"Probe_loss_tracker\")\n            self.probe_metrics = [keras.metrics.Accuracy(name=\"probe_accuracy\"),\n                                  keras.metrics.F1Score(average=\"micro\", threshold=None, name=\"probe_micro_f1_score\"),\n                                 keras.metrics.TruePositives(name = \"probe_TP\"),\n                                 keras.metrics.TrueNegatives(name = \"probe_TN\"),\n                                 keras.metrics.FalsePositives(name = \"probe_FP\"),\n                                 keras.metrics.FalseNegatives(name = \"probe_FN\")]\n            \n            self.probe_categories = probe_heads\n            self.probe_act = probe_activation\n            self.linear_probe = keras.layers.Dense(units = probe_heads, activation = self.probe_act)\n            if self.probe_act == \"sigmoid\":\n                self.probe_loss_fn = keras.losses.BinaryCrossentropy(reduction = None, label_smoothing = 0.1)\n            elif self.probe_act == \"softmax\":\n                self.probe_loss_fn = keras.losses.CategoricalCrossentropy(reduction = None, label_smoothing = 0.1)\n    def compile(self, optimizer, probe_optimizer = None, **kwargs):\n        super().compile(**kwargs)\n        self.optimizer = optimizer\n        if self.linear_probing:\n            self.probe_optimizer = probe_optimizer     \n    def get_config(self):\n        return {\"feature_extractor_name\" : self.feature_extractor.name,\n               \"embed_dims\" : self.embed_dims,\n                 \"Multiview(>2)\" : self.multiview,\n               'Variance_coefficient' : self.var_coef,\n               'Invariance_coefficient' : self.invar_coef,\n               \"Covariance_coefficient\" : self.cov_coef,\n                \"Variance_gamma\" : self.gamma,\n               \"SSL_method\" : \"VICReg\",\n               \"Linear Probe\" : self.linear_probing,\n               \"N_Categories\" : self.probe_categories if self.linear_probing else 0,\n               \"Probe Activation\" : self.probe_act if self.linear_probing else \"NA\"}\n    def get_projector(self):\n        model = keras.Sequential([Dense(units = self.embed_dims),\n                                 Dense(units = self.embed_dims),\n                                 Dense(units = self.embed_dims,dtype = \"float32\")]\n                                )\n        return model\n    \n    def compute_loss(self, rep_a, rep_b):\n        invar_loss = invariance_loss(rep_a, rep_b) * self.invar_coef\n        invar_loss = ops.mean(invar_loss)\n        \n        variance_a = variance(rep_a, gamma = self.gamma)\n        variance_a = ops.mean(variance_a)\n        \n        covariance_a = covariance(rep_a)\n        \n        variance_b = variance(rep_b, gamma = self.gamma)\n        variance_b = ops.mean(variance_b)\n        \n        covariance_b = covariance(rep_b)\n        #Variance loss -> variance가 toward gamma\n        # covariance -> covariance가 toward zero\n        var_loss = (variance_a + variance_b) * self.var_coef\n        covar_loss = (covariance_a + covariance_b) * self.cov_coef\n        loss = invar_loss + var_loss + covar_loss\n        return loss, invar_loss, var_loss, covar_loss\n        \n    def train_step(self, dataset):\n        if self.linear_probing:\n            data, aug_data, labels = dataset\n        else:\n            if self.multiview:\n                data = dataset[0]\n                aug_data = dataset[1:] ; n_augs = len(aug_data)\n                aug_data = ops.concatenate(aug_data, axis = 0)\n            else:\n                data, aug_data = dataset\n            \n        with tf.GradientTape() as tape:\n            \n            if len(self.feature_extractor.outputs) == 2 :\n                feature_seq, weights = self.feature_extractor(data)\n                feature_seq_aug, weights_aug = self.feature_extractor(aug_data)\n            elif len(self.feature_extractor.outputs) == 1:\n                feature_seq = self.feature_extractor(data)\n                feature_seq_aug = self.feature_extractor(aug_data)\n            \n            if len(ops.shape(feature_seq)) == 3: \n                rep_vector = keras.layers.GlobalAveragePooling1D()(feature_seq)\n                rep_vector_aug = keras.layers.GlobalAveragePooling1D()(feature_seq_aug)\n            elif len(ops.shape(feature_seq)) == 4:\n                rep_vector = keras.layers.GlobalAveragePooling2D()(feature_seq)\n                rep_vector_aug = keras.layers.GlobalAveragePooling2D()(feature_seq_aug)\n            elif len(ops.shape(feature_seq)) == 2:\n                rep_vector = feature_seq\n                rep_vector_aug = feature_seq_aug\n            \n            \n            rep_vector = self.projector_a(rep_vector)\n            rep_vector_aug = self.projector_b(rep_vector_aug)\n            if self.multiview:\n                loss, invar_loss, var_loss, covar_loss = 0.0, 0.0, 0.0, 0.0\n                rep_vector_aug_set = ops.split(rep_vector_aug, n_augs, 0)\n                for idx in range(n_augs):\n                    loss_, invar_loss_, var_loss_, covar_loss_ = self.compute_loss(rep_vector, rep_vector_aug_set[idx])\n                    loss += loss_\n                    invar_loss += invar_loss_\n                    var_loss += var_loss_\n                    covar_loss += covar_loss_\n                loss /= n_augs\n                invar_loss /= n_augs\n                var_loss /= n_augs\n                covar_loss /= n_augs\n                \n            else:\n                loss, invar_loss, var_loss, covar_loss = self.compute_loss(rep_vector, rep_vector_aug)\n        \n        feature_extractor_var = self.feature_extractor.trainable_variables\n        proj_a_var = self.projector_a.trainable_variables\n        proj_b_var = self.projector_b.trainable_variables\n        \n        trainable_variables = (feature_extractor_var + proj_a_var + proj_b_var)\n        gradients = tape.gradient(loss, trainable_variables)\n        self.optimizer.apply_gradients(zip(gradients, trainable_variables))\n        \n        self.loss_tracker.update_state(loss)\n        self.invar_loss_tracker.update_state(invar_loss)\n        self.var_loss_tracker.update_state(var_loss)\n        self.covar_loss_tracker.update_state(covar_loss)\n        output_dict = {'loss' : self.loss_tracker.result(),\n                        \"invariance_loss\" : self.invar_loss_tracker.result(),\n                        'variance_loss' : self.var_loss_tracker.result(),\n                        'covariance_loss' : self.covar_loss_tracker.result()\n                       }\n        \n        if self.linear_probing:\n            ##2. linear probing and calculate metrics##\n            \n            with tf.GradientTape() as tape:\n                if len(self.feature_extractor.outputs) == 2 :\n                    feature_seq, weights = self.feature_extractor(data, training = False)\n                elif len(self.feature_extractor.outputs) == 1:\n                    feature_seq = self.feature_extractor(data, training = False)\n                if len(ops.shape(feature_seq)) == 3: \n                    original_rep_vector = keras.layers.GlobalAveragePooling1D()(feature_seq)\n                elif len(ops.shape(feature_seq)) == 4:\n                    original_rep_vector = keras.layers.GlobalAveragePooling2D()(feature_seq)\n                elif len(ops.shape(feature_seq)) == 2:\n                    original_rep_vector = feature_seq\n                    \n                class_logits = self.linear_probe(original_rep_vector, training = True)\n                class_probe_loss = self.probe_loss_fn(y_true = labels, y_pred = class_logits)\n                class_probe_loss = ops.mean(class_probe_loss)\n            gradients = tape.gradient(class_probe_loss, \n                                            self.linear_probe.trainable_weights)\n            self.probe_optimizer.apply_gradients(zip(gradients, self.linear_probe.trainable_weights))\n            self.probe_loss_tracker.update_state(class_probe_loss)\n            output_dict.update({\"probe_classification_loss\" : self.probe_loss_tracker.result()})\n            #metrics\n            for comp in self.probe_metrics:\n                comp.update_state(labels, class_logits)\n                \n            probe_metric_dict = {comp.name :  comp.result() for comp in self.probe_metrics}\n            output_dict.update(probe_metric_dict)\n        else:\n            pass\n        del tape\n        return output_dict\n    def test_step(self, dataset):\n        if self.linear_probing:\n            data, aug_data, labels = dataset\n        else:\n            if self.multiview:\n                data = dataset[0]\n                aug_data = dataset[1:] ; n_augs = len(aug_data)\n                aug_data = ops.concatenate(aug_data, axis = 0)\n            else:\n                data, aug_data = dataset\n        \n        if len(self.feature_extractor.outputs) == 2 :\n            feature_seq, weights = self.feature_extractor(data)\n            feature_seq_aug, weights_aug = self.feature_extractor(aug_data)\n        elif len(self.feature_extractor.outputs) == 1:\n            feature_seq = self.feature_extractor(data)\n            feature_seq_aug = self.feature_extractor(aug_data)\n        \n        if len(ops.shape(feature_seq)) == 3: \n            rep_vector = keras.layers.GlobalAveragePooling1D()(feature_seq)\n            rep_vector_aug = keras.layers.GlobalAveragePooling1D()(feature_seq_aug)\n        elif len(ops.shape(feature_seq)) == 4:\n            rep_vector = keras.layers.GlobalAveragePooling2D()(feature_seq)\n            rep_vector_aug = keras.layers.GlobalAveragePooling2D()(feature_seq_aug)\n        elif len(ops.shape(feature_seq)) == 2:\n            rep_vector = feature_seq\n            rep_vector_aug = feature_seq_aug\n            \n        original_rep_vector = rep_vector\n        \n        rep_vector = self.projector_a(rep_vector, training = False)\n        rep_vector_aug = self.projector_b(rep_vector_aug, training = False)\n        if self.multiview:\n            loss, invar_loss, var_loss, covar_loss = [],[],[],[]\n            rep_vector_aug_set = ops.split(rep_vector_aug, n_augs, 0)\n            for idx in range(n_augs):\n                loss_, invar_loss_, var_loss_, covar_loss_ = self.compute_loss(rep_vector, rep_vector_aug_set[idx])\n                loss.append(loss_)\n                invar_loss.append(invar_loss_)\n                var_loss.append(var_loss_)\n                covar_loss.append(covar_loss_)\n            loss = ops.mean(loss)\n            invar_loss = ops.mean(invar_loss)\n            var_loss = ops.mean(var_loss)\n            covar_loss = ops.mean(covar_loss)    \n        else:\n            loss, invar_loss, var_loss, covar_loss = self.compute_loss(rep_vector, rep_vector_aug)\n        \n        self.loss_tracker.update_state(loss)\n        self.invar_loss_tracker.update_state(invar_loss)\n        self.var_loss_tracker.update_state(var_loss)\n        self.covar_loss_tracker.update_state(covar_loss)\n        \n        output_dict =  {'loss' : self.loss_tracker.result(),\n                \"invariance_loss\" : self.invar_loss_tracker.result(),\n                'variance_loss' : self.var_loss_tracker.result(),\n                'covariance_loss' : self.covar_loss_tracker.result()\n               }\n        if self.linear_probing:\n            ##2. linear probing and calculate metrics##\n            self.feature_extractor.trainable = False\n            class_logits = self.linear_probe(original_rep_vector, training = False)\n            class_probe_loss = self.probe_loss_fn(y_true = labels, y_pred = class_logits)\n            class_probe_loss = ops.mean(class_probe_loss)\n            self.probe_loss_tracker.update_state(class_probe_loss)\n            output_dict.update({\"probe_classification_loss\" : self.probe_loss_tracker.result()})\n            #metrics\n            for comp in self.probe_metrics:\n                comp.update_state(labels, class_logits)\n                \n            probe_metric_dict = {comp.name :  comp.result() for comp in self.probe_metrics}\n            output_dict.update(probe_metric_dict)\n            self.feature_extractor.trainable = True\n        return output_dict\n    def get_classifier_model(self):\n        self.feature_extractor.trainable = True\n        if self.linear_probing == False:\n            return self.feature_extractor\n        self.linear_probe.trainable = True\n        inputs = self.feature_extractor.input\n        outputs = self.feature_extractor.outputs\n        if len(self.feature_extractor.outputs) == 2 :\n            feature_map, att_weights = outputs\n        elif len(self.feature_extractor.outputs) == 1:\n            feature_map = outputs[0]\n        \n        if len(ops.shape(feature_map)) == 3: \n            rep_vector = keras.layers.GlobalAveragePooling1D()(feature_map)\n        elif len(ops.shape(feature_map)) == 4:\n            rep_vector = keras.layers.GlobalAveragePooling2D()(feature_map)\n        elif len(ops.shape(feature_seq)) == 2:\n            rep_vector = feature_map\n            \n        class_logits = self.linear_probe(rep_vector)\n        classifier_model = keras.Model(inputs, class_logits,\n                                      name = f\"SSL_pretrained_{self.feature_extractor.name}\")\n        return classifier_model\n    def call(self, dataset):\n        return self.test_step(dataset)","metadata":{"execution":{"iopub.status.busy":"2024-06-04T07:54:32.503021Z","iopub.execute_input":"2024-06-04T07:54:32.503536Z","iopub.status.idle":"2024-06-04T07:54:32.570620Z","shell.execute_reply.started":"2024-06-04T07:54:32.503432Z","shell.execute_reply":"2024-06-04T07:54:32.569297Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"-------------\n# Contrastive-based method\n- SimSiam\n- SimCLR\n- SwAV\n- DINO\n- MoCo and moco-based learnings:\n    - [Dense Contrastive learning](https://arxiv.org/abs/2011.09157v2) -> DCL\n    - [Nearest-Neighbor Contrastive Learning of Visual Representations](https://arxiv.org/abs/2104.14548v2) -> NNCLR","metadata":{}},{"cell_type":"code","source":"class SimSiam(keras.Model):\n    def __init__(self, feature_extractor, embed_dims = 256, multiview = False,\n                 probe = False, probe_heads = None, probe_activation = \"sigmoid\", **kwargs):\n        super().__init__(**kwargs)\n        self.feature_extractor = feature_extractor #attentive pooling을 거친 encoder로 가정 : rep_vector, attention_Weight를 output으로\n        self.embed_dims = embed_dims\n        self.multiview = multiview\n        \n        self.train_type = 'SimSiam'\n        self.loss_tracker = keras.metrics.Mean(name = f\"{self.train_type}_loss\")\n        self.probe = probe ; self.probe_heads = probe_heads\n        self.probe_activation = probe_activation\n        \n        print(\"Train with Gradient Accumulation is recommended!\")\n        \n        self.embed_dims = embed_dims\n        self.predictor = keras.Sequential([keras.layers.Dense(units = self.embed_dims,\n                                                            use_bias = False,\n                                                            kernel_regularizer = keras.regularizers.l2(5e-4),\n                                                            ),\n                                          keras.layers.Activation(\"gelu\"),\n                                          keras.layers.BatchNormalization(),\n                                          keras.layers.Dense(units = self.embed_dims),\n                                          ], name = f\"{self.train_type}_predictor\")\n        if self.probe:\n            self.linear_probe = Dense(units = self.probe_heads, \n                                     activation = self.probe_activation)\n            self.probe_loss_tracker = tf.keras.metrics.Mean(\"Probe_loss_tracker\")\n            self.probe_metrics = [keras.metrics.Accuracy(name=\"probe_accuracy\"),\n                                  keras.metrics.F1Score(average=\"micro\", threshold=None, name=\"probe_micro_f1_score\"),\n                                 keras.metrics.TruePositives(name = \"probe_TP\"),\n                                 keras.metrics.TrueNegatives(name = \"probe_TN\"),\n                                 keras.metrics.FalsePositives(name = \"probe_FP\"),\n                                 keras.metrics.FalseNegatives(name = \"probe_FN\")]\n            if self.probe_activation == \"sigmoid\":\n                self.probe_loss_fn = keras.losses.BinaryCrossentropy(reduction = None, label_smoothing = 0.1)\n            elif self.probe_activation == \"softmax\":\n                self.probe_loss_fn = keras.losses.CategoricalCrossentropy(reduction = None, label_smoothing = 0.1)\n    def get_config(self):\n        return {\"feature_extractor_name\" : self.feature_extractor.name,\n               \"embed_dims\" : self.embed_dims,\n                \"Multiview(>2)\" : self.multiview,\n               \"SSL_method\" : \"SimSiam\",\n               \"Linear Probe\" : self.probe,\n               \"N_Categories\" : self.probe_categories if self.probe else 0,\n               \"Probe Activation\" : self.probe_act if self.probe else \"NA\"}\n    def compile(self, optimizer, probe_optimizer = None, **kwargs):\n        super().compile(**kwargs)\n        self.optimizer = optimizer\n        if self.probe:\n            self.probe_optimizer = probe_optimizer \n    def compute_loss(self, p, z, epsilon = 1e-5):\n        # stop gradient is essential in SimSiam structure\n        # p, z is representation vectors : batch_size, embed_dims\n        \n        z = ops.stop_gradient(p)\n        z = keras.utils.normalize(z, axis = -1, order = 2)\n        p = keras.utils.normalize(p, axis = -1, order = 2)\n        cos_sim = ops.mean(ops.sum(z*p, axis = -1)\n                          ) #batchwise mean\n        return -cos_sim\n\n    def train_step(self, dataset):\n        if self.probe:\n            img1, img2, labels = dataset\n        else:\n            if self.multiview:\n                img1 = dataset[0]\n                img2 = dataset[1:] ; n_augs = len(img2)\n                img2 = ops.concatenate(img2, axis = 0)\n            else:\n                img1, img2 = dataset\n        \n        with tf.GradientTape() as tape:\n            if len(self.feature_extractor.outputs) == 2:\n                z1, weight1 = self.feature_extractor(img1, training = True)\n                z2, weight2 = self.feature_extractor(img2, training = True)\n            elif len(self.feature_extractor.outputs) == 1:\n                z1 = self.feature_extractor(img1, training = True)\n                z2 = self.feature_extractor(img2, training = True)\n            v1, v2 = self.predictor(z1, training = True), self.predictor(z2, training = True)\n            \n            if self.multiview:\n                v2 = ops.split(v2, n_augs, 0)\n                loss = []\n                for idx in range(n_augs):\n                    loss.append(self.compute_loss(v1, v2[idx])\n                               )\n                loss = ops.mean(loss)\n            else:\n                loss = 0.5*(self.compute_loss(v1, v2) + self.compute_loss(v2, v1))\n        trainable_params = self.feature_extractor.trainable_variables + self.predictor.trainable_variables\n        grads = tape.gradient(loss, trainable_params)\n        self.optimizer.apply_gradients(zip(grads, trainable_params))\n        self.loss_tracker.update_state(loss)\n        output_dict = {self.loss_tracker.name : self.loss_tracker.result()}\n        \n        if self.probe:\n            with tf.GradientTape() as tape:\n                class_logits = self.linear_probe(z1)\n                probe_loss = self.probe_loss_fn(y_true = labels, y_pred = class_logits)\n                probe_loss = ops.mean(probe_loss)\n            grads = tape.gradient(probe_loss, self.linear_probe.trainable_weights)\n            self.probe_optimizer.apply_gradients(zip(grads, self.linear_probe.trainable_weights))\n            self.probe_loss_tracker.update_state(probe_loss)\n            output_dict.update({\"probe_classification_loss\" : self.probe_loss_tracker.result()})\n            #metrics\n            for comp in self.probe_metrics:\n                comp.update_state(labels, class_logits)\n                \n            probe_metric_dict = {comp.name :  comp.result() for comp in self.probe_metrics}\n            output_dict.update(probe_metric_dict)\n        del tape\n        return output_dict\n    \n    def test_step(self, dataset):\n        if self.probe:\n            img1, img2, labels = dataset\n        else:\n            if self.multiview:\n                img1 = dataset[0]\n                img2 = dataset[1:] ; n_augs = len(img2)\n                img2 = ops.concatenate(img2, axis = 0)\n            else:\n                img1, img2 = dataset\n        \n        if len(self.feature_extractor.outputs) == 2:\n            z1, weight1 = self.feature_extractor(img1, training = False)\n            z2, weight2 = self.feature_extractor(img2, training = False)\n        elif len(self.feature_extractor.outputs) == 1:\n            z1 = self.feature_extractor(img1, training = False)\n            z2 = self.feature_extractor(img2, training = False)\n        v1, v2 = self.predictor(z1, training = False), self.predictor(z2, training = False)\n        if self.multiview:\n            v2 = ops.split(v2, n_augs, 0)\n            loss = []\n            for idx in range(n_augs):\n                loss.append(self.compute_loss(v1, v2[idx])\n                            )\n            loss = ops.mean(loss)\n        else:\n            loss = 0.5*(self.compute_loss(v1, v2) + self.compute_loss(v2, v1))\n        \n        self.loss_tracker.update_state(loss)\n        output_dict = {self.loss_tracker.name : self.loss_tracker.result()}\n        \n        if self.probe:\n            class_logits = self.linear_probe(z1)\n            probe_loss = self.probe_loss_fn(y_true = labels, y_pred = class_logits)\n            probe_loss = ops.mean(probe_loss)\n            self.probe_loss_tracker.update_state(probe_loss)\n            output_dict.update({\"probe_classification_loss\" : self.probe_loss_tracker.result()})\n            for comp in self.probe_metrics:\n                comp.update_state(labels, class_logits)\n                \n            probe_metric_dict = {comp.name :  comp.result() for comp in self.probe_metrics}\n            output_dict.update(probe_metric_dict)\n        \n        return output_dict\n    \n    def call(self, dataset):\n        return self.test_step(dataset)","metadata":{"execution":{"iopub.status.busy":"2024-06-04T07:54:32.572690Z","iopub.execute_input":"2024-06-04T07:54:32.573206Z","iopub.status.idle":"2024-06-04T07:54:32.617915Z","shell.execute_reply.started":"2024-06-04T07:54:32.573145Z","shell.execute_reply":"2024-06-04T07:54:32.616756Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"class SimCLR(keras.Model):\n    def __init__(self, feature_extractor, embed_dims = 256, multiview = False, probe = False,\n                 temperature = 0.1,\n                 **kwargs):\n        super().__init__(**kwargs)\n        self.feature_extractor = feature_extractor #attentive pooling을 거친 encoder로 가정 : rep_vector, attention_Weight를 output으로\n        self.embed_dims = embed_dims\n        self.multiview = multiview\n        self.t = temperature\n        self.train_type = 'SimCLR'\n        print(\"Train with Gradient Accumulation is recommended!\")\n        \n        self.embed_dims = embed_dims\n        self.predictor = keras.Sequential([keras.layers.Dense(units = self.embed_dims,\n                                                            use_bias = False,\n                                                            ),\n                                          keras.layers.Activation(\"gelu\"),\n                                          keras.layers.LayerNormalization(),\n                                          keras.layers.Dense(units = self.embed_dims),\n                                          keras.layers.Activation(\"gelu\"),\n                                          keras.layers.LayerNormalization(),\n                                          keras.layers.Dense(units = self.embed_dims),\n                                          ], name = f\"{self.train_type}_predictor\")\n        self.probe = False\n        \n        self.loss_tracker = keras.metrics.Mean(name = f\"{self.train_type}_loss\")\n        \n    def get_config(self):\n        return {\"feature_extractor_name\" : self.feature_extractor.name,\n               \"embed_dims\" : self.embed_dims,\n                \"Multiview(>2)\" : self.multiview,\n               \"SSL_method\" : \"SimCLR\",\n               \"Linear Probe\" : self.probe,\n               \"N_Categories\" : self.probe_categories if self.probe else 0,\n               \"Probe Activation\" : self.probe_act if self.probe else \"NA\"}\n    def compile(self, optimizer, probe_optimizer = None, **kwargs):\n        super().compile(**kwargs)\n        self.optimizer = optimizer\n        if self.probe:\n            self.probe_optimizer = probe_optimizer \n\n    def train_step(self, dataset):\n        if self.probe:\n            img1, img2, labels = dataset\n        else:\n            if self.multiview:\n                img1 = dataset[0]\n                img2 = dataset[1:] ; n_augs = len(img2)\n                img2 = ops.concatenate(img2, axis = 0)\n            else:\n                img1, img2 = dataset\n        \n        with tf.GradientTape() as tape:\n            if len(self.feature_extractor.outputs) == 2:\n                z1, weight1 = self.feature_extractor(img1, training = True)\n                z2, weight2 = self.feature_extractor(img2, training = True)\n            elif len(self.feature_extractor.outputs) == 1:\n                z1 = self.feature_extractor(img1, training = True)\n                z2 = self.feature_extractor(img2, training = True)\n            v1, v2 = self.predictor(z1, training = True), self.predictor(z2, training = True)\n            \n            if self.multiview:\n                v2 = ops.split(v2, n_augs, 0)\n                loss = []\n                for idx in range(n_augs):\n                    loss.append(keras_cv.losses.SimCLRLoss(temperature = 0.1, reduction = None)(v1, v2[idx]))\n                    \n                loss = ops.mean(loss)\n                \n            else:\n                loss = keras_cv.losses.SimCLRLoss(temperature = 0.1)(v1, v2)\n        trainable_params = self.feature_extractor.trainable_variables + self.predictor.trainable_variables\n        grads = tape.gradient(loss, trainable_params)\n        self.optimizer.apply_gradients(zip(grads, trainable_params))\n        self.loss_tracker.update_state(loss)\n        output_dict = {self.loss_tracker.name : self.loss_tracker.result(),\n                      }\n        return output_dict\n    \n    def test_step(self, dataset):\n        if self.probe:\n            img1, img2, labels = dataset\n        else:\n            if self.multiview:\n                img1 = dataset[0]\n                img2 = dataset[1:] ; n_augs = len(img2)\n                img2 = ops.concatenate(img2, axis = 0)\n            else:\n                img1, img2 = dataset\n        \n        if len(self.feature_extractor.outputs) == 2:\n            z1, weight1 = self.feature_extractor(img1, training = True)\n            z2, weight2 = self.feature_extractor(img2, training = True)\n        elif len(self.feature_extractor.outputs) == 1:\n            z1 = self.feature_extractor(img1, training = True)\n            z2 = self.feature_extractor(img2, training = True)\n        v1, v2 = self.predictor(z1, training = False), self.predictor(z2, training = False)\n            \n        if self.multiview:\n            v2 = ops.split(v2, n_augs, 0)\n            loss = []\n            for idx in range(n_augs):\n                loss.append(keras_cv.losses.SimCLRLoss(temperature = 0.1, reduction = None)(v1, v2[idx]))        \n            loss = ops.mean(loss)\n            \n        else:\n            loss = keras_cv.losses.SimCLRLoss(temperature = 0.1)(v1, v2)\n        \n        self.loss_tracker.update_state(loss)\n        output_dict = {self.loss_tracker.name : self.loss_tracker.result(),\n                      }\n        return output_dict\n    \n    def call(self, dataset):\n        return self.test_step(dataset)","metadata":{"execution":{"iopub.status.busy":"2024-06-04T07:54:32.619328Z","iopub.execute_input":"2024-06-04T07:54:32.619667Z","iopub.status.idle":"2024-06-04T07:54:32.652177Z","shell.execute_reply.started":"2024-06-04T07:54:32.619639Z","shell.execute_reply":"2024-06-04T07:54:32.650853Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"class Moco(keras.Model):\n    def __init__(self, feature_extractor, embed_dims, q_size = 2**13, pool_heads = 8, t = 0.07,\n                 momentum_coefficient = 0.999,\n                 **kwargs):\n        super().__init__(**kwargs)\n        self.feature_extractor = feature_extractor\n        self.momentum_encoder = feature_extractor\n        self.momentum_encoder.set_weights(self.feature_extractor.get_weights())\n        self.m = momentum_coefficient\n        self.pool_heads = pool_heads\n        self.t = t\n        self.q_size = q_size\n        self.embed_dims = embed_dims\n        self.feature_queue = keras.Variable(\n            keras.utils.normalize(\n                keras.random.normal(shape=(self.q_size, self.embed_dims)),\n                axis=1,\n                order=2,\n            ),\n            trainable=False, dtype = \"float32\"\n        )\n        self.projector = keras.Sequential([keras.layers.Dense(units = embed_dims),\n                                           keras.layers.Activation(\"relu\"),\n                                          keras.layers.Dense(units = embed_dims, dtype = \"float32\")])\n        self.momentum_projector = keras.models.clone_model(self.projector)\n        \n        self.ce_loss_fn = keras.losses.SparseCategoricalCrossentropy()\n        self.loss_tracker = keras.metrics.Mean(name = \"MoCo_loss\")\n    def get_config(self):\n        return {\"feature_extractor_name\" : self.feature_extractor.name,\n               \"embed_dims\" : self.embed_dims,\n               \"SSL_method\" : \"MoCo\",\n               \"Queue size\" : self.q_size,\n               \"temperature\" : self.t,\n               \"Momentum\" : self.m}\n    def compute_loss(self, z_a, z_b, training = True): #Info-NCE loss in the original paper\n        z_b = ops.stop_gradient(z_b)\n        z_a, z_b = keras.utils.normalize(z_a, axis = -1, order = 2), keras.utils.normalize(z_b, axis = -1, order = 2)\n        z_a, z_b = ops.cast(z_a, \"float32\"), ops.cast(z_b, \"float32\")\n        \n        pos_sim_ = ops.diagonal(z_a@ops.transpose(z_b)) ; pseudolabel = ops.zeros_like(pos_sim_)\n        pos_pair_similarity = ops.expand_dims(pos_sim_, axis = -1) ; del pos_sim_\n        \n        neg_pair_similarity = z_a@ops.cast(ops.transpose(self.feature_queue), \"float32\")\n        logits = ops.concatenate([pos_pair_similarity, neg_pair_similarity], axis = 1)\n        logits = ops.exp(logits/self.t)\n        logits = logits / (ops.sum(logits, axis = -1, keepdims = True) + 1e-8)\n        loss = self.ce_loss_fn(y_true = pseudolabel, y_pred = logits)\n        \n        if training:\n            self.feature_queue.assign(\n                ops.concatenate([z_a, ops.cast(self.feature_queue[:-ops.shape(z_a)[0] ],\n                                              \"float32\")\n                                ], axis=0)\n            )\n\n        return loss\n    def train_step(self, dataset):\n        img1, img2 = dataset\n        with tf.GradientTape() as tape:\n            if len(self.feature_extractor.outputs) == 2:\n                z1, weight1 = self.feature_extractor(img1)\n                z2, weight2 = self.momentum_encoder(img2)\n            elif len(self.feature_extractor.outputs) == 1:\n                z1 = self.feature_extractor(img1)\n                z2 = self.momentum_encoder(img2)\n            v1, v2 = self.projector(z1), self.momentum_projector(z2)\n            moco_loss = self.compute_loss(v1, v2)\n        grads = tape.gradient(moco_loss, \n                             self.feature_extractor.trainable_weights + self.projector.trainable_weights)\n        self.optimizer.apply_gradients(zip(grads, \n                                          self.feature_extractor.trainable_weights + self.projector.trainable_weights)\n                                      )\n        self.loss_tracker.update_state(moco_loss)\n        \n        #momentum update\n        for weight, m_weight in zip(self.feature_extractor.weights, self.momentum_encoder.weights):\n            m_weight.assign(self.m * m_weight + (1-self.m)*weight)\n        for weight, m_weight in zip(self.projector.weights, self.momentum_projector.weights):\n            m_weight.assign(self.m * m_weight + (1-self.m)*weight)\n        return {self.loss_tracker.name : self.loss_tracker.result()}\n    def test_step(self, dataset):\n        img1, img2 = dataset\n        if len(self.feature_extractor.outputs) == 2:\n            z1, weight1 = self.feature_extractor(img1)\n            z2, weight2 = self.momentum_encoder(img2)\n        elif len(self.feature_extractor.outputs) == 1:\n            z1 = self.feature_extractor(img1)\n            z2 = self.momentum_encoder(img2)\n        v1, v2 = self.projector(z1), self.momentum_projector(z2)\n        moco_loss = self.compute_loss(v1, v2, training = False)\n        self.loss_tracker.update_state(moco_loss)\n        return {self.loss_tracker.name : self.loss_tracker.result()}\n    def call(self, dataset):\n        img1, img2 = dataset\n        if len(self.feature_extractor.outputs) == 2:\n            z1, weight1 = self.feature_extractor(img1)\n            z2, weight2 = self.momentum_encoder(img2)\n        elif len(self.feature_extractor.outputs) == 1:\n            z1 = self.feature_extractor(img1)\n            z2 = self.momentum_encoder(img2)\n        v1, v2 = self.projector(z1), self.momentum_projector(z2)\n        moco_loss = self.compute_loss(v1, v2, training = False)\n        self.loss_tracker.update_state(moco_loss)\n        return {self.loss_tracker.name : self.loss_tracker.result()}","metadata":{"execution":{"iopub.status.busy":"2024-06-04T07:54:32.653896Z","iopub.execute_input":"2024-06-04T07:54:32.654444Z","iopub.status.idle":"2024-06-04T07:54:32.689787Z","shell.execute_reply.started":"2024-06-04T07:54:32.654400Z","shell.execute_reply":"2024-06-04T07:54:32.688736Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"----------","metadata":{}},{"cell_type":"markdown","source":"# Clustering & Distillation\n- SwAV, DINO\n\n> DINO architecture:\n\n![](https://miro.medium.com/v2/resize:fit:1400/1*huuMgEbBryxXUufW33uhvQ.png)","metadata":{}},{"cell_type":"code","source":"class DINO(keras.Model): #g : feature extractor + predictor\n    def __init__(self, feature_extractor, apply_simclr, apply_barlow,\n                 embed_dims = 1024, multiview = False, probe = False,\n                 teacher_t = 0.1, student_t = 0.1,\n                 **kwargs):\n        super().__init__(**kwargs)\n        self.feature_extractor = feature_extractor #output as feature vector and attention weight\n        self.student_extractor = feature_extractor\n        for f_t, f_p in zip(self.feature_extractor.weights, self.student_extractor.weights):\n            f_t.assign(f_p) \n            \n        self.embed_dims = embed_dims\n        self.multiview = multiview\n        self.teacher_t = teacher_t\n        self.student_t = student_t\n        self.c = tf.Variable(keras.random.normal(shape = (embed_dims,),\n                                      dtype = tf.float32)\n                            )\n        self.train_type = 'DINO'\n        self.embed_dims = embed_dims\n        self.teacher_predictor = keras.Sequential([keras.layers.Dense(units = self.embed_dims,\n                                                            use_bias = False,\n                                                            ),\n                                          keras.layers.Activation(\"relu\"),\n                                          keras.layers.LayerNormalization(),\n                                          keras.layers.Dense(units = self.embed_dims),\n                                        keras.layers.Activation(\"relu\"),\n                                          keras.layers.LayerNormalization(),\n                                          keras.layers.Dense(units = self.embed_dims),\n                                            keras.layers.Activation('relu', dtype = 'float32')\n                                          ], name = f\"Teacher_{self.train_type}_predictor\")\n        self.student_predictor = keras.Sequential([keras.layers.Dense(units = self.embed_dims,\n                                                            use_bias = False,\n                                                            ),\n                                          keras.layers.Activation(\"relu\"),\n                                          keras.layers.LayerNormalization(),\n                                          keras.layers.Dense(units = self.embed_dims),\n                                        keras.layers.Activation(\"relu\"),\n                                          keras.layers.LayerNormalization(),\n                                          keras.layers.Dense(units = self.embed_dims),\n                                            keras.layers.Activation('relu', dtype = 'float32')\n                                          ], name = f\"Student_{self.train_type}_predictor\")\n        for s_t, s_p in zip(self.teacher_predictor.weights, self.student_predictor.weights):\n            s_t.assign(s_p)\n        \n        self.probe = False\n        self.apply_simclr = apply_simclr\n        self.apply_barlow = apply_barlow\n        self.loss_tracker = keras.metrics.Mean(name = \"Total_loss\")\n        self.dino_tracker = keras.metrics.Mean(name = f\"{self.train_type}_loss\")\n        self.simclr_tracker = keras.metrics.Mean(name = \"SimCLR_loss\")\n        self.barlow_tracker = keras.metrics.Mean(name = \"Barlow_loss\")\n    def get_config(self):\n        return {\"feature_extractor_name\" : self.feature_extractor.name,\n               \"embed_dims\" : self.embed_dims,\n                \"Multiview(>2)\" : self.multiview,\n               \"SSL_method\" : \"DINOv1\",\n               \"Linear Probe\" : self.probe,\n               \"N_Categories\" : self.probe_categories if self.probe else 0,\n               \"Probe Activation\" : self.probe_act if self.probe else \"NA\",\n               \"Apply SimCLR\" : self.apply_simclr,\n               \"Apply Barlow\" : self.apply_barlow}\n    def compile(self, optimizer, probe_optimizer = None, **kwargs):\n        super().compile(**kwargs)\n        self.optimizer = optimizer\n        if self.probe:\n            self.probe_optimizer = probe_optimizer \n    def compute_h(self, t, s):\n        # t, s : embedding vector of teacher/student network (feature extractor + MLPs)\n        # C: centering coefficient, updated as EMA (teacher output의 평균)\n        c = self.c\n        t = tf.stop_gradient(t)\n        s = ops.softmax(s/self.student_t, \n                        axis = -1)\n        t = ops.softmax(((t - c)/self.teacher_t),\n                        axis = -1)\n        loss = -ops.mean(t*ops.log(s + 1e-5))\n        \n        return loss\n        \n    def train_step(self, dataset):\n        view_indices = list(range(len(dataset)))\n        if self.probe:\n            global_view, local_view, labels = dataset\n            n_global, n_local = 1, 1\n            total_view = ops.concatenate(dataset, axis = 0)\n        else:\n            if self.multiview:\n                global_view = dataset[0:2]\n                \n                n_global = len(global_view)\n                n_local = len(dataset) - n_global\n                \n                global_view = ops.concatenate(global_view, axis = 0)\n                total_view = ops.concatenate(dataset, axis = 0) #global_1, global_2, locals\n            else:\n                global_view, local_view = dataset\n                n_global, n_local = 1, 1\n                total_view = ops.concatenate(dataset, axis = 0)\n        # teacher -> global views, student -> global and local views\n        \n        with tf.GradientTape() as tape:\n            if len(self.feature_extractor.outputs) == 2:\n                z_global_teacher, global_weight_teacher = self.feature_extractor(global_view, training = True)\n                z_total_student, total_weight_student = self.student_extractor(total_view, training = True)\n                \n            elif len(self.feature_extractor.outputs) == 1:\n                z_global_teacher = self.feature_extractor(global_view, training = True)\n                z_total_student = self.student_extractor(total_view, training = True)\n                \n            v_teacher = self.teacher_predictor(z_global_teacher)\n            v_student = self.student_predictor(z_total_student)\n            \n            if self.multiview:\n                dino_loss = []\n                simclr_loss = []\n                barlow_loss = []\n                #start from here    \n                teacher_set = ops.split(v_teacher, n_global, 0)\n                student_set = ops.split(v_student, (n_global + n_local), 0)\n                teacher_indices = list(range(n_global))\n                \n                for idx in teacher_indices: #0, 1\n                    t = teacher_set[idx]\n                    view_indices.remove(idx)\n                    for j in view_indices: #(0), 1, 2, 3\n                        s = student_set[j]\n                        loss_ = self.compute_h(t, s) ; loss_ = ops.clip(loss_, -10.0, 10**4)\n                        dino_loss.append(loss_)\n                        simclr_loss_ = keras_cv.losses.SimCLRLoss(temperature = self.teacher_t, \n                                                                  name = \"simclr_loss_compute\")(t,s)\n                        simclr_loss.append(simclr_loss_)\n                        b_loss_ = BarlowLoss()(t, s)\n                        barlow_loss.append(b_loss_)\n                    view_indices.append(idx)\n                dino_loss = ops.mean(dino_loss)\n                simclr_loss = ops.mean(simclr_loss)\n                barlow_loss = ops.mean(barlow_loss)\n                loss = dino_loss + (self.apply_simclr * 0.5 * simclr_loss) + (self.apply_barlow * barlow_loss)\n            else:\n                dino_loss = self.compute_h(v_teacher, v_student)\n                simclr_loss = keras_cv.losses.SimCLRLoss(temperature = self.teacher_t, name = \"simclr_loss_compute\")(v_teacher, v_student)\n                barlow_loss = BarlowLoss()(v_teacher, v_student)\n                loss = dino_loss + (self.apply_simclr * 0.5 * simclr_loss) + (self.apply_barlow * barlow_loss)\n        #Track hyperparams and metrics\n        mean_val = ops.mean(v_teacher, axis = 0)\n        self.c.assign(0.99*self.c + (1-0.99)*mean_val)\n        \n        self.loss_tracker.update_state(loss)\n        self.dino_tracker.update_state(dino_loss)\n        self.simclr_tracker.update_state(simclr_loss)\n        self.barlow_tracker.update_state(barlow_loss)\n        \n        output_dict = {self.loss_tracker.name : self.loss_tracker.result(),\n                       self.dino_tracker.name : self.dino_tracker.result(),\n                       self.simclr_tracker.name : self.simclr_tracker.result(),\n                       self.barlow_tracker.name : self.barlow_tracker.result(),\n                      \"DINO_feature_std\" : ops.std(z_global_teacher)\n                      }\n        \n        #Update student params as backprop\n        student_params = self.student_extractor.trainable_variables + self.student_predictor.trainable_variables\n        grads = tape.gradient(loss, student_params)\n        self.optimizer.apply_gradients(zip(grads, student_params))\n        #Update teacher params as EMA\n        #lambda = 0.999\n        teacher_feature_w, teacher_predictor_w = [],[]\n        l_ = 0.999\n        #print(\"teacher EMA\")\n        for f_teacher_part, f_student_part in zip(self.feature_extractor.weights, self.student_extractor.weights):\n            f_teacher_part.assign(l_*f_teacher_part + (1-l_)*f_student_part)\n        #self.feature_extractor.set_weights(teacher_feature_w)\n        \n        #print(\"predictor EMA\")\n        for p_teacher_part, p_student_part in zip(self.teacher_predictor.weights, self.student_predictor.weights):\n            p_teacher_part.assign(l_*p_teacher_part + (1-l_)*p_student_part)\n        #self.teacher_predictor.set_weights(teacher_predictor_w)\n        \n        return output_dict\n    \n    def test_step(self, dataset):\n        view_indices = list(range(len(dataset)))\n        if self.probe:\n            global_view, local_view, labels = dataset\n            n_global, n_local = 1, 1\n            total_view = ops.concatenate(dataset, axis = 0)\n        else:\n            if self.multiview:\n                global_view = dataset[0:2]\n                \n                n_global = len(global_view)\n                n_local = len(dataset) - n_global\n                \n                global_view = ops.concatenate(global_view, axis = 0)\n                total_view = ops.concatenate(dataset, axis = 0) #global_1, global_2, locals\n            else:\n                global_view, local_view = dataset\n                n_global, n_local = 1, 1\n                total_view = ops.concatenate(dataset, axis = 0)\n        # teacher -> global views, student -> global and local views\n        \n        if len(self.feature_extractor.outputs) == 2:\n            z_global_teacher, global_weight_teacher = self.feature_extractor(global_view, training = True)\n            z_total_student, total_weight_student = self.student_extractor(total_view, training = True)\n                \n        elif len(self.feature_extractor.outputs) == 1:\n            z_global_teacher = self.feature_extractor(global_view, training = True)\n            z_total_student = self.student_extractor(total_view, training = True)\n                \n        v_teacher = self.teacher_predictor(z_global_teacher)\n        v_student = self.student_predictor(z_total_student)\n        \n        if self.multiview:\n            #start from here \n            dino_loss = []\n            simclr_loss = []\n            barlow_loss = []\n            teacher_set = ops.split(v_teacher, n_global, 0)\n            student_set = ops.split(v_student, (n_global + n_local), 0)\n            teacher_indices = list(range(n_global))\n                \n            for idx in teacher_indices:\n                t = teacher_set[idx]\n                view_indices.remove(idx)\n                for j in view_indices:\n                    s = student_set[j]\n                    loss_ = self.compute_h(t, s) ; loss_ = ops.clip(loss_, -10.0, 10**4)\n                    dino_loss.append(loss_)\n                    simclr_loss_ = keras_cv.losses.SimCLRLoss(temperature = self.teacher_t, name = \"simclr_loss_compute\")(t,s)\n                    simclr_loss.append(simclr_loss_)\n                    b_loss_ = BarlowLoss()(t,s)\n                    barlow_loss.append(b_loss_)\n                view_indices.append(idx)\n            dino_loss = ops.mean(dino_loss)\n            simclr_loss = ops.mean(simclr_loss)\n            barlow_loss = ops.mean(barlow_loss)\n            loss = dino_loss + (self.apply_simclr * 0.5 * simclr_loss) + (self.apply_barlow * barlow_loss)\n        else:\n            dino_loss = self.compute_h(v_teacher, v_student)\n            simclr_loss = keras_cv.losses.SimCLRLoss(temperature = self.teacher_t, name = \"simclr_loss_compute\")(v_teacher, v_student)\n            barlow_loss = BarlowLoss()(v_teacher, v_student)\n            loss = dino_loss + (self.apply_simclr * 0.5 * simclr_loss) + (self.apply_barlow * barlow_loss)\n        #Track hyperparams and metrics\n        self.loss_tracker.update_state(loss)\n        self.dino_tracker.update_state(dino_loss)\n        self.simclr_tracker.update_state(simclr_loss)\n        self.barlow_tracker.update_state(barlow_loss)\n        \n        output_dict = {self.loss_tracker.name : self.loss_tracker.result(),\n                       self.dino_tracker.name : self.dino_tracker.result(),\n                       self.simclr_tracker.name : self.simclr_tracker.result(),\n                       self.barlow_tracker.name : self.barlow_tracker.result(),\n                      \"DINO_feature_std\" : ops.std(z_global_teacher)\n                      }\n        \n        return output_dict\n    \n    def call(self, dataset):\n        return self.test_step(dataset)","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-06-04T07:54:32.691764Z","iopub.execute_input":"2024-06-04T07:54:32.692125Z","iopub.status.idle":"2024-06-04T07:54:32.755193Z","shell.execute_reply.started":"2024-06-04T07:54:32.692094Z","shell.execute_reply":"2024-06-04T07:54:32.753940Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"> iBOT 및 DINOv2\n\n- iBOT architecture: MIM, online tokenizing!\n![](https://velog.velcdn.com/images%2Frucola-pizza%2Fpost%2F4dc0785f-7072-4d03-8374-5bbccd8391b4%2F%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202022-03-10%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%202.01.16.png)\n\n- DINOv2:\n    - iBOT + Sinkhorn-knopp in teacher softmax + untying projection heads (student, teacher)","metadata":{}},{"cell_type":"code","source":"def sinkhorn(feature_vector_logits):\n    \"\"\"\n    Applies the Sinkhorn-Knopp algorithm to normalize feature vector logits.\n    \n    Args:\n    - feature_vector_logits (tf.Tensor): The input logits to be normalized.\n    \n    Returns:\n    - tf.Tensor: The doubly stochastic matrix.\n    \"\"\"\n    \n    # Exponentiate the logits\n    Q = tf.transpose(tf.exp(feature_vector_logits))\n    \n    # Normalize the entire matrix\n    Q /= tf.reduce_sum(Q)\n    \n    # Get dimensions K (number of rows) and B (number of columns)\n    K = tf.shape(Q)[0]\n    B = tf.shape(Q)[1]\n    \n    # Initialize u, r, and c\n    u = tf.zeros(K, dtype=tf.float32)\n    r = tf.ones(K, dtype=tf.float32) / (tf.cast(K, tf.float32) + 1e-7)\n    c = tf.ones(B, dtype=tf.float32) / (tf.cast(B, tf.float32) + 1e-7)\n    \n    # Sinkhorn iterations\n    for _ in range(3):\n        u = tf.reduce_sum(Q, axis=1)\n        Q *= tf.expand_dims((r / u), axis=1)\n        Q *= tf.expand_dims(c / tf.reduce_sum(Q, axis=0), 0)\n    \n    # Final normalization\n    final_quantity = Q / (tf.reduce_sum(Q, axis=0, keepdims=True) + 1e-7)\n    final_quantity = tf.transpose(final_quantity)\n    \n    return final_quantity\n\ndef compute_h(s, t,\n              c = 0.0, t_t = 0.05, t_s = 0.05, teacher_softmax = True):\n    t = tf.stop_gradient(t)\n    s = ops.softmax(s/t_s , axis = -1)\n    if teacher_softmax:\n        t = ops.softmax((  (t - c)/t_t  ),\n                            axis = -1)\n    else:\n        t = sinkhorn((t - c)/t_t)\n    return -ops.sum(t*ops.log(s + 1e-6) , axis = -1)\n\ndef compute_cls_loss(cls_a, cls_b, c, t_t, t_s, teacher_softmax):\n    cls_loss = compute_h(cls_a, cls_b, teacher_softmax = teacher_softmax,\n                            c = c, t_t = t_t, t_s = t_s)\n    return cls_loss\ndef compute_mim_loss(m_u, #masked parameters : 0 if non-masked, 1 if masked\n                         patch_u_s, patch_u_t, #patch embedding\n                         c, t_t, t_s, teacher_softmax):\n        #m_u : batch, seq_len shape\n        #patches : batch, seq_len, dim shape\n    mim_loss_u = m_u*compute_h(patch_u_s, patch_u_t, teacher_softmax = teacher_softmax,\n                            c = c, t_t = t_t, t_s = t_s)\n    mim_loss_u = ops.sum(mim_loss_u, axis = 1)/ops.sum(mim_loss_u, axis = 1)\n    return mim_loss_u\n\n\ndef compute_iBOT_loss(cls_v_student, cls_v_teacher,\n                      cls_u_student, cls_u_teacher,\n                      c_cls, t_t_cls, t_s_cls,\n                      \n                      m_u, patch_u_s, patch_u_t,\n                      m_v, patch_v_s, patch_v_t,\n                      c_mim, t_t_mim, t_s_mim,\n                      teacher_softmax):\n    \"\"\"Compute one-pair cls, mim loss\"\"\"\n    cls_v_loss = compute_cls_loss(cls_a = cls_v_student, cls_b = cls_v_teacher, \n                                  c = c_cls, t_t = t_t_cls, t_s = t_s_cls, teacher_softmax = teacher_softmax)\n    cls_u_loss = compute_cls_loss(cls_a = cls_u_student, cls_b = cls_u_teacher, \n                                  c = c_cls, t_t = t_t_cls, t_s = t_s_cls, teacher_softmax = teacher_softmax)\n    cls_loss = ops.mean(cls_v_loss + cls_u_loss)\n    \n    mim_u_loss = compute_mim_loss(m_u, patch_u_s, patch_u_t, c_mim, t_t_mim, t_s_mim, teacher_softmax)\n    mim_v_loss = compute_mim_loss(m_v, patch_v_s, patch_v_t, c_mim, t_t_mim, t_s_mim, teacher_softmax)\n        \n    mim_loss = ops.mean(mim_u_loss + mim_v_loss)\n    return cls_loss, mim_loss","metadata":{"execution":{"iopub.status.busy":"2024-06-04T07:54:32.756865Z","iopub.execute_input":"2024-06-04T07:54:32.757266Z","iopub.status.idle":"2024-06-04T07:54:32.780778Z","shell.execute_reply.started":"2024-06-04T07:54:32.757235Z","shell.execute_reply":"2024-06-04T07:54:32.779631Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"> iBOT loss의 경우,\n\n- 원본 논문에서 아래와 같은 비교 실험을 진행하였고,\n- 그 결과 case b가 가장 성능이 높았음\n    - $x$ : global view\n    - $y$ : local view\n    - mask generation : generate_mask function 이용, m_i , masked patches return\n    - image patching & embedding : ImagePatchEmbedding layer 이용 (att: patch_size, embed_dim)\n    \n![](https://ar5iv.labs.arxiv.org/html/2111.07832/assets/x9.png)","metadata":{}},{"cell_type":"code","source":"ibot_brainstorming = 0\nif ibot_brainstorming : \n    #generate_mask 이용 -> m_i, masked patches\n    batch_size_ = 16\n    embed_dims_ = 768\n    n_patches_ = 256\n    mask_rate_ = tf.random.uniform(shape = (), minval = 0.5, maxval = 0.8, seed = 1)\n    mask_token = tf.random.normal([1, embed_dims_])\n\n    # teacher -> global image only (2)\n    # student -> global and local views.\n\n    teacher_output_1 = [tf.random.normal([batch_size_, embed_dims_]), \n                     tf.ones([batch_size_, n_patches_, embed_dims_], dtype = tf.float32)]\n\n    teacher_output_2 = [tf.random.normal([batch_size_, embed_dims_]), \n                     tf.ones([batch_size_, n_patches_, embed_dims_], dtype = tf.float32)]\n\n    student_output_1, student_output_2, student_output_3, student_output_4 = tf.random.normal([batch_size_, n_patches_, embed_dims_]), tf.random.normal([batch_size_, n_patches_, embed_dims_]), tf.random.normal([batch_size_, n_patches_, embed_dims_]), tf.random.normal([batch_size_, n_patches_, embed_dims_])\n    m_s1, patch_s1 = generate_mask(student_output_1, tf.random.uniform(shape = (), minval = 0.4, maxval = 0.8, seed = 1), mask_token)\n    m_s2, patch_s2 = generate_mask(student_output_2, tf.random.uniform(shape = (), minval = 0.4, maxval = 0.8, seed = 1), mask_token)\n    m_s3, patch_s3 = generate_mask(student_output_3, tf.random.uniform(shape = (), minval = 0.4, maxval = 0.8, seed = 1), mask_token)\n    m_s4, patch_s4 = generate_mask(student_output_4, tf.random.uniform(shape = (), minval = 0.4, maxval = 0.8, seed = 1), mask_token)\n\n    ######compute loss####\n    mim_loss_a = compute_mim_loss(m_s1, patch_s1, teacher_output_2[-1], c = 0.5, t_t = 0.1, t_s = 0.1, teacher_softmax = True)\n    mim_loss_b = compute_mim_loss(m_s2, patch_s2, teacher_output_1[-1], c = 0.5, t_t = 0.1, t_s = 0.1, teacher_softmax = True)\n    mim_loss = ops.mean(mim_loss_a + mim_loss_b)\n\n    pool = keras.layers.GlobalAveragePooling1D()\n    cls_s1, cls_s2, cls_s3, cls_s4 = pool(patch_s1), pool(patch_s2), pool(patch_s3), pool(patch_s4)\n    cls_t1, cls_t2 = teacher_output_1[0], teacher_output_2[0]\n\n    cls_loss = []\n    cls_loss.append(compute_cls_loss(cls_s2, cls_t1, c = 0.5, t_t = 0.1, t_s = 0.1, teacher_softmax = True))\n    cls_loss.append(compute_cls_loss(cls_s3, cls_t1, c = 0.5, t_t = 0.1, t_s = 0.1, teacher_softmax = True))\n    cls_loss.append(compute_cls_loss(cls_s4, cls_t1, c = 0.5, t_t = 0.1, t_s = 0.1, teacher_softmax = True))\n    cls_loss.append(compute_cls_loss(cls_s1, cls_t1, c = 0.5, t_t = 0.1, t_s = 0.1, teacher_softmax = True))\n    cls_loss.append(compute_cls_loss(cls_s3, cls_t1, c = 0.5, t_t = 0.1, t_s = 0.1, teacher_softmax = True))\n    cls_loss.append(compute_cls_loss(cls_s4, cls_t1, c = 0.5, t_t = 0.1, t_s = 0.1, teacher_softmax = True))\n    cls_loss = ops.mean(cls_loss)\n    loss = mim_loss + cls_loss\n    print(f\"MIM Loss : {mim_loss}, CLS loss : {cls_loss}, Total loss : {loss}\")","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-06-04T07:54:32.787821Z","iopub.execute_input":"2024-06-04T07:54:32.788315Z","iopub.status.idle":"2024-06-04T07:54:32.809710Z","shell.execute_reply.started":"2024-06-04T07:54:32.788279Z","shell.execute_reply":"2024-06-04T07:54:32.808367Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"class iBOT(keras.Model): #g : feature extractor + predictor\n    def __init__(self, att_depth, att_dims, att_heads,\n                 patch_size = 16, embed_dims = 1024, multiview = False, probe = False,\n                 teacher_t = 0.1, student_t = 0.1, apply_simclr = False,\n                 **kwargs):\n        super().__init__(**kwargs)\n        self.att_depth = att_depth\n        self.att_dims = att_dims\n        self.att_heads = att_heads\n        self.patch_size = patch_size\n        self.embed_dims = embed_dims\n        self.multiview = multiview\n        self.train_type = 'iBOT'\n        self.embed_dims = embed_dims\n        self.probe = False\n        \n        # modelling\n        # image input -> patch embedding -> patch for teacher, masked patch for student -> get rep vector, get embed. patches\n        \n        self.patch_embedding_fn = ImagePatchEmbedding(patch_size = self.patch_size, embed_dim = self.embed_dims)\n        self.f_t = TRBlock(att_depth = self.att_depth, att_dims = self.att_dims, att_heads = self.att_heads, name = \"Teacher_Encoder\")\n        self.f_s = TRBlock(att_depth = self.att_depth, att_dims = self.att_dims, att_heads = self.att_heads, name = \"Student_Encoder\")\n        self.f_t.set_weights(self.f_s.get_weights())\n        self.teacher_predictor = keras.Sequential([keras.layers.Dense(units = self.embed_dims,\n                                                            use_bias = False,\n                                                            ),\n                                          keras.layers.LeakyReLU(negative_slope=0.3),\n                                          keras.layers.LayerNormalization(),\n                                          keras.layers.Dense(units = self.embed_dims),\n                                        keras.layers.LeakyReLU(negative_slope=0.3),\n                                          keras.layers.LayerNormalization(),\n                                          keras.layers.Dense(units = self.embed_dims),\n                                            keras.layers.LeakyReLU(negative_slope=0.3)\n                                          ], name = f\"Teacher_{self.train_type}_predictor\")\n        self.student_predictor = keras.Sequential([keras.layers.Dense(units = self.embed_dims, use_bias = False,),\n                                          keras.layers.LeakyReLU(negative_slope=0.3),\n                                          keras.layers.LayerNormalization(),\n                                          keras.layers.Dense(units = self.embed_dims),\n                                        keras.layers.LeakyReLU(negative_slope=0.3),\n                                          keras.layers.LayerNormalization(),\n                                          keras.layers.Dense(units = self.embed_dims),\n                                            keras.layers.LeakyReLU(negative_slope=0.3)\n                                          ], name = f\"Student_{self.train_type}_predictor\")\n        \n        self.teacher_patch_predictor = keras.Sequential([keras.layers.Dense(units = self.embed_dims,\n                                                            use_bias = False,\n                                                            ),\n                                          keras.layers.LeakyReLU(negative_slope=0.3),\n                                          keras.layers.LayerNormalization(),\n                                          keras.layers.Dense(units = self.embed_dims),\n                                        keras.layers.LeakyReLU(negative_slope=0.3),\n                                          keras.layers.LayerNormalization(),\n                                          keras.layers.Dense(units = self.embed_dims),\n                                            keras.layers.LeakyReLU(negative_slope=0.3)\n                                          ], name = f\"Teacher_{self.train_type}_predictor\")\n        self.student_patch_predictor = self.teacher_patch_predictor\n        \n        for s_t, s_p in zip(self.teacher_predictor.weights, self.student_predictor.weights):\n            s_t.assign(s_p)\n        for s_t, s_p in zip(self.teacher_patch_predictor.weights, self.student_patch_predictor.weights):\n            s_t.assign(s_p)\n        # About loss calculation\n        self.loss_tracker = keras.metrics.Mean(name = \"iBOT_loss\")\n        self.cls_tracker = keras.metrics.Mean(name = \"CLS_loss\") ; self.mim_tracker = keras.metrics.Mean(name = \"MIM_loss\")\n        self.apply_simclr = apply_simclr\n        self.simclr_tracker = keras.metrics.Mean(name = \"SimCLR_loss\")\n        \n        self.c_cls = tf.Variable(keras.random.normal(shape = (embed_dims,),\n                                      dtype = tf.float32)\n                            )\n        self.c_mim = tf.Variable(keras.random.normal(shape = (embed_dims,),\n                                      dtype = tf.float32)\n                            )\n        self.mask_token = tf.Variable(keras.random.normal(shape = (embed_dims,),\n                                      dtype = tf.float32)\n                            )\n        self.teacher_t = teacher_t\n        self.student_t = student_t\n    def get_config(self):\n        return {\"feature_extractor_name\" : self.feature_extractor.name,\n               \"embed_dims\" : self.embed_dims,\n                \"Multiview(>2)\" : self.multiview,\n               \"SSL_method\" : \"iBOT\",\n               \"Linear Probe\" : self.probe,\n               \"N_Categories\" : self.probe_categories if self.probe else 0,\n               \"Probe Activation\" : self.probe_act if self.probe else \"NA\",\n               \"Apply SimCLR\" : bool(self.apply_simclr),\n               }\n    def compile(self, optimizer, probe_optimizer = None, **kwargs):\n        super().compile(**kwargs)\n        self.optimizer = optimizer\n        if self.probe:\n            self.probe_optimizer = probe_optimizer \n\n        \n    def train_step(self, dataset):\n        view_indices = list(range(len(dataset)))\n        batch_size = ops.shape(dataset[0])[0]\n        \n        if self.probe:\n            global_view, local_view, labels = dataset\n            n_global, n_local = 1, 1\n            total_view = ops.concatenate(dataset, axis = 0)\n        else:\n            if self.multiview:\n                n_global = 2\n                n_local = len(dataset) - n_global\n                \n                #global_view = ops.concatenate(global_view, axis = 0)\n                total_view = ops.concatenate(dataset, axis = 0) #global_1, global_2, locals\n            else:\n                global_view, local_view = dataset\n                n_global, n_local = 1, 1\n                total_view = ops.concatenate(dataset, axis = 0)\n        # teacher -> global views, student -> global and local views\n        teacher_indices = list(range(n_global))\n        with tf.GradientTape() as tape:\n            total_view = self.patch_embedding_fn(total_view)\n            global_view = total_view[:n_global*batch_size, ...] #2B, patch_length, dims\n            local_view = total_view[n_global*batch_size:, ...] #N, patch_length, dims\n            \n            m_set, global_view_masked = generate_mask(global_view, \n                                                      tf.random.uniform(shape = (), minval = 0.4, maxval = 0.8), #<- mask probability\n                                                      self.mask_token)\n            \n            global_z_t, global_patches_t, att_weights_t = self.f_t([global_view, global_view])\n            global_z_masked_s, global_patches_masked_s, _ = self.f_s([global_view_masked, global_view_masked])\n            local_z_s, local_patches_s, _ = self.f_s([local_view, local_view])\n            del _, local_patches_s\n            \n            global_z_t, global_patches_t = self.teacher_predictor(global_z_t), self.teacher_patch_predictor(global_patches_t)\n            global_z_masked_s, global_patches_masked_s = self.student_predictor(global_z_masked_s), self.student_patch_predictor(global_patches_masked_s)\n            local_z_s = self.student_predictor(local_z_s)\n            \n            teacher_cls_set = ops.split(global_z_t, n_global, 0) ; del global_z_t\n            teacher_patches_set = ops.split(global_patches_t, n_global, 0) ; del global_patches_t\n                \n            student_global_cls_set = ops.split(global_z_masked_s, n_global, 0) ; del global_z_masked_s\n            student_global_patches_set = ops.split(global_patches_masked_s, n_global, 0); del global_patches_masked_s\n            m_set = ops.split(m_set, n_global, 0)\n            student_local_cls_set = ops.split(local_z_s, n_local, 0) ; del local_z_s\n            \n            student_cls_set = student_global_cls_set\n            student_cls_set.extend(student_local_cls_set)\n            \n            ##PAIRWISE LOSS CALCUATION##\n            total_loss = []\n            mim_loss = []\n            cls_loss = []\n            simclr_loss = []\n                \n                #teacher-student combination\n                #global-global : mim, cls loss\n                #global-local : cls loss\n                # cls loss = DINO and SimCLR loss\n                \n                #1. patch-patch mim loss\n            for m_, patch_u_s, patch_u_t in zip(m_set, student_global_patches_set, teacher_patches_set):\n                mim_loss.append(compute_mim_loss(m_, patch_u_s, patch_u_t, self.c_mim, self.teacher_t, self.student_t, teacher_softmax = True))\n            mim_loss = ops.mean(mim_loss)\n                #2. cls-cls DINO, simclr loss\n            for idx in teacher_indices: #0, 1\n                t = teacher_cls_set[idx]\n                view_indices.remove(idx)\n                for j in view_indices: #(0), 1, 2, 3\n                    s = student_cls_set[j]\n                    loss_ = compute_cls_loss(s, t, teacher_softmax = True, c = self.c_cls, t_t = self.teacher_t, t_s = self.student_t)\n                    loss_ = ops.clip(loss_, -10.0, 10**4)\n                    cls_loss.append(loss_)\n                    simclr_loss_ = keras_cv.losses.SimCLRLoss(temperature = self.teacher_t, \n                                                                  name = \"simclr_loss_compute\")(t,s)\n                    simclr_loss.append(simclr_loss_)\n                        \n                view_indices.append(idx)\n            cls_loss = ops.mean(cls_loss)\n            simclr_loss = ops.mean(simclr_loss)\n            loss = cls_loss + mim_loss + (self.apply_simclr * 0.5 * simclr_loss)\n            \n        #Track hyperparams and metrics\n        mean_cls = ops.mean(ops.concatenate(teacher_cls_set, axis = 0), \n                            axis = 0)\n        mean_patch = ops.mean(ops.concatenate(teacher_patches_set, axis = 0), \n                              axis = (0,1))\n        self.c_cls.assign(0.99*self.c_cls + (1-0.99)*mean_cls)\n        self.c_mim.assign(0.99*self.c_mim + (1-0.99)*mean_patch)\n        \n        self.loss_tracker.update_state(loss)\n        self.cls_tracker.update_state(cls_loss)\n        self.mim_tracker.update_state(mim_loss)\n        self.simclr_tracker.update_state(simclr_loss)\n        \n        output_dict = {self.loss_tracker.name : self.loss_tracker.result(),\n                       self.cls_tracker.name : self.cls_tracker.result(),\n                       self.simclr_tracker.name : self.simclr_tracker.result(),\n                       self.mim_tracker.name : self.mim_tracker.result(),\n                      }\n        \n        #Update student params as backprop\n        student_params = self.patch_embedding_fn + self.f_s.trainable_variables + self.student_predictor.trainable_variables + self.student_patch_predictor.trainable_variables + list(self.mask_token)\n        grads = tape.gradient(loss, student_params)\n        self.optimizer.apply_gradients(zip(grads, student_params))\n        #Update teacher params as EMA\n        #lambda = 0.999\n        teacher_feature_w, teacher_predictor_w = [],[]\n        l_ = 0.999\n        #print(\"teacher EMA\")\n        for f_teacher_part, f_student_part in zip(self.f_t.weights, self.f_s.weights):\n            f_teacher_part.assign(l_*f_teacher_part + (1-l_)*f_student_part)\n        #self.feature_extractor.set_weights(teacher_feature_w)\n        \n        #print(\"predictor EMA\")\n        for p_teacher_part, p_student_part in zip(self.teacher_predictor.weights, self.student_predictor.weights):\n            p_teacher_part.assign(l_*p_teacher_part + (1-l_)*p_student_part)\n        #self.teacher_predictor.set_weights(teacher_predictor_w)\n        for p_teacher_part, p_student_part in zip(self.teacher_patch_predictor.weights, self.student_patch_predictor.weights):\n            p_teacher_part.assign(l_*p_teacher_part + (1-l_)*p_student_part)\n        return output_dict\n    \n    def test_step(self, dataset):\n        view_indices = list(range(len(dataset)))\n        batch_size = ops.shape(dataset[0])[0]\n        \n        if self.probe:\n            global_view, local_view, labels = dataset\n            n_global, n_local = 1, 1\n            total_view = ops.concatenate(dataset, axis = 0)\n        else:\n            if self.multiview:\n                n_global = 2\n                n_local = len(dataset) - n_global\n                \n                #global_view = ops.concatenate(global_view, axis = 0)\n                total_view = ops.concatenate(dataset, axis = 0) #global_1, global_2, locals\n            else:\n                global_view, local_view = dataset\n                n_global, n_local = 1, 1\n                total_view = ops.concatenate(dataset, axis = 0)\n        # teacher -> global views, student -> global and local views\n        teacher_indices = list(range(n_global))\n        if True:\n            total_view = self.patch_embedding_fn(total_view)\n            global_view = total_view[:n_global*batch_size, ...] #2B, patch_length, dims\n            local_view = total_view[n_global*batch_size:, ...] #N, patch_length, dims\n            \n            m_set, global_view_masked = generate_mask(global_view, \n                                                      tf.random.uniform(shape = (), minval = 0.4, maxval = 0.8), #<- mask probability\n                                                      self.mask_token)\n            \n            global_z_t, global_patches_t, att_weights_t = self.f_t([global_view, global_view])\n            global_z_masked_s, global_patches_masked_s, _ = self.f_s([global_view_masked, global_view_masked])\n            local_z_s, local_patches_s, _ = self.f_s([local_view, local_view])\n            del _, local_patches_s\n            \n            global_z_t, global_patches_t = self.teacher_predictor(global_z_t), self.teacher_patch_predictor(global_patches_t)\n            global_z_masked_s, global_patches_masked_s = self.student_predictor(global_z_masked_s), self.student_patch_predictor(global_patches_masked_s)\n            local_z_s = self.student_predictor(local_z_s)\n            \n            teacher_cls_set = ops.split(global_z_t, n_global, 0) ; del global_z_t\n            teacher_patches_set = ops.split(global_patches_t, n_global, 0) ; del global_patches_t\n                \n            student_global_cls_set = ops.split(global_z_masked_s, n_global, 0) ; del global_z_masked_s\n            student_global_patches_set = ops.split(global_patches_masked_s, n_global, 0); del global_patches_masked_s\n            m_set = ops.split(m_set, n_global, 0)\n            student_local_cls_set = ops.split(local_z_s, n_local, 0) ; del local_z_s\n            \n            student_cls_set = student_global_cls_set\n            student_cls_set.extend(student_local_cls_set)\n            \n            ##PAIRWISE LOSS CALCUATION##\n            total_loss = []\n            mim_loss = []\n            cls_loss = []\n            simclr_loss = []\n                \n                #teacher-student combination\n                #global-global : mim, cls loss\n                #global-local : cls loss\n                # cls loss = DINO and SimCLR loss\n                \n                #1. patch-patch mim loss\n            for m_, patch_u_s, patch_u_t in zip(m_set, student_global_patches_set, teacher_patches_set):\n                mim_loss.append(compute_mim_loss(m_, patch_u_s, patch_u_t, self.c_mim, self.teacher_t, self.student_t, teacher_softmax = True))\n            mim_loss = ops.mean(mim_loss)\n                #2. cls-cls DINO, simclr loss\n            for idx in teacher_indices: #0, 1\n                t = teacher_cls_set[idx]\n                view_indices.remove(idx)\n                for j in view_indices: #(0), 1, 2, 3\n                    s = student_cls_set[j]\n                    loss_ = compute_cls_loss(s, t, teacher_softmax = True, c = self.c_cls, t_t = self.teacher_t, t_s = self.student_t)\n                    loss_ = ops.clip(loss_, -10.0, 10**4)\n                    cls_loss.append(loss_)\n                    simclr_loss_ = keras_cv.losses.SimCLRLoss(temperature = self.teacher_t, \n                                                                  name = \"simclr_loss_compute\")(t,s)\n                    simclr_loss.append(simclr_loss_)\n                        \n                view_indices.append(idx)\n            cls_loss = ops.mean(cls_loss)\n            simclr_loss = ops.mean(simclr_loss)\n            loss = cls_loss + mim_loss + (self.apply_simclr * 0.5 * simclr_loss)\n            \n        #Track hyperparams and metrics\n        mean_cls = ops.mean(ops.concatenate(teacher_cls_set, axis = 0), \n                            axis = 0)\n        mean_patch = ops.mean(ops.concatenate(teacher_patches_set, axis = 0), \n                              axis = (0,1))\n        self.c_cls.assign(0.99*self.c_cls + (1-0.99)*mean_cls)\n        self.c_mim.assign(0.99*self.c_mim + (1-0.99)*mean_patch)\n        \n        self.loss_tracker.update_state(loss)\n        self.cls_tracker.update_state(cls_loss)\n        self.mim_tracker.update_state(mim_loss)\n        self.simclr_tracker.update_state(simclr_loss)\n        \n        output_dict = {self.loss_tracker.name : self.loss_tracker.result(),\n                       self.cls_tracker.name : self.cls_tracker.result(),\n                       self.simclr_tracker.name : self.simclr_tracker.result(),\n                       self.mim_tracker.name : self.mim_tracker.result(),\n                      }\n        \n        return output_dict\n    \n    def call(self, dataset):\n        return self.test_step(dataset)\n    def get_full_model(self):\n        inputs = keras.layers.Input([res,res,3], name = \"ImageInput\")\n        patches = self.patch_embedding_fn(patches)\n        cls_token, patches, att_weight = self.f_t([patches, patches])\n        model = Model(inputs, [cls_token, patches, att_weight],\n                     name = f\"{self.train_type}_ViT\")\n        return model","metadata":{"execution":{"iopub.status.busy":"2024-06-04T07:54:32.811563Z","iopub.execute_input":"2024-06-04T07:54:32.811934Z","iopub.status.idle":"2024-06-04T07:54:32.893464Z","shell.execute_reply.started":"2024-06-04T07:54:32.811903Z","shell.execute_reply":"2024-06-04T07:54:32.891995Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"----------------\n# Multimodal Contrastive method\n- CLIP\n- SigLIP","metadata":{}},{"cell_type":"code","source":"class CLIP(keras.Model): #original CLIP + CLIP surgery\n    def __init__(self, image_encoder, text_encoder,\n                embed_dims, pool_heads = 8, t = 2.0, **kwargs):\n        super().__init__(**kwargs)\n        self.image_encoder = image_encoder\n        self.text_encoder = text_encoder\n        self.embed_dims = embed_dims\n        self.t = t\n        self.mlp_image = keras.layers.Dense(units = embed_dims)\n        self.mlp_text = keras.layers.Dense(units = embed_dims)\n        self.pe_fn = keras_nlp.layers.RotaryEmbedding(name = \"RotatoryPE\")\n        self.image_pooler = AttentionPooling(pool_heads, embed_dims, name = \"ImageAttentionPooler\")\n        self.text_pooler = AttentionPooling(pool_heads, embed_dims, name = \"TextAttentionPooler\")\n        self.loss_tracker = keras.metrics.Mean(name = \"CLIP_loss\")\n    def get_config(self):\n        return {\"Image encoder name\": self.image_encoder.name,\n               \"Text encoder name\" : self.text_encoder.name,\n               \"embed_dims\" : self.embed_dims,\n               \"temperature\" : self.t,\n               \"SSL_method\" : \"CLIP_with_Attentional_Pooling\"}\n    def get_clip_loss(self, image_vector, text_vector):\n        batch_size = ops.shape(image_vector)[0]\n        image_vector = keras.utils.normalize(image_vector, axis = -1, order = 2)\n        text_vector = keras.utils.normalize(text_vector, axis = -1, order = 2)\n        \n        cor_mat = tf.einsum(\"ab, cb->ac\", image_vector, text_vector) / self.t\n        cor_mat = ops.reshape(cor_mat, [batch_size, batch_size])\n        pseudo_label = ops.zeros_like(cor_mat)\n        diags = tf.linalg.diag_part(pseudo_label) + 1.0\n        pseudo_label = tf.linalg.set_diag(pseudo_label, diags)\n        pseudo_label = pseudo_label - ops.ones_like(pseudo_label)\n        loss = 0.5*(ops.mean(keras.losses.CategoricalFocalCrossentropy(reduction = None, label_smoothing = 0.05)(pseudo_label, cor_mat)) + \n                    ops.mean(keras.losses.CategoricalFocalCrossentropy(reduction = None, label_smoothing = 0.05)(pseudo_label, ops.transpose(cor_mat)))\n                   )\n        return loss\n        \n    def train_step(self, dataset):\n        image, text = dataset\n        with tf.GradientTape() as tape: \n            image_feature, text_feature = self.image_encoder(image, training = True), self.text_encoder(text, training = True)\n            batch_size, w, h, dims = ops.shape(image_feature) ; batch_size = ops.shape(image_feature)[0]\n            \n            image_feature = ops.reshape(image_feature, [batch_size, w*h, dims])\n            image_feature = self.pe_fn(image_feature)\n            text_vector = keras.layers.GlobalAveragePooling1D()(text_feature)\n            if len(ops.shape(image_feature)) == 3:\n                image_vector = keras.layers.GlobalAveragePooling1D()(image_feature)\n            elif len(ops.shape(image_feature)) == 4:\n                image_vector = keras.layers.GlobalAveragePooling2D()(image_feature)\n\n            image_vector, text_vector = self.mlp_image(image_vector, training = True), self.mlp_text(text_vector, training = True)\n            image_vector, text_vector = self.image_pooler([image_vector, image_feature], training = True)[0], self.text_pooler([text_vector, text_feature], training = True)[0]\n            loss = self.get_clip_loss(image_vector, text_vector)\n        \n        encoder_weights = self.image_encoder.trainable_weights + self.text_encoder.trainable_weights\n        mlp_weights = self.mlp_image.trainable_weights + self.mlp_text.trainable_weights\n        pool_weights = self.image_pooler.trainable_weights + self.text_pooler.trainable_weights\n        trainable_weights = encoder_weights + mlp_weights + pool_weights + self.pe_fn.trainable_weights\n        \n        grads = tape.gradient(loss, trainable_weights)\n        self.optimizer.apply_gradients(zip(grads, trainable_weights))\n        self.loss_tracker.update_state(loss)\n        return {self.loss_tracker.name : self.loss_tracker.result()}\n    \n    def test_step(self, dataset):\n        image, text = dataset\n        image_feature, text_feature = self.image_encoder(image, training = False), self.text_encoder(text, training = False)\n        batch_size, w, h, dims = ops.shape(image_feature) ; batch_size = ops.shape(image_feature)[0]\n            \n        image_feature = ops.reshape(image_feature, [batch_size, w*h, dims])\n        image_feature = self.pe_fn(image_feature)\n        \n        text_vector = keras.layers.GlobalAveragePooling1D()(text_feature)\n        if len(ops.shape(image_feature)) == 3:\n            image_vector = keras.layers.GlobalAveragePooling1D()(image_feature)\n        elif len(ops.shape(image_feature)) == 4:\n            image_vector = keras.layers.GlobalAveragePooling2D()(image_feature)\n\n        image_vector, text_vector = self.mlp_image(image_vector, training = False), self.mlp_text(text_vector, training = False)\n        image_vector, text_vector = self.image_pooler([image_vector, image_feature], training = False)[0], self.text_pooler([text_vector, text_feature], training = False)[0]\n        loss = self.get_clip_loss(image_vector, text_vector)\n        self.loss_tracker.update_state(loss)\n        return {self.loss_tracker.name : self.loss_tracker.result()}\n    def call(self, dataset):\n        return self.test_step(dataset)\n    def get_full_model(self):\n        inputs = self.image_encoder.inputs\n        feature = self.image_encoder.output\n        if len(ops.shape(feature)) == 4:\n            batch_size, w, h, dims = ops.shape(feature)\n            batch_size = ops.shape(feature)[0]\n            feature = ops.reshape(feature, [-1, w*h, dims])\n        feature = self.pe_fn(feature)\n        image_vector = keras.layers.GlobalAveragePooling1D()(feature)\n        z_image = self.mlp_image(image_vector)\n        outputs = self.image_pooler([z_image, feature])\n        return keras.Model(inputs, outputs,\n                          name = f\"FullModel_{self.image_encoder.name}\")","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-06-04T07:54:32.895040Z","iopub.execute_input":"2024-06-04T07:54:32.896233Z","iopub.status.idle":"2024-06-04T07:54:32.932671Z","shell.execute_reply.started":"2024-06-04T07:54:32.896164Z","shell.execute_reply":"2024-06-04T07:54:32.931384Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"class SigLIP(keras.Model):\n    def __init__(self, image_encoder, text_encoder,\n                embed_dims, pool_heads = 8, **kwargs):\n        super().__init__(**kwargs)\n        self.image_encoder = image_encoder\n        self.text_encoder = text_encoder\n        self.pe_fn = keras_nlp.layers.RotaryEmbedding(name = \"RotatoryPE\")\n        self.embed_dims = embed_dims\n        self.mlp_image = keras.layers.Dense(units = embed_dims)\n        self.mlp_text = keras.layers.Dense(units = embed_dims)\n        self.image_pooler = AttentionPooling(pool_heads, embed_dims, name = \"ImageAttentionPooler\")\n        self.text_pooler = AttentionPooling(pool_heads, embed_dims, name = \"TextAttentionPooler\")\n        self.loss_tracker = keras.metrics.Mean(name = \"SigLIP_loss\")\n        \n        self.t = tf.Variable(1.0, trainable = True, dtype = \"float32\")\n        self.b = tf.Variable(0.0, trainable = True, dtype = \"float32\")\n    def get_config(self):\n        return {\"Image encoder name\": self.image_encoder.name,\n               \"Text encoder name\" : self.text_encoder.name,\n               \"embed_dims\" : self.embed_dims,\n               \"SSL_method\" : \"SigLIP\"}\n    \n    def compute_loss(self, image_vector, text_vector):\n        batch_size = ops.shape(image_vector)[0]\n        z_img = keras.utils.normalize(image_vector, axis = -1, order = 2)\n        z_text = keras.utils.normalize(text_vector, axis = -1, order = 2)\n        z_img = tf.cast(z_img, tf.float32)\n        z_text = tf.cast(z_text, tf.float32)\n        cor_mat = ops.dot(z_img, ops.transpose(z_text)) * ops.exp(self.t) + self.b\n        cor_mat = ops.reshape(cor_mat, [batch_size, batch_size])\n        \n        pseudo_label = ops.zeros_like(cor_mat)\n        diags = tf.linalg.diag_part(pseudo_label) + 2.0\n        pseudo_label = tf.linalg.set_diag(pseudo_label, diags)\n        pseudo_label = pseudo_label - ops.ones_like(pseudo_label)\n        \n        loss = -ops.mean(ops.log_sigmoid(ops.multiply(cor_mat, pseudo_label)))\n        return loss\n        \n    def train_step(self, dataset):\n        image, text = dataset\n        with tf.GradientTape() as tape: \n            image_feature, text_feature = self.image_encoder(image, training = True), self.text_encoder(text, training = True)\n            batch_size, w, h, dims = ops.shape(image_feature) ; batch_size = ops.shape(image_feature)[0]\n            \n            image_feature = ops.reshape(image_feature, [batch_size, w*h, dims])\n            image_feature = self.pe_fn(image_feature)\n            \n            text_vector = keras.layers.GlobalAveragePooling1D()(text_feature)\n            if len(ops.shape(image_feature)) == 3:\n                image_vector = keras.layers.GlobalAveragePooling1D()(image_feature)\n            elif len(ops.shape(image_feature)) == 4:\n                image_vector = keras.layers.GlobalAveragePooling2D()(image_feature)\n\n            image_vector, text_vector = self.mlp_image(image_vector, training = True), self.mlp_text(text_vector, training = True)\n            image_vector, text_vector = self.image_pooler([image_vector, image_feature], training = True)[0], self.text_pooler([text_vector, text_feature], training = True)[0]\n            loss = 0.5*(self.compute_loss(image_vector, text_vector) + self.compute_loss(text_vector, image_vector))\n        \n        encoder_weights = self.image_encoder.trainable_weights + self.text_encoder.trainable_weights\n        mlp_weights = self.mlp_image.trainable_weights + self.mlp_text.trainable_weights\n        pool_weights = self.image_pooler.trainable_weights + self.text_pooler.trainable_weights\n        trainable_weights = encoder_weights + mlp_weights + pool_weights + [self.t, self.b] + self.pe_fn.trainable_weights\n        \n        grads = tape.gradient(loss, trainable_weights)\n        self.optimizer.apply_gradients(zip(grads, trainable_weights))\n        self.loss_tracker.update_state(loss)\n        return {self.loss_tracker.name : self.loss_tracker.result()}\n    \n    def test_step(self, dataset):\n        image, text = dataset\n        image_feature, text_feature = self.image_encoder(image, training = False), self.text_encoder(text, training = False)\n        batch_size, w, h, dims = ops.shape(image_feature) ; batch_size = ops.shape(image_feature)[0]\n            \n        image_feature = ops.reshape(image_feature, [batch_size, w*h, dims])\n        image_feature = self.pe_fn(image_feature)\n        text_vector = keras.layers.GlobalAveragePooling1D()(text_feature)\n        if len(ops.shape(image_feature)) == 3:\n            image_vector = keras.layers.GlobalAveragePooling1D()(image_feature)\n        elif len(ops.shape(image_feature)) == 4:\n            image_vector = keras.layers.GlobalAveragePooling2D()(image_feature)\n\n        image_vector, text_vector = self.mlp_image(image_vector, training = False), self.mlp_text(text_vector, training = False)\n        image_vector, text_vector = self.image_pooler([image_vector, image_feature], training = False)[0], self.text_pooler([text_vector, text_feature], training = False)[0]\n        loss = 0.5*(self.compute_loss(image_vector, text_vector) + self.compute_loss(text_vector, image_vector))\n        self.loss_tracker.update_state(loss)\n        return {self.loss_tracker.name : self.loss_tracker.result()}\n    def call(self, dataset):\n        return self.test_step(dataset)\n    def get_full_model(self):\n        inputs = self.image_encoder.inputs\n        feature = self.image_encoder.output\n        if len(ops.shape(feature)) == 4:\n            batch_size, w, h, dims = ops.shape(feature)\n            batch_size = ops.shape(feature)[0]\n            feature = ops.reshape(feature, [-1, w*h, dims])\n        feature = self.pe_fn(feature)\n        \n        image_vector = keras.layers.GlobalAveragePooling1D()(feature)\n        z_image = self.mlp_image(image_vector)\n        outputs = self.image_pooler([z_image, feature])\n        return keras.Model(inputs, outputs,\n                          name = f\"FullModel_{self.image_encoder.name}\")","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-06-04T07:54:32.934393Z","iopub.execute_input":"2024-06-04T07:54:32.934777Z","iopub.status.idle":"2024-06-04T07:54:32.973342Z","shell.execute_reply.started":"2024-06-04T07:54:32.934736Z","shell.execute_reply":"2024-06-04T07:54:32.972119Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"class SPARC(keras.Model): \n    #Reference : https://arxiv.org/abs/2401.09865 \"Improving fine-grained understanding in image-text pre-training\"\n    def __init__(self, image_encoder, text_encoder,\n                embed_dims, pool_heads = 8, preprocessor = None,\n                 **kwargs):\n        super().__init__(**kwargs)\n        self.image_encoder = image_encoder\n        self.text_encoder = text_encoder\n        self.preprocessor = preprocessor\n        self.pe_fn = keras_nlp.layers.RotaryEmbedding(name = \"RotatoryPE\")\n        self.embed_dims = embed_dims\n        self.mlp_image = keras.layers.Dense(units = embed_dims)\n        self.mlp_text = keras.layers.Dense(units = embed_dims)\n        self.image_pooler = AttentionPooling(pool_heads, embed_dims, name = \"ImageAttentionPooler\")\n        self.text_pooler = AttentionPooling(pool_heads, embed_dims, name = \"TextAttentionPooler\")\n        self.ce_fn = keras.losses.CategoricalCrossentropy(from_logits=True, reduction = None)\n        self.loss_tracker = keras.metrics.Mean(name = \"SPARC_loss\")\n        self.global_loss_tracker = keras.metrics.Mean(name = \"SPARC_global_loss\")\n        self.local_loss_tracker = keras.metrics.Mean(name = \"SPARC_local_loss\")\n        \n        self.t = tf.Variable(1.0, trainable = True, dtype = \"float32\")\n        self.b = tf.Variable(0.0, trainable = True, dtype = \"float32\")\n        self.threshold = tf.Variable(0.5, trainable = True, dtype = \"float32\")\n        \n    def get_config(self):\n        return {\"Image encoder name\": self.image_encoder.name,\n               \"Text encoder name\" : self.text_encoder.name,\n               \"embed_dims\" : self.embed_dims,\n               \"SSL_method\" : \"SPARC\"}\n    \n    def compute_global_loss(self, image_vector, text_vector): #<- SigLIP\n        batch_size = ops.shape(image_vector)[0]\n        z_img = keras.utils.normalize(image_vector, axis = -1, order = 2)\n        z_text = keras.utils.normalize(text_vector, axis = -1, order = 2)\n        z_img = tf.cast(z_img, tf.float32)\n        z_text = tf.cast(z_text, tf.float32)\n        cor_mat = ops.dot(z_img, ops.transpose(z_text)) * ops.exp(self.t) + self.b\n        cor_mat = ops.reshape(cor_mat, [batch_size, batch_size])\n        \n        pseudo_label = ops.zeros_like(cor_mat)\n        diags = tf.linalg.diag_part(pseudo_label) + 2.0\n        pseudo_label = tf.linalg.set_diag(pseudo_label, diags)\n        pseudo_label = pseudo_label - ops.ones_like(pseudo_label)\n        \n        loss = -ops.mean(ops.log_sigmoid(ops.multiply(cor_mat, pseudo_label)))\n        loss = tf.cast(loss, tf.float32)\n        return loss\n    \n    def text_compare(self, text_label, text_logit, mask = None):\n        loss = self.ce_fn(text_label, text_logit)\n        loss = tf.cast(loss, tf.float32)\n        if mask == None:\n            return ops.mean(loss)\n        else:\n            loss *= tf.cast(mask, tf.float32)\n            loss = (ops.sum(loss))/(ops.sum(tf.cast(mask, tf.float32)) + 1e-4)\n            return loss\n        \n    def compute_loss(self, image_sequence, text_sequence, mask = None):\n        batch_size = ops.shape(image_sequence)[0]\n        _, token_len, dim_ = ops.shape(text_sequence)\n        if len(ops.shape(image_sequence)) == 3:\n            _, image_len, dim_ = ops.shape(image_sequence)\n        elif len(ops.shape(image_sequence)) == 4:\n            _, w, h, dim_ = ops.shape(image_sequence)\n            image_sequence = ops.reshape(image_sequence, [batch_size, w*h, dim_])\n            image_len = w*h\n            \n        image_sequence, text_sequence = self.mlp_image(image_sequence), self.mlp_text(text_sequence) \n        image_sequence = self.pe_fn(image_sequence)\n        # [batch, image_len, embed_dims] / [batch, token_len, embed_dims], respectively.\n        \n        z_image, att_weight = self.image_pooler([keras.layers.GlobalAveragePooling1D()(image_sequence),\n                                                     image_sequence])\n        z_text, att_weight_ = self.text_pooler([keras.layers.GlobalAveragePooling1D()(text_sequence),\n                                                     text_sequence])\n        #1. global alignment loss\n        global_loss = 0.5*(self.compute_global_loss(z_image, z_text) + self.compute_global_loss(z_text, z_image))\n        #2. Fine Grained local loss\n        \n        #a. get similarity matrix b/w text sequence and image sequence\n        sim_matrix = ops.einsum(\"atd, aid -> ati\", text_sequence, image_sequence) #batch, token_len, image_len\n        sim_matrix = (sim_matrix - ops.min(sim_matrix, axis = -1, keepdims = True)) / (1e-4 + ops.max(sim_matrix, axis = -1, keepdims = True) - ops.min(sim_matrix, axis = -1, keepdims = True))\n        sim_matrix = tf.cast(sim_matrix, tf.float32)\n        \n        sim_matrix = ops.clip(sim_matrix, self.threshold, 1e4)\n        attended_text_sequence = ops.einsum(\"ati, aid -> atd\", sim_matrix, image_sequence)\n        \n        text_logit = ops.einsum(\"atd, aqd -> atq\", \n                                keras.utils.normalize(text_sequence, axis = -1, order = 2), \n                                keras.utils.normalize(attended_text_sequence, axis = -1, order = 2))/self.t\n        \n        pseudo_text_label = ops.tile(ops.expand_dims(ops.eye(token_len), axis = 0),\n                                     [batch_size,1,1])\n        local_loss = self.text_compare(pseudo_text_label, text_logit, mask)\n        loss = 0.5*global_loss + 1.0*local_loss\n        return loss, global_loss, local_loss\n        \n    def train_step(self, dataset):\n        image, text = dataset\n        if self.preprocessor != None:\n            text_mask = self.preprocessor(text)[\"padding_mask\"]\n            text_mask = tf.cast(text_mask, dtype = tf.int32)\n        else:\n            text_mask = None\n        with tf.GradientTape() as tape: \n            image_feature, text_feature = self.image_encoder(image, training = True), self.text_encoder(text, training = True)\n            loss, global_loss, local_loss = self.compute_loss(image_feature, text_feature, text_mask)\n        \n        encoder_weights = self.image_encoder.trainable_weights + self.text_encoder.trainable_weights\n        mlp_weights = self.mlp_image.trainable_weights + self.mlp_text.trainable_weights\n        pool_weights = self.image_pooler.trainable_weights + self.text_pooler.trainable_weights\n        trainable_weights = encoder_weights + mlp_weights + pool_weights + [self.t, self.b] + self.pe_fn.trainable_weights\n        \n        grads = tape.gradient(loss, trainable_weights)\n        self.optimizer.apply_gradients(zip(grads, trainable_weights))\n        self.loss_tracker.update_state(loss)\n        self.global_loss_tracker.update_state(global_loss)\n        self.local_loss_tracker.update_state(local_loss)\n        return {self.loss_tracker.name : self.loss_tracker.result(),\n               self.global_loss_tracker.name : self.global_loss_tracker.result(),\n               self.local_loss_tracker.name : self.local_loss_tracker.result()\n               }\n    \n    def test_step(self, dataset):\n        image, text = dataset\n        if self.preprocessor != None:\n            text_mask = self.preprocessor(text)[\"padding_mask\"]\n            text_mask = tf.cast(text_mask, tf.int32)\n        else:\n            text_mask = None\n            \n        image_feature, text_feature = self.image_encoder(image, training = False), self.text_encoder(text, training = False)\n        loss, global_loss, local_loss = self.compute_loss(image_feature, text_feature, text_mask)\n        \n        self.loss_tracker.update_state(loss)\n        self.global_loss_tracker.update_state(global_loss)\n        self.local_loss_tracker.update_state(local_loss)\n        return {self.loss_tracker.name : self.loss_tracker.result(),\n               self.global_loss_tracker.name : self.global_loss_tracker.result(),\n               self.local_loss_tracker.name : self.local_loss_tracker.result()\n               }\n    \n    def call(self, dataset):\n        return self.test_step(dataset)\n    def get_full_model(self):\n        inputs = self.image_encoder.inputs\n        feature = self.image_encoder.output\n        feature = self.mlp_image(feature)\n        if len(ops.shape(feature)) == 4:\n            batch_size, w, h, dims = ops.shape(feature)\n            batch_size = ops.shape(feature)[0]\n            feature = ops.reshape(feature, [-1, w*h, dims])\n        feature = self.pe_fn(feature)\n        \n        image_vector = keras.layers.GlobalAveragePooling1D()(feature)\n        outputs = self.image_pooler([image_vector, feature])\n        return keras.Model(inputs, outputs,\n                          name = f\"FullModel_{self.image_encoder.name}\")","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-06-04T07:54:32.975126Z","iopub.execute_input":"2024-06-04T07:54:32.975536Z","iopub.status.idle":"2024-06-04T07:54:33.026456Z","shell.execute_reply.started":"2024-06-04T07:54:32.975495Z","shell.execute_reply":"2024-06-04T07:54:33.025090Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"# Visualization","metadata":{}},{"cell_type":"code","source":"def att_visualize(model, images, res, thresholding = True,\n                 impose_alpha = 0.5):\n    # Use jet colormap to colorize heatmap\n    try:\n        model = model.get_full_model()\n    except Exception as e:\n        print(\"Using Raw model with Att pooling\",\"\\n\",\"Possible error:\",\"\\n\",e)\n        pass\n    try:\n        outputs = model.predict_on_batch(images) \n        att_weights = outputs[-1]\n        att_weights_ = att_weights[:, :, 0, :] #batch, heads, cls_token, w*h\n        _, heads, token_length = ops.shape(att_weights_) ; batch_size = ops.shape(att_weights_)[0]\n        token_length = tf.cast(token_length, tf.float32)\n\n        w = tf.cast(tf.math.sqrt(token_length), tf.int32)\n        heatmap = tf.reshape(att_weights_, [batch_size, heads, w, w])\n        M = tf.reduce_max(heatmap, axis = [2, 3], keepdims = True)\n        m = tf.reduce_min(heatmap, axis = [2, 3], keepdims = True)\n        heatmap = (heatmap - m) / (M-m + 1e-4)\n        if thresholding:\n            threshold = ops.median(heatmap, [2,3], keepdims = True)\n            heatmap = ops.where(heatmap < threshold, 0.0, heatmap)\n        else:\n            pass\n        heatmap = ops.reshape(heatmap, [-1, w, w])\n        heatmap = np.array(255.0*heatmap).astype(\"uint8\")\n        # Use jet colormap to colorize heatmap\n        cmap = mpl.colormaps[\"jet\"]\n        # Use RGB values of the colormap\n        _colors = cmap(np.arange(256))[:, :3]\n        heatmap = _colors[heatmap]\n        \n        heatmap = [keras.utils.array_to_img(h) for h in heatmap]\n        heatmap = [h.resize((res,res)) for h in heatmap]\n        heatmap = [keras.utils.img_to_array(h) for h in heatmap]\n        heatmap = np.array(heatmap)\n        heatmap = ops.reshape(heatmap, [batch_size, heads, res, res, 3])\n        \n        imposed = ((1-impose_alpha)*tf.cast(images[:, tf.newaxis, ...], tf.float32)) + (impose_alpha*tf.cast(heatmap, tf.float32))\n        imposed = tf.cast(imposed, tf.uint8)\n        tf.keras.backend.clear_session()\n        return imposed\n    except Exception as e:\n        print(\"Error raised during visualization\",\"\\n\",e)\n        pass","metadata":{"execution":{"iopub.status.busy":"2024-06-04T07:54:33.028152Z","iopub.execute_input":"2024-06-04T07:54:33.028645Z","iopub.status.idle":"2024-06-04T07:54:33.049278Z","shell.execute_reply.started":"2024-06-04T07:54:33.028602Z","shell.execute_reply":"2024-06-04T07:54:33.048144Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"# Test - drive","metadata":{}},{"cell_type":"code","source":"test_drive = 0","metadata":{"execution":{"iopub.status.busy":"2024-06-04T07:54:33.050819Z","iopub.execute_input":"2024-06-04T07:54:33.051302Z","iopub.status.idle":"2024-06-04T07:54:33.070306Z","shell.execute_reply.started":"2024-06-04T07:54:33.051257Z","shell.execute_reply":"2024-06-04T07:54:33.068665Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"if test_drive:\n    res = 256\n    batch_size = 8\n    gc_configs = get_gcvit_configs(res, 64, \"GC_ViT_xxtiny\")\n    gc_configs[\"level_depth\"] = [1,1,2,2]\n    print(gc_configs)\n    dataset = tfds.load(\"beans\", split = 'test')\n    def map_fn(dataset):\n        image = dataset[\"image\"]\n        image = tf.image.resize_with_pad(image, res, res, antialias = True)\n        image = tf.cast(image, tf.uint8)\n        return image\n    train_ds = dataset.map(map_fn).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n    \n    ssl_fn = get_map_fn(res, \"supervised\", \"ssl\", 5)\n    train_ds = train_ds.unbatch().map(ssl_fn).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n    for imgs in train_ds.take(1):\n        sets = imgs\n    for index in range(batch_size):\n        fig, axes = plt.subplots(2,2, figsize = (7,7))\n        axes = axes.flatten()\n        for i, ax in enumerate(axes):\n            ax.imshow(ops.cast(sets[i][index], \"uint8\"))\n            ax.set_title(f\"In-Batch index : {index}\")\n        print(\"=================================================\")\n        plt.show()","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-06-04T07:54:33.072501Z","iopub.execute_input":"2024-06-04T07:54:33.073320Z","iopub.status.idle":"2024-06-04T07:54:33.087844Z","shell.execute_reply.started":"2024-06-04T07:54:33.073271Z","shell.execute_reply":"2024-06-04T07:54:33.086617Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"if test_drive:\n    heads = 8\n    #vit_model = get_full_model(kimm.models.MobileViTS(input_shape = [res,res,3], include_top = False), embed_dims = 512,\n    #                           res = res, pe_type = None, att_depth = 1,\n    #                          att_heads = heads)\n    #vit_model = get_full_model(\"convmixer_32_12_768\", pe_type = None,\n    #                       res = res, att_depth = 1, att_heads = 16, embed_dims = 768)\n    #vit_model = get_full_model(\"vit\", res = res, return_patches = False, mask = False, \n    #                           att_depth = 6, att_heads = heads,\n    #                          pe_type = \"learnable\", embed_dims = 768)\n    \n    barlow = iBOT(4,768,8,\n                  embed_dims = 768, \n                  multiview = True, apply_simclr = True\n                 )\n    barlow.compile(optimizer = keras.optimizers.Adam(learning_rate = 1e-4))\n    print(barlow(sets))\n    f_ext = barlow.get_full_model()\n    print(f\"\\n\\n\\nGFlops : {get_flops(f_ext, imgs[:1])}\")\n    barlow.summary()\n    #result = barlow.train_step(sets)\n    result = barlow.fit(train_ds, epochs = 1, steps_per_epoch = 10)\n    print(result)\n    ","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-06-04T07:54:33.089228Z","iopub.execute_input":"2024-06-04T07:54:33.089592Z","iopub.status.idle":"2024-06-04T07:54:33.103775Z","shell.execute_reply.started":"2024-06-04T07:54:33.089562Z","shell.execute_reply":"2024-06-04T07:54:33.102431Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"if test_drive:\n    imgs = att_visualize(barlow.get_full_model(), sets[0], res = res,\n                        thresholding = True)\n    fig, axes = plt.subplots(batch_size, heads, figsize = (50,50))\n    for row in range(batch_size):\n        for col in range(heads):\n            axes[row][col].imshow(imgs[row, col, ...])\n    plt.show()","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-06-04T07:54:33.105307Z","iopub.execute_input":"2024-06-04T07:54:33.105728Z","iopub.status.idle":"2024-06-04T07:54:33.121391Z","shell.execute_reply.started":"2024-06-04T07:54:33.105688Z","shell.execute_reply":"2024-06-04T07:54:33.120258Z"},"trusted":true},"execution_count":37,"outputs":[]}]}