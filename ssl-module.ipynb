{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a202eddb",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-11-08T06:26:38.998428Z",
     "iopub.status.busy": "2024-11-08T06:26:38.997793Z",
     "iopub.status.idle": "2024-11-08T06:26:58.063508Z",
     "shell.execute_reply": "2024-11-08T06:26:58.062444Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 19.102848,
     "end_time": "2024-11-08T06:26:58.065718",
     "exception": false,
     "start_time": "2024-11-08T06:26:38.962870",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirements loaded, keras : v3.3.3, Tensorflow : v2.16.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ML tools \n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "import keras #; keras.config.set_dtype_policy(\"mixed_float16\")\n",
    "import keras_nlp\n",
    "import keras_cv\n",
    "from keras import ops as ops\n",
    "\n",
    "import cv2\n",
    "import tensorflow_io as tfio\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from keras import Input, Model, layers\n",
    "from keras.models import load_model\n",
    "from keras.layers import Layer\n",
    "from keras.layers import Conv2D, DepthwiseConv2D, Dense, Activation, BatchNormalization, LayerNormalization, MultiHeadAttention, Embedding, Subtract, Add, Multiply, GlobalAveragePooling2D, GlobalAveragePooling1D, LayerNormalization\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.applications import *\n",
    "import os, sys\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn.decomposition import PCA\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "from tqdm.notebook import tqdm\n",
    "print(f\"Requirements loaded, keras : v{keras.__version__}, Tensorflow : v{tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ccb37c",
   "metadata": {
    "papermill": {
     "duration": 0.032516,
     "end_time": "2024-11-08T06:26:58.130369",
     "exception": false,
     "start_time": "2024-11-08T06:26:58.097853",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataloader setting\n",
    "- original dataset(input as tf.ds) : image only or image/label paired dataset\n",
    "- target dataset(output as tf.ds) : (image1, image2), (image1, image2, image3), (image1, image2, label)\n",
    "    - Also, implement the mixing function : merges 2 homogenous dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b27ee6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T06:26:58.198131Z",
     "iopub.status.busy": "2024-11-08T06:26:58.197440Z",
     "iopub.status.idle": "2024-11-08T06:27:01.619049Z",
     "shell.execute_reply": "2024-11-08T06:27:01.618014Z"
    },
    "papermill": {
     "duration": 3.458257,
     "end_time": "2024-11-08T06:27:01.621409",
     "exception": false,
     "start_time": "2024-11-08T06:26:58.163152",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandAug Component in this SSL module :  ['random_contrast_1', 'random_brightness_1', 'random_shear', 'random_shear_1', 'random_translation', 'random_translation_1']\n"
     ]
    }
   ],
   "source": [
    "basic_aug = keras.Sequential([keras.layers.RandomFlip(), \n",
    "                              keras.layers.RandomRotation(factor = 0.4),\n",
    "                             keras.layers.RandomContrast(factor = 0.4),\n",
    "                             keras.layers.RandomBrightness(factor=0.4), \n",
    "                             ])\n",
    "input_layer = Input([None, None, 3])\n",
    "output = basic_aug(input_layer)\n",
    "output = (output - ops.min(output)) / (1e-4 + ops.max(output) - ops.min(output))\n",
    "output *= 255.0\n",
    "output = ops.cast(output, \"uint8\")\n",
    "basic_aug = keras.Model(input_layer, output)\n",
    "\n",
    "aug_layers = keras_cv.layers.RandAugment.get_standard_policy(\n",
    "    value_range=(0, 255), magnitude=0.2, magnitude_stddev=0.1\n",
    ")\n",
    "\n",
    "aug_layers.pop(0); aug_layers.pop(0) ; aug_layers.pop(0); aug_layers.pop(0)\n",
    "print(\"RandAug Component in this SSL module : \",[layer.name for layer in aug_layers])\n",
    "randaug = keras_cv.layers.RandomAugmentationPipeline(\n",
    "    layers=aug_layers, augmentations_per_image=1, auto_vectorize = True\n",
    ")\n",
    "input_layer_ = Input([None, None, 3])\n",
    "rand_output = randaug(input_layer_)\n",
    "randaug_model= keras.Model(input_layer_, rand_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be941ad6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T06:27:01.687439Z",
     "iopub.status.busy": "2024-11-08T06:27:01.686723Z",
     "iopub.status.idle": "2024-11-08T06:27:01.693668Z",
     "shell.execute_reply": "2024-11-08T06:27:01.692778Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.042061,
     "end_time": "2024-11-08T06:27:01.695677",
     "exception": false,
     "start_time": "2024-11-08T06:27:01.653616",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_other_augs_(res):\n",
    "    \n",
    "    \n",
    "    crop_resize_global = keras.Sequential([keras.layers.RandomCrop(int(0.95*res), int(0.95*res)),\n",
    "                                        keras.layers.Resizing(res, res)\n",
    "                                       ])\n",
    "    crop_resize_local = keras.Sequential([keras.layers.RandomCrop(int(0.5*res), int(0.5*res)),\n",
    "                                        keras.layers.Resizing(res, res)\n",
    "                                       ])\n",
    "    crop_resize_medium = keras.Sequential([keras.layers.RandomCrop(int(0.75*res), int(0.75*res)),\n",
    "                                        keras.layers.Resizing(res, res)\n",
    "                                       ])\n",
    "\n",
    "    return crop_resize_global, crop_resize_medium, crop_resize_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "767e44c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T06:27:01.761251Z",
     "iopub.status.busy": "2024-11-08T06:27:01.760947Z",
     "iopub.status.idle": "2024-11-08T06:27:01.765833Z",
     "shell.execute_reply": "2024-11-08T06:27:01.764954Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.039566,
     "end_time": "2024-11-08T06:27:01.767749",
     "exception": false,
     "start_time": "2024-11-08T06:27:01.728183",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def random_color_change(images):\n",
    "    if tf.random.uniform(shape = (), minval = 1, maxval = 10, dtype = \"int32\") <= 3:\n",
    "        return 255 - images\n",
    "    else:\n",
    "        return images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e174b34a",
   "metadata": {
    "papermill": {
     "duration": 0.031056,
     "end_time": "2024-11-08T06:27:01.830202",
     "exception": false,
     "start_time": "2024-11-08T06:27:01.799146",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- get_map_fn\n",
    "    - output : original image(anchor), augmented images, (labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "914eb7d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T06:27:01.895009Z",
     "iopub.status.busy": "2024-11-08T06:27:01.894614Z",
     "iopub.status.idle": "2024-11-08T06:27:01.907506Z",
     "shell.execute_reply": "2024-11-08T06:27:01.906618Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.047734,
     "end_time": "2024-11-08T06:27:01.909351",
     "exception": false,
     "start_time": "2024-11-08T06:27:01.861617",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_map_fn(res, input_type = None, output_type = None, n_view = 2, grayscale = True):\n",
    "    # input_type as \"supervised\", \"with_label\", \"with label\" / OR / \"unsupervised\", \"without label\", \"without_label\", \"image_only\", \"image\"\n",
    "    # output_ytpe as \"ssl\", \"ssl_with_label\" / if ssl, image output : image1, image2, ..., image_n_view\n",
    "    assert n_view >= 2, \"Augmented View number must be >= 2\"\n",
    "    assert input_type in [\"supervised\", \"with_label\", \"with label\", \"unsupervised\", \"without label\", \"without_label\", \"image_only\", \"image\"], 'Pick one of input_type : \"supervised\", \"with_label\", \"with label\",\\n \"unsupervised\", \"without label\", \"without_label\", \"image_only\", \"image\"'\n",
    "    assert output_type in [\"ssl\", \"ssl_with_label\"], 'Pick one of output_type : \"ssl\", \"ssl_with_label\"'\n",
    "    crop_resize_global, crop_resize_medium, crop_resize_local = get_other_augs_(res)\n",
    "    def map_fn(image, label = None):\n",
    "        if grayscale:\n",
    "            try:\n",
    "                image = tf.image.rgb_to_grayscale(image)\n",
    "            except:\n",
    "                pass\n",
    "        else:\n",
    "            try:\n",
    "                image = tf.image.grayscale_to_rgb(image)\n",
    "            except:\n",
    "                pass\n",
    "        if input_type in [\"supervised\", \"with_label\", \"with label\"]:\n",
    "            image, label = image, label\n",
    "        elif input_type in [\"unsupervised\", \"without label\", \"without_label\", \"image_only\", \"image\"]:\n",
    "            image = image\n",
    "            label = None\n",
    "        batch_size = ops.shape(image)[0]\n",
    "        \n",
    "        aug_ = basic_aug(image)\n",
    "        aug_ = randaug(aug_)\n",
    "        global_image = crop_resize_global(aug_)\n",
    "        global_image = random_color_change(global_image)\n",
    "        if n_view == 2:\n",
    "            if output_type == \"ssl\":\n",
    "                return (image, global_image)\n",
    "            elif output_type == \"ssl_with_label\":\n",
    "                return (image, global_image, label)\n",
    "        elif n_view == 3:\n",
    "            medium_image = crop_resize_medium(aug_)\n",
    "            medium_image = random_color_change(medium_image)\n",
    "            if output_type == \"ssl\":\n",
    "                return (image, global_image, medium_image)\n",
    "            elif output_type == \"ssl_with_label\":\n",
    "                return (image, global_image, medium_image, label)\n",
    "        elif n_view > 3:\n",
    "            medium_image = crop_resize_medium(aug_)\n",
    "            medium_image = random_color_change(medium_image)\n",
    "            local_images = [random_color_change(crop_resize_local(image)) for _ in range(n_view - 3)]\n",
    "            local_images = tuple(local_images)\n",
    "            outputs = (image, global_image, medium_image) + local_images\n",
    "            if output_type == \"ssl\":\n",
    "                return outputs\n",
    "            elif output_type == \"ssl_with_label\":\n",
    "                return outputs+(label)\n",
    "    return map_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a01c6c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T06:27:01.973649Z",
     "iopub.status.busy": "2024-11-08T06:27:01.973351Z",
     "iopub.status.idle": "2024-11-08T06:27:01.986071Z",
     "shell.execute_reply": "2024-11-08T06:27:01.985167Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.047235,
     "end_time": "2024-11-08T06:27:01.987870",
     "exception": false,
     "start_time": "2024-11-08T06:27:01.940635",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MaskLayer(keras.layers.Layer):\n",
    "    def __init__(self, masking_rate, update_value, **kwargs):\n",
    "        super(MaskLayer, self).__init__(**kwargs)\n",
    "        self.masking_rate = masking_rate\n",
    "        self.update_value = update_value\n",
    "\n",
    "    def call(self, patches):\n",
    "        sequence, masked_patches = self.generate_mask(patches, self.masking_rate, self.update_value)\n",
    "        return [sequence, masked_patches]\n",
    "\n",
    "    def generate_mask(self, patches, masking_rate, update_value):\n",
    "        batch_size = tf.shape(patches)[0]\n",
    "        length = tf.shape(patches)[1]\n",
    "        dims = tf.shape(patches)[2]\n",
    "        num_ones = tf.cast(tf.cast(masking_rate, tf.float32) * tf.cast(length, tf.float32), \n",
    "                           tf.int32)\n",
    "        \n",
    "        # Ensure num_ones does not exceed length\n",
    "        num_ones = tf.minimum(num_ones, length)\n",
    "        \n",
    "        # Initialize the sequence with zeros\n",
    "        sequence = tf.zeros((batch_size, length), dtype=tf.int32)\n",
    "\n",
    "        def mask_single_batch(batch_idx):\n",
    "            # Generate random indices for the ones\n",
    "            indices = tf.random.shuffle(tf.range(length))[:num_ones]\n",
    "            scatter_indices = tf.stack([tf.ones([num_ones], dtype=tf.int32) * batch_idx, indices], axis=1)\n",
    "            return scatter_indices\n",
    "\n",
    "        # Apply the mask generation function to each batch using tf.map_fn\n",
    "        all_indices = tf.map_fn(mask_single_batch, tf.range(batch_size), dtype=tf.int32)\n",
    "\n",
    "        # Reshape to apply the updates\n",
    "        all_indices = tf.reshape(all_indices, [-1, 2])\n",
    "\n",
    "        # Update the sequence with ones at the specified indices\n",
    "        sequence = tf.tensor_scatter_nd_update(sequence, all_indices, tf.ones([tf.shape(all_indices)[0]], dtype=tf.int32))\n",
    "\n",
    "        mask_indices = tf.where(sequence == 1)\n",
    "\n",
    "        # Ensure update_value is correctly shaped for high-dimensional replacement\n",
    "        updates_value = tf.tile(tf.expand_dims(update_value, 0), [tf.shape(mask_indices)[0], 1])\n",
    "        \n",
    "        masked_patches_shape = tf.concat([tf.shape(patches)[:2], [dims]], axis=0)\n",
    "        masked_patches = tf.tensor_scatter_nd_update(patches, mask_indices, updates_value)\n",
    "\n",
    "        return tf.cast(sequence, \"float32\"), masked_patches\n",
    "# Test the function with Keras input\n",
    "#inputs = keras.Input(shape=(None, 128))\n",
    "#update_value = tf.Variable(99*tf.ones([128]), trainable=True)\n",
    "#MaskLayer(masking_rate=0.5, update_value=update_value)(tf.random.normal([2,64,128]))\n",
    "#sequence, masked_patches = mask_layer(inputs)\n",
    "#model = keras.Model(inputs=inputs, outputs=[sequence, masked_patches])\n",
    "#model(tf.random.normal([2,64,128]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "569eb780",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T06:27:02.053246Z",
     "iopub.status.busy": "2024-11-08T06:27:02.052710Z",
     "iopub.status.idle": "2024-11-08T06:27:02.068238Z",
     "shell.execute_reply": "2024-11-08T06:27:02.067343Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.050391,
     "end_time": "2024-11-08T06:27:02.070271",
     "exception": false,
     "start_time": "2024-11-08T06:27:02.019880",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PatchMask(keras.layers.Layer):\n",
    "    def __init__(self, mode, ratio, patch_size = 16,\n",
    "                 fill_mode = 'constant', fill_value = 0.0, \n",
    "                 output_mode = \"sequence\", #or image\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        mode in ['patch', \"grid\"], \"Mode should be one of the following : patch, grid\"\n",
    "        assert (ratio < 1) and (ratio > 0), \"Ratio should be 0~1\"\n",
    "        assert fill_mode in ['constant', 'gaussian', 'gaussian_noise'], \"fill_mode should be one of the following : 'constant', 'gaussian', 'gaussian_noise' \"\n",
    "        assert output_mode in [\"image\", 'sequence'], \"output_mode should be one of the following : 'image', 'sequence' \"\n",
    "        self.mode = mode #patch, grid\n",
    "        self.ratio = ratio\n",
    "        self.fill_mode = fill_mode\n",
    "        self.fill_value = fill_value\n",
    "        self.patch_size = patch_size\n",
    "        self.output_mode = output_mode\n",
    "    def build(self, input_shape):\n",
    "        batch_size, h, w, dims = input_shape\n",
    "        self.res = h\n",
    "        self.original_channels = dims\n",
    "        self.seq_len = int(h//self.patch_size) * int(w//self.patch_size)\n",
    "        \n",
    "    def call(self, image, training = True):\n",
    "        batch_size = tf.shape(image)[0]\n",
    "        image = ops.cast(image, \"float32\")\n",
    "        if training == False:\n",
    "            if self.output_mode == \"sequence\":\n",
    "                return ops.image.extract_patches(image, self.patch_size)\n",
    "            else:\n",
    "                return image\n",
    "        ################################################\n",
    "        if self.mode in [\"grid\", 'Grid', \"GridMask\", \"gridmask\"]:\n",
    "            image = keras_cv.layers.GridMask(ratio_factor = self.ratio,\n",
    "                                            fill_mode = self.fill_mode,\n",
    "                                            fill_value = self.fill_value)\n",
    "            patches = ops.image.extract_patches(image, self.patch_size)\n",
    "        else:\n",
    "            image /= 255.0\n",
    "            patches = ops.image.extract_patches(image, self.patch_size)\n",
    "            _, w_, h_, dims_ = ops.shape(patches)\n",
    "            patches = ops.reshape(patches, [-1, w_*h_, dims_])\n",
    "            if self.fill_mode == \"constant\":\n",
    "                update_value = tf.zeros((dims_,), dtype = \"float32\") + float(self.fill_value)\n",
    "            elif self.fill_mode in [\"gaussian\", \"gaussian_noise\"]:\n",
    "                update_value = tf.random.normal((dims_,))\n",
    "            mask_ids, patches = MaskLayer(masking_rate = self.ratio, update_value = update_value)(patches)\n",
    "            patches = patches * 255.0\n",
    "            patches = ops.cast(patches, \"uint8\")\n",
    "        if self.output_mode == \"sequence\":\n",
    "            return mask_ids, patches\n",
    "        else:\n",
    "            patches = ops.reshape(patches, [-1, w_, h_, self.patch_size, self.patch_size,\n",
    "                                           self.original_channels])\n",
    "            patches = ops.transpose(patches, [0, 1, 3, 2, 4, 5])\n",
    "            patches = ops.reshape(patches, [-1, self.res, self.res, self.original_channels])\n",
    "            return mask_ids, patches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dd0223",
   "metadata": {
    "papermill": {
     "duration": 0.031573,
     "end_time": "2024-11-08T06:27:02.133336",
     "exception": false,
     "start_time": "2024-11-08T06:27:02.101763",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Hyperbolic layer\n",
    "- Image를 hyperbolic space에 embedding하기 위해\n",
    "- Hierarchial structure를 반영할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31a0651b",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-11-08T06:27:02.197383Z",
     "iopub.status.busy": "2024-11-08T06:27:02.197070Z",
     "iopub.status.idle": "2024-11-08T06:27:02.205362Z",
     "shell.execute_reply": "2024-11-08T06:27:02.204499Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.042576,
     "end_time": "2024-11-08T06:27:02.207313",
     "exception": false,
     "start_time": "2024-11-08T06:27:02.164737",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def multihead_poincare_distance(A, B):\n",
    "    # A, B의 모양 확인\n",
    "    batch_size, n_patches, n_heads, proj_dims = ops.shape(A)\n",
    "\n",
    "    # Compute norms\n",
    "    norm_A = tf.norm(A, axis=-1, keepdims=True)  # shape: [batch_size, n_patches, n_heads, 1]\n",
    "    norm_B = tf.norm(B, axis=-1, keepdims=True)  # shape: [batch_size, n_patches, n_heads, 1]\n",
    "\n",
    "    # Calculate pairwise differences\n",
    "    A_expanded = tf.expand_dims(A, axis=2)  # shape: [batch_size, n_patches, 1, n_heads, proj_dims]\n",
    "    B_expanded = tf.expand_dims(B, axis=1)  # shape: [batch_size, 1, n_patches, n_heads, proj_dims]\n",
    "    differences = A_expanded - B_expanded  # shape: [batch_size, n_patches, n_patches, n_heads, proj_dims]\n",
    "\n",
    "    # Calculate norms of differences\n",
    "    norm_uv = tf.norm(differences, axis=-1, keepdims=True)  # shape: [batch_size, n_patches, n_patches, n_heads, 1]\n",
    "\n",
    "    # Reshape norms for broadcasting\n",
    "    norm_A_reshaped = tf.expand_dims(norm_A, axis=2)  # shape: [batch_size, n_patches, 1, n_heads, 1]\n",
    "    norm_B_reshaped = tf.expand_dims(norm_B, axis=1)  # shape: [batch_size, 1, n_patches, n_heads, 1]\n",
    "\n",
    "    # Calculate Poincare distance\n",
    "    cosh_distance = 1 + 2 * tf.square(norm_uv) / \\\n",
    "                    ((1 - tf.square(norm_A_reshaped)) * (1 - tf.square(norm_B_reshaped) + 1e-8))  # Add epsilon for numerical stability\n",
    "    poincare_distances = tf.math.acosh(cosh_distance)  # shape: [batch_size, n_patches, n_patches, n_heads, 1]\n",
    "    poincare_distances = tf.squeeze(poincare_distances, axis=-1)  # shape: [batch_size, n_patches, n_patches, n_heads]\n",
    "    return poincare_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd62e003",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-11-08T06:27:02.272295Z",
     "iopub.status.busy": "2024-11-08T06:27:02.271985Z",
     "iopub.status.idle": "2024-11-08T06:27:02.286622Z",
     "shell.execute_reply": "2024-11-08T06:27:02.285844Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.049392,
     "end_time": "2024-11-08T06:27:02.288683",
     "exception": false,
     "start_time": "2024-11-08T06:27:02.239291",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HyperbolicDense(keras.layers.Layer):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.weight = self.add_weight(\n",
    "            shape=(int(input_shape[-1]), self.units),\n",
    "            initializer='glorot_uniform',\n",
    "            trainable=True,\n",
    "            name='weight'\n",
    "        )\n",
    "        self.bias = self.add_weight(\n",
    "            shape=(self.units,),\n",
    "            initializer='zeros',\n",
    "            trainable=True,\n",
    "            name='bias'\n",
    "        )\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        norm = tf.norm(inputs, axis=-1, keepdims=True)\n",
    "        max_norm = 1 - 1e-5\n",
    "        projected = tf.where(norm < max_norm, inputs, inputs / norm * max_norm)\n",
    "        output = tf.matmul(projected, self.weight) + self.bias\n",
    "        return output\n",
    "\n",
    "\n",
    "class HyperbolicMHA(keras.layers.Layer):\n",
    "    def __init__(self, num_heads, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.query_dense = HyperbolicDense(embed_dim)\n",
    "        self.key_dense = HyperbolicDense(embed_dim)\n",
    "        self.value_dense = HyperbolicDense(embed_dim)\n",
    "        \n",
    "    \n",
    "    def call(self, query, key = None, value = None, return_attention_scores = True):\n",
    "        if key is None:\n",
    "            key = query\n",
    "        if value is None:\n",
    "            value = query\n",
    "            \n",
    "        batch_size = tf.shape(query)[0]\n",
    "        _, n_patch, dims = ops.shape(query)\n",
    "        query = self.query_dense(query)\n",
    "        key = self.key_dense(key)\n",
    "        value = self.value_dense(value)\n",
    "        \n",
    "        # Multi-head로 분할\n",
    "        query = ops.reshape(query, [-1, n_patch, self.num_heads, self.projection_dim])\n",
    "        key = ops.reshape(key, [-1, n_patch, self.num_heads, self.projection_dim])\n",
    "        value = ops.reshape(value, [-1, n_patch, self.num_heads, self.projection_dim])\n",
    "        \n",
    "        # Poincare distance를 사용하여 Query와 Key 간의 거리를 계산하여 attention scores 계산\n",
    "        attention_scores = -multihead_poincare_distance(query, key)\n",
    "        \n",
    "        # softmax를 적용하여 attention scores를 확률 분포로 변환\n",
    "        attention_scores = tf.nn.softmax(attention_scores, axis=-1)\n",
    "        # Value에 대해 가중 평균을 계산하여 context 벡터 생성\n",
    "        context = ops.einsum(\"bpph, bphd -> bphd\", attention_scores, value)\n",
    "        context = ops.reshape(context, [-1, n_patch, self.embed_dim])\n",
    "        if return_attention_scores:\n",
    "            return context, ops.transpose(attention_scores, [0, 3,1,2])\n",
    "        else:\n",
    "            return context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c23b8c7",
   "metadata": {
    "papermill": {
     "duration": 0.031108,
     "end_time": "2024-11-08T06:27:02.351274",
     "exception": false,
     "start_time": "2024-11-08T06:27:02.320166",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fec9452",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-11-08T06:27:02.418331Z",
     "iopub.status.busy": "2024-11-08T06:27:02.417824Z",
     "iopub.status.idle": "2024-11-08T06:27:02.427701Z",
     "shell.execute_reply": "2024-11-08T06:27:02.426865Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.044811,
     "end_time": "2024-11-08T06:27:02.429487",
     "exception": false,
     "start_time": "2024-11-08T06:27:02.384676",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_flops(model, model_inputs) -> float:\n",
    "        \"\"\"\n",
    "        Calculate FLOPS [GFLOPs] for a tf.keras.Model or tf.keras.Sequential model\n",
    "        in inference mode. It uses tf.compat.v1.profiler under the hood.\n",
    "        Code reference : https://github.com/tensorflow/tensorflow/issues/32809\n",
    "        \"\"\"\n",
    "        # if not hasattr(model, \"model\"):\n",
    "        #     raise wandb.Error(\"self.model must be set before using this method.\")\n",
    "        \n",
    "        if not isinstance(\n",
    "            model, (tf.keras.models.Sequential, tf.keras.models.Model, keras.models.Model, keras.Sequential, keras.Model)\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                \"Calculating FLOPS is only supported for \"\n",
    "                \"`tf.keras.Model` and `tf.keras.Sequential` instances.\"\n",
    "            )\n",
    "\n",
    "        from tensorflow.python.framework.convert_to_constants import (\n",
    "            convert_variables_to_constants_v2_as_graph,\n",
    "        )\n",
    "\n",
    "        # Compute FLOPs for one sample\n",
    "        batch_size = 1\n",
    "        inputs = [\n",
    "            tf.TensorSpec([batch_size] + inp.shape[1:], inp.dtype)\n",
    "            for inp in model_inputs\n",
    "        ]\n",
    "\n",
    "        # convert tf.keras model into frozen graph to count FLOPs about operations used at inference\n",
    "        real_model = tf.function(model).get_concrete_function(inputs)\n",
    "        frozen_func, _ = convert_variables_to_constants_v2_as_graph(real_model)\n",
    "\n",
    "        # Calculate FLOPs with tf.profiler\n",
    "        run_meta = tf.compat.v1.RunMetadata()\n",
    "        opts = (\n",
    "            tf.compat.v1.profiler.ProfileOptionBuilder(\n",
    "                tf.compat.v1.profiler.ProfileOptionBuilder().float_operation()\n",
    "            )\n",
    "            .with_empty_output()\n",
    "            .build()\n",
    "        )\n",
    "\n",
    "        flops = tf.compat.v1.profiler.profile(\n",
    "            graph=frozen_func.graph, run_meta=run_meta, cmd=\"scope\", options=opts\n",
    "        )\n",
    "\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "\n",
    "        # convert to GFLOPs\n",
    "        return (flops.total_float_ops / 1e9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f889f9d4",
   "metadata": {
    "papermill": {
     "duration": 0.031143,
     "end_time": "2024-11-08T06:27:02.492203",
     "exception": false,
     "start_time": "2024-11-08T06:27:02.461060",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "> custom losses\n",
    "- keras_cv에서 simclr loss 지원\n",
    "- Barlow Twins가 배경 vs 전체 사물의 구별에 효과적(empirical) -> 다른 SSL과 같이 쓰자\n",
    "    - awesome code reference in [keras.io](https://keras.io/examples/vision/barlow_twins/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d4aca03",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-11-08T06:27:02.556089Z",
     "iopub.status.busy": "2024-11-08T06:27:02.555756Z",
     "iopub.status.idle": "2024-11-08T06:27:02.567154Z",
     "shell.execute_reply": "2024-11-08T06:27:02.566287Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.045509,
     "end_time": "2024-11-08T06:27:02.569019",
     "exception": false,
     "start_time": "2024-11-08T06:27:02.523510",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#helper functions\n",
    "# For Barlow Twins\n",
    "def get_cor_matrix(z1, z2, e = 1e-5):\n",
    "    #z1, z2 = [batch, embed_dims] shape tensor\n",
    "    # 각각 Normalize한 뒤 내적 -> dims by dims correlation matrix\n",
    "    z1_mean, z1_std = ops.mean(z1, axis = 0), ops.std(z1, axis = 0)\n",
    "    z2_mean, z2_std = ops.mean(z2, axis = 0), ops.std(z2, axis = 0)\n",
    "    z1 = (z1 - z1_mean) / (z1_std + e)\n",
    "    z2 = (z2 - z2_mean) / (z2_std + e)\n",
    "    bs = tf.cast(ops.shape(z1)[0], tf.float32)\n",
    "    matrix = (ops.transpose(z1)@z2) / bs\n",
    "    matrix = tf.cast(matrix, tf.float32)\n",
    "    return matrix\n",
    "\n",
    "# For VICreg\n",
    "def invariance_loss(za, zb): #invariance\n",
    "    l2_distances = keras.losses.MeanSquaredError(reduction = None)(za, zb)\n",
    "    return ops.cast(l2_distances, \"float32\")\n",
    "\n",
    "def variance(za, e = 1e-4, gamma = 5.0):\n",
    "    mu, var = tf.nn.moments(za, axes = 0)\n",
    "    mu = tf.cast(mu, tf.float32)\n",
    "    var = tf.cast(var, tf.float32)\n",
    "    var += tf.cast(e, tf.float32)\n",
    "    \n",
    "    s_xe_hinge = gamma - tf.math.sqrt(var)\n",
    "    s_val = tf.math.maximum(0.0, s_xe_hinge)\n",
    "    return ops.cast(s_val, \"float32\")\n",
    "\n",
    "def covariance(z, e = 1e-4, testing = False):\n",
    "    N, D = ops.shape(z)\n",
    "    z = z - tf.reduce_mean(z, axis=0, keepdims = True)\n",
    "    cov = (ops.transpose(z) @ z) / (N - 1)\n",
    "    \n",
    "    # 대각 요소를 0으로 만들어 off-diagonal 요소만 선택\n",
    "    mask = tf.ones_like(cov) - tf.eye(D)\n",
    "    cov_off_diag = cov * mask\n",
    "    \n",
    "    loss = tf.reduce_sum(tf.square(cov_off_diag)) / D\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ecb5656",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T06:27:02.632745Z",
     "iopub.status.busy": "2024-11-08T06:27:02.632442Z",
     "iopub.status.idle": "2024-11-08T06:27:02.639705Z",
     "shell.execute_reply": "2024-11-08T06:27:02.638950Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.041387,
     "end_time": "2024-11-08T06:27:02.641583",
     "exception": false,
     "start_time": "2024-11-08T06:27:02.600196",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BarlowLoss(keras.losses.Loss):\n",
    "    def __init__(self, diag = 0.6, off_diag = 0.4):\n",
    "        super().__init__()\n",
    "        self.diag = diag\n",
    "        self.off_diag = off_diag\n",
    "        \n",
    "    def compute_loss(self, correlation_matrix):\n",
    "        diag_component = tf.linalg.diag_part(correlation_matrix)\n",
    "        zero_diag = ops.zeros(correlation_matrix.shape[-1])\n",
    "        off_diag_matrix = tf.linalg.set_diag(correlation_matrix, zero_diag)\n",
    "        \n",
    "        diag_loss = tf.pow(diag_component-1, 2) * self.diag\n",
    "        off_diag_loss = tf.pow(off_diag_matrix, 2) * self.off_diag\n",
    "        \n",
    "        loss = ops.mean(diag_loss) + ops.mean(off_diag_loss)\n",
    "        loss = tf.cast(loss, tf.float32)\n",
    "        return loss\n",
    "    def call(self, z_a, z_b):\n",
    "        cor_matrix = get_cor_matrix(z_a, z_b)\n",
    "        return self.compute_loss(cor_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be54ef0d",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-11-08T06:27:02.705342Z",
     "iopub.status.busy": "2024-11-08T06:27:02.705064Z",
     "iopub.status.idle": "2024-11-08T06:27:02.721059Z",
     "shell.execute_reply": "2024-11-08T06:27:02.720352Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.050193,
     "end_time": "2024-11-08T06:27:02.723051",
     "exception": false,
     "start_time": "2024-11-08T06:27:02.672858",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VicReg(keras.losses.Loss):\n",
    "    \"\"\"VicReg Loss.\n",
    "\n",
    "    [VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning](https://arxiv.org/abs/2105.04906)\n",
    "    \"\"\"  # noqa\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        std_const: float = 1e-4,\n",
    "        lambda_: float = 25,\n",
    "        mu: float = 25,\n",
    "        nu: float = 1,\n",
    "        reduction = tf.keras.losses.Reduction.NONE,\n",
    "        name: str = \"vicreg_loss\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(reduction=reduction, name=name, **kwargs)\n",
    "        self.lambda_ = lambda_\n",
    "        self.mu = mu\n",
    "        self.nu = nu\n",
    "        self.std_const = std_const\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def call(self, za, zb) :\n",
    "        \"\"\"Compute the lost.\n",
    "        Args:\n",
    "            za: Embedding A\n",
    "            zb: Embedding B\n",
    "        Returns:\n",
    "            loss\n",
    "        \"\"\"\n",
    "        # compute the diagonal\n",
    "        batch_size = tf.shape(za)[0]\n",
    "\n",
    "        # distance loss to measure similarity between representations\n",
    "        sim_loss = tf.keras.losses.MeanSquaredError(reduction=self.reduction)(za, zb)\n",
    "        sim_loss = tf.keras.losses.MeanSquaredError(reduction=\"none\")(za, zb)\n",
    "\n",
    "        za = self.mean_center_columns(za)\n",
    "        zb = self.mean_center_columns(zb)\n",
    "\n",
    "        # std loss to maximize variance(information)\n",
    "        std_za = tf.sqrt(tf.math.reduce_variance(za, 0) + self.std_const)\n",
    "        std_zb = tf.sqrt(tf.math.reduce_variance(zb, 0) + self.std_const)\n",
    "\n",
    "        std_loss_za = tf.reduce_mean(tf.math.maximum(0.0, 1 - std_za))\n",
    "        std_loss_zb = tf.reduce_mean(tf.math.maximum(0.0, 1 - std_zb))\n",
    "\n",
    "        std_loss = std_loss_za / 2 + std_loss_zb / 2\n",
    "\n",
    "        off_diag_ca = self.cov_loss_each(za, batch_size)\n",
    "        off_diag_cb = self.cov_loss_each(zb, batch_size)\n",
    "\n",
    "        # covariance loss(1d tensor) for redundancy reduction\n",
    "        cov_loss = off_diag_ca + off_diag_cb\n",
    "\n",
    "        loss = self.lambda_ * sim_loss + self.mu * std_loss + self.nu * cov_loss\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def get_config(self) :\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"std_const\": self.std_const,\n",
    "                \"lambda_\": self.lambda_,\n",
    "                \"mu\": self.mu,\n",
    "                \"nu\": self.nu,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "\n",
    "    def off_diagonal(self, x) :\n",
    "        n = tf.shape(x)[0]\n",
    "        flattened = tf.reshape(x, [-1])[:-1]\n",
    "        off_diagonals = tf.reshape(flattened, (n - 1, n + 1))[:, 1:]\n",
    "        off_diag = tf.reshape(off_diagonals, [-1])\n",
    "        return off_diag\n",
    "\n",
    "    def cov_loss_each(self, z, batch_size):\n",
    "        # cross-correlation matrix axa\n",
    "        c = tf.matmul(z, z, transpose_a=True)\n",
    "        c = c / tf.cast(batch_size - 1, dtype=c.dtype)\n",
    "\n",
    "        num_features = tf.shape(c)[0]\n",
    "\n",
    "        off_diag_c = self.off_diagonal(c)\n",
    "        off_diag_c = tf.math.pow(off_diag_c, 2)\n",
    "\n",
    "        off_diag_c = tf.math.reduce_sum(off_diag_c) / tf.cast(num_features, dtype=c.dtype)\n",
    "\n",
    "        return off_diag_c\n",
    "\n",
    "    def mean_center_columns(self, x) :\n",
    "        col_mean = tf.math.reduce_mean(x, axis=0)\n",
    "\n",
    "        norm_col = x - col_mean\n",
    "        return norm_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3214ded9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T06:27:02.824000Z",
     "iopub.status.busy": "2024-11-08T06:27:02.823248Z",
     "iopub.status.idle": "2024-11-08T06:27:02.837138Z",
     "shell.execute_reply": "2024-11-08T06:27:02.836251Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.048756,
     "end_time": "2024-11-08T06:27:02.839003",
     "exception": false,
     "start_time": "2024-11-08T06:27:02.790247",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HyperbolicContrastiveLoss(keras.losses.Loss):\n",
    "    def __init__(self, temperature=0.1, curvature=0.6, name=\"hyperbolic_contrastive_loss\"):\n",
    "        super().__init__(name=name)\n",
    "        self.temperature = temperature\n",
    "        self.curvature = curvature\n",
    "        self.epsilon = 1e-4\n",
    "\n",
    "    def expmap0(self, u):\n",
    "        u_norm = ops.norm(u, axis=-1, keepdims=True)\n",
    "        return ops.tanh(ops.sqrt(self.curvature) * u_norm) * u / (ops.sqrt(self.curvature) * u_norm + self.epsilon)\n",
    "\n",
    "    def hyperbolic_distance(self, x, y):\n",
    "        x_norm = ops.sum(x**2, axis=-1, keepdims=True)\n",
    "        y_norm = ops.sum(y**2, axis=-1, keepdims=True)\n",
    "        xy_inner = ops.sum(x * y, axis=-1, keepdims=True)\n",
    "        \n",
    "        num = 2 * self.curvature * xy_inner\n",
    "        denom = (1 - self.curvature * x_norm) * (1 - self.curvature * y_norm) + self.epsilon\n",
    "        \n",
    "        return ops.arccosh(1 + num / denom) / ops.sqrt(self.curvature)\n",
    "\n",
    "    def call(self, y_true, y_pred = None):\n",
    "        \n",
    "        total_loss = 0\n",
    "        n = len(y_true)\n",
    "        batch_size = ops.shape(y_true[0])[0]\n",
    "\n",
    "        # Project all representations to hyperbolic space\n",
    "        hyperbolic_reps = [self.expmap0(rep) for rep in y_true]\n",
    "\n",
    "        for i in range(n):\n",
    "            for j in range(i+1, n):\n",
    "                # Calculate pairwise distances\n",
    "                distances = self.hyperbolic_distance(hyperbolic_reps[i], hyperbolic_reps[j])\n",
    "                \n",
    "                # Positive pairs are on the diagonal\n",
    "                pos_distances = ops.diagonal(distances)\n",
    "                \n",
    "                # Create a mask to exclude self-comparisons\n",
    "                mask = ops.ones((batch_size, batch_size)) - ops.eye(batch_size)\n",
    "                \n",
    "                # Calculate negative distances (all off-diagonal elements)\n",
    "                neg_distances = ops.reshape(distances * mask, (batch_size, -1))\n",
    "                \n",
    "                # Contrastive loss\n",
    "                pos_loss = ops.exp(-pos_distances / self.temperature)\n",
    "                neg_loss = ops.sum(ops.exp(-neg_distances / self.temperature), axis=-1)\n",
    "                \n",
    "                loss = -ops.log(pos_loss / (pos_loss + neg_loss + self.epsilon))\n",
    "                total_loss += ops.mean(loss)\n",
    "\n",
    "        # Average loss over all pairs\n",
    "        avg_loss = total_loss / (n * (n - 1) / 2)\n",
    "        return avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd37797d",
   "metadata": {
    "papermill": {
     "duration": 0.031216,
     "end_time": "2024-11-08T06:27:02.901641",
     "exception": false,
     "start_time": "2024-11-08T06:27:02.870425",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "> Attention weight loss\n",
    "- works directly to the cls-patches attention weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4162471a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T06:27:02.965831Z",
     "iopub.status.busy": "2024-11-08T06:27:02.965525Z",
     "iopub.status.idle": "2024-11-08T06:27:02.972074Z",
     "shell.execute_reply": "2024-11-08T06:27:02.971137Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.041184,
     "end_time": "2024-11-08T06:27:02.974055",
     "exception": false,
     "start_time": "2024-11-08T06:27:02.932871",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pairwise_kl_divergence(v_a, v_b):\n",
    "    \"\"\"\n",
    "    Compute pairwise KL divergence between attention weights.\n",
    "    \n",
    "    Args:\n",
    "    v_a, v_b: Tensors of shape [batch_size, num_heads, n_patches]\n",
    "    \n",
    "    Returns:\n",
    "    KL divergence matrix of shape [batch_size, num_heads, num_heads]\n",
    "    \"\"\"\n",
    "    # Ensure inputs are float32\n",
    "    v_a = tf.cast(v_a, tf.float32)\n",
    "    v_b = tf.cast(v_b, tf.float32)\n",
    "    \n",
    "    # Add small epsilon to avoid log(0)\n",
    "    epsilon = 1e-5\n",
    "    v_a = v_a + epsilon\n",
    "    v_b = v_b + epsilon\n",
    "    \n",
    "    # Expand dimensions for broadcasting\n",
    "    v_a_expanded = tf.expand_dims(v_a, axis=2)  # [batch_size, num_heads, 1, n_patches]\n",
    "    v_b_expanded = tf.expand_dims(v_b, axis=1)  # [batch_size, 1, num_heads, n_patches]\n",
    "    \n",
    "    # Compute KL divergence\n",
    "    kl_div = ops.sum(v_a_expanded * (ops.log(v_a_expanded) - ops.log(v_b_expanded)), axis=-1)\n",
    "    \n",
    "    return kl_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "211278a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T06:27:03.038158Z",
     "iopub.status.busy": "2024-11-08T06:27:03.037830Z",
     "iopub.status.idle": "2024-11-08T06:27:03.046468Z",
     "shell.execute_reply": "2024-11-08T06:27:03.045577Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.043143,
     "end_time": "2024-11-08T06:27:03.048400",
     "exception": false,
     "start_time": "2024-11-08T06:27:03.005257",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def attention_loss(w, alpha=0, beta=0): #w : batch, heads, 1, n_patches.\n",
    "    # Normalize attention weights along the last dimension\n",
    "    w_normalized = w\n",
    "    w_normalized = w_normalized[..., 0, :] #batch, heads, n_patches\n",
    "    batch_size, heads, n_patches = ops.shape(w_normalized)\n",
    "    w_transpose = ops.reshape(w_normalized, [-1, n_patches])\n",
    "    \n",
    "    entropy = -ops.sum(w_transpose * ops.log(w_transpose + 1e-5), axis=-1)\n",
    "    mean_entropy = ops.mean(entropy)\n",
    "    entropy_loss = 1/ops.mean(ops.square(mean_entropy))\n",
    "    #entropy_loss = ops.mean(1/ops.square(entropy))\n",
    "    #entropy_loss = ops.sqrt(5-mean_entropy)\n",
    "    \n",
    "    mask = ops.ones([1, heads, heads], dtype = \"float32\") - ops.eye(heads, heads, dtype = \"float32\")[tf.newaxis, ...]\n",
    "    # Calculate diversity term : cosine similarity matrix\n",
    "    #cos_mat = ops.einsum(\"bhd, bcd -> bhc\",\n",
    "    #                    ops.norm(w, axis = -1),\n",
    "    #                    ops.norm(w, axis = -1)\n",
    "    #                    ) #batch, heads, heads\n",
    "    #cos_mat *= mask\n",
    "    #headwise_similarity = ops.mean(cos_mat)\n",
    "    \n",
    "    kl_matrix = pairwise_kl_divergence(w_normalized, w_normalized)\n",
    "    kl_matrix *= mask\n",
    "    headwise_kl_divergence = ops.mean(kl_matrix) #KL 커질 수록 두 분포는 다른 분포\n",
    "    \n",
    "    # Combine entropy and diversity loss\n",
    "    total_loss = alpha * (entropy_loss) - beta * headwise_kl_divergence\n",
    "    #loss 감소 = target entropy diff 감소 / headwise cos sim 감소 / kl divergence headwise 증가\n",
    "    return total_loss, mean_entropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8937d265",
   "metadata": {
    "papermill": {
     "duration": 0.031116,
     "end_time": "2024-11-08T06:27:03.110933",
     "exception": false,
     "start_time": "2024-11-08T06:27:03.079817",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# CREATING NEW BACKBONES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ec35bf1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T06:27:03.177268Z",
     "iopub.status.busy": "2024-11-08T06:27:03.176667Z",
     "iopub.status.idle": "2024-11-08T06:27:03.197397Z",
     "shell.execute_reply": "2024-11-08T06:27:03.196657Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.055262,
     "end_time": "2024-11-08T06:27:03.199482",
     "exception": false,
     "start_time": "2024-11-08T06:27:03.144220",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TRBlock(keras.layers.Layer): #ViT Transformer block with register token, attention weight return\n",
    "    def __init__(self, att_depth, att_heads, att_dims,\n",
    "                 embed_dims = None,\n",
    "                 return_att_weight = True,\n",
    "                n_register_token = 4, dropout_rate = 0.25,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.att_depth = att_depth\n",
    "        self.att_dims = att_dims\n",
    "        self.return_weight = return_att_weight\n",
    "        self.n_reg = n_register_token\n",
    "        if embed_dims is None:\n",
    "            self.embed_dims = self.att_dims\n",
    "        else:\n",
    "            self.embed_dims = embed_dims\n",
    "            \n",
    "        self.ln_before_set = [keras.layers.LayerNormalization(name = f'LN_before_{i+1}') for i in range(self.att_depth)]\n",
    "        self.ln_after_set = [keras.layers.LayerNormalization(name = f'LN_after_{i+1}') for i in range(self.att_depth)]\n",
    "        self.mha_set = [keras.layers.MultiHeadAttention(att_heads, att_dims, name = f\"MHA_{i+1}\", use_bias = False) for i in range(self.att_depth)]\n",
    "        self.att_dropout = keras.layers.Dropout(dropout_rate)\n",
    "        self.proj_dropout = keras.layers.Dropout(dropout_rate)\n",
    "    def build(self, input_shape):\n",
    "        #query, key, value\n",
    "        if len(input_shape) == 3:\n",
    "            query_shape, key_shape, value_shape = input_shape[0], input_shape[1], input_shape[2]\n",
    "        elif len(input_shape) == 2:\n",
    "            query_shape, key_shape = input_shape[0], input_shape[1]\n",
    "            value_shape = key_shape\n",
    "        batch_size = query_shape[0]\n",
    "        self.query_length = query_shape[1]\n",
    "        self.embed_dims = query_shape[2]\n",
    "        self.embed_dim = self.embed_dims\n",
    "        self.dense_set = [Dense(units = self.embed_dims, name = f\"Dense_{i+1}\", activation = \"gelu\") for i in range(self.att_depth)]\n",
    "        self.first_embedding = Dense(units = self.embed_dims, activation = 'gelu')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if len(inputs) == 2:\n",
    "            query = inputs[0]\n",
    "            key = inputs[1]\n",
    "            value = key\n",
    "        elif len(inputs) == 3:\n",
    "            query, key, value = inputs\n",
    "        tok_ = keras.layers.GlobalAveragePooling1D()(query)\n",
    "        tok_ = tok_[:, tf.newaxis, :]\n",
    "        if self.n_reg > 0:\n",
    "            self.register_tokens = ops.cast(ops.ones_like(tok_), \"float32\")\n",
    "            self.register_tokens = ops.tile(self.register_tokens, [1, self.n_reg, 1])\n",
    "        else:\n",
    "            self.register_tokens = None\n",
    "        self.cls_token = ops.cast(ops.ones_like(tok_), \"float32\")\n",
    "        \n",
    "        \n",
    "        if self.register_tokens is not None:\n",
    "            encoded_patches = ops.concatenate([query, \n",
    "                                              self.cls_token,\n",
    "                                              self.register_tokens], axis = 1)\n",
    "        else:\n",
    "            encoded_patches = ops.concatenate([query, \n",
    "                                              self.cls_token], axis = 1)\n",
    "        encoded_patches = self.first_embedding(encoded_patches)\n",
    "        \n",
    "        for idx in range(self.att_depth):\n",
    "            x0 = self.ln_before_set[idx](encoded_patches)\n",
    "            x1_ = self.mha_set[idx](query = encoded_patches, key = key, value = value,\n",
    "                                    return_attention_scores = self.return_weight)\n",
    "\n",
    "            if self.return_weight:\n",
    "                x1, att_weights = x1_\n",
    "                del x1_\n",
    "            else:\n",
    "                x1 = x1_\n",
    "                del x1_\n",
    "            x1 = self.att_dropout(x1)\n",
    "            x2 = x0 + x1\n",
    "            x3 = self.ln_after_set[idx](x2)\n",
    "            x4 = self.dense_set[idx](x3)\n",
    "            x4 = self.proj_dropout(x4)\n",
    "            encoded_patches = x2+x4\n",
    "        patches, cls_token = encoded_patches[:, :self.query_length, :], encoded_patches[:, self.query_length, :]\n",
    "        del encoded_patches\n",
    "        if self.return_weight:\n",
    "            return cls_token, patches, att_weights[:, :, self.query_length:self.query_length+1, :self.query_length]\n",
    "        else:\n",
    "            return cls_token, patches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8461d4f3",
   "metadata": {
    "papermill": {
     "duration": 0.031427,
     "end_time": "2024-11-08T06:27:03.262149",
     "exception": false,
     "start_time": "2024-11-08T06:27:03.230722",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Brand-New TransformerEncoder block\n",
    "- input tensor : patches (PE를 거치지 않은)\n",
    "    - [batch_size, sequence_length, c] shape tensor. 각각 q, k 혹은 q, k, v\n",
    "- ViT에서 제시된 Transformer Encoder 블록을 유지하되, Self-Attention 자리에:\n",
    "    - simple MHA\n",
    "    - gated MLP 및 gaMLP(-> PE 거치지 않고 attention-like op 수행)\n",
    "    - MLP mixer\n",
    "    - ConvMixer\n",
    "    - Focal Modulation\n",
    "    - 들이 들어갈 수 있게 함. \n",
    "        - Ensemble : 모든 경우의 수의 가중 합\n",
    "- Code reference: [MLP and FNet](https://keras.io/examples/vision/mlp_image_classification/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "645cc049",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T06:27:03.326318Z",
     "iopub.status.busy": "2024-11-08T06:27:03.325948Z",
     "iopub.status.idle": "2024-11-08T06:27:03.343617Z",
     "shell.execute_reply": "2024-11-08T06:27:03.342729Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.052014,
     "end_time": "2024-11-08T06:27:03.345514",
     "exception": false,
     "start_time": "2024-11-08T06:27:03.293500",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DeformableConv2D(keras.layers.Layer):\n",
    "    def __init__(self, filters, kernel_size, groups = None, **kwargs):\n",
    "        super(DeformableConv2D, self).__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv = keras.layers.Conv2D(filters, kernel_size, padding='same', use_bias = True, groups = groups,\n",
    "                                       kernel_regularizer = keras.regularizers.L1L2(l1=0.01, l2=0.01))\n",
    "        self.offset_conv = keras.layers.Conv2D(2 * kernel_size * kernel_size, kernel_size, padding='same', use_bias=False,\n",
    "                                              kernel_regularizer = keras.regularizers.L1L2(l1=0.01, l2=0.01))\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        c = ops.shape(inputs)[-1]\n",
    "        offsets = self.offset_conv(inputs)\n",
    "        grid_y, grid_x = tf.meshgrid(tf.range(tf.shape(inputs)[1]), tf.range(tf.shape(inputs)[2]))\n",
    "        grid = tf.stack((grid_y, grid_x), axis=-1)  # shape (H, W, 2)\n",
    "        grid = tf.expand_dims(grid, axis=0)  # shape (1, H, W, 2)\n",
    "        grid = tf.tile(grid, [tf.shape(inputs)[0], 1, 1, 1])  # shape (N, H, W, 2)\n",
    "        \n",
    "        offsets = tf.reshape(offsets, [tf.shape(inputs)[0], tf.shape(inputs)[1], tf.shape(inputs)[2], -1, 2])\n",
    "        grid = tf.expand_dims(grid, axis=3)\n",
    "        grid = tf.cast(grid, \"float32\")\n",
    "        \n",
    "        offsets = offsets + grid\n",
    "        offsets = tf.cast(offsets, \"float32\")\n",
    "        \n",
    "        x_offset = tf.clip_by_value(offsets[..., 1], 0.0, \n",
    "                            tf.cast(tf.shape(inputs)[2] - 1, \"float32\")\n",
    "                           )\n",
    "        y_offset = tf.clip_by_value(offsets[..., 0], 0.0, \n",
    "                            tf.cast(tf.shape(inputs)[1] - 1, \"float32\")\n",
    "                           )\n",
    "        x_offset, y_offset = tf.cast(x_offset, \"int32\"), tf.cast(y_offset, \"int32\")\n",
    "        \n",
    "        batch_indices = tf.tile(tf.range(tf.shape(inputs)[0])[:, tf.newaxis, tf.newaxis, tf.newaxis], [1, tf.shape(inputs)[1], tf.shape(inputs)[2], self.kernel_size * self.kernel_size])\n",
    "        batch_indices = tf.reshape(batch_indices, [-1])\n",
    "        y_offset = tf.reshape(y_offset, [-1])\n",
    "        x_offset = tf.reshape(x_offset, [-1])\n",
    "        \n",
    "        indices = tf.stack([batch_indices, y_offset, x_offset], axis=-1)\n",
    "        sampled_features = tf.gather_nd(inputs, indices)\n",
    "        sampled_features = tf.reshape(sampled_features, \n",
    "                                      [tf.shape(inputs)[0], tf.shape(inputs)[1], tf.shape(inputs)[2], self.kernel_size * self.kernel_size, -1])\n",
    "        \n",
    "        sampled_features = tf.transpose(sampled_features, [0, 1, 2, 4, 3])\n",
    "        sampled_features = tf.reshape(sampled_features, [tf.shape(inputs)[0], tf.shape(inputs)[1], tf.shape(inputs)[2], c*self.kernel_size * self.kernel_size])\n",
    "        output = self.conv(sampled_features)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d044455",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T06:27:03.411359Z",
     "iopub.status.busy": "2024-11-08T06:27:03.411042Z",
     "iopub.status.idle": "2024-11-08T06:27:03.423851Z",
     "shell.execute_reply": "2024-11-08T06:27:03.422952Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.047588,
     "end_time": "2024-11-08T06:27:03.425826",
     "exception": false,
     "start_time": "2024-11-08T06:27:03.378238",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FNetLayer(layers.Layer):\n",
    "    def __init__(self, embedding_dim, dropout_rate, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "    def call(self, inputs):\n",
    "        # Apply fourier transformations.\n",
    "        real_part = inputs\n",
    "        im_part = keras.ops.zeros_like(inputs)\n",
    "        x = keras.ops.fft2((real_part, im_part))[0]\n",
    "        return x\n",
    "\n",
    "class gDense(layers.Layer): # 원본 논문에서, channel proj, Activation ~ SGU 및 그 이후 Channel proj까지 구현\n",
    "    def __init__(self, embed_dims, dropout_rate = 0.25, use_attention = False,\n",
    "                 *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.embed_dims = embed_dims\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.gelu = keras.layers.Activation('gelu')\n",
    "        self.channel_proj_first = Dense(units = embed_dims, use_bias = False)\n",
    "        self.use_attention = use_attention\n",
    "        self.ln = keras.layers.LayerNormalization(name = \"LN_in_SGU\")        \n",
    "    def build(self, input_shape):\n",
    "        batch_size, self.n_patches, c = input_shape\n",
    "        self.channel_proj_second = Dense(units = c, use_bias = False)\n",
    "        self.spatial_proj = Dense(units = self.n_patches, bias_initializer = keras.initializers.Ones(), \n",
    "                                  kernel_initializer = keras.initializers.Zeros())\n",
    "        if self.use_attention:\n",
    "            self.tiny_attn = MultiHeadAttention(4,64,dropout = self.dropout_rate, use_bias = False, output_shape = self.embed_dims//2)\n",
    "    def call(self, inputs):\n",
    "        x = self.channel_proj_first(inputs)\n",
    "        x = self.gelu(x)\n",
    "        #Spatial Gating unit\n",
    "        u, v = keras.ops.split(x, indices_or_sections=2, axis=2)\n",
    "        v_proj = self.ln(v)\n",
    "        v_proj = keras.ops.transpose(v_proj, (0,2,1))\n",
    "        v_proj = self.spatial_proj(v_proj)\n",
    "        v_proj = keras.ops.transpose(v_proj, (0,2,1))\n",
    "        if self.use_attention:\n",
    "            attn_ = self.tiny_attn(query = v, key = v, value = v)\n",
    "            v_proj += attn_\n",
    "        x = u*v_proj\n",
    "        x = self.channel_proj_second(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea89b4c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T06:27:03.490260Z",
     "iopub.status.busy": "2024-11-08T06:27:03.489610Z",
     "iopub.status.idle": "2024-11-08T06:27:03.505767Z",
     "shell.execute_reply": "2024-11-08T06:27:03.504871Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.05046,
     "end_time": "2024-11-08T06:27:03.507614",
     "exception": false,
     "start_time": "2024-11-08T06:27:03.457154",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Focal modulation implementation -> From keras, https://keras.io/examples/vision/focal_modulation_network/#focal-modulation-layer\n",
    "class FocalModulationLayer(layers.Layer):\n",
    "    \"\"\"The Focal Modulation layer includes query projection & context aggregation.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Projection dimension.\n",
    "        focal_window (int): Window size for focal modulation.\n",
    "        focal_level (int): The current focal level.\n",
    "        focal_factor (int): Factor of focal modulation.\n",
    "        proj_drop_rate (float): Rate of dropout.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        focal_window: int,\n",
    "        focal_level: int,\n",
    "        focal_factor: int = 2,\n",
    "        proj_drop_rate: float = 0.0,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dim = dim\n",
    "        self.focal_window = focal_window\n",
    "        self.focal_level = focal_level\n",
    "        self.focal_factor = focal_factor\n",
    "        self.proj_drop_rate = proj_drop_rate\n",
    "\n",
    "        # Project the input feature into a new feature space using a\n",
    "        # linear layer. Note the `units` used. We will be projecting the input\n",
    "        # feature all at once and split the projection into query, context,\n",
    "        # and gates.\n",
    "        self.initial_proj = layers.Dense(\n",
    "            units=(2 * self.dim) + (self.focal_level + 1),\n",
    "            use_bias=True,\n",
    "        )\n",
    "        self.focal_layers = list()\n",
    "        self.kernel_sizes = list()\n",
    "        for idx in range(self.focal_level):\n",
    "            kernel_size = (self.focal_factor * idx) + self.focal_window\n",
    "            depth_gelu_block = keras.Sequential(\n",
    "                [\n",
    "                    layers.ZeroPadding2D(padding=(kernel_size // 2, kernel_size // 2)),\n",
    "                    layers.Conv2D(\n",
    "                        filters=self.dim,\n",
    "                        kernel_size=kernel_size,\n",
    "                        activation=keras.activations.gelu,\n",
    "                        groups=self.dim,\n",
    "                        use_bias=False,\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "            self.focal_layers.append(depth_gelu_block)\n",
    "            self.kernel_sizes.append(kernel_size)\n",
    "        self.activation = keras.activations.gelu\n",
    "        self.gap = layers.GlobalAveragePooling2D(keepdims=True)\n",
    "        self.modulator_proj = layers.Conv2D(\n",
    "            filters=self.dim,\n",
    "            kernel_size=(1, 1),\n",
    "            use_bias=True,\n",
    "        )\n",
    "        self.proj = layers.Dense(units=self.dim)\n",
    "        self.proj_drop = layers.Dropout(self.proj_drop_rate)\n",
    "\n",
    "    def call(self, x: tf.Tensor, training = True) -> tf.Tensor:\n",
    "        \"\"\"Forward pass of the layer.\n",
    "\n",
    "        Args:\n",
    "            x: Tensor of shape (B, H, W, C)\n",
    "        \"\"\"\n",
    "        # Apply the linear projecion to the input feature map\n",
    "        x_proj = self.initial_proj(x)\n",
    "\n",
    "        # Split the projected x into query, context and gates\n",
    "        query, context, self.gates = tf.split(\n",
    "            value=x_proj,\n",
    "            num_or_size_splits=[self.dim, self.dim, self.focal_level + 1],\n",
    "            axis=-1,\n",
    "        )\n",
    "\n",
    "        # Context aggregation\n",
    "        context = self.focal_layers[0](context)\n",
    "        context_all = context * self.gates[..., 0:1]\n",
    "        for idx in range(1, self.focal_level):\n",
    "            context = self.focal_layers[idx](context)\n",
    "            context_all += context * self.gates[..., idx : idx + 1]\n",
    "\n",
    "        # Build the global context\n",
    "        context_global = self.activation(self.gap(context))\n",
    "        context_all += context_global * self.gates[..., self.focal_level :]\n",
    "\n",
    "        # Focal Modulation\n",
    "        self.modulator = self.modulator_proj(context_all)\n",
    "        x_output = query * self.modulator\n",
    "\n",
    "        # Project the output and apply dropout\n",
    "        x_output = self.proj(x_output)\n",
    "        x_output = self.proj_drop(x_output)\n",
    "\n",
    "        return x_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "48577474",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T06:27:03.572288Z",
     "iopub.status.busy": "2024-11-08T06:27:03.571944Z",
     "iopub.status.idle": "2024-11-08T06:27:03.593176Z",
     "shell.execute_reply": "2024-11-08T06:27:03.592298Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.055862,
     "end_time": "2024-11-08T06:27:03.595174",
     "exception": false,
     "start_time": "2024-11-08T06:27:03.539312",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# input, delta, A, B, C, D로 SSM을 구현\n",
    "def _ssm(x, delta, A, B, C, D):\n",
    "    \"\"\"\n",
    "    c : input tensor x의 channel (paper notation = n)\n",
    "    d : 모델 내부에서 projection dimension (attention dims 비슷한 역할)\n",
    "    \n",
    "    x : input tensor, [batch, length, c]\n",
    "    delta : 이산화를 위한 parameter\n",
    "    A, B, C, D : SSM에 쓰이는 Tensors\n",
    "    \n",
    "    delta : [batch, length, d]\n",
    "    A : [d, c]\n",
    "    B : [batch, length, d]\n",
    "    input : X, output : X와 같은 shape의 tensor\n",
    "    \"\"\"\n",
    "    #1. discretize\n",
    "    dA = tf.einsum(\"bld, dc -> bldc\", delta, A) #외적\n",
    "    dBu = tf.einsum(\"bld, bld, blc -> bldc\", delta, x, B) #B_bar x inputs(x)\n",
    "    #2. pre-calcuate the kernel\n",
    "    dA_cumulateSum = tf.pad(dA[:, 1:], [[0, 0], [1, 1], [0, 0], [0, 0]])[:, 1:, :, :] # <- Zero padding\n",
    "    dA_cumulateSum = tf.reverse(dA_cumulateSum, axis=[1]) # <- later token 가중치는 이전 가중치들의 총합이라 뒤집어 계산하는게 편함\n",
    "    dA_cumulateSum = tf.math.cumsum(A, axis = 1)\n",
    "    dA_cumulateSum = tf.exp(dA_cumulateSum) #A_bar = exp(delta * A)\n",
    "    dA_cumulateSum = tf.reverse(dA_cumulateSum, axis = [1])\n",
    "    \n",
    "    hidden = dBu * dA_cumulateSum\n",
    "    hidden = tf.math.cumsum(hidden, axis = 1)/(dA_cumulateSum + 1e-5)\n",
    "    y = tf.einsum(\"bldc, blc -> bld\", hidden, C)\n",
    "    return y + x*D\n",
    "\n",
    "class MambaBlock(keras.layers.Layer):\n",
    "    def __init__(self, hidden_dims, projection_expand_factor = 2,\n",
    "                kernel_size = 5, conv_use_bias = True, dense_use_bias = False,\n",
    "                **kwargs) : \n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden_dims = hidden_dims\n",
    "        assert isinstance(projection_expand_factor, int), 'projection_expand_factor should be integer' \n",
    "        self.proj_exp_factor = projection_expand_factor\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv_use_bias = conv_use_bias\n",
    "        self.dense_use_bias = dense_use_bias\n",
    "        \n",
    "        self.model_internal_dim = self.hidden_dims * self.proj_exp_factor\n",
    "        self.conv1d = keras.layers.Conv1D(filters = self.model_internal_dim,\n",
    "                                         use_bias = conv_use_bias,\n",
    "                                         kernel_size = kernel_size,\n",
    "                                         groups = self.model_internal_dim,\n",
    "                                         data_format = \"channels_first\", #ssm 구현 파트에서 tensor axis transpose가 일어난다!!\n",
    "                                         padding = \"causal\")\n",
    "        # Mamba에서는 A만 input indep. 학습대상, B, C, D, delta는 input dependent\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        batch_size, seq_len, d_in = input_shape\n",
    "        self.seq_len = seq_len\n",
    "        self.d_in = d_in\n",
    "        self.in_proj = keras.layers.Dense(units = 2*self.model_internal_dim, \n",
    "                                         input_shape = (d_in,),\n",
    "                                         use_bias = False)\n",
    "        self.dt_rank = int(d_in // 16)\n",
    "        # 특정 시점의 input tensor를 받아 delta, B, C 계산\n",
    "        self.x_proj = keras.layers.Dense(units = self.dt_rank + 2*self.hidden_dims, \n",
    "                                         use_bias = False)\n",
    "        # delta tensor를 받아 model internal dim으로 embed.\n",
    "        self.dt_proj = keras.layers.Dense(self.model_internal_dim,\n",
    "                                         input_shape = (self.dt_rank,),\n",
    "                                         use_bias = True)\n",
    "        ### A, D setting\n",
    "        self.A = self.add_weight(shape = (self.model_internal_dim, self.hidden_dims),\n",
    "                                 initializer = \"glorot_normal\",\n",
    "                                 trainable = True,\n",
    "                                 dtype = \"float32\")\n",
    "        self.D = self.add_weight(shape = (self.model_internal_dim,),\n",
    "                                 initializer = \"ones\",\n",
    "                                 trainable = True,\n",
    "                                 dtype = \"float32\")\n",
    "        self.out_proj = keras.layers.Dense(units = self.d_in, input_shape = (self.model_internal_dim,),\n",
    "                                          use_bias = self.dense_use_bias)\n",
    "    def compute_ssm(self, x):\n",
    "        #input x : [batch, seq_len, internal_dim] shape tensor\n",
    "        \n",
    "        #1. discretize delta, A~D\n",
    "        #A shape : d_in, n == model_internal_dim, hidden dim\n",
    "        A = -ops.exp(self.A)\n",
    "        D = ops.cast(self.D, \"float32\")\n",
    "        x_dbl = self.x_proj(x) #[batch, dt_rank + 2*hid_dim] shape tensor\n",
    "        (delta, B, C) = tf.split(x_dbl, num_or_size_splits = [self.dt_rank, self.hidden_dims, self.hidden_dims],\n",
    "                                axis = -1)\n",
    "        delta = self.dt_proj(delta)\n",
    "        delta = keras.activations.softplus(delta)\n",
    "        ssm_output = _ssm(x, delta, A, B, C, D)\n",
    "        return ssm_output\n",
    "    def call(self, x):\n",
    "        #x = [batch_size, seq_len, embed_dim] shape tensor\n",
    "        try:\n",
    "            x_with_shortcut = self.in_proj(x)\n",
    "            (x, shortcut) = tf.split(x_with_shortcut, num_or_size_splits = [self.model_internal_dim, self.model_internal_dim],\n",
    "                                    axis = -1)\n",
    "            x = ops.transpose(x, [0, 2, 1])\n",
    "            x = self.conv1d(x)\n",
    "            x = ops.transpose(x, [0, 2, 1])\n",
    "            x = keras.activations.swish(x)\n",
    "            x_ssm = self.compute_ssm(x)\n",
    "            x_ssm *= keras.activations.swish(shortcut)\n",
    "            output = self.out_proj(x_ssm)\n",
    "            return output\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7bcf8ba3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T06:27:03.659376Z",
     "iopub.status.busy": "2024-11-08T06:27:03.658728Z",
     "iopub.status.idle": "2024-11-08T06:27:03.678682Z",
     "shell.execute_reply": "2024-11-08T06:27:03.677822Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.054029,
     "end_time": "2024-11-08T06:27:03.680492",
     "exception": false,
     "start_time": "2024-11-08T06:27:03.626463",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MetaEncoder(keras.layers.Layer):\n",
    "    def __init__(self, operation_type = 'attention', att_dims = 256, att_heads = 8, dropout_rate = 0.2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        assert operation_type in [\"attention\", \"gMLP\", 'gaMLP', 'mlpmixer', \"fnet\",\n",
    "                                  'focal_modulation', \"mamba\", \"Mamba\", \"S6\"]\n",
    "        self.op_type = operation_type\n",
    "        self.att_dims = att_dims\n",
    "        self.att_heads = att_heads\n",
    "        \n",
    "        self.layernorm1 = keras.layers.LayerNormalization()\n",
    "        self.layernorm2 = keras.layers.LayerNormalization()\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "    def build(self, input_shape): #image input\n",
    "        if len(input_shape) == 3:\n",
    "            q_shape, k_shape, v_shape = input_shape\n",
    "        elif len(input_shape) == 2:\n",
    "            q_shape, k_shape = input_shape\n",
    "            v_shape = k_shape\n",
    "        elif len(input_shape) == 1:\n",
    "            q_shape = input_shape[0]\n",
    "            v_shape, k_shape = q_shape, q_shape\n",
    "        \n",
    "        batch_size, self.n_patches, self.embed_dims = q_shape\n",
    "        self.res_ = tf.cast(tf.cast(self.n_patches, \"float32\")**0.5,\n",
    "                      \"int32\")\n",
    "        self.proj = Dense(units = self.embed_dims, activation = \"gelu\")\n",
    "        if self.op_type == 'attention':\n",
    "            self.op_fn = keras.layers.MultiHeadAttention(self.att_heads, self.att_dims, \n",
    "                                                        dropout = self.dropout_rate, use_bias = False)\n",
    "        elif self.op_type == \"mlpmixer\":\n",
    "            self.op_fn = keras.Sequential([keras.layers.Permute((2,1)),\n",
    "                                           keras.layers.Dense(units = self.n_patches, use_bias = False),\n",
    "                                           keras.layers.Activation(\"gelu\"),\n",
    "                                           keras.layers.Dropout(self.dropout_rate),\n",
    "                                           keras.layers.Dense(units = self.n_patches, use_bias = False),\n",
    "                                          keras.layers.Permute((2,1))]\n",
    "                                         )\n",
    "        elif self.op_type == \"gMLP\":\n",
    "            self.op_fn = gDense(embed_dims = 2*self.embed_dims, \n",
    "                                use_attention = False)\n",
    "        elif self.op_type == \"gaMLP\":\n",
    "            self.op_fn = gDense(embed_dims = 2*self.embed_dims, \n",
    "                                use_attention = True)\n",
    "        elif self.op_type == 'fnet':\n",
    "            self.op_fn = FNetLayer(self.embed_dims, self.dropout_rate)\n",
    "        elif self.op_type == \"focal_modulation\":\n",
    "            self.op_fn = FocalModulationLayer(self.embed_dims, \n",
    "                                              focal_window = 3, \n",
    "                                              focal_level = 3,\n",
    "                                             proj_drop_rate = self.dropout_rate)\n",
    "        elif self.op_type in [\"mamba\", \"Mamba\", \"S6\"] :\n",
    "            self.op_fn = MambaBlock(self.att_dims, name = \"ForwardMamba\")\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        if len(inputs) == 3:\n",
    "            q, k, v = inputs\n",
    "        elif len(inputs) == 2:\n",
    "            q, k = inputs\n",
    "            v = k\n",
    "        elif len(inputs) == 1:\n",
    "            q = inputs[0]\n",
    "            k = q\n",
    "            v = q\n",
    "        shortcut = q\n",
    "        q = self.layernorm1(q)\n",
    "        if self.op_type == \"attention\":\n",
    "            q, att_weight = self.op_fn(query = q, key = k, value = v, return_attention_scores = True)\n",
    "        else:\n",
    "            if self.op_type == \"focal_modulation\":\n",
    "                q = ops.reshape(q, [-1, self.res_, self.res_, self.embed_dims])\n",
    "                q = self.op_fn(q)\n",
    "                q = ops.reshape(q, [-1, self.n_patches, self.embed_dims])\n",
    "            elif self.op_type in [\"mamba\", \"Mamba\", \"S6\"]:\n",
    "                q = 0.5*(self.op_fn(q) + self.op_fn(tf.reverse(q, axis = [1])\n",
    "                                                   )\n",
    "                        )\n",
    "            else:\n",
    "                q = self.op_fn(q)\n",
    "        q = q + shortcut\n",
    "        q2 = q\n",
    "        q = self.layernorm2(q)\n",
    "        q = self.proj(q)\n",
    "        q += q2 ; del q2\n",
    "        return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "67ed92c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T06:27:03.745171Z",
     "iopub.status.busy": "2024-11-08T06:27:03.744875Z",
     "iopub.status.idle": "2024-11-08T06:27:03.763480Z",
     "shell.execute_reply": "2024-11-08T06:27:03.762612Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.053353,
     "end_time": "2024-11-08T06:27:03.765387",
     "exception": false,
     "start_time": "2024-11-08T06:27:03.712034",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AttentionPooling(keras.layers.Layer):\n",
    "    def __init__(self, attention_heads, attention_dims = None, bias = False, scale = None, \n",
    "                 dropout_rate = 0.05, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.n_heads = attention_heads\n",
    "        self.n_dims = attention_dims\n",
    "        self.bias = bias\n",
    "        self.scale = scale\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # query, key\n",
    "        query_dims = input_shape[0][-1]\n",
    "        key_length = input_shape[1][1]\n",
    "        \n",
    "        if self.n_dims == None:\n",
    "            embed_dims = query_dims\n",
    "        else:\n",
    "            embed_dims = self.n_dims\n",
    "        self.embed_dims = embed_dims\n",
    "        self.per_head_dims = embed_dims//self.n_heads\n",
    "        \n",
    "        self.scale = self.scale if self.scale != None else embed_dims**-0.5\n",
    "        \n",
    "        self.query_embed_fn = keras.layers.Dense(units = embed_dims, use_bias = self.bias,\n",
    "                                               name = \"Q_Embedding_Dense_layer\")\n",
    "        self.key_embed_fn = keras.layers.Dense(units = embed_dims, use_bias = self.bias,\n",
    "                                               name = \"K_Embedding_Dense_layer\")\n",
    "        self.value_embed_fn = keras.layers.Dense(units = embed_dims, use_bias = self.bias,\n",
    "                                               name = \"V_Embedding_Dense_layer\")\n",
    "        \n",
    "        self.softmax = keras.layers.Activation(\"softmax\", name = \"AttentionWeightSoftmax\")\n",
    "        self.proj = keras.layers.Dense(units = query_dims, use_bias = self.bias, \n",
    "                                      name = \"ProjectToOriginalDimension\")\n",
    "        self.att_dropout = keras.layers.Dropout(self.dropout_rate)\n",
    "        self.proj_dropout = keras.layers.Dropout(self.dropout_rate)\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, inputs, **kwargs):\n",
    "        if len(inputs) == 2:\n",
    "            q, k = inputs\n",
    "            value_ = False\n",
    "        elif len(inputs) == 3:\n",
    "            q, k, v = inputs\n",
    "            value_ = True\n",
    "            \n",
    "        if len(ops.shape(q)) == 2:\n",
    "            q = q[:, tf.newaxis, :]\n",
    "        if len(ops.shape(k)) == 4:\n",
    "            b_, w_, h_, dims_ = ops.shape(k)\n",
    "            k = ops.reshape(k, [b_, w_*h_, dims_])\n",
    "        if (value_) and (len(ops.shape(v))) == 4:\n",
    "            b_, w_, h_, dims_ = ops.shape(v)\n",
    "            v = ops.reshape(v, [b_, w_*h_, dims_])\n",
    "            \n",
    "        batch_size, query_length, q_dims = ops.shape(q)\n",
    "        _, key_length, k_dms = ops.shape(k)\n",
    "        \n",
    "        query = self.query_embed_fn(q) * self.scale #batch, 1(or, query length), q_dims\n",
    "        query = ops.reshape(query, [batch_size, query_length, self.n_heads, self.per_head_dims])\n",
    "        \n",
    "        key = self.key_embed_fn(k)\n",
    "        key = ops.reshape(key, [batch_size, key_length, self.n_heads, self.per_head_dims])\n",
    "        \n",
    "        if value_:\n",
    "            value = self.value_embed_fn(v)#각각 batch, token_length, heads, per_head_dims\n",
    "        else:\n",
    "            value = self.value_embed_fn(k)\n",
    "        value = ops.reshape(value, [batch_size, key_length, self.n_heads, self.per_head_dims])\n",
    "        attention_score = keras.ops.einsum(\"abhd, achd -> ahbc\",\n",
    "                                          query, key) #b = query length, c = key length\n",
    "        attention_weight = self.softmax(attention_score)\n",
    "        attention_weight = self.att_dropout(attention_weight)\n",
    "        self.attention_weight = attention_weight \n",
    "        attended_output = keras.ops.einsum(\"ahbc, achd -> abhd\",\n",
    "                                          attention_weight, value)\n",
    "        attended_output = keras.ops.reshape(attended_output, \n",
    "                                           [batch_size, query_length, self.n_heads*self.per_head_dims]\n",
    "                                           )\n",
    "        attended_output = self.proj(attended_output)\n",
    "        attended_output = self.proj_dropout(attended_output)\n",
    "        return attended_output, attention_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e1c57ecb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T06:27:03.829285Z",
     "iopub.status.busy": "2024-11-08T06:27:03.828975Z",
     "iopub.status.idle": "2024-11-08T06:27:03.838711Z",
     "shell.execute_reply": "2024-11-08T06:27:03.837846Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.044025,
     "end_time": "2024-11-08T06:27:03.840539",
     "exception": false,
     "start_time": "2024-11-08T06:27:03.796514",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ImagePatchEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, patch_size, embed_dim, groups, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = embed_dim\n",
    "        #self.conv = DeformableConv2D(filters = self.embed_dim, kernel_size = 4, groups = groups)\n",
    "        self.conv = keras.layers.Conv2D(filters = self.embed_dim, kernel_size = 3, \n",
    "                                        groups = groups, activation = \"gelu\", padding = 'same',\n",
    "                                       kernel_regularizer = keras.regularizers.L1L2(l1=0.01, l2=0.01)\n",
    "                                       )\n",
    "    def build(self, input_shape): #image input\n",
    "        batch_size, h, w, c = input_shape\n",
    "        self.n_patches = (h//self.patch_size) * (w//self.patch_size)\n",
    "        self.pe_coefficient = c**-0.5\n",
    "        self.position_embedding= keras.layers.Embedding(\n",
    "            input_dim=self.n_patches, output_dim=self.embed_dim\n",
    "        )\n",
    "    def call(self, image):\n",
    "        \n",
    "        positions = ops.expand_dims(\n",
    "            ops.arange(start=0, stop=self.n_patches, step=1), axis=0\n",
    "        )\n",
    "\n",
    "        image = ops.image.extract_patches(image, size= self.patch_size, padding = 'same')\n",
    "        image = self.conv(image)\n",
    "        batch, w, h, dims = ops.shape(image)\n",
    "        image = ops.reshape(image, [-1, w*h, dims])\n",
    "        image += self.pe_coefficient * (self.position_embedding(positions))\n",
    "       \n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "761603a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T06:27:03.904721Z",
     "iopub.status.busy": "2024-11-08T06:27:03.904036Z",
     "iopub.status.idle": "2024-11-08T06:27:03.911346Z",
     "shell.execute_reply": "2024-11-08T06:27:03.910469Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.041342,
     "end_time": "2024-11-08T06:27:03.913170",
     "exception": false,
     "start_time": "2024-11-08T06:27:03.871828",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PatchEncoder(keras.layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = keras.layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = keras.layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = ops.expand_dims(\n",
    "            ops.arange(start=0, stop=self.num_patches, step=1), axis=0\n",
    "        )\n",
    "        projected_patches = self.projection(patch)\n",
    "        encoded = projected_patches + self.position_embedding(positions)\n",
    "        return encoded\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"num_patches\": self.num_patches})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dcebb4",
   "metadata": {
    "papermill": {
     "duration": 0.031097,
     "end_time": "2024-11-08T06:27:03.975307",
     "exception": false,
     "start_time": "2024-11-08T06:27:03.944210",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# General Context Vision Transformer implementation\n",
    "- Reference : [another kaggle notebook, identical implementation](https://www.kaggle.com/code/hskimjjys/general-context-vit-script/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b3668076",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T06:27:04.040087Z",
     "iopub.status.busy": "2024-11-08T06:27:04.039526Z",
     "iopub.status.idle": "2024-11-08T06:27:04.126945Z",
     "shell.execute_reply": "2024-11-08T06:27:04.126240Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.122689,
     "end_time": "2024-11-08T06:27:04.128987",
     "exception": false,
     "start_time": "2024-11-08T06:27:04.006298",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SE(keras.layers.Layer):\n",
    "    def __init__(self, output_dim = None, squeeze_rate = 0.25, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.output_dim = output_dim\n",
    "        self.rate = squeeze_rate\n",
    "    def build(self, input_shape) : #batch_size, h, w, dims\n",
    "        if self.output_dim == None:\n",
    "            self.output_dim = input_shape[-1]\n",
    "        else:\n",
    "            pass\n",
    "        self.avg_pool = keras.layers.GlobalAveragePooling2D(keepdims = True, name = \"AvgPooling\")\n",
    "        self.mlps = keras.Sequential([keras.layers.Dense(units = int(self.rate * self.output_dim),\n",
    "                                                        use_bias = False, name = \"Dense1\"),\n",
    "                                      keras.layers.Activation(\"gelu\", name = \"GeluAct\"),\n",
    "                                      keras.layers.Dense(units = self.output_dim, use_bias = False, name = \"Dense2\"),\n",
    "                                      keras.layers.Activation(\"sigmoid\", name = \"Excitation_Sigmoid\")\n",
    "                                     ])\n",
    "        #super().build(input_shape)\n",
    "    def call(self, inputs, **kwargs):\n",
    "        pooled = self.avg_pool(inputs)\n",
    "        weights = self.mlps(pooled)\n",
    "        return inputs * weights\n",
    "    \n",
    "class DownSampler(keras.layers.Layer):\n",
    "    def __init__(self, keepdims = False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.keepdims = keepdims\n",
    "    def build(self, input_shape):\n",
    "        embed_dims = input_shape[-1]\n",
    "        out_dim = embed_dims if self.keepdims else 2*embed_dims\n",
    "        self.fused_mbconv = keras.Sequential([keras.layers.DepthwiseConv2D(kernel_size = 3, padding = 'same', use_bias = False, name = \"DWConv\"),\n",
    "                                             keras.layers.Activation(\"gelu\", name = 'GeluAct'),\n",
    "                                             SE(name = \"SqueezeAndExcitation2D\"),\n",
    "                                             keras.layers.Conv2D(filters = embed_dims, kernel_size = 1, padding = 'same', use_bias = False, name = \"PointWiseConv\")],\n",
    "                                            name = \"Fused_MBConvLayer\")\n",
    "        self.down_conv = keras.layers.Conv2D(filters = out_dim, kernel_size = 3, strides = 2, padding = 'same', use_bias = False, name = \"DownConvolution\")\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon = 1e-5, name = 'LayerNorm1')\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon = 1e-5, name = 'LayerNorm2')\n",
    "    def call(self, inputs, **kwargs):\n",
    "        x = self.layernorm1(inputs)\n",
    "        x += self.fused_mbconv(inputs)\n",
    "        x = self.down_conv(x)\n",
    "        return self.layernorm2(x)\n",
    "    \n",
    "class MLP(keras.layers.Layer):\n",
    "    def __init__(self, middle_dim = None, output_dim = None,\n",
    "                activation = 'gelu', dropout = 0.2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.middle_dim = middle_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.activation = activation\n",
    "        self.dropout_rate = dropout\n",
    "    def build(self, input_shape):\n",
    "        self.input_dims = input_shape[-1]\n",
    "        self.middle_dim = int(1.5*self.input_dims) if self.middle_dim == None else self.middle_dim\n",
    "        self.output_dim = self.input_dims if self.output_dim == None else self.output_dim\n",
    "        self.mlp1 = keras.layers.Dense(units = self.middle_dim, name = \"FirstMLP\")\n",
    "        self.act = keras.layers.Activation(self.activation, name = \"MiddleActivation\")\n",
    "        self.mlp2 = keras.layers.Dense(units = self.output_dim, name = \"SecondMLP\")\n",
    "        self.drop1 = keras.layers.Dropout(self.dropout_rate, name = \"Dropout1\")\n",
    "        self.drop2 = keras.layers.Dropout(self.dropout_rate, name = \"Dropout2\")\n",
    "    def call(self, inputs, **kwargs):\n",
    "        x = self.mlp1(inputs)\n",
    "        x = self.act(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.mlp2(x)\n",
    "        x = self.drop2(x)\n",
    "        return x\n",
    "    \n",
    "class PatchEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, patching_type = \"conv\", #conv or tokenlearner\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.patching_type = patching_type\n",
    "    def build(self, input_shape):\n",
    "        if (self.patching_type == \"tokenlearner\") or (self.patching_type == \"token_learner\"):\n",
    "            self.proj = keras.layers.Conv2D(self.embed_dim, kernel_size = 3, strides = 2, padding = 'same', name = \"projection_conv\") #Overlapping patches\n",
    "            #token learner implementation\n",
    "            batch_size, w, h, filters = input_shape\n",
    "            n_tokens = (w//4) * (h//4) ; self.resized_w, self.resized_h = int(w//4), int(h/4)\n",
    "            self.input_seq_flatten = keras.layers.Reshape([1, -1, self.embed_dim], name = \"image_to_sequence_reshape\")\n",
    "            self.layer_norm = keras.layers.LayerNormalization(epsilon = 1e-5)\n",
    "            self.attention_ops = keras.Sequential([keras.layers.Conv2D(n_tokens, kernel_size = 3, activation = \"gelu\", use_bias = False, padding = 'same'),\n",
    "                                                  keras.layers.Conv2D(n_tokens, kernel_size = 3, activation = \"gelu\", use_bias = False, padding = 'same'),\n",
    "                                                  keras.layers.Conv2D(n_tokens, kernel_size = 3, activation = \"sigmoid\", use_bias = False, padding = 'same'),\n",
    "                                                  keras.layers.Reshape([-1, n_tokens]), #batch_size, HW, n_tokens\n",
    "                                                  keras.layers.Permute([2,1])], #batch_size, n_tokens, HW\n",
    "                                                 name = \"Conv_for_attention_weight\")\n",
    "        else:\n",
    "            self.proj = keras.layers.Conv2D(self.embed_dim, kernel_size = 3, strides = 2, padding = 'same') #Overlapping patches\n",
    "            self.down_sample = DownSampler(keepdims = True, name = \"DownSampler_after_projection\")\n",
    "    def call(self, inputs, **kwargs):\n",
    "        if (self.patching_type == \"tokenlearner\") or (self.patching_type == \"token_learner\"):\n",
    "            #token learner implementation\n",
    "            norm_input = self.layer_norm(inputs)\n",
    "            proj_inputs = self.proj(norm_input)\n",
    "            seq_inputs = self.input_seq_flatten(proj_inputs) #batch, 1, HW, embed_dims\n",
    "            att_weights = self.attention_ops(proj_inputs) #batch, n_tokens, HW\n",
    "            att_weights = ops.expand_dims(att_weights, axis = -1) #batch, n_tokens, HW, 1\n",
    "            attended = att_weights * seq_inputs #batch, n_tokens, HW, embed_dims\n",
    "            attended = ops.mean(attended, axis = 2) #batch, n_tokens, embed_dims\n",
    "            #reshape to 2D array\n",
    "            attended = ops.reshape(attended, [-1, self.resized_w, self.resized_h, self.embed_dim]\n",
    "                                  )\n",
    "            return attended\n",
    "        else:\n",
    "            x = self.proj(inputs)\n",
    "            x = self.down_sample(x)\n",
    "            return x\n",
    "        \n",
    "class FeatExtract(keras.layers.Layer):\n",
    "    def __init__(self, keepdims = False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.keepdims = keepdims\n",
    "    def build(self, input_shape):\n",
    "        batch_size, H, W, embed_dims = input_shape\n",
    "        self.fused_mbconv = keras.Sequential([keras.layers.DepthwiseConv2D(kernel_size = 3, padding = 'same', use_bias = False, name = \"DWConv\"),\n",
    "                                             keras.layers.Activation(\"gelu\", name = 'GeluAct'),\n",
    "                                             SE(name = \"SqueezeAndExcitation2D\"),\n",
    "                                             keras.layers.Conv2D(filters = embed_dims, kernel_size = 1, padding = 'same', use_bias = False, name = \"PointWiseConv\")],\n",
    "                                            name = \"Fused_MBConvLayer\")\n",
    "        if self.keepdims == False:\n",
    "            self.pool = keras.layers.MaxPooling2D(name = \"FeatExtractMaxPool2D\")\n",
    "    def call(self, inputs):\n",
    "        x = inputs + self.fused_mbconv(inputs)\n",
    "        if self.keepdims == False:\n",
    "            return self.pool(x)\n",
    "        return x\n",
    "    \n",
    "class GlobalQueryGenerator(keras.layers.Layer):\n",
    "    def __init__(self, keepdims = False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.keepdims = keepdims #Keepdims는 여기서 0과 1로 이루어진 list도 될 수 있다 -> FeatExtract layer를 keepdims의 원소 갯수만큼 repeat!\n",
    "    def build(self, input_shape):\n",
    "        self.q_generator = keras.Sequential([FeatExtract(keepdims = keepdim, name = f\"FeatureExtraction_{idx+1}\") for idx, keepdim in enumerate(self.keepdims)])\n",
    "    def call(self, inputs):\n",
    "        return self.q_generator(inputs)\n",
    "    \n",
    "class WindowAttention(keras.layers.Layer):\n",
    "    def __init__(self, window_size, n_heads, global_query, #제공된다면 global, 아니라면 local mHSA -> 0 or 1\n",
    "                qkv_bias = True, qk_scale = None,\n",
    "                dropout_rate = 0.05, return_attention_weights = False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.window_size = (window_size, window_size)\n",
    "        self.n_heads = n_heads\n",
    "        self.global_query = global_query\n",
    "        self.bias = qkv_bias\n",
    "        self.scale = qk_scale\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.return_attention_weights = return_attention_weights\n",
    "    def build(self, input_shape) :\n",
    "        #input = [query, key, value]\n",
    "        embed_dims = input_shape[0][-1]\n",
    "        head_dims = embed_dims//self.n_heads\n",
    "        self.scale = self.scale if self.scale != None else embed_dims**-0.5\n",
    "        self.qkv_size = 3-int(self.global_query)\n",
    "        self.qkv_embed_fn = keras.layers.Dense(units = embed_dims * self.qkv_size, use_bias = self.bias,\n",
    "                                 name = \"QKV_Embedding_Dense_layer\")\n",
    "        self.softmax = keras.layers.Activation(\"softmax\", name = \"AttentionWeightSoftmax\") #for attention weight computation\n",
    "        self.proj = keras.layers.Dense(units = embed_dims, use_bias = self.bias, name = \"Projection\")\n",
    "        self.attention_dropout = keras.layers.Dropout(self.dropout_rate)\n",
    "        self.projection_dropout = keras.layers.Dropout(self.dropout_rate)\n",
    "        self.relative_position_bias_table = self.add_weight(\n",
    "            name=\"relative_position_bias_table\",\n",
    "            shape=[\n",
    "                (2 * self.window_size[0] - 1) * (2 * self.window_size[1] - 1),\n",
    "                self.n_heads,\n",
    "            ],\n",
    "            initializer=keras.initializers.TruncatedNormal(stddev=0.02),\n",
    "            trainable=True,\n",
    "            dtype=self.dtype,\n",
    "        ) #<- learnable weight of relational position. 위 window size = 4의 예시에서, 임의의 두 지점 간 거리의 경우의 수는 총 49개 -> 이에 해당하는 weight tensor를 만듬.\n",
    "        super().build(input_shape)\n",
    "    def get_relative_position_index(self): #<- window 내 2 지점 간 거리의 index matrix.\n",
    "        coords_h = ops.arange(self.window_size[0])\n",
    "        coords_w = ops.arange(self.window_size[1])\n",
    "        coords = ops.stack(ops.meshgrid(coords_h, coords_w, indexing=\"ij\"), axis=0)\n",
    "        coords_flatten = ops.reshape(coords, [2, -1])\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
    "        relative_coords = ops.transpose(relative_coords, axes=[1, 2, 0])\n",
    "        relative_coords_xx = relative_coords[:, :, 0] + self.window_size[0] - 1\n",
    "        relative_coords_yy = relative_coords[:, :, 1] + self.window_size[1] - 1\n",
    "        relative_coords_xx = relative_coords_xx * (2 * self.window_size[1] - 1)\n",
    "        relative_position_index = relative_coords_xx + relative_coords_yy\n",
    "        return relative_position_index\n",
    "    def call(self, inputs, **kwargs):\n",
    "        #input : key(=value), global query OR key only\n",
    "        # input component shape : batch*n_windows, h_window*w_window, embed_dims -> level/block 설계 시 repeat 처리 후 attention에 feed\n",
    "        if self.global_query :\n",
    "            inputs, q_global = inputs\n",
    "            batch_size = ops.shape(q_global)[0]\n",
    "        else:\n",
    "            inputs = inputs[0]\n",
    "        batch_, token_length, embed_dims = ops.shape(inputs) #global query는 query generator에 의해 token_length 개 만큼의 token으로 전체 이미지/feature map을 압축한 상태\n",
    "        \n",
    "        qkv = self.qkv_embed_fn(inputs) #batch*n_windows, h_w * w_w, qkv_size * embed_dims\n",
    "        \n",
    "        qkv = ops.reshape(qkv, [-1, token_length, self.qkv_size, self.n_heads, embed_dims//self.n_heads])\n",
    "        qkv = ops.transpose(qkv, [2, 0, 3, 1, 4]) #qkv_size, batch_, n_heads, token_length, C\n",
    "        \n",
    "        #QKV 분리\n",
    "        if self.global_query:\n",
    "            k, v = ops.split(qkv, 2, axis = 0) #각각 batch_, n_heads, token_length, C\n",
    "            #repeat the global query tensor\n",
    "            # batch_size, n_query_tokens, dims -> batch_(=batch * n_windows), n_query_tokens, dims\n",
    "            q_global = ops.repeat(q_global, batch_//batch_size, axis = 0) #->batch_, n_query_tokens, dims\n",
    "            q = ops.reshape(q_global, [batch_, token_length, self.n_heads, embed_dims//self.n_heads])\n",
    "            q = ops.transpose(q, [0, 2, 1, 3]\n",
    "                             )\n",
    "        else:\n",
    "            q, k, v = ops.split(qkv, 3, axis = 0)\n",
    "            q = ops.squeeze(q, axis = 0)\n",
    "        k = ops.squeeze(k, axis = 0)\n",
    "        v = ops.squeeze(v, axis = 0)\n",
    "        \n",
    "        q *= self.scale #batch_, n_heads, token_length, dimension_per_heads(=C)\n",
    "        attention_score = q@ops.transpose(k, [0, 1, 3, 2]) #batch_, n_heads, token_length, token_length\n",
    "        \n",
    "        #positional encoding(bias) 계산 -> attention score에 더해 주기\n",
    "        # Code from original keras homepage\n",
    "        relative_position_bias = ops.take(\n",
    "            self.relative_position_bias_table,\n",
    "            ops.reshape(self.get_relative_position_index(), [-1]),\n",
    "        )\n",
    "        relative_position_bias = ops.reshape(\n",
    "            relative_position_bias,\n",
    "            [\n",
    "                self.window_size[0] * self.window_size[1],\n",
    "                self.window_size[0] * self.window_size[1],\n",
    "                -1,\n",
    "            ],\n",
    "        )\n",
    "        relative_position_bias = ops.transpose(relative_position_bias, axes=[2, 0, 1])\n",
    "        attention_score += relative_position_bias[None,]\n",
    "        attention_weight = self.softmax(attention_score)\n",
    "        attention_weight = self.attention_dropout(attention_weight) #batch_, n_heads, token_length, token_length\n",
    "        #value tensor shape : batch_, n_heads, token_length, dimension_per_heads(=C)\n",
    "        attended_output = attention_weight@v\n",
    "        attended_output = ops.transpose(attended_output, [0, 2, 1, 3])\n",
    "        attended_output = ops.reshape(attended_output, [batch_, token_length, embed_dims])\n",
    "        attended_output = self.projection_dropout(self.proj(attended_output))\n",
    "        self.attention_weight = attention_weight\n",
    "        if self.return_attention_weights:\n",
    "            return attended_output, attention_weight\n",
    "        else:\n",
    "            return attended_output\n",
    "        \n",
    "        \n",
    "class Block(keras.layers.Layer):\n",
    "    def __init__(self, #이하는 Window Attention configurations\n",
    "                 window_size, num_heads, global_query, \n",
    "                 qkv_bias = True, qk_scale = None, dropout_rate = 0.05, \n",
    "                 # 이하는 MLP module의 configuration\n",
    "                 mlp_ratio = 4.0, layer_scale = None, return_attention_weights = False,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.window_size = window_size\n",
    "        self.n_heads = num_heads\n",
    "        self.global_query = global_query\n",
    "        self.qkv_bias = qkv_bias\n",
    "        self.qk_scale = qk_scale\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        self.layer_scale = layer_scale\n",
    "        self.return_attention_weights = return_attention_weights\n",
    "    def build(self, input_shape):\n",
    "        #input tensor : list of key/query or key only\n",
    "        # each tensor is batch_size, w, h, channel dims shape tensor\n",
    "        batch_size, H, W, dims = input_shape[0]\n",
    "        self.norm1 = keras.layers.LayerNormalization(epsilon = 1e-5)\n",
    "        self.norm2 = keras.layers.LayerNormalization(epsilon = 1e-5)\n",
    "        self.window_attention = WindowAttention(window_size = self.window_size,\n",
    "                                               n_heads = self.n_heads,\n",
    "                                               global_query = self.global_query,\n",
    "                                               qkv_bias = self.qkv_bias,\n",
    "                                               qk_scale = self.qk_scale,\n",
    "                                               dropout_rate = self.dropout_rate,\n",
    "                                                return_attention_weights = self.return_attention_weights)\n",
    "        self.mlps = MLP(middle_dim = int(self.mlp_ratio * dims), dropout = self.dropout_rate)\n",
    "        if self.layer_scale != None:\n",
    "            self.gamma1 = self.add_weight(shape = [dims], name = \"Gamma1\", trainable = True,\n",
    "                                         initializer = keras.initializer.Constant(self.layer_scale), dtype = self.dtype)\n",
    "            self.gamma2 = self.add_weight(shape = [dims], name = \"Gamma2\", trainable = True,\n",
    "                                         initializer = keras.initializer.Constant(self.layer_scale), dtype = self.dtype)\n",
    "        else:\n",
    "            self.gamma1, self.gamma2 = 1.0, 1.0\n",
    "        self.n_windows = int(H//self.window_size) * int(W//self.window_size)\n",
    "        \n",
    "    #input feature map을 일정 크기의 window로 partition을 만들어주는 함수 및\n",
    "    # 그 partition을 받아 원래의 feature map으로 돌려주는 함수를 만들자\n",
    "    def window_partition(self, inputs): #feature map -> multiple windows\n",
    "        batch_size, H, W, dims = ops.shape(inputs)\n",
    "        h, w = H//self.window_size, W//self.window_size\n",
    "        inputs = ops.reshape(inputs, [batch_size, \n",
    "                                      h, self.window_size,\n",
    "                                     w, self.window_size, \n",
    "                                     dims])\n",
    "        inputs = ops.transpose(inputs, [0,#batch_size\n",
    "                                        1,3, #h, w\n",
    "                                        2,4, #winsize, winsize\n",
    "                                        5])\n",
    "        return ops.reshape(inputs, [-1, self.window_size, self.window_size, dims]) #batch_size*n_windows, window_size, window_size, dims\n",
    "        \n",
    "    def window_reverse(self, inputs, H, W, dims): #window partition -> original feature map\n",
    "        x = ops.reshape(inputs, [-1, H//self.window_size, W//self.window_size, self.window_size, self.window_size, dims])\n",
    "        x = ops.transpose(x, [0, 1, 3, 2, 4, 5])\n",
    "        return ops.reshape(x, [-1, H, W, dims])\n",
    "    \n",
    "    def call(self, inputs, **kwargs):\n",
    "        if self.global_query:\n",
    "            inputs, global_query = inputs\n",
    "        else:\n",
    "            inputs = inputs[0]\n",
    "        batch_size, H, W, dims = ops.shape(inputs)\n",
    "        x = self.norm1(inputs)\n",
    "        x = self.window_partition(x) \n",
    "        x = ops.reshape(x, [-1, self.window_size*self.window_size, dims])\n",
    "        if self.global_query:\n",
    "            outputs_ = self.window_attention([x, global_query]\n",
    "                                     )\n",
    "        else:\n",
    "            outputs_ = self.window_attention([x])\n",
    "        if self.return_attention_weights:\n",
    "            x, attention_weight = outputs_\n",
    "        else:\n",
    "            x = outputs_\n",
    "        x = self.window_reverse(x, H, W, dims)\n",
    "        x = inputs + self.gamma1*x\n",
    "        x += self.gamma2*(self.mlps(self.norm2(x)))\n",
    "        if self.return_attention_weights:\n",
    "            return x, attention_weight\n",
    "        else:\n",
    "            return x\n",
    "    \n",
    "class Level(keras.layers.Layer):\n",
    "    def __init__(self, \n",
    "                depth, #<- Block repetition depth\n",
    "                num_heads, window_size, keepdims, #downsampler 및 block의 hyperparameter\n",
    "                downsample = True, mlp_ratio = 4.0,\n",
    "                qkv_bias = True, qk_scale = None,\n",
    "                dropout = 0.05, layer_scale = None, return_attention_weights = True,\n",
    "                **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.depth = depth\n",
    "        self.n_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.keepdims = keepdims\n",
    "        self.downsample = downsample\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        self.qkv_bias = qkv_bias\n",
    "        self.qk_scale = qk_scale\n",
    "        self.dropout_rate = dropout\n",
    "        self.layer_scale = layer_scale\n",
    "        self.return_attention_weights = return_attention_weights\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        #input tensor : feature map / patches\n",
    "        batch_size, H, W, dims = input_shape\n",
    "        self.blocks = [Block(window_size = self.window_size, num_heads = self.n_heads, global_query = bool(idx%2), \n",
    "                             qkv_bias = self.qkv_bias, qk_scale = self.qk_scale, dropout_rate = self.dropout_rate, \n",
    "                             mlp_ratio = self.mlp_ratio, layer_scale = self.layer_scale, return_attention_weights = self.return_attention_weights,\n",
    "                             name = f\"GCViTBlock{idx+1}\") for idx in range(self.depth)]\n",
    "        self.downsampler = DownSampler(name = \"Downsampler\")\n",
    "        self.query_generator = GlobalQueryGenerator(keepdims = self.keepdims, name = \"GlobalQueryGenerator\")\n",
    "        \n",
    "    def call(self, inputs, **kwargs):\n",
    "        patches = inputs\n",
    "        global_query = self.query_generator(inputs)\n",
    "        for idx, block in enumerate(self.blocks):\n",
    "            if idx % 2 :\n",
    "                outputs_ = block([patches, global_query])\n",
    "            else:\n",
    "                outputs_ = block([patches])\n",
    "            if self.return_attention_weights:\n",
    "                patches, attention_weights = outputs_\n",
    "            else:\n",
    "                patches = outputs_\n",
    "        if self.downsample == False:\n",
    "            return patches\n",
    "        else:\n",
    "            return self.downsampler(patches)\n",
    "def get_gcvit_configs(res, initial_embedding_dims, name = None):\n",
    "    return {'res' : res,\n",
    "            'embed_dims' : initial_embedding_dims,\n",
    "            \"patch_embedding_type\" : \"conv\", #conv or tokenlearner\n",
    "            \"level_depth\" : [2,4,6,8],\n",
    "            \"level_heads\" : [2,4,8,16],\n",
    "            \"level_keepdims\" : [[0,0,0],\n",
    "                                   [0,0],\n",
    "                                   [1], \n",
    "                                    [1]\n",
    "                                   ], #3번째 level부터는 window attention == global attention\n",
    "            \"level_window_size\" : [res//32, res//32, res//16, res//32],\n",
    "            \"model_name\" : f\"GCViT_res{res}\" if name == None else name\n",
    "                }\n",
    "def get_gcvit(configs):\n",
    "    res = configs[\"res\"]\n",
    "    inputs = Input([res,res,3], name = \"ImageInput\")\n",
    "    patcher = PatchEmbedding(embed_dim = configs['embed_dims'], patching_type = configs[\"patch_embedding_type\"],\n",
    "                             name = \"PatchEmbedding\")\n",
    "    patches = patcher(inputs)\n",
    "    \n",
    "    for idx, (depth, heads, keepdims, window_size) in enumerate(zip(configs[\"level_depth\"], configs[\"level_heads\"], configs[\"level_keepdims\"], configs[\"level_window_size\"])):\n",
    "        if idx == len(configs['level_depth'])-1:\n",
    "            downsample = False\n",
    "        else:\n",
    "            downsample = True\n",
    "        level = Level(depth = depth, num_heads = heads, window_size = window_size, keepdims = keepdims, downsample = downsample,\n",
    "                      name = f\"GCViT_Lv{idx+1}_downsample_{downsample}\")\n",
    "        patches = level(patches)\n",
    "    model = keras.Model(inputs, patches,\n",
    "                       name = configs[\"model_name\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687e17a7",
   "metadata": {
    "papermill": {
     "duration": 0.031097,
     "end_time": "2024-11-08T06:27:04.191552",
     "exception": false,
     "start_time": "2024-11-08T06:27:04.160455",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65d8627",
   "metadata": {
    "papermill": {
     "duration": 0.03132,
     "end_time": "2024-11-08T06:27:04.254106",
     "exception": false,
     "start_time": "2024-11-08T06:27:04.222786",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# MLP-mixer\n",
    "- for lower resource demand!\n",
    "- Code Reference : [Keras.io](https://keras.io/examples/vision/mlp_image_classification/#the-mlpmixer-model)\n",
    "\n",
    "![](https://velog.velcdn.com/images/minkyu4506/post/0237ee55-74eb-4836-952c-6bd33694aecc/image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0ce87299",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T06:27:04.318484Z",
     "iopub.status.busy": "2024-11-08T06:27:04.318132Z",
     "iopub.status.idle": "2024-11-08T06:27:04.327356Z",
     "shell.execute_reply": "2024-11-08T06:27:04.326489Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.043896,
     "end_time": "2024-11-08T06:27:04.329287",
     "exception": false,
     "start_time": "2024-11-08T06:27:04.285391",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NaivePatchesExtraction(keras.layers.Layer): #untrainable layer\n",
    "    def __init__(self, patch_size, output_type = \"seq\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.patch_size = patch_size\n",
    "        self.output_type = output_type\n",
    "        assert output_type in [\"seq\", \"sequence\", \"Sequence\", \"3D\", \"conv\", \"feature_map\", \"2D\"]\n",
    "    def call(self, x):\n",
    "        if len(ops.shape(x)) == 3:\n",
    "            x = ops.expand_dims(x, 0)\n",
    "        patches = keras.ops.image.extract_patches(x, self.patch_size)\n",
    "        batch_size = keras.ops.shape(patches)[0]\n",
    "        num_patches = keras.ops.shape(patches)[1] * keras.ops.shape(patches)[2]\n",
    "        patch_dim = keras.ops.shape(patches)[3]\n",
    "        if self.output_type in [\"seq\", \"sequence\", \"Sequence\", \"3D\"] : \n",
    "            out = keras.ops.reshape(patches, (batch_size, num_patches, patch_dim))\n",
    "        elif self.output_type in [\"conv\", \"feature_map\", \"2D\"]:\n",
    "            out = keras.ops.reshape(patches, (batch_size, keras.ops.shape(patches)[1], keras.ops.shape(patches)[2],\n",
    "                                             patch_dim))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8f66ef69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T06:27:04.394599Z",
     "iopub.status.busy": "2024-11-08T06:27:04.394322Z",
     "iopub.status.idle": "2024-11-08T06:27:04.407209Z",
     "shell.execute_reply": "2024-11-08T06:27:04.406375Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.048558,
     "end_time": "2024-11-08T06:27:04.409123",
     "exception": false,
     "start_time": "2024-11-08T06:27:04.360565",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLPMixer(keras.layers.Layer): #untrainable layer\n",
    "    def __init__(self, units = None, dropout_rate = 0.2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.layernorm1 = keras.layers.LayerNormalization()\n",
    "        self.layernorm2 = keras.layers.LayerNormalization()\n",
    "        self.drop_rate = dropout_rate\n",
    "        self.units = units\n",
    "        \n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        #batch, seq_len, dims\n",
    "        batch_size = input_shape[0]\n",
    "        n_tokens = input_shape[1]\n",
    "        channels = input_shape[2]\n",
    "        embed_dim_middle = channels if self.units is None else self.units\n",
    "        \n",
    "        self.mlp1 = keras.Sequential([keras.layers.Dense(units = n_tokens, use_bias = False),\n",
    "                                     keras.layers.Activation(\"gelu\"),\n",
    "                                     keras.layers.Dropout(self.drop_rate),\n",
    "                                     keras.layers.Dense(units = n_tokens, use_bias = False),],\n",
    "                                    name = \"MLP1_TokenWiseMixing\") #output : batch, embedding_dims(channels), n_tokens\n",
    "        \n",
    "        self.mlp2 = keras.Sequential([keras.layers.Dense(units = embed_dim_middle, use_bias = False),\n",
    "                                     keras.layers.Activation(\"gelu\"),\n",
    "                                     keras.layers.Dropout(self.drop_rate),\n",
    "                                     keras.layers.Dense(units = channels, use_bias = False),],\n",
    "                                    name = \"MLP2_ChannelWiseMixing\")\n",
    "    def enable_lora(self, rank):\n",
    "        print(\"Use only in pretrained model!\\n\\n\")\n",
    "        if rank == 0:\n",
    "            print(\"lora disabled\\n\\n\")\n",
    "            pass\n",
    "        else:\n",
    "            self.layernorm1.trainable = False\n",
    "            self.layernorm2.trainable = False\n",
    "            for layer in self.mlp1.layers:\n",
    "                if isinstance(layer, keras.layers.Dense):\n",
    "                    layer.enable_lora(rank)\n",
    "                else:\n",
    "                    layer.trainable = False\n",
    "            for layer in self.mlp2.layers:\n",
    "                if isinstance(layer, keras.layers.Dense):\n",
    "                    layer.enable_lora(rank)\n",
    "                else:\n",
    "                    layer.trainable = False\n",
    "    def call(self, x):\n",
    "        # x : patches, [batch, n_patches, embed_dims]\n",
    "        normed_patch = self.layernorm1(x)\n",
    "        normed_patch = ops.transpose(normed_patch, [0,2,1]) #batch, embed_dims, n_patches\n",
    "        normed_patch = self.mlp1(normed_patch)\n",
    "        normed_patch = ops.transpose(normed_patch, [0,2,1]) #batch, n_patches, embed_dims\n",
    "        normed_patch += x\n",
    "        normed_patch2 = self.layernorm2(normed_patch)\n",
    "        normed_patch2 = self.mlp2(normed_patch2)\n",
    "        normed_patch2 += normed_patch\n",
    "        return normed_patch2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "101d16f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T06:27:04.472891Z",
     "iopub.status.busy": "2024-11-08T06:27:04.472585Z",
     "iopub.status.idle": "2024-11-08T06:27:04.479513Z",
     "shell.execute_reply": "2024-11-08T06:27:04.478672Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.040862,
     "end_time": "2024-11-08T06:27:04.481430",
     "exception": false,
     "start_time": "2024-11-08T06:27:04.440568",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_mlp_mixer(res, name = \"mlpmixer_16_4_768\", mask = False):\n",
    "    _, patch_size, depth, embed_dims = name.split(\"_\")\n",
    "    patch_size = int(patch_size)\n",
    "    depth = int(depth)\n",
    "    embed_dims = int(embed_dims)\n",
    "    \n",
    "    model_name = f\"Res{res}_{name}\"\n",
    "    inputs = Input([res,res,3], name = \"ImageInput\")\n",
    "    scaled_input = inputs/255.0\n",
    "    patches = NaivePatchesExtraction(patch_size, name = \"PatchExtraction\")(scaled_input)\n",
    "    patches_embedding = PatchEncoder(num_patches = ops.shape(patches)[1], projection_dim = embed_dims)(patches)\n",
    "    if mask:\n",
    "        patches_embedding *= mask\n",
    "    for idx in range(depth):\n",
    "        patches_embedding = MLPMixer(name = f\"MLPMixer_{idx+1}\")(patches_embedding)\n",
    "    model = Model(inputs, patches_embedding,\n",
    "                 name = model_name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a935aec5",
   "metadata": {
    "papermill": {
     "duration": 0.030764,
     "end_time": "2024-11-08T06:27:04.543346",
     "exception": false,
     "start_time": "2024-11-08T06:27:04.512582",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ConvMixer\n",
    "- Dense layer inside MLPmixer --> Conv2D\n",
    "- Code Reference : [Keras.io](https://keras.io/examples/vision/convmixer/)\n",
    "\n",
    "![](https://i.imgur.com/yF8actg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7094e4f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T06:27:04.607280Z",
     "iopub.status.busy": "2024-11-08T06:27:04.606975Z",
     "iopub.status.idle": "2024-11-08T06:27:04.619145Z",
     "shell.execute_reply": "2024-11-08T06:27:04.618265Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.04629,
     "end_time": "2024-11-08T06:27:04.621117",
     "exception": false,
     "start_time": "2024-11-08T06:27:04.574827",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConvMixer(keras.layers.Layer): #untrainable layer\n",
    "    def __init__(self, filters = None, kernel_size = 5, dropout_rate = 0.2, output_type = \"2D\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.layernorm1 = keras.layers.LayerNormalization()\n",
    "        self.layernorm2 = keras.layers.LayerNormalization()\n",
    "        self.drop_rate = dropout_rate ; self.drop = keras.layers.Dropout(self.drop_rate)\n",
    "        \n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.output_type = output_type\n",
    "        assert output_type in [\"seq\", \"sequence\", \"Sequence\", \"3D\", \"conv\", \"feature_map\", \"2D\"]\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        #batch, seq_len, dims\n",
    "        batch_size = input_shape[0]\n",
    "        h = input_shape[1]\n",
    "        w = input_shape[2]\n",
    "        channels = input_shape[3]\n",
    "        c = channels if self.filters is None else self.filters\n",
    "        \n",
    "        self.depthconv = keras.layers.DepthwiseConv2D(kernel_size = self.kernel_size, padding = \"SAME\", use_bias = False)\n",
    "        self.pointconv = keras.layers.Conv2D(filters = c, kernel_size = 1, padding = 'SAME', use_bias = False)\n",
    "        self.gelu = keras.layers.Activation('gelu')\n",
    "\n",
    "    def call(self, x):\n",
    "        # x : feature_map, [batch, h,w, embed_dims]\n",
    "        fmap = self.depthconv(x)\n",
    "        fmap = self.gelu(fmap)\n",
    "        fmap = self.layernorm1(fmap) + x\n",
    "        fmap = self.pointconv(fmap)\n",
    "        fmap = self.gelu(fmap)\n",
    "        fmap = self.layernorm2(fmap)\n",
    "        \n",
    "        batch_size = ops.shape(fmap)[0]\n",
    "        h = ops.shape(fmap)[1]\n",
    "        w = ops.shape(fmap)[2]\n",
    "        dims_ = ops.shape(fmap)[3]\n",
    "        \n",
    "        if self.output_type in [\"seq\", \"sequence\", \"Sequence\", \"3D\"] : \n",
    "            fmap = keras.ops.reshape(fmap, (batch_size, h*w, dims_))\n",
    "            return fmap\n",
    "        elif self.output_type in [\"conv\", \"feature_map\", \"2D\"]:\n",
    "            return fmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f248d5ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T06:27:04.684766Z",
     "iopub.status.busy": "2024-11-08T06:27:04.684460Z",
     "iopub.status.idle": "2024-11-08T06:27:04.692942Z",
     "shell.execute_reply": "2024-11-08T06:27:04.692153Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.042464,
     "end_time": "2024-11-08T06:27:04.694818",
     "exception": false,
     "start_time": "2024-11-08T06:27:04.652354",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_conv_mixer(res, name = \"convmixer_16_4_768\", mask = False):\n",
    "    _, patch_size, depth, embed_dims = name.split(\"_\")\n",
    "    patch_size = int(patch_size)\n",
    "    depth = int(depth)\n",
    "    embed_dims = int(embed_dims)\n",
    "    \n",
    "    model_name = f\"Res{res}_{name}\"\n",
    "    inputs = Input([res,res,3], name = \"ImageInput\")\n",
    "    scaled_input = inputs/255.0\n",
    "    \n",
    "    patches = NaivePatchesExtraction(patch_size, name = \"PatchExtraction\")(scaled_input)\n",
    "    patches_embedding = PatchEncoder(num_patches = ops.shape(patches)[1], projection_dim = embed_dims)(patches)\n",
    "    batch, n_patches, dims = ops.shape(patches_embedding) ; res_ = int(ops.sqrt(ops.cast(n_patches, \"float32\")))\n",
    "    patches_embedding = ops.reshape(patches_embedding, (-1, res_, res_, dims))\n",
    "    if mask:\n",
    "        patches_embedding *= mask\n",
    "    for idx in range(depth):\n",
    "        if idx == depth-1:\n",
    "            types = \"seq\"\n",
    "        else:\n",
    "            types = \"2D\"\n",
    "        patches_embedding = ConvMixer(name = f\"ConvMixer_{idx+1}\", output_type = types)(patches_embedding)\n",
    "    model = Model(inputs, patches_embedding,\n",
    "                 name = model_name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c20c4c",
   "metadata": {
    "papermill": {
     "duration": 0.030959,
     "end_time": "2024-11-08T06:27:04.756789",
     "exception": false,
     "start_time": "2024-11-08T06:27:04.725830",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Get final Feature extractor\n",
    "- according to [recent study](https://arxiv.org/abs/2309.16588), add cls token and register tokens \"after\" the patches!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a960272b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T06:27:04.820739Z",
     "iopub.status.busy": "2024-11-08T06:27:04.820444Z",
     "iopub.status.idle": "2024-11-08T06:27:04.835753Z",
     "shell.execute_reply": "2024-11-08T06:27:04.835089Z"
    },
    "papermill": {
     "duration": 0.049563,
     "end_time": "2024-11-08T06:27:04.837626",
     "exception": false,
     "start_time": "2024-11-08T06:27:04.788063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ViT_rollout(keras.Model):\n",
    "    def __init__(self, res, patch_size, att_depth, att_heads, att_dims, embed_dims, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.res = res\n",
    "        self.patch_size = patch_size\n",
    "        self.att_depth = att_depth\n",
    "        self.att_heads = att_heads\n",
    "        self.att_dims = att_dims\n",
    "        self.embed_dims = embed_dims\n",
    "        \n",
    "        self.pre_lns = [keras.layers.LayerNormalization(name = f\"PreLN{idx+1}\") for idx in range(self.att_depth)]\n",
    "        self.tr_layers = [keras.layers.MultiHeadAttention(self.att_heads, self.att_dims, name = f\"MHSA{idx+1}\") for idx in range(self.att_depth)]\n",
    "        self.mlps = [keras.layers.Dense(units = self.embed_dims, activation = \"gelu\", name = f\"MLP{idx+1}\") for idx in range(self.att_depth)]\n",
    "        self.post_lns = [keras.layers.LayerNormalization(name = f\"PostLN{idx+1}\")  for idx in range(self.att_depth)]\n",
    "        self.drops = [keras.layers.Dropout(rate = 0.2, name = f\"Dropout{idx+1}\")  for idx in range(self.att_depth)]\n",
    "        self.num_patches = (self.res//self.patch_size)**2\n",
    "        self.patch_conv = keras.layers.Conv2D(filters = embed_dims, kernel_size = patch_size, strides = patch_size, padding = 'same', activation = 'gelu', name = \"ConvolutionForPatching\")\n",
    "        \n",
    "        #self.cls_token = self.add_weight(shape=(1, 1, self.embed_dims), initializer=\"zeros\")\n",
    "        self.pos_embed = self.add_weight(shape=(1, self.num_patches, self.embed_dims), initializer=\"zeros\")\n",
    "        \n",
    "    def call(self, image):\n",
    "        image = image / 255\n",
    "        patches = self.patch_conv(image) ; bs = ops.shape(image)[0]\n",
    "        patches = ops.reshape(patches, [bs, self.num_patches, self.embed_dims])\n",
    "        patches += self.pos_embed\n",
    "        \n",
    "        cls_tokens = ops.zeros_like(keras.layers.GlobalAveragePooling1D(keepdims = True)(patches))\n",
    "        patches = ops.concatenate([cls_tokens, patches], axis = 1)\n",
    "        att_weights_patches = []\n",
    "        \n",
    "        for pre_ln, mhsa, mlp, post_ln, drop in zip(self.pre_lns, self.tr_layers, self.mlps, self.post_lns, self.drops):\n",
    "            x0 = pre_ln(patches)\n",
    "            x1, att_w_whole = mhsa(query = x0, key = x0, value = x0, \n",
    "                                  return_attention_scores = True) ; att_weights_patches.append(att_w_whole)\n",
    "            x2 = patches + x1\n",
    "            x3 = post_ln(x2)\n",
    "            x4 = mlp(x3)\n",
    "            x5 = x2 + x4\n",
    "            patches = drop(x5)\n",
    "        cls_tokens = patches[:, 0, :]\n",
    "        patches = patches[:, 1:, :]\n",
    "        final_att_weights = att_weights_patches[-1]\n",
    "        return cls_tokens, patches, {\"total_weight\" : att_weights_patches,\n",
    "                                    \"final_layer_weight\" : final_att_weights}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7152d794",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T06:27:04.901697Z",
     "iopub.status.busy": "2024-11-08T06:27:04.901396Z",
     "iopub.status.idle": "2024-11-08T06:27:04.918355Z",
     "shell.execute_reply": "2024-11-08T06:27:04.917686Z"
    },
    "papermill": {
     "duration": 0.051533,
     "end_time": "2024-11-08T06:27:04.920238",
     "exception": false,
     "start_time": "2024-11-08T06:27:04.868705",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_metaformer(meta_type, res, att_depth, att_heads, att_dims, \n",
    "                   patch_size = 16, embed_dims = 1024, grayscale = False, register_tokens = 0,\n",
    "                   pretrained_encoder = None, return_patches = False,\n",
    "                  ):\n",
    "    assert isinstance(register_tokens, int), \"register_tokens should be integer\"\n",
    "    name = f\"Metaformer_res{res}_type_{meta_type}\"\n",
    "    if grayscale:\n",
    "        inputs = Input([res,res,1], name = \"Input_images_grayscale\")\n",
    "    else:\n",
    "        inputs = Input([res,res,3], name = \"Input_images\")\n",
    "    if pretrained_encoder is None:\n",
    "        scaled_inputs = keras.layers.Rescaling(scale=1./127.5, offset=-1, name = \"RescalingImage\")(inputs)\n",
    "        patches = ImagePatchEmbedding(patch_size = patch_size, embed_dim = embed_dims, groups = att_heads//2,\n",
    "                                      name = \"ImagePatchingWithLearnablePE\")(scaled_inputs)\n",
    "    else:\n",
    "        assert grayscale is False, \"To use pretrained network, input images must have 3 channels\"\n",
    "        patches = pretrained_encoder(inputs)\n",
    "        _, w_, h_, dims_ = ops.shape(patches)\n",
    "        patches = ops.reshape(patches, [-1, w_*h_, dims_])\n",
    "        patches = keras.layers.Dense(embed_dims, name = \"EmbeddingMLP\", activation = \"gelu\")(patches)\n",
    "        name = f\"{pretrained_encoder.name}_Metaformer_res{res}_type_{meta_type}\"\n",
    "    \n",
    "    n_patches = ops.shape(patches)[1]\n",
    "    initial_0 = keras.layers.GlobalAveragePooling1D(name = \"pool1d\", keepdims = True)(patches) #batch, 1, dims\n",
    "    initial_0 = keras.ops.zeros_like(initial_0)\n",
    "    \n",
    "    cls_token = keras.layers.Dense(embed_dims, name=\"cls_token\")(initial_0)\n",
    "    \n",
    "    if register_tokens > 0:\n",
    "        reg_tokens = keras.layers.Dense(embed_dims, name = \"register_token\")(initial_0)\n",
    "        reg_tokens = ops.concatenate([ops.ones_like(reg_tokens, dtype = \"float32\") for _ in range(register_tokens)],\n",
    "                                    axis = 1)\n",
    "        patches_ = ops.concatenate([patches, cls_token, reg_tokens], axis = 1)\n",
    "    else:\n",
    "        patches_ = ops.concatenate([patches, cls_token], axis = 1)\n",
    "    for idx in range(att_depth):\n",
    "        patches_ = MetaEncoder(operation_type = meta_type, att_dims = att_dims, att_heads = att_heads,\n",
    "                                 name = f\"MetaEncoder_{meta_type}_{idx+1}\")([patches_])    \n",
    "    patches = patches_[:, :n_patches, :]\n",
    "    cls_token = patches_[:, n_patches:n_patches+1, :] ; del patches_\n",
    "    if len(ops.shape(cls_token)) == 2:\n",
    "        cls_token = cls_token[:, tf.newaxis, :]\n",
    "    #total_patches = ops.concatenate([cls_token, patches], axis = 1)\n",
    "    #attention_mask = ops.ones([1, n_patches + 1, n_patches + 1])\n",
    "    #attention_mask = tf.linalg.set_diag(attention_mask, \n",
    "    #                                   ops.zeros([1, n_patches + 1]))\n",
    "    cls_token, weights = MultiHeadAttention(att_heads, att_dims, use_bias = False, name = \"CLS_Att_Pooling\")(query = cls_token,\n",
    "                                                                                                         key = patches,\n",
    "                                                                                                         value = patches,\n",
    "                                                                                                         return_attention_scores = True)\n",
    "    \n",
    "    #patches_, weights_ = MultiHeadAttention(att_heads, att_dims, use_bias = True, name = \"AttentionPooling\")(query = total_patches, key = total_patches, \n",
    "    #                                                                                        value = total_patches,\n",
    "    #                                                                                       return_attention_scores = True,\n",
    "    #                                                                                       attention_mask = attention_mask\n",
    "    #                                                                                                                        )\n",
    "    #patches = patches_[:, -n_patches:, :]\n",
    "    #cls_token = patches_[:, 0, :] + cls_token_\n",
    "    \n",
    "    cls_token = keras.layers.Identity(name = \"Representation_vector\")(cls_token[:,0,:])\n",
    "    attention_weight = keras.layers.Identity(name = \"Attention_weight\")(weights)\n",
    "    patches = keras.layers.Identity(name = \"Encoded_Patches\")(patches)\n",
    "    if return_patches:\n",
    "        return keras.Model(inputs, [cls_token, patches, attention_weight],\n",
    "                          name = name)\n",
    "    else:\n",
    "        return keras.Model(inputs, [cls_token, attention_weight],\n",
    "                          name = name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3c76397e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T06:27:04.984458Z",
     "iopub.status.busy": "2024-11-08T06:27:04.984153Z",
     "iopub.status.idle": "2024-11-08T06:27:05.009440Z",
     "shell.execute_reply": "2024-11-08T06:27:05.008601Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.059743,
     "end_time": "2024-11-08T06:27:05.011318",
     "exception": false,
     "start_time": "2024-11-08T06:27:04.951575",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_feature_extractor(conv_base, #if None, Vanilla ViT\n",
    "                         embed_dims, res, pe_type = \"learnable\",\n",
    "                          patch_size = 16,\n",
    "                        att_depth = 4, att_heads = 16, mask = False, \n",
    "                          return_patches = False) : \n",
    "    inputs = Input([res,res,3], name = \"Input_images\")\n",
    "    batch_size = ops.shape(inputs)[0]\n",
    "    if conv_base in [None, 'vit', \"ViT\"] : #Vanilla Vision Transformer\n",
    "        scaled_inputs = inputs/255.0\n",
    "        patches = ops.image.extract_patches(scaled_inputs,\n",
    "                                           size = patch_size,\n",
    "                                           padding = 'same')\n",
    "        \n",
    "        _, w, h, dims = ops.shape(patches) ; n_patches = w*h \n",
    "        \n",
    "        patches = ops.reshape(patches, [-1, w*h, dims])\n",
    "        if pe_type in ['rotary', 'rotation', 'rotatory', 'roformer']:\n",
    "            patches = keras_nlp.layers.RotaryEmbedding(name = \"RotaryPositionalEmbedding\")(patches)\n",
    "            patches = Dense(units = embed_dims, activation = \"gelu\", name = \"EmbeddingAfterRPE\")(patches)\n",
    "        elif pe_type in [\"learnable\", 'absolute']:\n",
    "            patches = PatchEncoder(num_patches = w*h, projection_dim = embed_dims)(patches)\n",
    "        elif pe_type == None:\n",
    "            pass\n",
    "        \n",
    "        if mask:\n",
    "            patches *= mask\n",
    "        \n",
    "        learned_token, patches, attention_score = TRBlock(att_depth = att_depth, \n",
    "                                                          att_dims = embed_dims, \n",
    "                                                          att_heads = att_heads)([patches, patches])\n",
    "        \n",
    "        learned_token = keras.layers.Identity(name = \"feature_vector\")(learned_token)\n",
    "        patches = keras.layers.Identity(name = \"encoded_patches\")(patches)\n",
    "        attention_score = keras.layers.Identity(name = \"attention_weight\")(attention_score) \n",
    "        model_name = f\"ViT_depth{att_depth}_dims{embed_dims}_heads{att_heads}_patch{patch_size}\"\n",
    "    else:\n",
    "        feature_map = conv_base(inputs)\n",
    "        dims = ops.shape(feature_map)[-1] ; batch_size = ops.shape(feature_map)[0]\n",
    "        if len(ops.shape(feature_map)) == 4:\n",
    "            _, w, h, dims = ops.shape(feature_map)\n",
    "            feature_map = ops.reshape(feature_map, [-1, w*h, dims])\n",
    "        n_patches = ops.shape(feature_map)[1]\n",
    "        \n",
    "        if pe_type in ['rotary', 'rotation', 'rotatory', 'roformer']:\n",
    "            feature_map = keras_nlp.layers.RotaryEmbedding(name = \"RotaryPositionalEmbedding\")(feature_map)\n",
    "            feature_map = Dense(units = embed_dims, name = \"FeatureEmbedding\", activation = \"gelu\")(feature_map)\n",
    "        elif pe_type in [\"learnable\", 'absolute']:\n",
    "            feature_map = PatchEncoder(num_patches = n_patches, projection_dim = embed_dims)(feature_map)\n",
    "        elif pe_type == None:\n",
    "            feature_map = Dense(units = embed_dims, name = \"FeatureEmbedding\", activation = \"gelu\")(feature_map)\n",
    "        learned_token = keras.layers.GlobalAveragePooling1D()(feature_map)\n",
    "        learned_token = learned_token[:, tf.newaxis, :]\n",
    "        learned_token, attention_score = AttentionPooling(attention_heads = att_heads, \n",
    "                                                          attention_dims = att_heads*32)([learned_token, feature_map])\n",
    "        learned_token = ops.squeeze(learned_token, axis = 1)\n",
    "        \n",
    "        learned_token = keras.layers.Identity(name = \"feature_vector\")(learned_token)\n",
    "        patches = keras.layers.Identity(name = \"encoded_patches\")(feature_map)\n",
    "        attention_score = keras.layers.Identity(name = \"attention_weight\")(attention_score) \n",
    "        \n",
    "        model_name = f\"{conv_base.name}_depth{att_depth}_dims{embed_dims}_heads{att_heads}\"\n",
    "    if return_patches:\n",
    "        \n",
    "        model = Model(inputs,\n",
    "                     [learned_token, patches, \n",
    "                      attention_score],\n",
    "                     name = model_name + \"_withPatches\")\n",
    "    else:\n",
    "        model = Model(inputs, [learned_token, attention_score],\n",
    "                      name = model_name)\n",
    "    return model\n",
    "\n",
    "def get_full_model(conv_base_name, res, embed_dims = 1280, patch_size = 16, pe_type = 'rotary',\n",
    "                   att_depth = 4, att_heads = 8,\n",
    "                  extra_configs = None, mask = False,\n",
    "                   return_patches = False):\n",
    "    if isinstance(conv_base_name, keras.Model):\n",
    "        conv_base = conv_base_name\n",
    "        #pe_type = \"learnable\"\n",
    "    elif conv_base_name in [\"effnet\", 'EfficientNet']:\n",
    "        conv_base = keras.applications.EfficientNetV2B1(input_shape = [res,res,3],\n",
    "                                                       include_top = False)\n",
    "    elif conv_base_name in [\"effnet_small\", \"EfficientNetSmall\"]:\n",
    "        conv_base = keras.applications.EfficientNetV2S(input_shape = [res,res,3],\n",
    "                                                       include_top = False)\n",
    "    elif conv_base_name in [\"effnet_base\", \"EfficientNetBase\"]:\n",
    "        conv_base = keras.applications.EfficientNetV2M(input_shape = [res,res,3],\n",
    "                                                       include_top = False)\n",
    "    elif conv_base_name in [\"convnext\", 'ConvNeXt']:\n",
    "        conv_base = keras.applications.ConvNeXtTiny(input_shape = [res,res,3],\n",
    "                                                       include_top = False)\n",
    "    elif conv_base_name in [\"convnext_small\", 'ConvNeXtSmall']:\n",
    "        conv_base = keras.applications.ConvNeXtSmall(input_shape = [res,res,3],\n",
    "                                                       include_top = False)\n",
    "    elif conv_base_name in [\"convnext_base\", 'ConvNeXtBase']:\n",
    "        conv_base = keras.applications.ConvNeXtBase(input_shape = [res,res,3],\n",
    "                                                       include_top = False)\n",
    "    elif isinstance(conv_base_name, dict):\n",
    "        conv_base = get_gcvit(conv_base_name)\n",
    "    elif conv_base_name.split(\"_\")[0] in [\"MLPMixer\", \"mlpmixer\", \"MLP\", \"mlp\"]:\n",
    "        conv_base = get_mlp_mixer(res = res, name = conv_base_name, mask = mask)\n",
    "        pe_type = None\n",
    "    elif conv_base_name.split(\"_\")[0] in [\"ConvMixer\", \"convmixer\", \"Conv\", \"conv\"]:\n",
    "        conv_base = get_conv_mixer(res = res, name = conv_base_name, mask = mask)\n",
    "        pe_type = None\n",
    "    else:\n",
    "        conv_base = None\n",
    "    return get_feature_extractor(conv_base, pe_type = pe_type, res = res, patch_size = patch_size, embed_dims = embed_dims, att_depth = att_depth, att_heads = att_heads,\n",
    "                                return_patches = return_patches, mask = mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f153db9c",
   "metadata": {
    "papermill": {
     "duration": 0.031082,
     "end_time": "2024-11-08T06:27:05.073630",
     "exception": false,
     "start_time": "2024-11-08T06:27:05.042548",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---------\n",
    "# Information-maximization method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eac11872",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T06:27:05.138311Z",
     "iopub.status.busy": "2024-11-08T06:27:05.137978Z",
     "iopub.status.idle": "2024-11-08T06:27:05.187479Z",
     "shell.execute_reply": "2024-11-08T06:27:05.186690Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.084402,
     "end_time": "2024-11-08T06:27:05.189368",
     "exception": false,
     "start_time": "2024-11-08T06:27:05.104966",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BarlowModel(keras.Model):\n",
    "    def __init__(self, feature_extractor, embed_dims, probe = False, multiview = False,\n",
    "                 probe_heads = None, probe_activation = \"sigmoid\",\n",
    "                 diag = 0.6, off_diag = 0.4, \n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dims = embed_dims\n",
    "        self.multiview = multiview\n",
    "        self.feature_extractor = feature_extractor #weight sharing for VICreg, Barlow Twins\n",
    "        self.projector_a = self.get_projector()\n",
    "        self.projector_b = self.get_projector()\n",
    "        self.diag = diag ; self.off_diag = off_diag\n",
    "        self.loss_tracker = tf.keras.metrics.Mean(\"Barlow_loss_tracker\")\n",
    "        self.linear_probing = probe\n",
    "        if probe:\n",
    "            self.probe_loss_tracker = tf.keras.metrics.Mean(\"Probe_loss_tracker\")\n",
    "            self.probe_metrics = [\n",
    "                                  keras.metrics.F1Score(average=\"micro\", threshold=None, name=\"probe_micro_f1_score\"),\n",
    "                                 keras.metrics.TruePositives(name = \"probe_TP\"),\n",
    "                                 keras.metrics.TrueNegatives(name = \"probe_TN\"),\n",
    "                                 keras.metrics.FalsePositives(name = \"probe_FP\"),\n",
    "                                 keras.metrics.FalseNegatives(name = \"probe_FN\")]\n",
    "            \n",
    "            self.probe_categories = probe_heads\n",
    "            self.probe_act = probe_activation\n",
    "            self.linear_probe = keras.layers.Dense(units = probe_heads, activation = self.probe_act, dtype = \"float32\")\n",
    "            if self.probe_act == \"sigmoid\":\n",
    "                self.probe_loss_fn = keras.losses.BinaryCrossentropy(reduction = None, label_smoothing = 0.1)\n",
    "                self.probe_metrics.append(keras.metrics.BinaryAccuracy(name=\"probe_accuracy\", threshold=0.5))\n",
    "            elif self.probe_act == \"softmax\":\n",
    "                self.probe_loss_fn = keras.losses.CategoricalCrossentropy(reduction = None, label_smoothing = 0.1)\n",
    "                self.probe_metrics.append(keras.metrics.CategoricalAccuracy(name=\"probe_accuracy\", threshold=0.5))\n",
    "    def compile(self, optimizer, probe_optimizer = None, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "        self.optimizer = optimizer\n",
    "        if self.linear_probing:\n",
    "            self.probe_optimizer = probe_optimizer\n",
    "        \n",
    "    def get_env_config(self):\n",
    "        return {\"feature_extractor_name\" : self.feature_extractor.name,\n",
    "               \"embed_dims\" : self.embed_dims,\n",
    "                \"Multiview(>2)\" : self.multiview,\n",
    "               'diag_part_coefficient' : self.diag,\n",
    "               'off_diag_coefficient' : self.off_diag,\n",
    "               \"SSL_method\" : \"Barlow_Twins\",\n",
    "               \"Linear Probe\" : self.linear_probing,\n",
    "               \"N_Categories\" : self.probe_categories if self.linear_probing else 0,\n",
    "               \"Probe Activation\" : self.probe_act if self.linear_probing else \"NA\"}\n",
    "    \n",
    "    def get_projector(self):\n",
    "        \n",
    "        model = keras.Sequential([Dense(units = self.embed_dims),\n",
    "                                 Dense(units = self.embed_dims),\n",
    "                                 Dense(units = self.embed_dims, dtype = \"float32\"),\n",
    "                                 keras.layers.Lambda(lambda x : ops.normalize(x))]\n",
    "                                )\n",
    "        return model\n",
    "    \n",
    "    def compute_loss(self, correlation_matrix):\n",
    "        diag_component = tf.linalg.diag_part(correlation_matrix)\n",
    "        zero_diag = tf.zeros(correlation_matrix.shape[-1])\n",
    "        off_diag_matrix = tf.linalg.set_diag(correlation_matrix, zero_diag)\n",
    "        \n",
    "        diag_loss = tf.pow(diag_component-1, 2) * self.diag\n",
    "        off_diag_loss = tf.pow(off_diag_matrix, 2) * self.off_diag\n",
    "        loss = tf.reduce_mean(diag_loss) + tf.reduce_mean(off_diag_loss)\n",
    "        loss = tf.cast(loss, tf.float32)\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    def train_step(self, dataset):\n",
    "        \n",
    "        if self.linear_probing:\n",
    "            data, aug_data, labels = dataset\n",
    "        else:\n",
    "            if self.multiview:\n",
    "                data = dataset[0]\n",
    "                aug_data = dataset[1:] ; n_augs = len(aug_data)\n",
    "                aug_data = ops.concatenate(aug_data, axis = 0)\n",
    "            else:\n",
    "                data, aug_data = dataset\n",
    "        ## 1. SSL encoder loss ##\n",
    "        with tf.GradientTape() as tape:\n",
    "            if len(self.feature_extractor.outputs) == 2 :\n",
    "                feature_seq, weights = self.feature_extractor(data)\n",
    "                feature_seq_aug, weights_aug = self.feature_extractor(aug_data)\n",
    "            elif len(self.feature_extractor.outputs) == 1:\n",
    "                feature_seq = self.feature_extractor(data)\n",
    "                feature_seq_aug = self.feature_extractor(aug_data)\n",
    "                \n",
    "            if len(ops.shape(feature_seq)) == 3: \n",
    "                rep_vector = keras.layers.GlobalAveragePooling1D()(feature_seq)\n",
    "                rep_vector_aug = keras.layers.GlobalAveragePooling1D()(feature_seq_aug)\n",
    "            elif len(ops.shape(feature_seq)) == 4:\n",
    "                rep_vector = keras.layers.GlobalAveragePooling2D()(feature_seq)\n",
    "                rep_vector_aug = keras.layers.GlobalAveragePooling2D()(feature_seq_aug)\n",
    "            elif len(ops.shape(feature_seq)) == 2:\n",
    "                rep_vector = feature_seq\n",
    "                rep_vector_aug = feature_seq_aug\n",
    "            original_rep_vector = rep_vector #for Linear probing\n",
    "            \n",
    "            rep_vector = self.projector_a(rep_vector)\n",
    "            rep_vector_aug = self.projector_b(rep_vector_aug)\n",
    "            if self.multiview:\n",
    "                rep_vector_aug_set = ops.split(rep_vector_aug, n_augs, 0)\n",
    "                loss = []\n",
    "                for idx in range(n_augs):\n",
    "                    correlation_matrix = get_cor_matrix(rep_vector, rep_vector_aug_set[idx])\n",
    "                    loss.append(self.compute_loss(correlation_matrix)\n",
    "                               )\n",
    "                loss = ops.mean(loss)\n",
    "            else:\n",
    "                correlation_matrix = get_cor_matrix(rep_vector, rep_vector_aug)\n",
    "                loss = self.compute_loss(correlation_matrix)\n",
    "    \n",
    "\n",
    "        gradients = tape.gradient(loss, \n",
    "                                 (self.feature_extractor.trainable_variables + self.projector_a.trainable_variables + self.projector_b.trainable_variables))\n",
    "        self.optimizer.apply_gradients(zip(gradients, \n",
    "                                          (self.feature_extractor.trainable_variables + self.projector_a.trainable_variables + self.projector_b.trainable_variables)\n",
    "                                          ))\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        output_dict = {'Barlow_loss' : self.loss_tracker.result()}\n",
    "        if self.linear_probing:\n",
    "            ##2. linear probing and calculate metrics##\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                if len(self.feature_extractor.outputs) == 2 :\n",
    "                    feature_seq, weights = self.feature_extractor(data, training = False)\n",
    "                elif len(self.feature_extractor.outputs) == 1:\n",
    "                    feature_seq = self.feature_extractor(data, training = False)\n",
    "                if len(ops.shape(feature_seq)) == 3: \n",
    "                    original_rep_vector = keras.layers.GlobalAveragePooling1D()(feature_seq)\n",
    "                elif len(ops.shape(feature_seq)) == 4:\n",
    "                    original_rep_vector = keras.layers.GlobalAveragePooling2D()(feature_seq)\n",
    "                elif len(ops.shape(feature_seq)) == 2:\n",
    "                    rep_vector = feature_seq\n",
    "        \n",
    "                class_logits = self.linear_probe(original_rep_vector, training = True)\n",
    "                class_probe_loss = self.probe_loss_fn(y_true = labels, y_pred = class_logits)\n",
    "                class_probe_loss = ops.mean(class_probe_loss)\n",
    "            gradients = tape.gradient(class_probe_loss, \n",
    "                                            self.linear_probe.trainable_weights)\n",
    "            self.probe_optimizer.apply_gradients(zip(gradients, self.linear_probe.trainable_weights))\n",
    "            self.probe_loss_tracker.update_state(class_probe_loss)\n",
    "            output_dict.update({\"probe_classification_loss\" : self.probe_loss_tracker.result()})\n",
    "            #metrics\n",
    "            for comp in self.probe_metrics:\n",
    "                comp.update_state(labels, class_logits)\n",
    "                \n",
    "            probe_metric_dict = {comp.name :  comp.result() for comp in self.probe_metrics}\n",
    "            output_dict.update(probe_metric_dict)\n",
    "        else:\n",
    "            pass\n",
    "        del tape\n",
    "        return output_dict\n",
    "    def test_step(self, dataset):\n",
    "        if self.linear_probing:\n",
    "            data, aug_data, labels = dataset\n",
    "        else:\n",
    "            if self.multiview:\n",
    "                data = dataset[0]\n",
    "                aug_data = dataset[1:] ; n_augs = len(aug_data)\n",
    "                aug_data = ops.concatenate(aug_data, axis = 0)\n",
    "            else:\n",
    "                data, aug_data = dataset\n",
    "            \n",
    "        if len(self.feature_extractor.outputs) == 2 :\n",
    "            feature_seq, weights = self.feature_extractor(data)\n",
    "            feature_seq_aug, weights_aug = self.feature_extractor(aug_data)\n",
    "        elif len(self.feature_extractor.outputs) == 1:\n",
    "            feature_seq = self.feature_extractor(data)\n",
    "            feature_seq_aug = self.feature_extractor(aug_data)\n",
    "        if len(ops.shape(feature_seq)) == 3: \n",
    "            rep_vector = keras.layers.GlobalAveragePooling1D()(feature_seq)\n",
    "            rep_vector_aug = keras.layers.GlobalAveragePooling1D()(feature_seq_aug)\n",
    "        elif len(ops.shape(feature_seq)) == 4:\n",
    "            rep_vector = keras.layers.GlobalAveragePooling2D()(feature_seq)\n",
    "            rep_vector_aug = keras.layers.GlobalAveragePooling2D()(feature_seq_aug)\n",
    "        elif len(ops.shape(feature_seq)) == 2:\n",
    "            rep_vector = feature_seq\n",
    "            rep_vector_aug = feature_seq_aug\n",
    "        \n",
    "        rep_vector = self.projector_a(rep_vector)\n",
    "        rep_vector_aug = self.projector_b(rep_vector_aug)\n",
    "        \n",
    "        if self.multiview:\n",
    "            rep_vector_aug_set = ops.split(rep_vector_aug, n_augs, 0)\n",
    "            loss = []\n",
    "            for idx in range(n_augs):\n",
    "                correlation_matrix = get_cor_matrix(rep_vector, rep_vector_aug_set[idx])\n",
    "                loss.append(self.compute_loss(correlation_matrix)\n",
    "                            )\n",
    "            loss = ops.mean(loss)\n",
    "        else:\n",
    "            correlation_matrix = get_cor_matrix(rep_vector, rep_vector_aug)\n",
    "            loss = self.compute_loss(correlation_matrix)\n",
    "        \n",
    "        self.loss_tracker.update_state(loss)\n",
    "        output_dict = {'Barlow_loss' : self.loss_tracker.result()}\n",
    "        \n",
    "        if self.linear_probing:\n",
    "            ##2. linear probing and calculate metrics##\n",
    "            if len(self.feature_extractor.outputs) == 2 :\n",
    "                feature_seq, weights = self.feature_extractor(data, training = False)\n",
    "            elif len(self.feature_extractor.outputs) == 1:\n",
    "                feature_seq = self.feature_extractor(data, training = False)\n",
    "            if len(ops.shape(feature_seq)) == 3: \n",
    "                original_rep_vector = keras.layers.GlobalAveragePooling1D()(feature_seq)\n",
    "            elif len(ops.shape(feature_seq)) == 4:\n",
    "                original_rep_vector = keras.layers.GlobalAveragePooling2D()(feature_seq)\n",
    "            elif len(ops.shape(feature_seq)) == 2:\n",
    "                original_rep_vector = feature_seq\n",
    "\n",
    "            class_logits = self.linear_probe(original_rep_vector, training = False)\n",
    "            class_probe_loss = self.probe_loss_fn(y_true = labels, y_pred = class_logits)\n",
    "            class_probe_loss = ops.mean(class_probe_loss)\n",
    "            self.probe_loss_tracker.update_state(class_probe_loss)\n",
    "            output_dict.update({\"probe_classification_loss\" : self.probe_loss_tracker.result()})\n",
    "            #metrics\n",
    "            for comp in self.probe_metrics:\n",
    "                comp.update_state(labels, class_logits)\n",
    "                \n",
    "            probe_metric_dict = {comp.name :  comp.result() for comp in self.probe_metrics}\n",
    "            output_dict.update(probe_metric_dict)\n",
    "        else:\n",
    "            pass\n",
    "        return output_dict\n",
    "    def get_classifier_model(self):\n",
    "        self.feature_extractor.trainable = True\n",
    "        if self.linear_probing == False:\n",
    "            return self.feature_extractor\n",
    "        self.linear_probe.trainable = True\n",
    "        inputs = self.feature_extractor.input\n",
    "        outputs = self.feature_extractor.outputs\n",
    "        if len(self.feature_extractor.outputs) == 2 :\n",
    "            feature_map, att_weights = outputs\n",
    "        elif len(self.feature_extractor.outputs) == 1:\n",
    "            feature_map = outputs[0]\n",
    "        \n",
    "        if len(ops.shape(feature_map)) == 3: \n",
    "            rep_vector = keras.layers.GlobalAveragePooling1D()(feature_map)\n",
    "        elif len(ops.shape(feature_map)) == 4:\n",
    "            rep_vector = keras.layers.GlobalAveragePooling2D()(feature_map)\n",
    "        elif len(ops.shape(feature_map)) == 2:\n",
    "            rep_vector = feature_map\n",
    "            \n",
    "        class_logits = self.linear_probe(rep_vector)\n",
    "        classifier_model = keras.Model(inputs, class_logits,\n",
    "                                      name = f\"Barlow_pretrained_{self.feature_extractor.name}\")\n",
    "        return classifier_model\n",
    "    def call(self, dataset):\n",
    "        return self.test_step(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6b9cc27f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T06:27:05.254091Z",
     "iopub.status.busy": "2024-11-08T06:27:05.253751Z",
     "iopub.status.idle": "2024-11-08T06:27:05.307132Z",
     "shell.execute_reply": "2024-11-08T06:27:05.306357Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.088041,
     "end_time": "2024-11-08T06:27:05.309011",
     "exception": false,
     "start_time": "2024-11-08T06:27:05.220970",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VICRegModel(keras.Model):\n",
    "    def __init__(self, feature_extractor, embed_dims, multiview = False,\n",
    "                 probe = False, \n",
    "                 probe_heads = None, probe_activation = \"sigmoid\",\n",
    "                 variance_coeff = 20, invariance_coeff = 20, covariance_coeff = 1, \n",
    "                 variance_gamma = 5.0,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dims = embed_dims\n",
    "        self.multiview = multiview\n",
    "        self.feature_extractor = feature_extractor #weight sharing for VICreg, Barlow Twins\n",
    "        self.projector_a = self.get_projector()\n",
    "        self.projector_b = self.get_projector()\n",
    "        self.var_coef = variance_coeff ; self.invar_coef = invariance_coeff ; self.cov_coef = covariance_coeff\n",
    "        self.gamma = variance_gamma\n",
    "        \n",
    "        self.loss_tracker = tf.keras.metrics.Mean(\"VIC_loss_tracker\")\n",
    "        self.invar_loss_tracker = tf.keras.metrics.Mean(\"Invariance_loss_tracker\")\n",
    "        self.var_loss_tracker = tf.keras.metrics.Mean(\"Variance_loss_tracker\")\n",
    "        self.covar_loss_tracker = tf.keras.metrics.Mean(\"Covariance_loss_tracker\")\n",
    "        self.linear_probing = probe\n",
    "        if probe:\n",
    "            self.probe_loss_tracker = tf.keras.metrics.Mean(\"Probe_loss_tracker\")\n",
    "            self.probe_metrics = [keras.metrics.Accuracy(name=\"probe_accuracy\"),\n",
    "                                  keras.metrics.F1Score(average=\"micro\", threshold=None, name=\"probe_micro_f1_score\"),\n",
    "                                 keras.metrics.TruePositives(name = \"probe_TP\"),\n",
    "                                 keras.metrics.TrueNegatives(name = \"probe_TN\"),\n",
    "                                 keras.metrics.FalsePositives(name = \"probe_FP\"),\n",
    "                                 keras.metrics.FalseNegatives(name = \"probe_FN\")]\n",
    "            \n",
    "            self.probe_categories = probe_heads\n",
    "            self.probe_act = probe_activation\n",
    "            self.linear_probe = keras.layers.Dense(units = probe_heads, activation = self.probe_act)\n",
    "            if self.probe_act == \"sigmoid\":\n",
    "                self.probe_loss_fn = keras.losses.BinaryCrossentropy(reduction = None, label_smoothing = 0.1)\n",
    "            elif self.probe_act == \"softmax\":\n",
    "                self.probe_loss_fn = keras.losses.CategoricalCrossentropy(reduction = None, label_smoothing = 0.1)\n",
    "    def compile(self, optimizer, probe_optimizer = None, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "        self.optimizer = optimizer\n",
    "        if self.linear_probing:\n",
    "            self.probe_optimizer = probe_optimizer     \n",
    "    def get_env_config(self):\n",
    "        return {\"feature_extractor_name\" : self.feature_extractor.name,\n",
    "               \"embed_dims\" : self.embed_dims,\n",
    "                 \"Multiview(>2)\" : self.multiview,\n",
    "               'Variance_coefficient' : self.var_coef,\n",
    "               'Invariance_coefficient' : self.invar_coef,\n",
    "               \"Covariance_coefficient\" : self.cov_coef,\n",
    "                \"Variance_gamma\" : self.gamma,\n",
    "               \"SSL_method\" : \"VICReg\",\n",
    "               \"Linear Probe\" : self.linear_probing,\n",
    "               \"N_Categories\" : self.probe_categories if self.linear_probing else 0,\n",
    "               \"Probe Activation\" : self.probe_act if self.linear_probing else \"NA\"}\n",
    "    def get_projector(self):\n",
    "        model = keras.Sequential([Dense(units = self.embed_dims),\n",
    "                                  keras.layers.Activation(\"gelu\"),\n",
    "                                  keras.layers.Dropout(0.25),\n",
    "                                 Dense(units = self.embed_dims,dtype = \"float32\"),\n",
    "                                 keras.layers.Lambda(lambda x : ops.normalize(x))]\n",
    "                                )\n",
    "        return model\n",
    "    \n",
    "    def compute_loss(self, rep_a, rep_b):\n",
    "        invar_loss = invariance_loss(rep_a, rep_b) * self.invar_coef\n",
    "        invar_loss = ops.mean(invar_loss)\n",
    "        \n",
    "        variance_a = variance(rep_a, gamma = self.gamma)\n",
    "        variance_a = ops.mean(variance_a)\n",
    "        \n",
    "        covariance_a = covariance(rep_a)\n",
    "        \n",
    "        variance_b = variance(rep_b, gamma = self.gamma)\n",
    "        variance_b = ops.mean(variance_b)\n",
    "        \n",
    "        covariance_b = covariance(rep_b)\n",
    "        #Variance loss -> variance가 toward gamma\n",
    "        # covariance -> covariance가 toward zero\n",
    "        var_loss = (variance_a + variance_b) * self.var_coef\n",
    "        covar_loss = (covariance_a + covariance_b) * self.cov_coef\n",
    "        loss = invar_loss + var_loss + covar_loss\n",
    "        return loss, invar_loss, var_loss, covar_loss\n",
    "        \n",
    "    def train_step(self, dataset):\n",
    "        if self.linear_probing:\n",
    "            data, aug_data, labels = dataset\n",
    "        else:\n",
    "            if self.multiview:\n",
    "                data = dataset[0]\n",
    "                aug_data = dataset[1:] ; n_augs = len(aug_data)\n",
    "                aug_data = ops.concatenate(aug_data, axis = 0)\n",
    "            else:\n",
    "                data, aug_data = dataset\n",
    "            \n",
    "        with tf.GradientTape() as tape:\n",
    "            \n",
    "            if len(self.feature_extractor.outputs) == 2 :\n",
    "                feature_seq, weights = self.feature_extractor(data)\n",
    "                feature_seq_aug, weights_aug = self.feature_extractor(aug_data)\n",
    "            elif len(self.feature_extractor.outputs) == 1:\n",
    "                feature_seq = self.feature_extractor(data)\n",
    "                feature_seq_aug = self.feature_extractor(aug_data)\n",
    "            \n",
    "            if len(ops.shape(feature_seq)) == 3: \n",
    "                rep_vector = keras.layers.GlobalAveragePooling1D()(feature_seq)\n",
    "                rep_vector_aug = keras.layers.GlobalAveragePooling1D()(feature_seq_aug)\n",
    "            elif len(ops.shape(feature_seq)) == 4:\n",
    "                rep_vector = keras.layers.GlobalAveragePooling2D()(feature_seq)\n",
    "                rep_vector_aug = keras.layers.GlobalAveragePooling2D()(feature_seq_aug)\n",
    "            elif len(ops.shape(feature_seq)) == 2:\n",
    "                rep_vector = feature_seq\n",
    "                rep_vector_aug = feature_seq_aug\n",
    "            \n",
    "            \n",
    "            rep_vector = self.projector_a(rep_vector)\n",
    "            rep_vector_aug = self.projector_b(rep_vector_aug)\n",
    "            if self.multiview:\n",
    "                loss, invar_loss, var_loss, covar_loss = 0.0, 0.0, 0.0, 0.0\n",
    "                rep_vector_aug_set = ops.split(rep_vector_aug, n_augs, 0)\n",
    "                for idx in range(n_augs):\n",
    "                    loss_, invar_loss_, var_loss_, covar_loss_ = self.compute_loss(rep_vector, rep_vector_aug_set[idx])\n",
    "                    loss += loss_\n",
    "                    invar_loss += invar_loss_\n",
    "                    var_loss += var_loss_\n",
    "                    covar_loss += covar_loss_\n",
    "                loss /= n_augs\n",
    "                invar_loss /= n_augs\n",
    "                var_loss /= n_augs\n",
    "                covar_loss /= n_augs\n",
    "                \n",
    "            else:\n",
    "                loss, invar_loss, var_loss, covar_loss = self.compute_loss(rep_vector, rep_vector_aug)\n",
    "        \n",
    "        feature_extractor_var = self.feature_extractor.trainable_variables\n",
    "        proj_a_var = self.projector_a.trainable_variables\n",
    "        proj_b_var = self.projector_b.trainable_variables\n",
    "        \n",
    "        trainable_variables = (feature_extractor_var + proj_a_var + proj_b_var)\n",
    "        gradients = tape.gradient(loss, trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "        \n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.invar_loss_tracker.update_state(invar_loss)\n",
    "        self.var_loss_tracker.update_state(var_loss)\n",
    "        self.covar_loss_tracker.update_state(covar_loss)\n",
    "        output_dict = {'loss' : self.loss_tracker.result(),\n",
    "                        \"invariance_loss\" : self.invar_loss_tracker.result(),\n",
    "                        'variance_loss' : self.var_loss_tracker.result(),\n",
    "                        'covariance_loss' : self.covar_loss_tracker.result()\n",
    "                       }\n",
    "        \n",
    "        if self.linear_probing:\n",
    "            ##2. linear probing and calculate metrics##\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                if len(self.feature_extractor.outputs) == 2 :\n",
    "                    feature_seq, weights = self.feature_extractor(data, training = False)\n",
    "                elif len(self.feature_extractor.outputs) == 1:\n",
    "                    feature_seq = self.feature_extractor(data, training = False)\n",
    "                if len(ops.shape(feature_seq)) == 3: \n",
    "                    original_rep_vector = keras.layers.GlobalAveragePooling1D()(feature_seq)\n",
    "                elif len(ops.shape(feature_seq)) == 4:\n",
    "                    original_rep_vector = keras.layers.GlobalAveragePooling2D()(feature_seq)\n",
    "                elif len(ops.shape(feature_seq)) == 2:\n",
    "                    original_rep_vector = feature_seq\n",
    "                    \n",
    "                class_logits = self.linear_probe(original_rep_vector, training = True)\n",
    "                class_probe_loss = self.probe_loss_fn(y_true = labels, y_pred = class_logits)\n",
    "                class_probe_loss = ops.mean(class_probe_loss)\n",
    "            gradients = tape.gradient(class_probe_loss, \n",
    "                                            self.linear_probe.trainable_weights)\n",
    "            self.probe_optimizer.apply_gradients(zip(gradients, self.linear_probe.trainable_weights))\n",
    "            self.probe_loss_tracker.update_state(class_probe_loss)\n",
    "            output_dict.update({\"probe_classification_loss\" : self.probe_loss_tracker.result()})\n",
    "            #metrics\n",
    "            for comp in self.probe_metrics:\n",
    "                comp.update_state(labels, class_logits)\n",
    "                \n",
    "            probe_metric_dict = {comp.name :  comp.result() for comp in self.probe_metrics}\n",
    "            output_dict.update(probe_metric_dict)\n",
    "        else:\n",
    "            pass\n",
    "        del tape\n",
    "        return output_dict\n",
    "    def test_step(self, dataset):\n",
    "        if self.linear_probing:\n",
    "            data, aug_data, labels = dataset\n",
    "        else:\n",
    "            if self.multiview:\n",
    "                data = dataset[0]\n",
    "                aug_data = dataset[1:] ; n_augs = len(aug_data)\n",
    "                aug_data = ops.concatenate(aug_data, axis = 0)\n",
    "            else:\n",
    "                data, aug_data = dataset\n",
    "        \n",
    "        if len(self.feature_extractor.outputs) == 2 :\n",
    "            feature_seq, weights = self.feature_extractor(data)\n",
    "            feature_seq_aug, weights_aug = self.feature_extractor(aug_data)\n",
    "        elif len(self.feature_extractor.outputs) == 1:\n",
    "            feature_seq = self.feature_extractor(data)\n",
    "            feature_seq_aug = self.feature_extractor(aug_data)\n",
    "        \n",
    "        if len(ops.shape(feature_seq)) == 3: \n",
    "            rep_vector = keras.layers.GlobalAveragePooling1D()(feature_seq)\n",
    "            rep_vector_aug = keras.layers.GlobalAveragePooling1D()(feature_seq_aug)\n",
    "        elif len(ops.shape(feature_seq)) == 4:\n",
    "            rep_vector = keras.layers.GlobalAveragePooling2D()(feature_seq)\n",
    "            rep_vector_aug = keras.layers.GlobalAveragePooling2D()(feature_seq_aug)\n",
    "        elif len(ops.shape(feature_seq)) == 2:\n",
    "            rep_vector = feature_seq\n",
    "            rep_vector_aug = feature_seq_aug\n",
    "            \n",
    "        original_rep_vector = rep_vector\n",
    "        \n",
    "        rep_vector = self.projector_a(rep_vector, training = False)\n",
    "        rep_vector_aug = self.projector_b(rep_vector_aug, training = False)\n",
    "        if self.multiview:\n",
    "            loss, invar_loss, var_loss, covar_loss = [],[],[],[]\n",
    "            rep_vector_aug_set = ops.split(rep_vector_aug, n_augs, 0)\n",
    "            for idx in range(n_augs):\n",
    "                loss_, invar_loss_, var_loss_, covar_loss_ = self.compute_loss(rep_vector, rep_vector_aug_set[idx])\n",
    "                loss.append(loss_)\n",
    "                invar_loss.append(invar_loss_)\n",
    "                var_loss.append(var_loss_)\n",
    "                covar_loss.append(covar_loss_)\n",
    "            loss = ops.mean(loss)\n",
    "            invar_loss = ops.mean(invar_loss)\n",
    "            var_loss = ops.mean(var_loss)\n",
    "            covar_loss = ops.mean(covar_loss)    \n",
    "        else:\n",
    "            loss, invar_loss, var_loss, covar_loss = self.compute_loss(rep_vector, rep_vector_aug)\n",
    "        \n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.invar_loss_tracker.update_state(invar_loss)\n",
    "        self.var_loss_tracker.update_state(var_loss)\n",
    "        self.covar_loss_tracker.update_state(covar_loss)\n",
    "        \n",
    "        output_dict =  {'loss' : self.loss_tracker.result(),\n",
    "                \"invariance_loss\" : self.invar_loss_tracker.result(),\n",
    "                'variance_loss' : self.var_loss_tracker.result(),\n",
    "                'covariance_loss' : self.covar_loss_tracker.result()\n",
    "               }\n",
    "        if self.linear_probing:\n",
    "            ##2. linear probing and calculate metrics##\n",
    "            self.feature_extractor.trainable = False\n",
    "            class_logits = self.linear_probe(original_rep_vector, training = False)\n",
    "            class_probe_loss = self.probe_loss_fn(y_true = labels, y_pred = class_logits)\n",
    "            class_probe_loss = ops.mean(class_probe_loss)\n",
    "            self.probe_loss_tracker.update_state(class_probe_loss)\n",
    "            output_dict.update({\"probe_classification_loss\" : self.probe_loss_tracker.result()})\n",
    "            #metrics\n",
    "            for comp in self.probe_metrics:\n",
    "                comp.update_state(labels, class_logits)\n",
    "                \n",
    "            probe_metric_dict = {comp.name :  comp.result() for comp in self.probe_metrics}\n",
    "            output_dict.update(probe_metric_dict)\n",
    "            self.feature_extractor.trainable = True\n",
    "        return output_dict\n",
    "    def get_classifier_model(self):\n",
    "        self.feature_extractor.trainable = True\n",
    "        if self.linear_probing == False:\n",
    "            return self.feature_extractor\n",
    "        self.linear_probe.trainable = True\n",
    "        inputs = self.feature_extractor.input\n",
    "        outputs = self.feature_extractor.outputs\n",
    "        if len(self.feature_extractor.outputs) == 2 :\n",
    "            feature_map, att_weights = outputs\n",
    "        elif len(self.feature_extractor.outputs) == 1:\n",
    "            feature_map = outputs[0]\n",
    "        \n",
    "        if len(ops.shape(feature_map)) == 3: \n",
    "            rep_vector = keras.layers.GlobalAveragePooling1D()(feature_map)\n",
    "        elif len(ops.shape(feature_map)) == 4:\n",
    "            rep_vector = keras.layers.GlobalAveragePooling2D()(feature_map)\n",
    "        elif len(ops.shape(feature_seq)) == 2:\n",
    "            rep_vector = feature_map\n",
    "            \n",
    "        class_logits = self.linear_probe(rep_vector)\n",
    "        classifier_model = keras.Model(inputs, class_logits,\n",
    "                                      name = f\"SSL_pretrained_{self.feature_extractor.name}\")\n",
    "        return classifier_model\n",
    "    def call(self, dataset):\n",
    "        return self.test_step(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567c29e4",
   "metadata": {
    "papermill": {
     "duration": 0.031086,
     "end_time": "2024-11-08T06:27:05.371487",
     "exception": false,
     "start_time": "2024-11-08T06:27:05.340401",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "-------------\n",
    "# Contrastive-based method\n",
    "- SimSiam\n",
    "- SimCLR\n",
    "- SwAV\n",
    "- DINO\n",
    "- MoCo and moco-based learnings:\n",
    "    - [Dense Contrastive learning](https://arxiv.org/abs/2011.09157v2) -> DCL\n",
    "    - [Nearest-Neighbor Contrastive Learning of Visual Representations](https://arxiv.org/abs/2104.14548v2) -> NNCLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cb427cd6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T06:27:05.437319Z",
     "iopub.status.busy": "2024-11-08T06:27:05.436974Z",
     "iopub.status.idle": "2024-11-08T06:27:05.471741Z",
     "shell.execute_reply": "2024-11-08T06:27:05.470948Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.069598,
     "end_time": "2024-11-08T06:27:05.473574",
     "exception": false,
     "start_time": "2024-11-08T06:27:05.403976",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SimSiam(keras.Model):\n",
    "    def __init__(self, feature_extractor, embed_dims = 256, multiview = False,\n",
    "                 probe = False, probe_heads = None, probe_activation = \"sigmoid\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.feature_extractor = feature_extractor #attentive pooling을 거친 encoder로 가정 : rep_vector, attention_Weight를 output으로\n",
    "        self.embed_dims = embed_dims\n",
    "        self.multiview = multiview\n",
    "        \n",
    "        self.train_type = 'SimSiam'\n",
    "        self.loss_tracker = keras.metrics.Mean(name = f\"{self.train_type}_loss\")\n",
    "        self.probe = probe ; self.probe_heads = probe_heads\n",
    "        self.probe_activation = probe_activation\n",
    "        \n",
    "        print(\"Train with Gradient Accumulation is recommended!\")\n",
    "        \n",
    "        self.embed_dims = embed_dims\n",
    "        self.predictor = keras.Sequential([keras.layers.Dense(units = self.embed_dims,\n",
    "                                                            use_bias = False,\n",
    "                                                            kernel_regularizer = keras.regularizers.l2(5e-4),\n",
    "                                                            ),\n",
    "                                          keras.layers.Activation(\"gelu\"),\n",
    "                                          keras.layers.BatchNormalization(),\n",
    "                                          keras.layers.Dense(units = self.embed_dims),\n",
    "                                          ], name = f\"{self.train_type}_predictor\")\n",
    "        if self.probe:\n",
    "            self.linear_probe = Dense(units = self.probe_heads, \n",
    "                                     activation = self.probe_activation)\n",
    "            self.probe_loss_tracker = tf.keras.metrics.Mean(\"Probe_loss_tracker\")\n",
    "            self.probe_metrics = [keras.metrics.Accuracy(name=\"probe_accuracy\"),\n",
    "                                  keras.metrics.F1Score(average=\"micro\", threshold=None, name=\"probe_micro_f1_score\"),\n",
    "                                 keras.metrics.TruePositives(name = \"probe_TP\"),\n",
    "                                 keras.metrics.TrueNegatives(name = \"probe_TN\"),\n",
    "                                 keras.metrics.FalsePositives(name = \"probe_FP\"),\n",
    "                                 keras.metrics.FalseNegatives(name = \"probe_FN\")]\n",
    "            if self.probe_activation == \"sigmoid\":\n",
    "                self.probe_loss_fn = keras.losses.BinaryCrossentropy(reduction = None, label_smoothing = 0.1)\n",
    "            elif self.probe_activation == \"softmax\":\n",
    "                self.probe_loss_fn = keras.losses.CategoricalCrossentropy(reduction = None, label_smoothing = 0.1)\n",
    "    def get_env_config(self):\n",
    "        return {\"feature_extractor_name\" : self.feature_extractor.name,\n",
    "               \"embed_dims\" : self.embed_dims,\n",
    "                \"Multiview(>2)\" : self.multiview,\n",
    "               \"SSL_method\" : \"SimSiam\",\n",
    "               \"Linear Probe\" : self.probe,\n",
    "               \"N_Categories\" : self.probe_categories if self.probe else 0,\n",
    "               \"Probe Activation\" : self.probe_act if self.probe else \"NA\"}\n",
    "    def compile(self, optimizer, probe_optimizer = None, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "        self.optimizer = optimizer\n",
    "        if self.probe:\n",
    "            self.probe_optimizer = probe_optimizer \n",
    "    def compute_loss(self, p, z, epsilon = 1e-5):\n",
    "        # stop gradient is essential in SimSiam structure\n",
    "        # p, z is representation vectors : batch_size, embed_dims\n",
    "        \n",
    "        z = ops.stop_gradient(p)\n",
    "        z = keras.utils.normalize(z, axis = -1, order = 2)\n",
    "        p = keras.utils.normalize(p, axis = -1, order = 2)\n",
    "        cos_sim = ops.mean(ops.sum(z*p, axis = -1)\n",
    "                          ) #batchwise mean\n",
    "        return -cos_sim\n",
    "\n",
    "    def train_step(self, dataset):\n",
    "        if self.probe:\n",
    "            img1, img2, labels = dataset\n",
    "        else:\n",
    "            if self.multiview:\n",
    "                img1 = dataset[0]\n",
    "                img2 = dataset[1:] ; n_augs = len(img2)\n",
    "                img2 = ops.concatenate(img2, axis = 0)\n",
    "            else:\n",
    "                img1, img2 = dataset\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            if len(self.feature_extractor.outputs) == 2:\n",
    "                z1, weight1 = self.feature_extractor(img1, training = True)\n",
    "                z2, weight2 = self.feature_extractor(img2, training = True)\n",
    "            elif len(self.feature_extractor.outputs) == 1:\n",
    "                z1 = self.feature_extractor(img1, training = True)\n",
    "                z2 = self.feature_extractor(img2, training = True)\n",
    "            v1, v2 = self.predictor(z1, training = True), self.predictor(z2, training = True)\n",
    "            \n",
    "            if self.multiview:\n",
    "                v2 = ops.split(v2, n_augs, 0)\n",
    "                loss = []\n",
    "                for idx in range(n_augs):\n",
    "                    loss.append(self.compute_loss(v1, v2[idx])\n",
    "                               )\n",
    "                loss = ops.mean(loss)\n",
    "            else:\n",
    "                loss = 0.5*(self.compute_loss(v1, v2) + self.compute_loss(v2, v1))\n",
    "        trainable_params = self.feature_extractor.trainable_variables + self.predictor.trainable_variables\n",
    "        grads = tape.gradient(loss, trainable_params)\n",
    "        self.optimizer.apply_gradients(zip(grads, trainable_params))\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        output_dict = {self.loss_tracker.name : self.loss_tracker.result()}\n",
    "        \n",
    "        if self.probe:\n",
    "            with tf.GradientTape() as tape:\n",
    "                class_logits = self.linear_probe(z1)\n",
    "                probe_loss = self.probe_loss_fn(y_true = labels, y_pred = class_logits)\n",
    "                probe_loss = ops.mean(probe_loss)\n",
    "            grads = tape.gradient(probe_loss, self.linear_probe.trainable_weights)\n",
    "            self.probe_optimizer.apply_gradients(zip(grads, self.linear_probe.trainable_weights))\n",
    "            self.probe_loss_tracker.update_state(probe_loss)\n",
    "            output_dict.update({\"probe_classification_loss\" : self.probe_loss_tracker.result()})\n",
    "            #metrics\n",
    "            for comp in self.probe_metrics:\n",
    "                comp.update_state(labels, class_logits)\n",
    "                \n",
    "            probe_metric_dict = {comp.name :  comp.result() for comp in self.probe_metrics}\n",
    "            output_dict.update(probe_metric_dict)\n",
    "        del tape\n",
    "        return output_dict\n",
    "    \n",
    "    def test_step(self, dataset):\n",
    "        if self.probe:\n",
    "            img1, img2, labels = dataset\n",
    "        else:\n",
    "            if self.multiview:\n",
    "                img1 = dataset[0]\n",
    "                img2 = dataset[1:] ; n_augs = len(img2)\n",
    "                img2 = ops.concatenate(img2, axis = 0)\n",
    "            else:\n",
    "                img1, img2 = dataset\n",
    "        \n",
    "        if len(self.feature_extractor.outputs) == 2:\n",
    "            z1, weight1 = self.feature_extractor(img1, training = False)\n",
    "            z2, weight2 = self.feature_extractor(img2, training = False)\n",
    "        elif len(self.feature_extractor.outputs) == 1:\n",
    "            z1 = self.feature_extractor(img1, training = False)\n",
    "            z2 = self.feature_extractor(img2, training = False)\n",
    "        v1, v2 = self.predictor(z1, training = False), self.predictor(z2, training = False)\n",
    "        if self.multiview:\n",
    "            v2 = ops.split(v2, n_augs, 0)\n",
    "            loss = []\n",
    "            for idx in range(n_augs):\n",
    "                loss.append(self.compute_loss(v1, v2[idx])\n",
    "                            )\n",
    "            loss = ops.mean(loss)\n",
    "        else:\n",
    "            loss = 0.5*(self.compute_loss(v1, v2) + self.compute_loss(v2, v1))\n",
    "        \n",
    "        self.loss_tracker.update_state(loss)\n",
    "        output_dict = {self.loss_tracker.name : self.loss_tracker.result()}\n",
    "        \n",
    "        if self.probe:\n",
    "            class_logits = self.linear_probe(z1)\n",
    "            probe_loss = self.probe_loss_fn(y_true = labels, y_pred = class_logits)\n",
    "            probe_loss = ops.mean(probe_loss)\n",
    "            self.probe_loss_tracker.update_state(probe_loss)\n",
    "            output_dict.update({\"probe_classification_loss\" : self.probe_loss_tracker.result()})\n",
    "            for comp in self.probe_metrics:\n",
    "                comp.update_state(labels, class_logits)\n",
    "                \n",
    "            probe_metric_dict = {comp.name :  comp.result() for comp in self.probe_metrics}\n",
    "            output_dict.update(probe_metric_dict)\n",
    "        \n",
    "        return output_dict\n",
    "    \n",
    "    def call(self, dataset):\n",
    "        return self.test_step(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4cafb0bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T06:27:05.538415Z",
     "iopub.status.busy": "2024-11-08T06:27:05.538054Z",
     "iopub.status.idle": "2024-11-08T06:27:05.562290Z",
     "shell.execute_reply": "2024-11-08T06:27:05.561361Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.059259,
     "end_time": "2024-11-08T06:27:05.564303",
     "exception": false,
     "start_time": "2024-11-08T06:27:05.505044",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SimCLR(keras.Model):\n",
    "    def __init__(self, feature_extractor, embed_dims = 256, multiview = False, probe = False,\n",
    "                 temperature = 0.1,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.feature_extractor = feature_extractor #attentive pooling을 거친 encoder로 가정 : rep_vector, attention_Weight를 output으로\n",
    "        self.embed_dims = embed_dims\n",
    "        self.multiview = multiview\n",
    "        self.t = temperature\n",
    "        self.train_type = 'SimCLR'\n",
    "        print(\"Train with Gradient Accumulation is recommended!\")\n",
    "        \n",
    "        self.embed_dims = embed_dims\n",
    "        self.predictor = keras.Sequential([keras.layers.Dense(units = self.embed_dims,\n",
    "                                                            use_bias = False,\n",
    "                                                            ),\n",
    "                                          keras.layers.Activation(\"gelu\"),\n",
    "                                          keras.layers.LayerNormalization(),\n",
    "                                          keras.layers.Dense(units = self.embed_dims),\n",
    "                                          keras.layers.Activation(\"gelu\"),\n",
    "                                          keras.layers.LayerNormalization(),\n",
    "                                          keras.layers.Dense(units = self.embed_dims),\n",
    "                                           keras.layers.Lambda(lambda x : ops.normalize(x))\n",
    "                                          ], name = f\"{self.train_type}_predictor\")\n",
    "        self.probe = False\n",
    "        \n",
    "        self.loss_tracker = keras.metrics.Mean(name = f\"{self.train_type}_loss\")\n",
    "        \n",
    "    def get_config(self):\n",
    "        return {\"feature_extractor_name\" : self.feature_extractor.name,\n",
    "               \"embed_dims\" : self.embed_dims,\n",
    "                \"Multiview(>2)\" : self.multiview,\n",
    "               \"SSL_method\" : \"SimCLR\",\n",
    "               \"Linear Probe\" : self.probe,\n",
    "               \"N_Categories\" : self.probe_categories if self.probe else 0,\n",
    "               \"Probe Activation\" : self.probe_act if self.probe else \"NA\"}\n",
    "    def compile(self, optimizer, probe_optimizer = None, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "        self.optimizer = optimizer\n",
    "        if self.probe:\n",
    "            self.probe_optimizer = probe_optimizer \n",
    "\n",
    "    def train_step(self, dataset):\n",
    "        if self.probe:\n",
    "            img1, img2, labels = dataset\n",
    "        else:\n",
    "            if self.multiview:\n",
    "                img1 = dataset[0]\n",
    "                img2 = dataset[1:] ; n_augs = len(img2)\n",
    "                img2 = ops.concatenate(img2, axis = 0)\n",
    "            else:\n",
    "                img1, img2 = dataset\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            if len(self.feature_extractor.outputs) == 2:\n",
    "                z1, weight1 = self.feature_extractor(img1, training = True)\n",
    "                z2, weight2 = self.feature_extractor(img2, training = True)\n",
    "            elif len(self.feature_extractor.outputs) == 1:\n",
    "                z1 = self.feature_extractor(img1, training = True)\n",
    "                z2 = self.feature_extractor(img2, training = True)\n",
    "            v1, v2 = self.predictor(z1, training = True), self.predictor(z2, training = True)\n",
    "            \n",
    "            if self.multiview:\n",
    "                v2 = ops.split(v2, n_augs, 0)\n",
    "                loss = []\n",
    "                for idx in range(n_augs):\n",
    "                    loss.append(keras_cv.losses.SimCLRLoss(temperature = 0.1, reduction = None)(v1, v2[idx]))\n",
    "                    \n",
    "                loss = ops.mean(loss)\n",
    "                \n",
    "            else:\n",
    "                loss = keras_cv.losses.SimCLRLoss(temperature = 0.1)(v1, v2)\n",
    "        trainable_params = self.feature_extractor.trainable_variables + self.predictor.trainable_variables\n",
    "        grads = tape.gradient(loss, trainable_params)\n",
    "        self.optimizer.apply_gradients(zip(grads, trainable_params))\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        output_dict = {self.loss_tracker.name : self.loss_tracker.result(),\n",
    "                      }\n",
    "        return output_dict\n",
    "    \n",
    "    def test_step(self, dataset):\n",
    "        if self.probe:\n",
    "            img1, img2, labels = dataset\n",
    "        else:\n",
    "            if self.multiview:\n",
    "                img1 = dataset[0]\n",
    "                img2 = dataset[1:] ; n_augs = len(img2)\n",
    "                img2 = ops.concatenate(img2, axis = 0)\n",
    "            else:\n",
    "                img1, img2 = dataset\n",
    "        \n",
    "        if len(self.feature_extractor.outputs) == 2:\n",
    "            z1, weight1 = self.feature_extractor(img1, training = True)\n",
    "            z2, weight2 = self.feature_extractor(img2, training = True)\n",
    "        elif len(self.feature_extractor.outputs) == 1:\n",
    "            z1 = self.feature_extractor(img1, training = True)\n",
    "            z2 = self.feature_extractor(img2, training = True)\n",
    "        v1, v2 = self.predictor(z1, training = False), self.predictor(z2, training = False)\n",
    "            \n",
    "        if self.multiview:\n",
    "            v2 = ops.split(v2, n_augs, 0)\n",
    "            loss = []\n",
    "            for idx in range(n_augs):\n",
    "                loss.append(keras_cv.losses.SimCLRLoss(temperature = 0.1, reduction = None)(v1, v2[idx]))        \n",
    "            loss = ops.mean(loss)\n",
    "            \n",
    "        else:\n",
    "            loss = keras_cv.losses.SimCLRLoss(temperature = 0.1)(v1, v2)\n",
    "        \n",
    "        self.loss_tracker.update_state(loss)\n",
    "        output_dict = {self.loss_tracker.name : self.loss_tracker.result(),\n",
    "                      }\n",
    "        return output_dict\n",
    "    \n",
    "    def call(self, dataset):\n",
    "        return self.test_step(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "35be3c3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T06:27:05.628625Z",
     "iopub.status.busy": "2024-11-08T06:27:05.628300Z",
     "iopub.status.idle": "2024-11-08T06:27:05.654258Z",
     "shell.execute_reply": "2024-11-08T06:27:05.653399Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.060392,
     "end_time": "2024-11-08T06:27:05.656046",
     "exception": false,
     "start_time": "2024-11-08T06:27:05.595654",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Moco(keras.Model):\n",
    "    def __init__(self, feature_extractor, embed_dims, q_size = 2**13, pool_heads = 8, t = 0.07,\n",
    "                 momentum_coefficient = 0.999,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.momentum_encoder = feature_extractor\n",
    "        self.momentum_encoder.set_weights(self.feature_extractor.get_weights())\n",
    "        self.m = momentum_coefficient\n",
    "        self.pool_heads = pool_heads\n",
    "        self.t = t\n",
    "        self.q_size = q_size\n",
    "        self.embed_dims = embed_dims\n",
    "        self.feature_queue = keras.Variable(\n",
    "            keras.utils.normalize(\n",
    "                keras.random.normal(shape=(self.q_size, self.embed_dims)),\n",
    "                axis=1,\n",
    "                order=2,\n",
    "            ),\n",
    "            trainable=False, dtype = \"float32\"\n",
    "        )\n",
    "        self.projector = keras.Sequential([keras.layers.Dense(units = embed_dims),\n",
    "                                           keras.layers.Activation(\"relu\"),\n",
    "                                          keras.layers.Dense(units = embed_dims, dtype = \"float32\"),\n",
    "                                          ])\n",
    "        self.momentum_projector = keras.models.clone_model(self.projector)\n",
    "        \n",
    "        self.ce_loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
    "        self.loss_tracker = keras.metrics.Mean(name = \"MoCo_loss\")\n",
    "    def get_env_config(self):\n",
    "        return {\"feature_extractor_name\" : self.feature_extractor.name,\n",
    "               \"embed_dims\" : self.embed_dims,\n",
    "               \"SSL_method\" : \"MoCo\",\n",
    "               \"Queue size\" : self.q_size,\n",
    "               \"temperature\" : self.t,\n",
    "               \"Momentum\" : self.m}\n",
    "    def compute_loss(self, z_a, z_b, training = True): #Info-NCE loss in the original paper\n",
    "        z_b = ops.stop_gradient(z_b)\n",
    "        z_a, z_b = keras.utils.normalize(z_a, axis = -1, order = 2), keras.utils.normalize(z_b, axis = -1, order = 2)\n",
    "        z_a, z_b = ops.cast(z_a, \"float32\"), ops.cast(z_b, \"float32\")\n",
    "        \n",
    "        pos_sim_ = ops.diagonal(z_a@ops.transpose(z_b)) ; pseudolabel = ops.zeros_like(pos_sim_)\n",
    "        pos_pair_similarity = ops.expand_dims(pos_sim_, axis = -1) ; del pos_sim_\n",
    "        \n",
    "        neg_pair_similarity = z_a@ops.cast(ops.transpose(self.feature_queue), \"float32\")\n",
    "        logits = ops.concatenate([pos_pair_similarity, neg_pair_similarity], axis = 1)\n",
    "        logits = ops.exp(logits/self.t)\n",
    "        logits = logits / (ops.sum(logits, axis = -1, keepdims = True) + 1e-8)\n",
    "        loss = self.ce_loss_fn(y_true = pseudolabel, y_pred = logits)\n",
    "        \n",
    "        if training:\n",
    "            self.feature_queue.assign(\n",
    "                ops.concatenate([z_a, ops.cast(self.feature_queue[:-ops.shape(z_a)[0] ],\n",
    "                                              \"float32\")\n",
    "                                ], axis=0)\n",
    "            )\n",
    "\n",
    "        return loss\n",
    "    def train_step(self, dataset):\n",
    "        img1, img2 = dataset\n",
    "        with tf.GradientTape() as tape:\n",
    "            if len(self.feature_extractor.outputs) == 2:\n",
    "                z1, weight1 = self.feature_extractor(img1)\n",
    "                z2, weight2 = self.momentum_encoder(img2)\n",
    "            elif len(self.feature_extractor.outputs) == 1:\n",
    "                z1 = self.feature_extractor(img1)\n",
    "                z2 = self.momentum_encoder(img2)\n",
    "            v1, v2 = self.projector(z1), self.momentum_projector(z2)\n",
    "            moco_loss = self.compute_loss(v1, v2)\n",
    "        grads = tape.gradient(moco_loss, \n",
    "                             self.feature_extractor.trainable_weights + self.projector.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, \n",
    "                                          self.feature_extractor.trainable_weights + self.projector.trainable_weights)\n",
    "                                      )\n",
    "        self.loss_tracker.update_state(moco_loss)\n",
    "        \n",
    "        #momentum update\n",
    "        for weight, m_weight in zip(self.feature_extractor.weights, self.momentum_encoder.weights):\n",
    "            m_weight.assign(self.m * m_weight + (1-self.m)*weight)\n",
    "        for weight, m_weight in zip(self.projector.weights, self.momentum_projector.weights):\n",
    "            m_weight.assign(self.m * m_weight + (1-self.m)*weight)\n",
    "        return {self.loss_tracker.name : self.loss_tracker.result()}\n",
    "    def test_step(self, dataset):\n",
    "        img1, img2 = dataset\n",
    "        if len(self.feature_extractor.outputs) == 2:\n",
    "            z1, weight1 = self.feature_extractor(img1)\n",
    "            z2, weight2 = self.momentum_encoder(img2)\n",
    "        elif len(self.feature_extractor.outputs) == 1:\n",
    "            z1 = self.feature_extractor(img1)\n",
    "            z2 = self.momentum_encoder(img2)\n",
    "        v1, v2 = self.projector(z1), self.momentum_projector(z2)\n",
    "        moco_loss = self.compute_loss(v1, v2, training = False)\n",
    "        self.loss_tracker.update_state(moco_loss)\n",
    "        return {self.loss_tracker.name : self.loss_tracker.result()}\n",
    "    def call(self, dataset):\n",
    "        img1, img2 = dataset\n",
    "        if len(self.feature_extractor.outputs) == 2:\n",
    "            z1, weight1 = self.feature_extractor(img1)\n",
    "            z2, weight2 = self.momentum_encoder(img2)\n",
    "        elif len(self.feature_extractor.outputs) == 1:\n",
    "            z1 = self.feature_extractor(img1)\n",
    "            z2 = self.momentum_encoder(img2)\n",
    "        v1, v2 = self.projector(z1), self.momentum_projector(z2)\n",
    "        moco_loss = self.compute_loss(v1, v2, training = False)\n",
    "        self.loss_tracker.update_state(moco_loss)\n",
    "        return {self.loss_tracker.name : self.loss_tracker.result()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3deaf78a",
   "metadata": {
    "papermill": {
     "duration": 0.031304,
     "end_time": "2024-11-08T06:27:05.718466",
     "exception": false,
     "start_time": "2024-11-08T06:27:05.687162",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Neighbor Contrastive Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff860abf",
   "metadata": {
    "papermill": {
     "duration": 0.031117,
     "end_time": "2024-11-08T06:27:05.780530",
     "exception": false,
     "start_time": "2024-11-08T06:27:05.749413",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- [NNCLR](https://keras.io/examples/vision/nnclr/), [SNCLR](https://curiouscat.tistory.com/67)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eb3c1067",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-11-08T06:27:05.844205Z",
     "iopub.status.busy": "2024-11-08T06:27:05.843839Z",
     "iopub.status.idle": "2024-11-08T06:27:05.850550Z",
     "shell.execute_reply": "2024-11-08T06:27:05.849711Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.040784,
     "end_time": "2024-11-08T06:27:05.852436",
     "exception": false,
     "start_time": "2024-11-08T06:27:05.811652",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_pairwise_losses(V, loss_fn):\n",
    "    n = len(V)\n",
    "    batch_size = V[0].shape[0]\n",
    "    embed_dims = V[0].shape[1]\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            # V[i]와 V[j]를 연결 (각각 [batch_size, embed_dims])\n",
    "            combined = ops.concatenate([V[i], V[j]], axis=1)  # 결과: [batch_size, 2*embed_dims]\n",
    "            \n",
    "            # y_true는 사용하지 않지만, loss 함수의 인터페이스를 위해 더미 값 제공\n",
    "            y_true = ops.zeros_like(combined)\n",
    "            \n",
    "            # loss 계산\n",
    "            loss = loss_fn(y_true, combined)\n",
    "            total_loss += loss\n",
    "    \n",
    "    # 총 비교 횟수로 나누어 평균 loss 계산\n",
    "    avg_loss = total_loss / (n * (n - 1) / 2)\n",
    "    \n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dc17579b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T06:27:05.916578Z",
     "iopub.status.busy": "2024-11-08T06:27:05.916304Z",
     "iopub.status.idle": "2024-11-08T06:27:05.956458Z",
     "shell.execute_reply": "2024-11-08T06:27:05.955570Z"
    },
    "papermill": {
     "duration": 0.074769,
     "end_time": "2024-11-08T06:27:05.958435",
     "exception": false,
     "start_time": "2024-11-08T06:27:05.883666",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NCLR(keras.Model): #Neighbor Contrastive LeaRning\n",
    "    def __init__(self, feature_extractor, embed_dims, patch_size,\n",
    "                 subtype = \"nnclr\", #nnclr, snclr\n",
    "                 q_size = 2**15, t = 0.1, use_mim = False, hyperbolic = False,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.subtype = subtype #subtype에 따라 nearest neighbor 전략 or soft neighbor 전략 달라짐.\n",
    "        self.use_mim = use_mim ; self.patch_size = patch_size if self.use_mim else \"N/A\"\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.t = t\n",
    "        self.q_size = q_size\n",
    "        self.embed_dims = embed_dims\n",
    "        self.feature_q = keras.Variable(\n",
    "            keras.utils.normalize(\n",
    "                keras.random.normal(shape=(self.q_size, self.embed_dims)), #q_size, embed_dims shape matrix : FIFO로 update해야 함\n",
    "                axis=1,\n",
    "                order=2,\n",
    "            ),\n",
    "            trainable=False, dtype = \"float32\"\n",
    "        )\n",
    "        self.hyperbolic = hyperbolic\n",
    "        self.compute_vic = VicReg()\n",
    "        if hyperbolic:\n",
    "            self.h_loss = HyperbolicContrastiveLoss(temperature = self.t)\n",
    "        if use_mim:\n",
    "            self.projector = Dense(units = 3*(patch_size**2), activation = \"sigmoid\", name = 'RGB_regressor')\n",
    "            self.reg_fn = keras.losses.MeanAbsoluteError(reduction = None)\n",
    "\n",
    "    def get_env_config(self):\n",
    "        return {\"feature_extractor_name\" : self.feature_extractor.name,\n",
    "               \"embed_dims\" : self.embed_dims,\n",
    "               \"SSL_method\" : f\"NCLR_{self.subtype}_without_momentum\",\n",
    "               \"Queue size\" : self.q_size,\n",
    "               \"temperature\" : self.t,\n",
    "                \"MIM mix\" : self.use_mim\n",
    "               }\n",
    "    def call(self, images, training = False):\n",
    "        if self.use_mim:\n",
    "            mask_sequence, mask_image, image, original_image = images\n",
    "            images = [original_image, image, mask_image]\n",
    "        else:\n",
    "            pass\n",
    "            \n",
    "        if training or isinstance(images, list):\n",
    "            batch_size = ops.shape(images[0])[0] ; n_augs = len(images)\n",
    "            images = keras.ops.concatenate(images, axis = 0)\n",
    "        else:\n",
    "            n_augs = 1\n",
    "        print(ops.shape(images))\n",
    "        # images : batch_size * n_augs, res, res, c\n",
    "        if (isinstance(self.feature_extractor, ViT_rollout)) or (len(self.feature_extractor.outputs) == 3):\n",
    "            cls_tokens, patches, attention_weights = self.feature_extractor(images)\n",
    "        elif len(self.feature_extractor.outputs) == 2:\n",
    "            cls_tokens, attention_weights = self.feature_extractor(images)\n",
    "\n",
    "        if training or isinstance(images, list):\n",
    "            cls_tokens = ops.split(cls_tokens, n_augs, axis = 0) #[anchor token, aug_tokens1, 2, ....]\n",
    "            patches = ops.split(patches, n_augs, axis = 0)\n",
    "            try:\n",
    "                attention_weights = ops.split(attention_weights, n_augs, axis = 0)\n",
    "            except:\n",
    "                attention_weights = attention_weights[\"final_layer_weight\"]\n",
    "                attention_weights = ops.split(attention_weights, n_augs, axis = 0)\n",
    "            \n",
    "        if patches is not None:\n",
    "            return cls_tokens, patches, attention_weights\n",
    "        else:\n",
    "            return cls_tokens, attention_weights\n",
    "        \n",
    "    def get_neighbor(self, projections): \n",
    "        p = projections\n",
    "        cos_sim = ops.matmul(p, ops.transpose(self.feature_q)\n",
    "                            ) # batch, q_size\n",
    "        if self.subtype in ['nnclr', \"NNCLR\", 'hard']:\n",
    "            support_f = ops.take(self.feature_q,\n",
    "                                 ops.argmax(cos_sim, axis = 1), # <- q_size 개 중 가장 similarity가 높은 batch개만 고려\n",
    "                                 axis = 0)\n",
    "            return p + ops.stop_gradient(support_f-p) # <- NNCLR: support set 중 가장 similarity가 높은 support data만 고려\n",
    "        elif self.subtype in ['snclr', \"SNCLR\", 'soft']:\n",
    "            # softmax with temperature + 가중평균\n",
    "            w = ops.softmax(cos_sim/self.t) # batch, q size\n",
    "            support_f = ops.matmul(w, self.feature_q) ; del w #batch, embed_dims\n",
    "            return p + ops.stop_gradient(support_f - p)\n",
    "    \n",
    "    #########loss 구현부터######\n",
    "    def compute_loss(self, projections, training = True): #SimCLR-like loss shape\n",
    "        if isinstance(projections, list):\n",
    "            n_augs = len(projections) #original image, aug_image, masked_image\n",
    "            vic_loss = ops.mean(self.compute_vic(projections[0], projections[1]) + self.compute_vic(projections[0], projections[2]) + self.compute_vic(projections[2], projections[1]))\n",
    "        else:\n",
    "            n_augs = 1\n",
    "            vic_loss = 0.0\n",
    "        # NCLR\n",
    "        f_original, f_aug, f_mim = projections[0], projections[1], projections[2]\n",
    "        f_original, f_aug, f_mim = ops.normalize(f_original), ops.normalize(f_aug), ops.normalize(f_mim)\n",
    "        # 각각 batch, embed_dims shape tensor\n",
    "        f_original_n, f_aug_n, f_mim_n = self.get_neighbor(f_original), self.get_neighbor(f_aug), self.get_neighbor(f_mim)\n",
    "        sim_matrix_1_1 = ops.matmul(f_original, ops.transpose(f_aug_n)) / self.t\n",
    "        sim_matrix_1_2 = ops.matmul(f_original_n, ops.transpose(f_aug)) / self.t\n",
    "        sim_matrix_2_1 = ops.matmul(f_original, ops.transpose(f_mim_n)) / self.t\n",
    "        sim_matrix_2_2 = ops.matmul(f_original_n, ops.transpose(f_mim)) / self.t\n",
    "        batch_size = ops.shape(f_original)[0]\n",
    "        pseudo_label = ops.arange(ops.shape(f_original)[0])\n",
    "        \n",
    "        loss_1 = (keras.losses.sparse_categorical_crossentropy(pseudo_label, sim_matrix_1_1, from_logits = True) + \n",
    "                 keras.losses.sparse_categorical_crossentropy(pseudo_label, ops.transpose(sim_matrix_1_1), from_logits = True) + \n",
    "                 keras.losses.sparse_categorical_crossentropy(pseudo_label, sim_matrix_1_2, from_logits = True) + \n",
    "                 keras.losses.sparse_categorical_crossentropy(pseudo_label, ops.transpose(sim_matrix_1_2), from_logits = True))\n",
    "        \n",
    "        loss_2 = (keras.losses.sparse_categorical_crossentropy(pseudo_label, sim_matrix_2_1, from_logits = True) + \n",
    "                 keras.losses.sparse_categorical_crossentropy(pseudo_label, ops.transpose(sim_matrix_2_1), from_logits = True) + \n",
    "                 keras.losses.sparse_categorical_crossentropy(pseudo_label, sim_matrix_2_2, from_logits = True) + \n",
    "                 keras.losses.sparse_categorical_crossentropy(pseudo_label, ops.transpose(sim_matrix_2_2), from_logits = True))\n",
    "        \n",
    "        if training:\n",
    "            self.feature_q.assign(\n",
    "                ops.concatenate([f_original, \n",
    "                                 self.feature_q[:-batch_size ]\n",
    "                                ], axis=0)\n",
    "            )\n",
    "            \n",
    "        nclr_loss = ops.mean(loss_1 + loss_2)\n",
    "        loss = nclr_loss + 0.05 * vic_loss\n",
    "        \n",
    "        return loss, nclr_loss, vic_loss\n",
    "    \n",
    "\n",
    "    def compute_hyperbolic_loss(self, projections, training = True):\n",
    "        nclr_loss = self.h_loss(projections, y_pred = 0.0)\n",
    "        if isinstance(projections, list):\n",
    "            n_augs = len(projections)\n",
    "            projections = ops.concatenate(projections, axis = 0)\n",
    "        else:\n",
    "            n_augs = 1\n",
    "        var_loss, cov_loss = variance(projections), covariance(projections)\n",
    "        var_loss, cov_loss = ops.mean(var_loss), ops.mean(cov_loss)\n",
    "        loss = nclr_loss\n",
    "        \n",
    "        # QUEUE update, FIFO\n",
    "        if training:\n",
    "            self.feature_q.assign(\n",
    "                ops.concatenate([projections[:batch_size, :], \n",
    "                                 self.feature_q[:-batch_size ]\n",
    "                                ], axis=0)\n",
    "            )\n",
    "        return loss, nclr_loss, var_loss, cov_loss\n",
    "    \n",
    "    \n",
    "    def train_step(self, dataset, training = True):\n",
    "        if self.use_mim:\n",
    "            mask_sequence, mask_image, image, original_image = dataset\n",
    "            original_patches = original_image / 255.0 #batch, res, res, 3\n",
    "            original_patches = ops.image.extract_patches(original_patches, self.patch_size, padding = 'same')\n",
    "            n_patches = ops.shape(original_patches)[1] * ops.shape(original_patches)[2]\n",
    "            channels = ops.shape(original_patches)[-1]\n",
    "            original_patches = ops.reshape(original_patches, [-1, n_patches, self.patch_size, self.patch_size, 3])\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            if (isinstance(self.feature_extractor, ViT_rollout)) or (len(self.feature_extractor.outputs) == 3):\n",
    "                cls_tokens, patches, attention_weights = self(dataset, training = training)\n",
    "                if self.use_mim :\n",
    "                    patches_masked = ops.split(patches, 3, axis = 0)[0]\n",
    "                    \n",
    "                    patches = self.projector(patches_masked)\n",
    "                    patches = ops.reshape(patches, [-1, n_patches, self.patch_size, self.patch_size, 3])\n",
    "                    \n",
    "                    mim_loss = self.reg_fn(original_patches, patches)\n",
    "                    mim_loss = mim_loss * mask_sequence[..., tf.newaxis, tf.newaxis]\n",
    "                    mim_loss = ops.mean(mim_loss)\n",
    "                else:\n",
    "                    mim_loss = 0\n",
    "            elif len(self.feature_extractor.outputs) == 2:\n",
    "                cls_tokens, attention_weights = self(dataset, training = training)\n",
    "                mim_loss = 0\n",
    "            if self.hyperbolic:\n",
    "                loss, nclr_loss, _ = self.compute_hyperbolic_loss(cls_tokens, training=training)\n",
    "            else:\n",
    "                loss, nclr_loss, vic_loss = self.compute_loss(cls_tokens, training=training)\n",
    "            \n",
    "            if self.use_mim:\n",
    "                loss = loss + mim_loss\n",
    "            else:\n",
    "                mim_loss = 0\n",
    "                pass\n",
    "            \n",
    "            #attention_weight_loss, entropy = attention_loss(attention_weights[0])\n",
    "            #loss += attention_weight_loss\n",
    "        dims = ops.shape(attention_weights[0])[-1]\n",
    "        w_transpose = ops.reshape(attention_weights[0], [-1, dims])\n",
    "        entropy = ops.mean(-ops.sum(w_transpose * ops.log(w_transpose + 1e-5), axis=-1))\n",
    "            \n",
    "        grads = tape.gradient(loss, \n",
    "                             self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, \n",
    "                                          self.trainable_weights)\n",
    "                                      )\n",
    "        \n",
    "        return {\"Total_loss\" : loss, \"MIM_loss\" : mim_loss,\n",
    "                #\"Attention_weight_loss\" : attention_weight_loss,\n",
    "                \"Attention_weight_entropy\" : entropy,\n",
    "               \"NCLR_loss\" : nclr_loss, \"VIC_loss\" : vic_loss\n",
    "               }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d28d497",
   "metadata": {
    "papermill": {
     "duration": 0.031038,
     "end_time": "2024-11-08T06:27:06.020762",
     "exception": false,
     "start_time": "2024-11-08T06:27:05.989724",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32e020b",
   "metadata": {
    "papermill": {
     "duration": 0.031023,
     "end_time": "2024-11-08T06:27:06.083057",
     "exception": false,
     "start_time": "2024-11-08T06:27:06.052034",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Clustering & Distillation\n",
    "- SwAV, DINO\n",
    "\n",
    "> DINO architecture:\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:1400/1*huuMgEbBryxXUufW33uhvQ.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "796f2a20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T06:27:06.147347Z",
     "iopub.status.busy": "2024-11-08T06:27:06.147030Z",
     "iopub.status.idle": "2024-11-08T06:27:06.533678Z",
     "shell.execute_reply": "2024-11-08T06:27:06.532722Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.421751,
     "end_time": "2024-11-08T06:27:06.536086",
     "exception": false,
     "start_time": "2024-11-08T06:27:06.114335",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DINO(keras.Model): #g : feature extractor + predictor\n",
    "    def __init__(self, feature_extractor, apply_simclr = False, apply_barlow = False,\n",
    "                 student_model = None,\n",
    "                 embed_dims = 1024, multiview = False, probe = False,\n",
    "                 teacher_t = 0.1, student_t = 0.1,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.feature_extractor = feature_extractor #output as feature vector and attention weight\n",
    "        if student_model is None:\n",
    "            self.student_extractor = tf.keras.models.clone_model(feature_extractor)\n",
    "        else:\n",
    "            self.student_extractor = student_model\n",
    "            \n",
    "        for f_t, f_p in zip(self.feature_extractor.weights, self.student_extractor.weights):\n",
    "            f_t.assign(f_p) \n",
    "            \n",
    "        self.embed_dims = embed_dims\n",
    "        self.multiview = multiview\n",
    "        self.teacher_t = teacher_t\n",
    "        self.student_t = student_t\n",
    "        self.c = tf.Variable(keras.random.normal(shape = (embed_dims,),\n",
    "                                      dtype = tf.float32)\n",
    "                            )\n",
    "        self.train_type = 'DINO'\n",
    "        self.embed_dims = embed_dims\n",
    "        self.teacher_predictor = keras.Sequential([keras.layers.Dense(units = self.embed_dims, use_bias = False),\n",
    "                                          keras.layers.Dense(units = self.embed_dims),\n",
    "                                          keras.layers.Dense(units = self.embed_dims),\n",
    "                                            keras.layers.Lambda(lambda x : ops.normalize(x))\n",
    "                                          ], name = f\"Teacher_{self.train_type}_predictor\")\n",
    "        self.student_predictor = keras.Sequential([keras.layers.Dense(units = self.embed_dims, use_bias = False),\n",
    "                                          keras.layers.Dense(units = self.embed_dims),\n",
    "                                          keras.layers.Dense(units = self.embed_dims),\n",
    "                                            keras.layers.Lambda(lambda x : ops.normalize(x))\n",
    "                                          ], name = f\"Student_{self.train_type}_predictor\")\n",
    "        for s_t, s_p in zip(self.teacher_predictor.weights, self.student_predictor.weights):\n",
    "            s_t.assign(s_p)\n",
    "        \n",
    "        self.probe = False\n",
    "        self.apply_simclr = apply_simclr\n",
    "        self.apply_barlow = apply_barlow\n",
    "        self.loss_tracker = keras.metrics.Mean(name = \"Total_loss\")\n",
    "        self.dino_tracker = keras.metrics.Mean(name = f\"{self.train_type}_loss\")\n",
    "        self.simclr_tracker = keras.metrics.Mean(name = \"SimCLR_loss\")\n",
    "        self.barlow_tracker = keras.metrics.Mean(name = \"Barlow_loss\")\n",
    "    def get_env_config(self):\n",
    "        return {\"feature_extractor_name\" : self.feature_extractor.name,\n",
    "               \"embed_dims\" : self.embed_dims,\n",
    "                \"Multiview(>2)\" : self.multiview,\n",
    "               \"SSL_method\" : \"DINOv1\",\n",
    "               \"Linear Probe\" : self.probe,\n",
    "               \"N_Categories\" : self.probe_categories if self.probe else 0,\n",
    "               \"Probe Activation\" : self.probe_act if self.probe else \"NA\",\n",
    "               \"Apply SimCLR\" : self.apply_simclr,\n",
    "               \"Apply Barlow\" : self.apply_barlow}\n",
    "    def compile(self, optimizer, probe_optimizer = None, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "        self.optimizer = optimizer\n",
    "        if self.probe:\n",
    "            self.probe_optimizer = probe_optimizer \n",
    "    def compute_h(self, t, s):\n",
    "        # t, s : embedding vector of teacher/student network (feature extractor + MLPs)\n",
    "        # C: centering coefficient, updated as EMA (teacher output의 평균)\n",
    "        c = self.c\n",
    "        t = tf.stop_gradient(t)\n",
    "        s = ops.softmax(s/self.student_t, \n",
    "                        axis = -1)\n",
    "        t = ops.softmax(((t - c)/self.teacher_t),\n",
    "                        axis = -1)\n",
    "        loss = -ops.mean(t*ops.log(s + 1e-5))\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    def train_step(self, dataset):\n",
    "        view_indices = list(range(len(dataset)))\n",
    "        if self.probe:\n",
    "            global_view, local_view, labels = dataset\n",
    "            n_global, n_local = 1, 1\n",
    "            total_view = ops.concatenate(dataset, axis = 0)\n",
    "        else:\n",
    "            if self.multiview:\n",
    "                global_view = dataset[0:2]\n",
    "                \n",
    "                n_global = len(global_view)\n",
    "                n_local = len(dataset) - n_global\n",
    "                \n",
    "                global_view = ops.concatenate(global_view, axis = 0)\n",
    "                total_view = ops.concatenate(dataset, axis = 0) #global_1, global_2, locals\n",
    "            else:\n",
    "                global_view, local_view = dataset #x1, x2\n",
    "                n_global, n_local = 1, 1\n",
    "                total_view = ops.concatenate(dataset, axis = 0)\n",
    "        # teacher -> global views, student -> global and local views\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            if self.multiview:\n",
    "                if len(self.feature_extractor.outputs) == 2:\n",
    "                    z_global_teacher, global_weight_teacher = self.feature_extractor(global_view, training = True)\n",
    "                    z_total_student, total_weight_student = self.student_extractor(total_view, training = True)\n",
    "\n",
    "                elif len(self.feature_extractor.outputs) == 1:\n",
    "                    z_global_teacher = self.feature_extractor(global_view, training = True)\n",
    "                    z_total_student = self.student_extractor(total_view, training = True)\n",
    "                    \n",
    "                v_teacher = self.teacher_predictor(z_global_teacher)\n",
    "                v_student = self.student_predictor(z_total_student)\n",
    "                v_teacher_total = v_teacher\n",
    "            else:\n",
    "                if len(self.feature_extractor.outputs) == 2:\n",
    "                    z_total_teacher, _ = self.feature_extractor(total_view, training = True)\n",
    "                    z_total_student, _ = self.student_extractor(total_view, training = True)\n",
    "\n",
    "                elif len(self.feature_extractor.outputs) == 1:\n",
    "                    z_total_teacher = self.feature_extractor(total_view, training = True)\n",
    "                    z_total_student = self.student_extractor(total_view, training = True)\n",
    "                \n",
    "                v_teacher_total = self.teacher_predictor(z_total_teacher) ; del z_total_teacher, _\n",
    "                v_student_total = self.student_predictor(z_total_student) ; del z_total_student\n",
    "                \n",
    "                v_teacher_1, v_teacher_2 = ops.split(v_teacher_total, 2, 0)\n",
    "                v_student_1, v_student_2 = ops.split(v_student_total, 2, 0) ; del v_student_total\n",
    "\n",
    "\n",
    "            if self.multiview:\n",
    "                dino_loss = []\n",
    "                simclr_loss = []\n",
    "                barlow_loss = []\n",
    "                #start from here    \n",
    "                teacher_set = ops.split(v_teacher, n_global, 0)\n",
    "                student_set = ops.split(v_student, (n_global + n_local), 0)\n",
    "                teacher_indices = list(range(n_global))\n",
    "                \n",
    "                for idx in teacher_indices: #0, 1\n",
    "                    t = teacher_set[idx]\n",
    "                    view_indices.remove(idx)\n",
    "                    for j in view_indices: #(0), 1, 2, 3\n",
    "                        s = student_set[j]\n",
    "                        loss_ = self.compute_h(t, s) ; loss_ = ops.clip(loss_, -10.0, 10**4)\n",
    "                        dino_loss.append(loss_)\n",
    "                        simclr_loss_ = keras_cv.losses.SimCLRLoss(temperature = self.teacher_t, \n",
    "                                                                  name = \"simclr_loss_compute\")(tf.stop_gradient(t),\n",
    "                                                                                                s)\n",
    "                        simclr_loss.append(simclr_loss_)\n",
    "                        b_loss_ = BarlowLoss()(tf.stop_gradient(t), \n",
    "                                               s)\n",
    "                        barlow_loss.append(b_loss_)\n",
    "                    view_indices.append(idx)\n",
    "                dino_loss = ops.mean(dino_loss)\n",
    "                simclr_loss = ops.mean(simclr_loss)\n",
    "                barlow_loss = ops.mean(barlow_loss)\n",
    "                loss = dino_loss + (self.apply_simclr * simclr_loss) + (self.apply_barlow * barlow_loss)\n",
    "            else:\n",
    "                dino_loss = 0.5*(self.compute_h(v_teacher_1, v_student_2) + self.compute_h(v_teacher_2, v_student_1))\n",
    "                simclr_loss = 0.5*(keras_cv.losses.SimCLRLoss(temperature = self.teacher_t, name = \"simclr_loss_compute\")(tf.stop_gradient(v_teacher_1), v_student_2) + keras_cv.losses.SimCLRLoss(temperature = self.teacher_t, name = \"simclr_loss_compute\")(tf.stop_gradient(v_teacher_2), v_student_1))\n",
    "                barlow_loss = 0.5*(BarlowLoss()(tf.stop_gradient(v_teacher_1), v_student_2) + BarlowLoss()(tf.stop_gradient(v_teacher_2), v_student_1))\n",
    "                loss = dino_loss + (self.apply_simclr * simclr_loss) + (self.apply_barlow * barlow_loss)\n",
    "        #Track hyperparams and metrics\n",
    "        mean_val = ops.mean(v_teacher_total, axis = 0)\n",
    "        self.c.assign(0.99*self.c + (1-0.99)*mean_val)\n",
    "        \n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.dino_tracker.update_state(dino_loss)\n",
    "        self.simclr_tracker.update_state(simclr_loss)\n",
    "        self.barlow_tracker.update_state(barlow_loss)\n",
    "        \n",
    "        output_dict = {self.loss_tracker.name : self.loss_tracker.result(),\n",
    "                       self.dino_tracker.name : self.dino_tracker.result(),\n",
    "                       self.simclr_tracker.name : self.simclr_tracker.result(),\n",
    "                       self.barlow_tracker.name : self.barlow_tracker.result(),\n",
    "\n",
    "                      }\n",
    "        \n",
    "        #Update student params as backprop\n",
    "        student_params = self.student_extractor.trainable_variables + self.student_predictor.trainable_variables\n",
    "        grads = tape.gradient(loss, student_params)\n",
    "        self.optimizer.apply_gradients(zip(grads, student_params))\n",
    "        #Update teacher params as EMA\n",
    "        #lambda = 0.999\n",
    "        teacher_feature_w, teacher_predictor_w = [],[]\n",
    "        l_ = 0.999\n",
    "        #print(\"teacher EMA\")\n",
    "        for f_teacher_part, f_student_part in zip(self.feature_extractor.weights, self.student_extractor.weights):\n",
    "            f_teacher_part.assign(l_*f_teacher_part + (1-l_)*f_student_part)\n",
    "        #self.feature_extractor.set_weights(teacher_feature_w)\n",
    "        \n",
    "        #print(\"predictor EMA\")\n",
    "        for p_teacher_part, p_student_part in zip(self.teacher_predictor.weights, self.student_predictor.weights):\n",
    "            p_teacher_part.assign(l_*p_teacher_part + (1-l_)*p_student_part)\n",
    "        #self.teacher_predictor.set_weights(teacher_predictor_w)\n",
    "        \n",
    "        return output_dict\n",
    "    \n",
    "    def test_step(self, dataset):\n",
    "        view_indices = list(range(len(dataset)))\n",
    "        if self.probe:\n",
    "            global_view, local_view, labels = dataset\n",
    "            n_global, n_local = 1, 1\n",
    "            total_view = ops.concatenate(dataset, axis = 0)\n",
    "        else:\n",
    "            if self.multiview:\n",
    "                global_view = dataset[0:2]\n",
    "                \n",
    "                n_global = len(global_view)\n",
    "                n_local = len(dataset) - n_global\n",
    "                \n",
    "                global_view = ops.concatenate(global_view, axis = 0)\n",
    "                total_view = ops.concatenate(dataset, axis = 0) #global_1, global_2, locals\n",
    "            else:\n",
    "                global_view, local_view = dataset\n",
    "                n_global, n_local = 1, 1\n",
    "                total_view = ops.concatenate(dataset, axis = 0)\n",
    "        # teacher -> global views, student -> global and local views\n",
    "        if self.multiview:\n",
    "            if len(self.feature_extractor.outputs) == 2:\n",
    "                z_global_teacher, global_weight_teacher = self.feature_extractor(global_view, training = True)\n",
    "                z_total_student, total_weight_student = self.student_extractor(total_view, training = True)\n",
    "\n",
    "            elif len(self.feature_extractor.outputs) == 1:\n",
    "                z_global_teacher = self.feature_extractor(global_view, training = True)\n",
    "                z_total_student = self.student_extractor(total_view, training = True)\n",
    "                    \n",
    "            v_teacher = self.teacher_predictor(z_global_teacher)\n",
    "            v_student = self.student_predictor(z_total_student)\n",
    "            \n",
    "        else:\n",
    "            if len(self.feature_extractor.outputs) == 2:\n",
    "                z_total_teacher, _ = self.feature_extractor(total_view, training = True)\n",
    "                z_total_student, _ = self.student_extractor(total_view, training = True)\n",
    "\n",
    "            elif len(self.feature_extractor.outputs) == 1:\n",
    "                z_total_teacher = self.feature_extractor(total_view, training = True)\n",
    "                z_total_student = self.student_extractor(total_view, training = True)\n",
    "                \n",
    "            v_teacher_total = self.teacher_predictor(z_total_teacher) ; del z_total_teacher, _\n",
    "            v_student_total = self.student_predictor(z_total_student) ; del z_total_student\n",
    "                \n",
    "            v_teacher_1, v_teacher_2 = ops.split(v_teacher_total, 2, 0) ; del v_teacher_total\n",
    "            v_student_1, v_student_2 = ops.split(v_student_total, 2, 0) ; del v_student_total\n",
    "        \n",
    "        if self.multiview:\n",
    "            #start from here \n",
    "            dino_loss = []\n",
    "            simclr_loss = []\n",
    "            barlow_loss = []\n",
    "            teacher_set = ops.split(v_teacher, n_global, 0)\n",
    "            student_set = ops.split(v_student, (n_global + n_local), 0)\n",
    "            teacher_indices = list(range(n_global))\n",
    "                \n",
    "            for idx in teacher_indices:\n",
    "                t = teacher_set[idx]\n",
    "                view_indices.remove(idx)\n",
    "                for j in view_indices:\n",
    "                    s = student_set[j]\n",
    "                    loss_ = self.compute_h(t, s) ; loss_ = ops.clip(loss_, -10.0, 10**4)\n",
    "                    dino_loss.append(loss_)\n",
    "                    simclr_loss_ = keras_cv.losses.SimCLRLoss(temperature = self.teacher_t, name = \"simclr_loss_compute\")(t,s)\n",
    "                    simclr_loss.append(simclr_loss_)\n",
    "                    b_loss_ = BarlowLoss()(t,s)\n",
    "                    barlow_loss.append(b_loss_)\n",
    "                view_indices.append(idx)\n",
    "            dino_loss = ops.mean(dino_loss)\n",
    "            simclr_loss = ops.mean(simclr_loss)\n",
    "            barlow_loss = ops.mean(barlow_loss)\n",
    "            loss = dino_loss + (self.apply_simclr * simclr_loss) + (self.apply_barlow * barlow_loss)\n",
    "        else:\n",
    "            dino_loss = 0.5*(self.compute_h(v_teacher_1, v_student_2) + self.compute_h(v_teacher_2, v_student_1))\n",
    "            simclr_loss = 0.5*(keras_cv.losses.SimCLRLoss(temperature = self.teacher_t, name = \"simclr_loss_compute\")(tf.stop_gradient(v_teacher_1), v_student_2) + keras_cv.losses.SimCLRLoss(temperature = self.teacher_t, name = \"simclr_loss_compute\")(tf.stop_gradient(v_teacher_2), v_student_1))\n",
    "            barlow_loss = 0.5*(BarlowLoss()(tf.stop_gradient(v_teacher_1), v_student_2) + BarlowLoss()(tf.stop_gradient(v_teacher_2), v_student_1))\n",
    "            loss = dino_loss + (self.apply_simclr * simclr_loss) + (self.apply_barlow * barlow_loss)\n",
    "        #Track hyperparams and metrics\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.dino_tracker.update_state(dino_loss)\n",
    "        self.simclr_tracker.update_state(simclr_loss)\n",
    "        self.barlow_tracker.update_state(barlow_loss)\n",
    "        \n",
    "        output_dict = {self.loss_tracker.name : self.loss_tracker.result(),\n",
    "                       self.dino_tracker.name : self.dino_tracker.result(),\n",
    "                       self.simclr_tracker.name : self.simclr_tracker.result(),\n",
    "                       self.barlow_tracker.name : self.barlow_tracker.result(),\n",
    "                      }\n",
    "        \n",
    "        return output_dict\n",
    "    \n",
    "    def call(self, dataset):\n",
    "        return self.test_step(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8ea1c349",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T06:27:06.601388Z",
     "iopub.status.busy": "2024-11-08T06:27:06.601064Z",
     "iopub.status.idle": "2024-11-08T06:27:06.630398Z",
     "shell.execute_reply": "2024-11-08T06:27:06.629493Z"
    },
    "papermill": {
     "duration": 0.064523,
     "end_time": "2024-11-08T06:27:06.632424",
     "exception": false,
     "start_time": "2024-11-08T06:27:06.567901",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DINO_MIM(keras.Model): #g : feature extractor + predictor\n",
    "    # dataset image : mask_sequence, mask_image, image, original_image = images\n",
    "    # images = [original_image, image, mask_image]\n",
    "    def __init__(self, feature_extractor, student_model,\n",
    "                 teacher_t = 0.04, student_t = 0.1,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.feature_extractor = feature_extractor #output as feature vector, patch and attention weight\n",
    "        self.student_extractor = student_model\n",
    "        self.final_dims_ = 4096\n",
    "        self.teacher_t = tf.Variable(teacher_t, trainable = False)\n",
    "        self.student_t = student_t\n",
    "        self.train_type = 'DINO'\n",
    "        \n",
    "        self.teacher_predictor = keras.Sequential([keras.layers.Dense(units = 1024),\n",
    "                                                   keras.layers.Dense(units = 1024),\n",
    "                                                   keras.layers.Dense(units = 1024),\n",
    "                                                   keras.layers.Dense(units = self.final_dims_),\n",
    "                                            keras.layers.Lambda(lambda x : ops.normalize(x))\n",
    "                                          ], name = f\"Teacher_{self.train_type}_predictor\")\n",
    "        self.student_predictor = keras.Sequential([keras.layers.Dense(units = 1024),\n",
    "                                                   keras.layers.Dense(units = 1024),\n",
    "                                                   keras.layers.Dense(units = 1024),\n",
    "                                                   keras.layers.Dense(units = self.final_dims_),\n",
    "                                            keras.layers.Lambda(lambda x : ops.normalize(x))\n",
    "                                          ], name = f\"Student_{self.train_type}_predictor\")\n",
    "        self.feature_extractor.trainable = False #update via EMA from student network\n",
    "        self.teacher_predictor.trainable = False #update via EMA from student network\n",
    "        \n",
    "        self.c = tf.Variable(ops.normalize(keras.random.normal(shape = (self.final_dims_,)), axis = -1),\n",
    "                             trainable = False,\n",
    "                            ) \n",
    "        \n",
    "        for f_t, f_p in zip(self.feature_extractor.weights, self.student_extractor.weights):\n",
    "            f_t.assign(f_p) \n",
    "        for s_t, s_p in zip(self.teacher_predictor.weights, self.student_predictor.weights):\n",
    "            s_t.assign(s_p)\n",
    "        self.vic_fn = VicReg()\n",
    "        \n",
    "    def get_env_config(self):\n",
    "        return {\"feature_extractor_name\" : self.feature_extractor.name,\n",
    "               \"embed_dims\" : self.embed_dims,\n",
    "               \"SSL_method\" : \"DINO_with_MIM_and_VIC\",\n",
    "               } \n",
    "    def compute_h(self, t, s):\n",
    "        # t, s : embedding vector of teacher/student network (feature extractor + MLPs)\n",
    "        # C: centering coefficient, updated as EMA (teacher output의 평균)\n",
    "        \n",
    "        t = tf.stop_gradient(t)\n",
    "        s = ops.softmax(s/self.student_t, \n",
    "                        axis = -1)\n",
    "        t = ops.softmax(((t - self.c)/self.teacher_t),\n",
    "                        axis = -1)\n",
    "        loss = -ops.mean(t*ops.log(s + 1e-10))\n",
    "        \n",
    "        return loss\n",
    "    def call(self, dataset):\n",
    "        mask_sequence, mask_image, image, original_image = dataset\n",
    "        dataset = [original_image, image, mask_image] #global view, local view, impaired view\n",
    "        # according to original paper: teacher network(only global view) , student network(all views)\n",
    "        teacher_f, teacher_patches, teacher_w = self.feature_extractor(ops.concatenate([original_image, \n",
    "                                                                                       mask_image], axis = 0)\n",
    "                                                                      ) ; del teacher_patches\n",
    "        student_f, student_patches, student_w = self.student_extractor(ops.concatenate(dataset, axis = 0)) #global, local, mask\n",
    "        teacher_z, student_z = self.teacher_predictor(teacher_f), self.student_predictor(student_f)\n",
    "        student_z_set = ops.split(student_z, 3, axis = 0)\n",
    "        student_patches_set = ops.split(student_patches, 3, axis = 0) ; del student_patches\n",
    "        teacher_z_set = ops.split(teacher_z, 2, axis = 0)\n",
    "        teacher_w_original = ops.split(teacher_w, 2, axis = 0)[0]\n",
    "        \n",
    "        teacher_z_global, teacher_z_mask = teacher_z_set[0], teacher_z_set[1]\n",
    "        student_z_global, student_z_local, student_z_mask = student_z_set[0], student_z_set[1], student_z_set[2]\n",
    "        del teacher_z_set, student_z_set\n",
    "        student_patches_original, student_patches_masked = student_patches_set[0], student_patches_set[-1]\n",
    "        del student_patches_set\n",
    "        \n",
    "        return [teacher_z_global, student_z_local, student_z_mask, # DINO loss\n",
    "               teacher_z_mask, student_z_global, student_z_mask, #DINO loss\n",
    "               student_patches_original, student_patches_masked, # MIM loss (in feature space)\n",
    "               teacher_w_original]\n",
    "    def train_step(self, dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            teacher_z_global, student_z_local, student_z_mask, teacher_z_mask, student_z_global, student_z_mask, student_patches_original, student_patches_masked, teacher_w_original = self(dataset)\n",
    "            dino_global_local = self.compute_h(teacher_z_global, student_z_local)\n",
    "            dino_global_mask = self.compute_h(teacher_z_global, student_z_mask)\n",
    "            dino_mask_local = self.compute_h(teacher_z_mask, student_z_local)\n",
    "            dino_mask_global = self.compute_h(teacher_z_mask, student_z_global)\n",
    "            dino_global_global = self.compute_h(teacher_z_global, student_z_global)\n",
    "            dino_mask_mask = self.compute_h(teacher_z_mask, student_z_mask)\n",
    "            \n",
    "            dino_loss = ops.mean([dino_global_local,\n",
    "                                  dino_global_mask,\n",
    "                                  dino_mask_local,\n",
    "                                  dino_mask_global,\n",
    "                                  dino_global_global,\n",
    "                                  dino_mask_mask\n",
    "                                 ])\n",
    "\n",
    "            mim_loss = ops.mean(ops.square(student_patches_original - student_patches_masked))\n",
    "            vic_loss = ops.mean([self.vic_fn(student_z_global, student_z_local), \n",
    "                                  self.vic_fn(student_z_global, student_z_mask),\n",
    "                                 self.vic_fn(student_z_global, student_z_global),\n",
    "                                 #self.vic_fn(teacher_z_mask, student_z_local), \n",
    "                                 # self.vic_fn(teacher_z_mask, student_z_mask),\n",
    "                                 #self.vic_fn(teacher_z_mask, student_z_global)\n",
    "                                ]\n",
    "                               )\n",
    "            loss = (dino_loss) + (mim_loss) + (0.01 * vic_loss)\n",
    "            \n",
    "        #Track hyperparams and metrics\n",
    "        l_ = 0.996\n",
    "        \n",
    "        mean_val = ops.mean(ops.concatenate([teacher_z_global, teacher_z_mask], axis = 0), axis = 0)\n",
    "        self.c.assign(l_*self.c + (1-l_)*mean_val)\n",
    "        # teacher temperature decay\n",
    "        self.teacher_t.assign(ops.minimum(0.06, self.teacher_t * (1+1-l_))\n",
    "                             )\n",
    "        \n",
    "        #Update student params as backprop\n",
    "        student_params = self.student_extractor.trainable_variables + self.student_predictor.trainable_variables \n",
    "        grads = tape.gradient(loss, student_params)\n",
    "        self.optimizer.apply_gradients(zip(grads, student_params))\n",
    "        \n",
    "        #Update teacher params as EMA\n",
    "        for f_teacher_part, f_student_part in zip(self.feature_extractor.weights, self.student_extractor.weights):\n",
    "            f_teacher_part.assign(l_*f_teacher_part + (1-l_)*f_student_part)\n",
    "        for p_teacher_part, p_student_part in zip(self.teacher_predictor.weights, self.student_predictor.weights):\n",
    "            p_teacher_part.assign(l_*p_teacher_part + (1-l_)*p_student_part)\n",
    "        dims = teacher_w_original.shape[-1]\n",
    "        w_transpose = ops.reshape(teacher_w_original, [-1, dims])\n",
    "        entropy = ops.mean(-ops.sum(w_transpose * ops.log(w_transpose + 1e-5), axis=-1))\n",
    "        \n",
    "        return {\"Total_loss\" : loss, \"MIM_loss\" : mim_loss,\n",
    "                \"Attention_weight_entropy\" : entropy,\n",
    "               \"DINO_loss\" : dino_loss, \"VIC_loss\" : vic_loss,\n",
    "                \"Teacher Temperature\" : self.teacher_t,\n",
    "                \"DINO_G_G\" : dino_global_global,\n",
    "                'DINO_G_M' : dino_global_mask,\n",
    "                \"DINO_G_L\" : dino_global_local,\n",
    "                \"DINO_M_G\" : dino_mask_global,\n",
    "                \"DINO_M_L\" : dino_mask_local,\n",
    "                \"DINO_M_M\" : dino_mask_mask\n",
    "               }\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa4d433",
   "metadata": {
    "papermill": {
     "duration": 0.031485,
     "end_time": "2024-11-08T06:27:06.695147",
     "exception": false,
     "start_time": "2024-11-08T06:27:06.663662",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "> iBOT 및 DINOv2\n",
    "\n",
    "- iBOT architecture: MIM, online tokenizing!\n",
    "![](https://velog.velcdn.com/images%2Frucola-pizza%2Fpost%2F4dc0785f-7072-4d03-8374-5bbccd8391b4%2F%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202022-03-10%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%202.01.16.png)\n",
    "\n",
    "- DINOv2:\n",
    "    - iBOT + Sinkhorn-knopp in teacher softmax + untying projection heads (student, teacher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2455f39c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T06:27:06.759419Z",
     "iopub.status.busy": "2024-11-08T06:27:06.759097Z",
     "iopub.status.idle": "2024-11-08T06:27:06.776086Z",
     "shell.execute_reply": "2024-11-08T06:27:06.775148Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.051536,
     "end_time": "2024-11-08T06:27:06.778057",
     "exception": false,
     "start_time": "2024-11-08T06:27:06.726521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sinkhorn(feature_vector_logits):\n",
    "    \"\"\"\n",
    "    Applies the Sinkhorn-Knopp algorithm to normalize feature vector logits.\n",
    "    \n",
    "    Args:\n",
    "    - feature_vector_logits (tf.Tensor): The input logits to be normalized.\n",
    "    \n",
    "    Returns:\n",
    "    - tf.Tensor: The doubly stochastic matrix.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Exponentiate the logits\n",
    "    Q = tf.transpose(tf.exp(feature_vector_logits))\n",
    "    \n",
    "    # Normalize the entire matrix\n",
    "    Q /= tf.reduce_sum(Q)\n",
    "    \n",
    "    # Get dimensions K (number of rows) and B (number of columns)\n",
    "    K = tf.shape(Q)[0]\n",
    "    B = tf.shape(Q)[1]\n",
    "    \n",
    "    # Initialize u, r, and c\n",
    "    u = tf.zeros(K, dtype=tf.float32)\n",
    "    r = tf.ones(K, dtype=tf.float32) / (tf.cast(K, tf.float32) + 1e-7)\n",
    "    c = tf.ones(B, dtype=tf.float32) / (tf.cast(B, tf.float32) + 1e-7)\n",
    "    \n",
    "    # Sinkhorn iterations\n",
    "    for _ in range(3):\n",
    "        u = tf.reduce_sum(Q, axis=1)\n",
    "        Q *= tf.expand_dims((r / u), axis=1)\n",
    "        Q *= tf.expand_dims(c / tf.reduce_sum(Q, axis=0), 0)\n",
    "    \n",
    "    # Final normalization\n",
    "    final_quantity = Q / (tf.reduce_sum(Q, axis=0, keepdims=True) + 1e-7)\n",
    "    final_quantity = tf.transpose(final_quantity)\n",
    "    \n",
    "    return final_quantity\n",
    "\n",
    "def compute_h(s, t,\n",
    "              c = 0.0, t_t = 0.05, t_s = 0.05, teacher_softmax = True):\n",
    "    t = tf.stop_gradient(t)\n",
    "    s = ops.softmax(s/t_s , axis = -1)\n",
    "    if teacher_softmax:\n",
    "        t = ops.softmax((  (t - c)/t_t  ),\n",
    "                            axis = -1)\n",
    "    else:\n",
    "        t = sinkhorn((t - c)/t_t)\n",
    "    return -ops.sum(t*ops.log(s + 1e-6) , axis = -1)\n",
    "\n",
    "def compute_cls_loss(cls_a, cls_b, c, t_t, t_s, teacher_softmax):\n",
    "    cls_loss = compute_h(cls_a, cls_b, teacher_softmax = teacher_softmax,\n",
    "                            c = c, t_t = t_t, t_s = t_s)\n",
    "    return cls_loss\n",
    "def compute_mim_loss(m_u, patch_u_s, patch_u_t, c, t_t, t_s, teacher_softmax):\n",
    "    mim_loss_u = m_u * compute_h(patch_u_s, patch_u_t, teacher_softmax=teacher_softmax,\n",
    "                                 c=c, t_t=t_t, t_s=t_s)\n",
    "    # 마스킹된 위치의 손실을 유효한 위치에서만 계산\n",
    "    valid_positions = tf.reduce_sum(m_u, axis=1)\n",
    "    valid_mask = tf.cast(valid_positions > 0, tf.float32)\n",
    "    mim_loss_u = tf.reduce_sum(mim_loss_u, axis=1) / (valid_positions + 1e-6)\n",
    "    return tf.reduce_mean(mim_loss_u * valid_mask)\n",
    "\n",
    "\n",
    "def compute_iBOT_loss(cls_v_student, cls_v_teacher,\n",
    "                      cls_u_student, cls_u_teacher,\n",
    "                      c_cls, t_t_cls, t_s_cls,\n",
    "                      \n",
    "                      m_u, patch_u_s, patch_u_t,\n",
    "                      m_v, patch_v_s, patch_v_t,\n",
    "                      c_mim, t_t_mim, t_s_mim,\n",
    "                      teacher_softmax):\n",
    "    \"\"\"Compute one-pair cls, mim loss\"\"\"\n",
    "    cls_v_loss = compute_cls_loss(cls_a = cls_v_student, cls_b = cls_v_teacher, \n",
    "                                  c = c_cls, t_t = t_t_cls, t_s = t_s_cls, teacher_softmax = teacher_softmax)\n",
    "    cls_u_loss = compute_cls_loss(cls_a = cls_u_student, cls_b = cls_u_teacher, \n",
    "                                  c = c_cls, t_t = t_t_cls, t_s = t_s_cls, teacher_softmax = teacher_softmax)\n",
    "    cls_loss = ops.mean(cls_v_loss + cls_u_loss)\n",
    "    \n",
    "    mim_u_loss = compute_mim_loss(m_u, patch_u_s, patch_u_t, c_mim, t_t_mim, t_s_mim, teacher_softmax)\n",
    "    mim_v_loss = compute_mim_loss(m_v, patch_v_s, patch_v_t, c_mim, t_t_mim, t_s_mim, teacher_softmax)\n",
    "        \n",
    "    mim_loss = ops.mean(mim_u_loss + mim_v_loss)\n",
    "    return cls_loss, mim_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dee45e",
   "metadata": {
    "papermill": {
     "duration": 0.031164,
     "end_time": "2024-11-08T06:27:06.840772",
     "exception": false,
     "start_time": "2024-11-08T06:27:06.809608",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "> iBOT loss의 경우,\n",
    "\n",
    "- 원본 논문에서 아래와 같은 비교 실험을 진행하였고,\n",
    "- 그 결과 case b가 가장 성능이 높았음\n",
    "    - $x$ : global view\n",
    "    - $y$ : local view\n",
    "    - mask generation : generate_mask function 이용, m_i , masked patches return\n",
    "    - image patching & embedding : ImagePatchEmbedding layer 이용 (att: patch_size, embed_dim)\n",
    "    \n",
    "![](https://ar5iv.labs.arxiv.org/html/2111.07832/assets/x9.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "90b6c729",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T06:27:06.905553Z",
     "iopub.status.busy": "2024-11-08T06:27:06.905181Z",
     "iopub.status.idle": "2024-11-08T06:27:06.968418Z",
     "shell.execute_reply": "2024-11-08T06:27:06.967642Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.098637,
     "end_time": "2024-11-08T06:27:06.970579",
     "exception": false,
     "start_time": "2024-11-08T06:27:06.871942",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class iBOT(keras.Model): #g : feature extractor + predictor\n",
    "    def __init__(self, att_depth, att_dims, att_heads, \n",
    "                 patch_size = 16, embed_dims = 1024, multiview = False, probe = False,\n",
    "                 teacher_t = 0.07, student_t = 0.07, apply_simclr = False, grayscale = True,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.grayscale = grayscale\n",
    "        self.att_depth = att_depth\n",
    "        self.att_dims = att_dims\n",
    "        self.att_heads = att_heads\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dims = embed_dims\n",
    "        self.multiview = multiview\n",
    "        self.train_type = 'iBOT'\n",
    "        self.embed_dims = embed_dims\n",
    "        self.probe = False\n",
    "        \n",
    "        # modelling\n",
    "        # image input -> patch embedding -> patch for teacher, masked patch for student -> get rep vector, get embed. patches\n",
    "        \n",
    "        self.patch_embedding_fn = ImagePatchEmbedding(patch_size = self.patch_size, embed_dim = self.att_dims)\n",
    "        self.f_t = TRBlock(att_depth = self.att_depth, att_dims = self.att_dims, att_heads = self.att_heads, name = \"Teacher_Encoder\")\n",
    "        self.f_s = TRBlock(att_depth = self.att_depth, att_dims = self.att_dims, att_heads = self.att_heads, name = \"Student_Encoder\")\n",
    "        self.f_t.set_weights(self.f_s.get_weights())\n",
    "        self.teacher_predictor = keras.Sequential([keras.layers.Dense(units = self.embed_dims,\n",
    "                                                            use_bias = False,\n",
    "                                                            ),\n",
    "                                          keras.layers.Dense(units = self.embed_dims, use_bias = False),\n",
    "                                          keras.layers.Dense(units = self.embed_dims, use_bias = False),\n",
    "                                            ], name = f\"Teacher_{self.train_type}_predictor\")\n",
    "        self.student_predictor = keras.Sequential([keras.layers.Dense(units = self.embed_dims, use_bias = False,),\n",
    "                                          keras.layers.Dense(units = self.embed_dims, use_bias = False),\n",
    "                                          keras.layers.Dense(units = self.embed_dims, use_bias = False),\n",
    "                                          ], name = f\"Student_{self.train_type}_predictor\")\n",
    "        \n",
    "        self.teacher_patch_predictor = keras.Sequential([keras.layers.Dense(units = self.embed_dims,\n",
    "                                                            use_bias = False,\n",
    "                                                            ),\n",
    "                                          keras.layers.Dense(units = self.embed_dims, use_bias = False),\n",
    "                                          keras.layers.Dense(units = self.embed_dims, use_bias = False),\n",
    "                                            ], name = f\"Teacher_Patch_{self.train_type}_predictor\")\n",
    "        self.student_patch_predictor = keras.Sequential([keras.layers.Dense(units = self.embed_dims,\n",
    "                                                            use_bias = False,\n",
    "                                                            ),\n",
    "                                          keras.layers.Dense(units = self.embed_dims, use_bias = False),\n",
    "                                          keras.layers.Dense(units = self.embed_dims, use_bias = False),\n",
    "                                          ], name = f\"Student_Patch_{self.train_type}_predictor\")\n",
    "        \n",
    "        for s_t, s_p in zip(self.teacher_predictor.weights, self.student_predictor.weights):\n",
    "            s_t.assign(s_p)\n",
    "        for s_t, s_p in zip(self.teacher_patch_predictor.weights, self.student_patch_predictor.weights):\n",
    "            s_t.assign(s_p)\n",
    "        # About loss calculation\n",
    "        self.loss_tracker = keras.metrics.Mean(name = \"iBOT_loss\")\n",
    "        self.cls_tracker = keras.metrics.Mean(name = \"CLS_loss\") ; self.mim_tracker = keras.metrics.Mean(name = \"MIM_loss\")\n",
    "        self.apply_simclr = apply_simclr\n",
    "        self.simclr_tracker = keras.metrics.Mean(name = \"SimCLR_loss\")\n",
    "        \n",
    "        self.c_cls = self.add_weight(name='c_cls', shape=(self.embed_dims,), initializer='glorot_uniform', trainable=False)\n",
    "        self.c_mim = self.add_weight(name='c_mim', shape=(self.embed_dims,), initializer='glorot_uniform', trainable=False)\n",
    "        self.mask_token = self.add_weight(name='mask_token', shape=(self.att_dims,), initializer='glorot_uniform', trainable=True)\n",
    "        \n",
    "        \n",
    "        self.teacher_t = teacher_t\n",
    "        self.student_t = student_t\n",
    "    def get_env_config(self):\n",
    "        return {\"feature_extractor_name\" : f\"ViT_vanilla_depth{self.att_depth}_heads{self.att_heads}_dims{self.att_dims}\",\n",
    "               \"embed_dims\" : self.embed_dims,\n",
    "                \"Multiview(>2)\" : self.multiview,\n",
    "               \"SSL_method\" : \"iBOT\",\n",
    "               \"Linear Probe\" : self.probe,\n",
    "               \"N_Categories\" : self.probe_categories if self.probe else 0,\n",
    "               \"Probe Activation\" : self.probe_act if self.probe else \"NA\",\n",
    "               \"Apply SimCLR\" : bool(self.apply_simclr),\n",
    "               }\n",
    "    def compile(self, optimizer, probe_optimizer = None, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "        self.optimizer = optimizer\n",
    "        if self.probe:\n",
    "            self.probe_optimizer = probe_optimizer \n",
    "\n",
    "        \n",
    "    def train_step(self, dataset):\n",
    "        view_indices = list(range(len(dataset)))\n",
    "        batch_size = ops.shape(dataset[0])[0]\n",
    "        \n",
    "        if self.probe:\n",
    "            global_view, local_view, labels = dataset\n",
    "            n_global, n_local = 1, 1\n",
    "            total_view = ops.concatenate(dataset, axis = 0)\n",
    "        else:\n",
    "            if self.multiview:\n",
    "                n_global = 2\n",
    "                n_local = len(dataset) - n_global\n",
    "                \n",
    "                #global_view = ops.concatenate(global_view, axis = 0)\n",
    "                total_view = ops.concatenate(dataset, axis = 0) #global_1, global_2, locals\n",
    "            else:\n",
    "                global_view, local_view = dataset\n",
    "                n_global, n_local = 1, 1\n",
    "                total_view = ops.concatenate(dataset, axis = 0)\n",
    "        # teacher -> global views, student -> global and local views\n",
    "        teacher_indices = list(range(n_global))\n",
    "        with tf.GradientTape() as tape:\n",
    "            total_view = self.patch_embedding_fn(total_view)\n",
    "            global_view = total_view[:n_global*batch_size, ...] #2B, patch_length, dims\n",
    "            local_view = total_view[n_global*batch_size:, ...] #N, patch_length, dims\n",
    "            m_set, global_view_masked = MaskLayer(masking_rate= 0.6, \n",
    "                                                  update_value = self.mask_token)(global_view)\n",
    "            \n",
    "            global_z_t, global_patches_t, att_weights_t = self.f_t([global_view, global_view])\n",
    "            global_z_masked_s, global_patches_masked_s, _ = self.f_s([global_view_masked, global_view_masked])\n",
    "            local_z_s, local_patches_s, _ = self.f_s([local_view, local_view])\n",
    "            del _, local_patches_s\n",
    "            \n",
    "            global_z_t, global_patches_t = self.teacher_predictor(global_z_t), self.teacher_patch_predictor(global_patches_t)\n",
    "            global_z_masked_s, global_patches_masked_s = self.student_predictor(global_z_masked_s), self.student_patch_predictor(global_patches_masked_s)\n",
    "            local_z_s = self.student_predictor(local_z_s)\n",
    "            \n",
    "            teacher_cls_set = ops.split(global_z_t, n_global, 0) ; del global_z_t\n",
    "            teacher_patches_set = ops.split(global_patches_t, n_global, 0) ; del global_patches_t\n",
    "                \n",
    "            student_global_cls_set = ops.split(global_z_masked_s, n_global, 0) ; del global_z_masked_s\n",
    "            student_global_patches_set = ops.split(global_patches_masked_s, n_global, 0); del global_patches_masked_s\n",
    "            m_set = ops.split(m_set, n_global, 0)\n",
    "            student_local_cls_set = ops.split(local_z_s, n_local, 0) ; del local_z_s\n",
    "            \n",
    "            student_cls_set = student_global_cls_set\n",
    "            student_cls_set.extend(student_local_cls_set)\n",
    "            \n",
    "            ##PAIRWISE LOSS CALCUATION##\n",
    "            total_loss = []\n",
    "            mim_loss = []\n",
    "            cls_loss = []\n",
    "            simclr_loss = []\n",
    "                \n",
    "                #teacher-student combination\n",
    "                #global-global : mim, cls loss\n",
    "                #global-local : cls loss\n",
    "                # cls loss = DINO and SimCLR loss\n",
    "                \n",
    "                #1. patch-patch mim loss\n",
    "            for m_, patch_u_s, patch_u_t in zip(m_set, student_global_patches_set, teacher_patches_set):\n",
    "                mim_loss.append(compute_mim_loss(m_, patch_u_s, patch_u_t, self.c_mim, self.teacher_t, self.student_t, teacher_softmax = True))\n",
    "            mim_loss = ops.mean(mim_loss)\n",
    "                #2. cls-cls DINO, simclr loss\n",
    "            for idx in teacher_indices: #0, 1\n",
    "                t = teacher_cls_set[idx]\n",
    "                view_indices.remove(idx)\n",
    "                for j in view_indices: #(0), 1, 2, 3\n",
    "                    s = student_cls_set[j]\n",
    "                    loss_ = compute_cls_loss(s, t, teacher_softmax = True, c = self.c_cls, t_t = self.teacher_t, t_s = self.student_t)\n",
    "                    loss_ = ops.clip(loss_, -10.0, 10**4)\n",
    "                    cls_loss.append(loss_)\n",
    "                    simclr_loss_ = keras_cv.losses.SimCLRLoss(temperature = self.teacher_t, \n",
    "                                                                  name = \"simclr_loss_compute\")(t,s)\n",
    "                    simclr_loss.append(simclr_loss_)\n",
    "                        \n",
    "                view_indices.append(idx)\n",
    "            cls_loss = ops.mean(cls_loss)\n",
    "            simclr_loss = ops.mean(simclr_loss)\n",
    "            loss = cls_loss + mim_loss + (self.apply_simclr * simclr_loss)\n",
    "            \n",
    "        #Track hyperparams and metrics\n",
    "        mean_cls = ops.mean(ops.concatenate(teacher_cls_set, axis = 0), \n",
    "                            axis = 0)\n",
    "        mean_patch = ops.mean(ops.concatenate(teacher_patches_set, axis = 0), \n",
    "                              axis = (0,1))\n",
    "        self.c_cls.assign(0.99*self.c_cls + (1-0.99)*mean_cls)\n",
    "        self.c_mim.assign(0.99*self.c_mim + (1-0.99)*mean_patch)\n",
    "        \n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.cls_tracker.update_state(cls_loss)\n",
    "        self.mim_tracker.update_state(mim_loss)\n",
    "        self.simclr_tracker.update_state(simclr_loss)\n",
    "        \n",
    "        output_dict = {self.loss_tracker.name : self.loss_tracker.result(),\n",
    "                       self.cls_tracker.name : self.cls_tracker.result(),\n",
    "                       self.simclr_tracker.name : self.simclr_tracker.result(),\n",
    "                       self.mim_tracker.name : self.mim_tracker.result(),\n",
    "                      }\n",
    "        \n",
    "        #Update student params as backprop\n",
    "        student_params = self.patch_embedding_fn.trainable_variables + self.f_s.trainable_variables + self.student_predictor.trainable_variables + self.student_patch_predictor.trainable_variables \n",
    "        student_params.append(self.mask_token)\n",
    "        \n",
    "        \n",
    "        grads = tape.gradient(loss, student_params)\n",
    "        self.optimizer.apply_gradients(zip(grads, student_params))\n",
    "        #Update teacher params as EMA\n",
    "        #lambda = 0.999\n",
    "        teacher_feature_w, teacher_predictor_w = [],[]\n",
    "        l_ = 0.999\n",
    "        #print(\"teacher EMA\")\n",
    "        for f_teacher_part, f_student_part in zip(self.f_t.weights, self.f_s.weights):\n",
    "            f_teacher_part.assign(l_*f_teacher_part + (1-l_)*f_student_part)\n",
    "        #self.feature_extractor.set_weights(teacher_feature_w)\n",
    "        \n",
    "        #print(\"predictor EMA\")\n",
    "        for p_teacher_part, p_student_part in zip(self.teacher_predictor.weights, self.student_predictor.weights):\n",
    "            p_teacher_part.assign(l_*p_teacher_part + (1-l_)*p_student_part)\n",
    "        #self.teacher_predictor.set_weights(teacher_predictor_w)\n",
    "        for p_teacher_part, p_student_part in zip(self.teacher_patch_predictor.weights, self.student_patch_predictor.weights):\n",
    "            p_teacher_part.assign(l_*p_teacher_part + (1-l_)*p_student_part)\n",
    "        return output_dict\n",
    "    \n",
    "    def test_step(self, dataset):\n",
    "        view_indices = list(range(len(dataset)))\n",
    "        batch_size = ops.shape(dataset[0])[0]\n",
    "        \n",
    "        if self.probe:\n",
    "            global_view, local_view, labels = dataset\n",
    "            n_global, n_local = 1, 1\n",
    "            total_view = ops.concatenate(dataset, axis = 0)\n",
    "        else:\n",
    "            if self.multiview:\n",
    "                n_global = 2\n",
    "                n_local = len(dataset) - n_global\n",
    "                \n",
    "                #global_view = ops.concatenate(global_view, axis = 0)\n",
    "                total_view = ops.concatenate(dataset, axis = 0) #global_1, global_2, locals\n",
    "            else:\n",
    "                global_view, local_view = dataset\n",
    "                n_global, n_local = 1, 1\n",
    "                total_view = ops.concatenate(dataset, axis = 0)\n",
    "        # teacher -> global views, student -> global and local views\n",
    "        teacher_indices = list(range(n_global))\n",
    "        if True:\n",
    "            total_view = self.patch_embedding_fn(total_view)\n",
    "            print(\"total view shape : \",ops.shape(total_view))\n",
    "            global_view = total_view[:n_global*batch_size, ...] #2B, patch_length, dims\n",
    "            local_view = total_view[n_global*batch_size:, ...] #N, patch_length, dims\n",
    "            m_set, global_view_masked = MaskLayer(masking_rate = 0.6, \n",
    "                                                  update_value = self.mask_token)(global_view)\n",
    "            #m_set, global_view_masked = generate_mask(global_view, \n",
    "            #                                          tf.random.uniform(shape = (), minval = 0.4, maxval = 0.8), #<- mask probability\n",
    "            #                                          self.mask_token,\n",
    "            #                                         self.batch_size)\n",
    "            \n",
    "            global_z_t, global_patches_t, att_weights_t = self.f_t([global_view, global_view])\n",
    "            global_z_masked_s, global_patches_masked_s, _ = self.f_s([global_view_masked, global_view_masked])\n",
    "            local_z_s, local_patches_s, _ = self.f_s([local_view, local_view])\n",
    "            del _, local_patches_s\n",
    "            \n",
    "            global_z_t, global_patches_t = self.teacher_predictor(global_z_t), self.teacher_patch_predictor(global_patches_t)\n",
    "            global_z_masked_s, global_patches_masked_s = self.student_predictor(global_z_masked_s), self.student_patch_predictor(global_patches_masked_s)\n",
    "            local_z_s = self.student_predictor(local_z_s)\n",
    "            \n",
    "            teacher_cls_set = ops.split(global_z_t, n_global, 0) ; del global_z_t\n",
    "            teacher_patches_set = ops.split(global_patches_t, n_global, 0) ; del global_patches_t\n",
    "                \n",
    "            student_global_cls_set = ops.split(global_z_masked_s, n_global, 0) ; del global_z_masked_s\n",
    "            student_global_patches_set = ops.split(global_patches_masked_s, n_global, 0); del global_patches_masked_s\n",
    "            m_set = ops.split(m_set, n_global, 0)\n",
    "            student_local_cls_set = ops.split(local_z_s, n_local, 0) ; del local_z_s\n",
    "            \n",
    "            student_cls_set = student_global_cls_set\n",
    "            student_cls_set.extend(student_local_cls_set)\n",
    "            \n",
    "            ##PAIRWISE LOSS CALCUATION##\n",
    "            total_loss = []\n",
    "            mim_loss = []\n",
    "            cls_loss = []\n",
    "            simclr_loss = []\n",
    "                \n",
    "                #teacher-student combination\n",
    "                #global-global : mim, cls loss\n",
    "                #global-local : cls loss\n",
    "                # cls loss = DINO and SimCLR loss\n",
    "                \n",
    "                #1. patch-patch mim loss\n",
    "            for m_, patch_u_s, patch_u_t in zip(m_set, student_global_patches_set, teacher_patches_set):\n",
    "                print(ops.shape(m_), ops.shape(patch_u_s), ops.shape(self.c_mim))\n",
    "                mim_loss.append(compute_mim_loss(m_, patch_u_s, patch_u_t, self.c_mim, self.teacher_t, self.student_t, teacher_softmax = True))\n",
    "            mim_loss = ops.mean(mim_loss)\n",
    "                #2. cls-cls DINO, simclr loss\n",
    "            for idx in teacher_indices: #0, 1\n",
    "                t = teacher_cls_set[idx]\n",
    "                view_indices.remove(idx)\n",
    "                for j in view_indices: #(0), 1, 2, 3\n",
    "                    s = student_cls_set[j]\n",
    "                    loss_ = compute_cls_loss(s, t, teacher_softmax = True, c = self.c_cls, t_t = self.teacher_t, t_s = self.student_t)\n",
    "                    loss_ = ops.clip(loss_, -10.0, 10**4)\n",
    "                    cls_loss.append(loss_)\n",
    "                    simclr_loss_ = keras_cv.losses.SimCLRLoss(temperature = self.teacher_t, \n",
    "                                                                  name = \"simclr_loss_compute\")(t,s)\n",
    "                    simclr_loss.append(simclr_loss_)\n",
    "                        \n",
    "                view_indices.append(idx)\n",
    "            cls_loss = ops.mean(cls_loss)\n",
    "            simclr_loss = ops.mean(simclr_loss)\n",
    "            loss = cls_loss + mim_loss + (self.apply_simclr * 0.5 * simclr_loss)\n",
    "            \n",
    "        #Track hyperparams and metrics\n",
    "        mean_cls = ops.mean(ops.concatenate(teacher_cls_set, axis = 0), \n",
    "                            axis = 0)\n",
    "        mean_patch = ops.mean(ops.concatenate(teacher_patches_set, axis = 0), \n",
    "                              axis = (0,1))\n",
    "        self.c_cls.assign(0.99*self.c_cls + (1-0.99)*mean_cls)\n",
    "        self.c_mim.assign(0.99*self.c_mim + (1-0.99)*mean_patch)\n",
    "        \n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.cls_tracker.update_state(cls_loss)\n",
    "        self.mim_tracker.update_state(mim_loss)\n",
    "        self.simclr_tracker.update_state(simclr_loss)\n",
    "        \n",
    "        output_dict = {self.loss_tracker.name : self.loss_tracker.result(),\n",
    "                       self.cls_tracker.name : self.cls_tracker.result(),\n",
    "                       self.simclr_tracker.name : self.simclr_tracker.result(),\n",
    "                       self.mim_tracker.name : self.mim_tracker.result(),\n",
    "                      }\n",
    "        \n",
    "        return output_dict\n",
    "    \n",
    "    def call(self, dataset):\n",
    "        return self.test_step(dataset)\n",
    "    def get_full_model(self, res):\n",
    "        if self.grayscale:\n",
    "            inputs =keras.layers.Input([res,res,1], name = \"ImageInput\") \n",
    "        else:\n",
    "            inputs = keras.layers.Input([res,res,3], name = \"ImageInput\")\n",
    "        patches = self.patch_embedding_fn(inputs)\n",
    "        cls_token, patches, att_weight = self.f_t([patches, patches])\n",
    "        model = Model(inputs, [cls_token, patches, att_weight],\n",
    "                     name = f\"{self.train_type}_ViT\")\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837ffd26",
   "metadata": {
    "papermill": {
     "duration": 0.030957,
     "end_time": "2024-11-08T06:27:07.033229",
     "exception": false,
     "start_time": "2024-11-08T06:27:07.002272",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "------------\n",
    "# MIM, Mixed SSL\n",
    "- MIM:\n",
    "    - SimMIM\n",
    "- inform. based + MIM based + contrastive based + self-distillation?\n",
    "- Unsupervised semantic segmentation + feature-vector SSL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6469d79",
   "metadata": {
    "papermill": {
     "duration": 0.030955,
     "end_time": "2024-11-08T06:27:07.095205",
     "exception": false,
     "start_time": "2024-11-08T06:27:07.064250",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "> SimMIM\n",
    "\n",
    "![](https://kimjy99.github.io/assets/img/simmim/simmim-fig1.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "396c7ee1",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-11-08T06:27:07.159678Z",
     "iopub.status.busy": "2024-11-08T06:27:07.159329Z",
     "iopub.status.idle": "2024-11-08T06:27:07.174954Z",
     "shell.execute_reply": "2024-11-08T06:27:07.174103Z"
    },
    "papermill": {
     "duration": 0.050412,
     "end_time": "2024-11-08T06:27:07.176976",
     "exception": false,
     "start_time": "2024-11-08T06:27:07.126564",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_masking_fn(grayscale = True, masking_rate = 0.5, update_value = 0.0, patch_size = 32):\n",
    "    \n",
    "    @tf.function\n",
    "    def mask_and_restore(image, \n",
    "                         masking_rate=masking_rate, update_value = update_value, patch_size = patch_size):\n",
    "        if len(ops.shape(image)) == 3:\n",
    "            single = True\n",
    "            image = image[tf.newaxis, ...]\n",
    "        else:\n",
    "            single = False\n",
    "        image = ops.cast(image, \"float32\") ; original_image = image\n",
    "        c = ops.shape(image)[-1] ; res = ops.shape(image)[-2]\n",
    "        \n",
    "        rate = 0.7\n",
    "        target_size = int(rate * tf.cast(res, \"float32\"))\n",
    "        bs = ops.shape(image)[0]\n",
    "        if np.random.randint(10) <= 3:\n",
    "            image = 255 - image\n",
    "        image = basic_aug(image)\n",
    "        \n",
    "        try:\n",
    "            image = tf.image.random_crop(image, size = (bs, target_size, target_size, c))\n",
    "        except:\n",
    "            image = tf.image.random_crop(image, size = (target_size, target_size, c))\n",
    "        image = tf.image.resize(image, [res, res], antialias = True)\n",
    "        \n",
    "        patches = keras.ops.image.extract_patches(original_image, size = patch_size, padding = 'same')\n",
    "        #_, w_, h_, dims = \n",
    "        dims = ops.shape(patches)[-1]\n",
    "        w_, h_ = ops.shape(patches)[-2], ops.shape(patches)[-3]\n",
    "        patches = keras.layers.Reshape([w_*h_, dims])(patches)\n",
    "\n",
    "        update_token = keras.ops.zeros(shape = (dims,), dtype = \"float32\")\n",
    "        sequence, masked_patches = MaskLayer(masking_rate=masking_rate, \n",
    "                                             update_value=update_token)(patches)\n",
    "        masked_patches = ops.reshape(masked_patches, [-1, res//patch_size, res//patch_size, \n",
    "                                                      patch_size, patch_size, c])\n",
    "        masked_patches = ops.transpose(masked_patches, [0,1,3,2,4,5])\n",
    "        masked_patches = ops.reshape(masked_patches, [-1, res, res, c])\n",
    "        \n",
    "        if grayscale:\n",
    "            try:\n",
    "                masked_patches = tf.image.rgb_to_grayscale(masked_patches)\n",
    "                image = tf.image.rgb_to_grayscale(image)\n",
    "                original_image = tf.image.rgb_to_grayscale(original_image)\n",
    "            except:\n",
    "                pass\n",
    "        else:\n",
    "            try:\n",
    "                masked_patches = tf.image.grayscale_to_rgb(masked_patches)\n",
    "                image = tf.image.grayscale_to_rgb(image)\n",
    "                original_image = tf.image.grayscale_to_rgb(original_image)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        masked_patches = ops.cast(masked_patches, \"uint8\")\n",
    "        image = ops.cast(image, \"uint8\")\n",
    "        if single:\n",
    "            image = image[0, ...]\n",
    "            masked_patches = masked_patches[0,...]\n",
    "            sequence = sequence[0,...]\n",
    "            original_image = original_image[0,...]\n",
    "        return (sequence, masked_patches, image, original_image)\n",
    "    return mask_and_restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0e87a022",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T06:27:07.241071Z",
     "iopub.status.busy": "2024-11-08T06:27:07.240475Z",
     "iopub.status.idle": "2024-11-08T06:27:07.261206Z",
     "shell.execute_reply": "2024-11-08T06:27:07.260300Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.055098,
     "end_time": "2024-11-08T06:27:07.263260",
     "exception": false,
     "start_time": "2024-11-08T06:27:07.208162",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#sim MIM:\n",
    "#masked image -> patchwise encoder -> original image와 patch별 L1 loss 구하고  mask sequence로 가중합\n",
    "# Loss의 argument로 들어가는 원소 : [batch, n_patches, patch_size, patch_size, 1 or 3]\n",
    "class SimMIM(keras.Model):\n",
    "    def __init__(self, feature_extractor, grayscale, patch_size,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.patch_size = patch_size\n",
    "        self.feature_extractor = feature_extractor #weight sharing for VICreg, Barlow Twins\n",
    "        self.grayscale = grayscale\n",
    "        if grayscale:\n",
    "            self.projector = Dense(units = patch_size**2 , activation = \"sigmoid\", name = \"Grayscale_Regressor\")\n",
    "        else:\n",
    "            self.projector = Dense(units = 3*(patch_size**2) , activation = \"sigmoid\", name = \"RGB_Regressor\")\n",
    "        \n",
    "        self.loss_tracker = keras.metrics.Mean(\"SimMIM_Regression_loss_tracker\")\n",
    "        self.reg_fn = keras.losses.MeanAbsoluteError(reduction = None)\n",
    "    \n",
    "    def get_env_config(self):\n",
    "        return {\"feature_extractor_name\" : self.feature_extractor.name,\n",
    "               \"SSL_method\" : \"SimMIM\",\n",
    "                \"Grayscale\" : self.grayscale,\n",
    "               }\n",
    "    \n",
    "    def compute_loss(self, original_patches, predicted_patches, mask_indices):\n",
    "        loss = self.reg_fn(y_true = original_patches, y_pred = predicted_patches)\n",
    "        return ops.sum(loss)/ops.sum(mask_indices)\n",
    "        \n",
    "    def train_step(self, dataset):\n",
    "        mask_indices, mask_image, image = dataset\n",
    "        mask_image = ops.cast(mask_image, \"float32\")\n",
    "        image = ops.cast(image, \"float32\")\n",
    "        _image =  keras.layers.Rescaling(scale=1./127.5, offset=-1, name = \"RescalingImage\")(image)\n",
    "        original_patches = ops.image.extract_patches(_image, self.patch_size, padding = \"same\")\n",
    "        n_patches = ops.shape(original_patches)[1] * ops.shape(original_patches)[2]\n",
    "        channels = ops.shape(original_patches)[-1]\n",
    "        \n",
    "        if self.grayscale:\n",
    "            original_dims = 1\n",
    "        else:\n",
    "            original_dims = 3\n",
    "        \n",
    "        original_patches = ops.reshape(original_patches, [-1, n_patches, self.patch_size, self.patch_size, original_dims ])\n",
    "        ## 1. SSL encoder loss ##\n",
    "        with tf.GradientTape() as tape:\n",
    "            feature_token, feature_seq, weights = self.feature_extractor(mask_image)\n",
    "            feature_seq = self.projector(feature_seq)\n",
    "            feature_seq = ops.reshape(feature_seq, [-1, n_patches, self.patch_size, self.patch_size, original_dims])\n",
    "            loss = self.compute_loss(original_patches, feature_seq, mask_indices)\n",
    "\n",
    "        gradients = tape.gradient(loss, \n",
    "                                 (self.feature_extractor.trainable_variables + self.projector.trainable_variables))\n",
    "        self.optimizer.apply_gradients(zip(gradients, \n",
    "                                          (self.feature_extractor.trainable_variables + self.projector.trainable_variables)\n",
    "                                          ))\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        output_dict = {'SimMIM_loss' : self.loss_tracker.result()}\n",
    "        return output_dict\n",
    "    def test_step(self, dataset):\n",
    "        mask_indices, mask_image, image = dataset\n",
    "        mask_image = ops.cast(mask_image, \"float32\")\n",
    "        image = ops.cast(image, \"float32\")\n",
    "        _image =  keras.layers.Rescaling(scale=1./127.5, offset=-1, name = \"RescalingImage\")(image)\n",
    "        \n",
    "        original_patches = ops.image.extract_patches(_image, self.patch_size, padding = \"same\")\n",
    "        n_patches = ops.shape(original_patches)[1] * ops.shape(original_patches)[2]\n",
    "        channels = ops.shape(original_patches)[-1]\n",
    "        if self.grayscale:\n",
    "            original_dims = 1\n",
    "        else:\n",
    "            original_dims = 3\n",
    "        original_patches = ops.reshape(original_patches, [-1, n_patches, self.patch_size, self.patch_size, original_dims ])\n",
    "        \n",
    "        if True:\n",
    "            feature_token, feature_seq, weights = self.feature_extractor(mask_image)\n",
    "            feature_seq = self.projector(feature_seq)\n",
    "            feature_seq = ops.reshape(feature_seq, [-1, n_patches, self.patch_size, self.patch_size, original_dims])\n",
    "            loss = self.compute_loss(original_patches, feature_seq, mask_indices)\n",
    "\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        output_dict = {'SimMIM_loss' : self.loss_tracker.result()}\n",
    "        return output_dict\n",
    "    \n",
    "    def call(self, dataset):\n",
    "        return self.test_step(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ef81ed",
   "metadata": {
    "papermill": {
     "duration": 0.031573,
     "end_time": "2024-11-08T06:27:07.326116",
     "exception": false,
     "start_time": "2024-11-08T06:27:07.294543",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "> Mixed SimMIM\n",
    "- Original SimMIM plus,\n",
    "- masked image (encoded) 및 image (encoded) feature map간 euclidian distance 최소화\n",
    "- representation vector의 VIC loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cb041662",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T06:27:07.391248Z",
     "iopub.status.busy": "2024-11-08T06:27:07.390743Z",
     "iopub.status.idle": "2024-11-08T06:27:07.428339Z",
     "shell.execute_reply": "2024-11-08T06:27:07.427376Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.073102,
     "end_time": "2024-11-08T06:27:07.430357",
     "exception": false,
     "start_time": "2024-11-08T06:27:07.357255",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MixedMIM(keras.Model):\n",
    "    def __init__(self, feature_extractor, grayscale, patch_size,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.patch_size = patch_size\n",
    "        self.feature_extractor = feature_extractor #weight sharing for VICreg, Barlow Twins\n",
    "        self.mha = keras.layers.MultiHeadAttention(8,512)\n",
    "        self.grayscale = grayscale\n",
    "        if grayscale:\n",
    "            self.projector = Dense(units = patch_size**2 , activation = \"sigmoid\", name = \"Grayscale_Regressor\")\n",
    "        else:\n",
    "            self.projector = Dense(units = 3*(patch_size**2) , activation = \"sigmoid\", name = \"RGB_Regressor\")\n",
    "        \n",
    "        self.loss_tracker = keras.metrics.Mean(\"MixedMIM_loss_tracker\")\n",
    "        self.sim_loss_tracker = keras.metrics.Mean(\"SimMIM_Regression_loss_tracker\")\n",
    "        self.feature_loss_tracker = keras.metrics.Mean(\"FeatureMap_distance_loss_tracker\")\n",
    "        self.vic_loss_tracker = keras.metrics.Mean(\"vic_loss_tracker\")\n",
    "        self.barlow_loss_tracker = keras.metrics.Mean(\"barlow_loss_tracker\")\n",
    "        \n",
    "        self.reg_fn = keras.losses.Huber(reduction = None)\n",
    "        self.distance_fn = keras.losses.Huber(reduction = None)\n",
    "        self.vic_loss = VICRegLoss()\n",
    "        self.barlow_loss = BarlowLoss()\n",
    "    def get_env_config(self):\n",
    "        return {\"feature_extractor_name\" : self.feature_extractor.name,\n",
    "               \"SSL_method\" : \"MixedMIM\",\n",
    "                \"Grayscale\" : self.grayscale,\n",
    "               }\n",
    "    \n",
    "    def compute_simmim_loss(self, original_patches, predicted_patches, mask_indices):\n",
    "        loss = self.reg_fn(y_true = original_patches, y_pred = predicted_patches)\n",
    "        loss = loss * mask_indices[..., tf.newaxis, tf.newaxis]\n",
    "        return ops.sum(loss)/ops.sum(mask_indices)\n",
    "    \n",
    "    def compute_fmap_loss(self, original_patches, predicted_patches, mask_indices):\n",
    "        loss = self.distance_fn(y_true = original_patches, y_pred = predicted_patches)\n",
    "        loss = loss * mask_indices[..., tf.newaxis, tf.newaxis]\n",
    "        return ops.sum(loss)/ops.sum(mask_indices)\n",
    "\n",
    "    \n",
    "    def train_step(self, dataset):\n",
    "        mask_indices, mask_image, image, original_image = dataset\n",
    "        mask_image = ops.cast(mask_image, \"float32\")\n",
    "        image = ops.cast(image, \"float32\")\n",
    "        image_ = keras.layers.Rescaling(scale=1./127.5, offset=-1, name = \"RescalingImage\")(image)\n",
    "        original_patches = ops.image.extract_patches(image_, self.patch_size, padding = \"same\")\n",
    "        n_patches = ops.shape(original_patches)[1] * ops.shape(original_patches)[2]\n",
    "        channels = ops.shape(original_patches)[-1]\n",
    "        \n",
    "        if self.grayscale:\n",
    "            original_dims = 1\n",
    "        else:\n",
    "            original_dims = 3\n",
    "        \n",
    "        original_patches_rgb = ops.reshape(original_patches, \n",
    "                                       [-1, n_patches, self.patch_size, self.patch_size, \n",
    "                                        original_dims ])\n",
    "        with tf.GradientTape() as tape:\n",
    "            feature_token, feature_seq, weights = self.feature_extractor(image)\n",
    "            feature_token_mask, feature_seq_mask, weights_mask = self.feature_extractor(mask_image)\n",
    "            original_token, original_seq, original_weights = self.feature_extractor(original_image)\n",
    "            \n",
    "            feature_seq_mask_rgb = self.projector(feature_seq_mask)\n",
    "            feature_seq_mask_rgb = ops.reshape(feature_seq_mask_rgb, [-1, n_patches, self.patch_size, self.patch_size, original_dims])\n",
    "            \n",
    "            # RGB regression loss\n",
    "            simmim_loss = self.compute_simmim_loss(original_patches_rgb, feature_seq_mask_rgb, mask_indices)\n",
    "            # Feature map loss\n",
    "            feature_seq_mask = self.mha(query = feature_seq_mask, key = feature_seq, value = feature_seq,\n",
    "                                       return_attention_scores = False,\n",
    "                                       attention_mask = 1-(mask_indices[:, tf.newaxis, :])) #original embedding 중 mask 되지 않은 부분을 context로 삼음.\n",
    "            feature_map_loss = self.compute_fmap_loss(feature_seq, feature_seq_mask, mask_indices)\n",
    "            # VIC loss\n",
    "            \n",
    "            vic_loss = ops.mean([self.vic_loss(feature_token_mask, feature_token), self.vic_loss(feature_token_mask, original_token),\n",
    "                                self.vic_loss(feature_token, original_token),\n",
    "                                self.vic_loss(feature_token, feature_token_mask), self.vic_loss(original_token, feature_token_mask),\n",
    "                                self.vic_loss(original_token, feature_token),\n",
    "                                ]\n",
    "                               )\n",
    "            barlow_loss = ops.mean([self.barlow_loss(feature_token_mask, feature_token), self.barlow_loss(feature_token_mask, original_token),\n",
    "                                self.barlow_loss(feature_token, original_token),\n",
    "                                   self.barlow_loss(feature_token, feature_token_mask), self.barlow_loss(original_token, feature_token_mask),\n",
    "                                self.barlow_loss(original_token, feature_token)\n",
    "                                   ]\n",
    "                                  )\n",
    "            \n",
    "            loss = simmim_loss/100 + feature_map_loss + vic_loss/50 + barlow_loss\n",
    "\n",
    "        gradients = tape.gradient(loss, \n",
    "                                 (self.feature_extractor.trainable_variables + self.projector.trainable_variables + self.mha.trainable_variables))\n",
    "        self.optimizer.apply_gradients(zip(gradients, \n",
    "                                          (self.feature_extractor.trainable_variables + self.projector.trainable_variables + self.mha.trainable_variables)\n",
    "                                          ))\n",
    "        \n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.sim_loss_tracker.update_state(simmim_loss)\n",
    "        self.feature_loss_tracker.update_state(feature_map_loss)\n",
    "        self.vic_loss_tracker.update_state(vic_loss)\n",
    "        self.barlow_loss_tracker.update_state(barlow_loss)\n",
    "        output_dict = {'MixedMIM_loss' : self.loss_tracker.result(),\n",
    "                      'SimMIM_loss' : self.sim_loss_tracker.result(),\n",
    "                      'Feature_Map_Distance' : self.feature_loss_tracker.result(),\n",
    "                      'CLS_token_VIC_loss' : self.vic_loss_tracker.result(),\n",
    "                       'CLS_token_Barlow_loss' : self.barlow_loss_tracker.result()\n",
    "                      }\n",
    "        return output_dict\n",
    "    \n",
    "    def test_step(self, dataset):\n",
    "        mask_indices, mask_image, image, original_image = dataset\n",
    "        mask_image = ops.cast(mask_image, \"float32\")\n",
    "        image = ops.cast(image, \"float32\")\n",
    "        image_ =  keras.layers.Rescaling(scale=1./127.5, offset=-1, name = \"RescalingImage\")(image)\n",
    "        original_patches = ops.image.extract_patches(image_, self.patch_size, padding = \"same\")\n",
    "        n_patches = ops.shape(original_patches)[1] * ops.shape(original_patches)[2]\n",
    "        channels = ops.shape(original_patches)[-1]\n",
    "        \n",
    "        if self.grayscale:\n",
    "            original_dims = 1\n",
    "        else:\n",
    "            original_dims = 3\n",
    "        \n",
    "        original_patches_rgb = ops.reshape(original_patches, \n",
    "                                       [-1, n_patches, self.patch_size, self.patch_size, \n",
    "                                        original_dims ])\n",
    "        if True:\n",
    "            feature_token, feature_seq, weights = self.feature_extractor(image)\n",
    "            feature_token_mask, feature_seq_mask, weights_mask = self.feature_extractor(mask_image)\n",
    "            original_token, original_seq, original_weights = self.feature_extractor(original_image)\n",
    "            \n",
    "            feature_seq_mask_rgb = self.projector(feature_seq_mask)\n",
    "            feature_seq_mask_rgb = ops.reshape(feature_seq_mask_rgb, [-1, n_patches, self.patch_size, self.patch_size, original_dims])\n",
    "            print(f\"original sequence shape : {original_patches_rgb.shape}, feature seq mask shape : {ops.shape(feature_seq_mask_rgb)}\")\n",
    "            # RGB regression loss\n",
    "            simmim_loss = self.compute_simmim_loss(original_patches_rgb, feature_seq_mask_rgb, mask_indices)\n",
    "            # Feature map loss\n",
    "            feature_seq_mask = self.mha(query = feature_seq_mask, key = feature_seq, value = feature_seq,\n",
    "                                       return_attention_scores = False,\n",
    "                                       attention_mask = 1-(mask_indices[:, tf.newaxis, :]))\n",
    "            feature_map_loss = self.compute_fmap_loss(feature_seq, feature_seq_mask, mask_indices)\n",
    "            # VIC loss\n",
    "            vic_loss = ops.mean([self.vic_loss(feature_token_mask, feature_token), self.vic_loss(feature_token_mask, original_token),\n",
    "                                self.vic_loss(feature_token, original_token),\n",
    "                                self.vic_loss(feature_token, feature_token_mask), self.vic_loss(original_token, feature_token_mask),\n",
    "                                self.vic_loss(original_token, feature_token),\n",
    "                                ]\n",
    "                               )\n",
    "            barlow_loss = ops.mean([self.barlow_loss(feature_token_mask, feature_token), self.barlow_loss(feature_token_mask, original_token),\n",
    "                                self.barlow_loss(feature_token, original_token),\n",
    "                                   self.barlow_loss(feature_token, feature_token_mask), self.barlow_loss(original_token, feature_token_mask),\n",
    "                                self.barlow_loss(original_token, feature_token)\n",
    "                                   ]\n",
    "                                  )\n",
    "            loss = simmim_loss/100 + feature_map_loss + vic_loss/50 + barlow_loss\n",
    "\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.sim_loss_tracker.update_state(simmim_loss)\n",
    "        self.feature_loss_tracker.update_state(feature_map_loss)\n",
    "        self.vic_loss_tracker.update_state(vic_loss)\n",
    "        self.barlow_loss_tracker.update_state(barlow_loss)\n",
    "        output_dict = {'MixedMIM_loss' : self.loss_tracker.result(),\n",
    "                      'SimMIM_loss' : self.sim_loss_tracker.result(),\n",
    "                      'Feature_Map_Distance' : self.feature_loss_tracker.result(),\n",
    "                      'CLS_token_VIC_loss' : self.vic_loss_tracker.result(),\n",
    "                       'CLS_token_Barlow_loss' : self.barlow_loss_tracker.result()\n",
    "                      }\n",
    "        return output_dict\n",
    "    \n",
    "    def call(self, dataset):\n",
    "        return self.test_step(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b4db7c",
   "metadata": {
    "papermill": {
     "duration": 0.031225,
     "end_time": "2024-11-08T06:27:07.493328",
     "exception": false,
     "start_time": "2024-11-08T06:27:07.462103",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "> DistilMIM\n",
    "\n",
    "- Mixed MIM with self-distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7e26dd23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T06:27:07.557869Z",
     "iopub.status.busy": "2024-11-08T06:27:07.557553Z",
     "iopub.status.idle": "2024-11-08T06:27:07.595549Z",
     "shell.execute_reply": "2024-11-08T06:27:07.594633Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.072999,
     "end_time": "2024-11-08T06:27:07.597497",
     "exception": false,
     "start_time": "2024-11-08T06:27:07.524498",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DistilMIM(keras.Model):\n",
    "    def __init__(self, feature_extractor, grayscale, patch_size, lambda_ = 0.99, multiview = False,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.patch_size = patch_size\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.channels = ops.shape(self.feature_extractor.outputs[0])[-1]\n",
    "        self.grayscale = grayscale\n",
    "        if grayscale:\n",
    "            self.projector = Dense(units = patch_size**2 , activation = \"sigmoid\", name = \"Grayscale_Regressor\")\n",
    "        else:\n",
    "            self.projector = Dense(units = 3*(patch_size**2) , activation = \"sigmoid\", name = \"RGB_Regressor\")\n",
    "        \n",
    "        ##student model setting\n",
    "        ##backprop -> student model, EMA -> teacher model (feature extractor, projector)\n",
    "        self.student_extractor = tf.keras.models.clone_model(feature_extractor)\n",
    "        self.student_extractor.set_weights(self.feature_extractor.get_weights())\n",
    "        \n",
    "        self.c_token = self.add_weight(name='center_token', shape=(self.channels,), initializer='glorot_uniform', trainable=False)\n",
    "        self.c_patches = self.add_weight(name='center_patch', shape=(self.channels,), initializer='glorot_uniform', trainable=False)\n",
    "        self.l = lambda_\n",
    "        ##\n",
    "        \n",
    "        self.loss_tracker = keras.metrics.Mean(\"MixedMIM_loss_tracker\")\n",
    "        self.sim_loss_tracker = keras.metrics.Mean(\"SimMIM_Regression_loss_tracker\")\n",
    "        self.feature_loss_tracker = keras.metrics.Mean(\"FeatureMap_H_loss_tracker\")\n",
    "        self.token_loss_tracker = keras.metrics.Mean(\"Token_H_loss_tracker\")\n",
    "        \n",
    "        self.reg_fn = keras.losses.Huber(reduction = None)\n",
    "        \n",
    "    def get_env_config(self):\n",
    "        return {\"feature_extractor_name\" : self.feature_extractor.name,\n",
    "               \"SSL_method\" : \"DistilMIM\",\n",
    "                \"Grayscale\" : self.grayscale,\n",
    "               }\n",
    "    def compute_h(self, t, s, center, mask = None):\n",
    "        t = tf.stop_gradient(t)\n",
    "        t = (t-center)/0.1 #centering and shapening\n",
    "        t = ops.softmax(t, axis = -1)\n",
    "        s = ops.softmax(s, axis = -1)\n",
    "        ce = -t*ops.log(s + 1e-5)\n",
    "        if mask is not None:\n",
    "            if len(ops.shape(mask)) == 2:\n",
    "                mask = mask[..., tf.newaxis]\n",
    "            ce = ops.multiply(ce, mask)\n",
    "        \n",
    "        if len(ops.shape(t)) == 2:\n",
    "            ce = ops.sum(ce, axis = -1)\n",
    "        elif len(ops.shape(t)) == 3 :\n",
    "            ce = ops.sum(ce, axis = [1, 2])\n",
    "        \n",
    "        if mask is not None:\n",
    "            return ops.sum(ce)/ops.sum(mask)\n",
    "        else:\n",
    "            return ops.mean(ce)\n",
    "        \n",
    "    def compute_simmim_loss(self, original_patches, predicted_patches, mask_indices):\n",
    "        loss = self.reg_fn(y_true = original_patches, y_pred = predicted_patches)\n",
    "        return ops.sum(loss)/ops.sum(mask_indices)\n",
    "    \n",
    "    def train_step(self, dataset):\n",
    "        mask_indices, mask_image, image = dataset\n",
    "        mask_image = ops.cast(mask_image, \"float32\")\n",
    "        image = ops.cast(image, \"float32\")\n",
    "        image =  keras.layers.Rescaling(scale=1./127.5, offset=-1, name = \"RescalingImage\")(image)\n",
    "        original_patches = ops.image.extract_patches(image, self.patch_size, padding = \"same\")\n",
    "        n_patches = ops.shape(original_patches)[1] * ops.shape(original_patches)[2]\n",
    "        channels = ops.shape(original_patches)[-1]\n",
    "        \n",
    "        if self.grayscale:\n",
    "            original_dims = 1\n",
    "        else:\n",
    "            original_dims = 3\n",
    "        \n",
    "        original_patches_rgb = ops.reshape(original_patches, \n",
    "                                       [-1, n_patches, self.patch_size, self.patch_size, \n",
    "                                        original_dims ])\n",
    "        with tf.GradientTape() as tape:\n",
    "            feature_token, feature_seq, weights = self.feature_extractor(image)\n",
    "            feature_token_mask, feature_seq_mask, weights_mask = self.feature_extractor(mask_image)\n",
    "            \n",
    "            feature_token_s, feature_seq_s, weights_s = self.student_extractor(image)\n",
    "            feature_token_mask_s, feature_seq_mask_s, weights_mask_s = self.student_extractor(mask_image)\n",
    "            \n",
    "            # RGB regression loss\n",
    "            feature_seq_mask_rgb = self.projector(feature_seq_mask_s)\n",
    "            feature_seq_mask_rgb = ops.reshape(feature_seq_mask_rgb, [-1, n_patches, self.patch_size, self.patch_size, original_dims])\n",
    "            simmim_loss = self.compute_simmim_loss(original_patches_rgb, feature_seq_mask_rgb, \n",
    "                                                   mask_indices)\n",
    "            # Feature map loss\n",
    "            ##feature_map_loss_1 = self.compute_h(t = feature_seq_mask, s = feature_seq_s, \n",
    "            ##                                 center = self.c_patches, mask = mask_indices)\n",
    "            feature_map_loss = self.compute_h(t = feature_seq, s = feature_seq_mask_s, \n",
    "                                             center = self.c_patches, mask = mask_indices)\n",
    "            \n",
    "            # token loss\n",
    "            token_loss_1 = self.compute_h(t = feature_token, s = feature_token_mask_s,\n",
    "                                       center = self.c_token, mask = None)\n",
    "            token_loss_2 = self.compute_h(t = feature_token_mask, s = feature_token_s,\n",
    "                                       center = self.c_token, mask = None)\n",
    "            token_loss = 0.5*(token_loss_1 + token_loss_2)\n",
    "            \n",
    "            loss = token_loss + feature_map_loss + simmim_loss/50\n",
    "        # A) update student network\n",
    "        gradients = tape.gradient(loss, \n",
    "                                 (self.student_extractor.trainable_variables + self.projector.trainable_variables))\n",
    "        self.optimizer.apply_gradients(zip(gradients, \n",
    "                                          (self.student_extractor.trainable_variables + self.projector.trainable_variables)\n",
    "                                          ))\n",
    "        # B) update center values for centering : c_token, c_patches\n",
    "        mean_cls = ops.mean(ops.concatenate([feature_token, feature_token_mask], axis = 0), axis = 0)\n",
    "        mean_patch = ops.mean(ops.concatenate([feature_seq, feature_seq_mask], axis = 0), axis = (0,1))\n",
    "        self.c_token.assign(self.l*self.c_token + (1.0-self.l)*mean_cls)\n",
    "        self.c_patches.assign(self.l*self.c_patches + (1.0-self.l)*mean_patch)\n",
    "        \n",
    "        #C) update teacher network via EMA\n",
    "        for f_teacher, f_student in zip(self.feature_extractor.weights, self.student_extractor.weights):\n",
    "            f_teacher.assign(self.l*f_teacher + (1-self.l)*f_student)\n",
    "        \n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.sim_loss_tracker.update_state(simmim_loss)\n",
    "        self.feature_loss_tracker.update_state(feature_map_loss)\n",
    "        self.token_loss_tracker.update_state(token_loss)\n",
    "        \n",
    "        output_dict = {'DistilMIM_loss' : self.loss_tracker.result(),\n",
    "                      'SimMIM_loss' : self.sim_loss_tracker.result(),\n",
    "                      'Feature_Map_H' : self.feature_loss_tracker.result(),\n",
    "                      'Token_H' : self.token_loss_tracker.result()\n",
    "                      }\n",
    "        return output_dict\n",
    "    \n",
    "    def test_step(self, dataset):\n",
    "        mask_indices, mask_image, image = dataset\n",
    "        mask_image = ops.cast(mask_image, \"float32\")\n",
    "        image = ops.cast(image, \"float32\")\n",
    "        image =  keras.layers.Rescaling(scale=1./127.5, offset=-1, name = \"RescalingImage\")(image)\n",
    "        original_patches = ops.image.extract_patches(image, self.patch_size, padding = \"same\")\n",
    "        n_patches = ops.shape(original_patches)[1] * ops.shape(original_patches)[2]\n",
    "        channels = ops.shape(original_patches)[-1]\n",
    "        \n",
    "        if self.grayscale:\n",
    "            original_dims = 1\n",
    "        else:\n",
    "            original_dims = 3\n",
    "        \n",
    "        original_patches_rgb = ops.reshape(original_patches, \n",
    "                                       [-1, n_patches, self.patch_size, self.patch_size, \n",
    "                                        original_dims ])\n",
    "        if True:\n",
    "            feature_token, feature_seq, weights = self.feature_extractor(image)\n",
    "            feature_token_mask, feature_seq_mask, weights_mask = self.feature_extractor(mask_image)\n",
    "            \n",
    "            feature_token_s, feature_seq_s, weights_s = self.student_extractor(image)\n",
    "            feature_token_mask_s, feature_seq_mask_s, weights_mask_s = self.student_extractor(mask_image)\n",
    "            \n",
    "            # RGB regression loss\n",
    "            feature_seq_mask_rgb = self.projector(feature_seq_mask_s)\n",
    "            feature_seq_mask_rgb = ops.reshape(feature_seq_mask_rgb, [-1, n_patches, self.patch_size, self.patch_size, original_dims])\n",
    "            simmim_loss = self.compute_simmim_loss(original_patches_rgb, feature_seq_mask_rgb, \n",
    "                                                   mask_indices)\n",
    "            # Feature map loss\n",
    "            ##feature_map_loss_1 = self.compute_h(t = feature_seq_mask, s = feature_seq_s, \n",
    "            ##                                 center = self.c_patches, mask = mask_indices)\n",
    "            feature_map_loss = self.compute_h(t = feature_seq, s = feature_seq_mask_s, \n",
    "                                             center = self.c_patches, mask = mask_indices)\n",
    "            \n",
    "            # token loss\n",
    "            token_loss_1 = self.compute_h(t = feature_token, s = feature_token_mask_s,\n",
    "                                       center = self.c_token, mask = None)\n",
    "            token_loss_2 = self.compute_h(t = feature_token_mask, s = feature_token_s,\n",
    "                                       center = self.c_token, mask = None)\n",
    "            token_loss = 0.5*(token_loss_1 + token_loss_2)\n",
    "            \n",
    "            loss = token_loss + feature_map_loss + simmim_loss/50\n",
    "        \n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.sim_loss_tracker.update_state(simmim_loss)\n",
    "        self.feature_loss_tracker.update_state(feature_map_loss)\n",
    "        self.token_loss_tracker.update_state(token_loss)\n",
    "        \n",
    "        output_dict = {'DistilMIM_loss' : self.loss_tracker.result(),\n",
    "                      'SimMIM_loss' : self.sim_loss_tracker.result(),\n",
    "                      'Feature_Map_H' : self.feature_loss_tracker.result(),\n",
    "                      'Token_H' : self.token_loss_tracker.result()\n",
    "                      }\n",
    "        return output_dict\n",
    "    \n",
    "    def call(self, dataset):\n",
    "        return self.test_step(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdb1242",
   "metadata": {
    "papermill": {
     "duration": 0.030987,
     "end_time": "2024-11-08T06:27:07.659721",
     "exception": false,
     "start_time": "2024-11-08T06:27:07.628734",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "> VICRegL helper functions and loss implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0d198afe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T06:27:07.723838Z",
     "iopub.status.busy": "2024-11-08T06:27:07.723538Z",
     "iopub.status.idle": "2024-11-08T06:27:07.747996Z",
     "shell.execute_reply": "2024-11-08T06:27:07.747144Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.058966,
     "end_time": "2024-11-08T06:27:07.749850",
     "exception": false,
     "start_time": "2024-11-08T06:27:07.690884",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def feature_based_matching(feature_map_a, feature_map_b, k=20):\n",
    "    if len(ops.shape(feature_map_a)) == 4:\n",
    "        batch_size, H, W, D = feature_map_a.shape\n",
    "    elif len(ops.shape(feature_map_a)) == 3 :\n",
    "        batch_size, seq_len, D = feature_map_a.shape\n",
    "        H = tf.sqrt(tf.cast(seq_len, tf.float32))\n",
    "        H = tf.cast(H, tf.int32)\n",
    "        W = H\n",
    "    feature_map_a_flat = tf.reshape(feature_map_a, [-1, H * W, D])\n",
    "    feature_map_b_flat = tf.reshape(feature_map_b, [-1, H * W, D])\n",
    "    dist = tf.norm(feature_map_a_flat[:, :, tf.newaxis, :] - feature_map_b_flat[:, tf.newaxis, :, :], axis=-1)\n",
    "    \n",
    "    matchings = ops.reshape(dist, [-1, H*W*H*W])\n",
    "    matchings = ops.cast(matchings, \"float32\")\n",
    "\n",
    "    # Top-k selection\n",
    "    top_k_values, _ = tf.nn.top_k(-matchings, k=int(k))\n",
    "    top_k_values = -top_k_values\n",
    "    matchings_top_k = tf.reduce_mean(top_k_values, axis=-1)\n",
    "    return matchings_top_k\n",
    "\n",
    "def location_based_matching(feature_map_a, feature_map_b, k=4):\n",
    "    if len(ops.shape(feature_map_a)) == 4:\n",
    "        batch_size, H, W, D = feature_map_a.shape\n",
    "    elif len(ops.shape(feature_map_a)) == 3 :\n",
    "        batch_size, seq_len, D = feature_map_a.shape\n",
    "        H = tf.sqrt(tf.cast(seq_len, tf.float32))\n",
    "        H = tf.cast(H, tf.int32)\n",
    "        W = H\n",
    "    coords = tf.stack(tf.meshgrid(tf.range(H), tf.range(W), indexing='ij'), axis=-1)\n",
    "    coords_flat = tf.cast(tf.reshape(coords, [H * W, 2]), tf.float32)\n",
    "    coord_dist = tf.norm(coords_flat[:, tf.newaxis, :] - coords_flat[tf.newaxis, :, :], axis=-1)\n",
    "    coord_dist = tf.reshape(coord_dist, [H * W, H * W])\n",
    "\n",
    "    feature_map_a_flat = tf.reshape(feature_map_a, [-1, H * W, D])\n",
    "    feature_map_b_flat = tf.reshape(feature_map_b, [-1, H * W, D])\n",
    "\n",
    "    dist = tf.norm(feature_map_a_flat[:, :, tf.newaxis, :] - feature_map_b_flat[:, tf.newaxis, :, :], axis=-1)\n",
    "    combined_dist = dist * tf.expand_dims(coord_dist, axis=0)\n",
    "    \n",
    "    matchings = ops.reshape(dist, [-1, H*W*H*W])\n",
    "    \n",
    "    matchings = ops.cast(matchings, \"float32\")\n",
    "\n",
    "    # Top-k selection\n",
    "    \n",
    "    top_k_values, _ = tf.nn.top_k(-matchings, k=int(k))\n",
    "    top_k_values = -top_k_values\n",
    "    matchings_top_k = tf.reduce_mean(top_k_values, axis=-1)\n",
    "    return matchings_top_k\n",
    "\n",
    "class VICRegL_Loss(keras.losses.Loss):\n",
    "    def __init__(self, alpha=0.75, lambda_v=1.0, mu=1.0, nu=1.0, name=\"VICRegL_Loss\"):\n",
    "        super(VICRegL_Loss, self).__init__(name=name)\n",
    "        self.alpha = alpha\n",
    "        self.lambda_v = lambda_v\n",
    "        self.mu = mu\n",
    "        self.nu = nu\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        z_a, patch_a = y_true\n",
    "        z_b, patch_b = y_pred\n",
    "        local_loss = self.local_criterion(patch_a, patch_b)\n",
    "        global_loss = self.global_criterion(z_a, z_b)\n",
    "        return self.alpha * global_loss + (1 - self.alpha) * local_loss\n",
    "\n",
    "    def global_criterion(self, z_a, z_b):\n",
    "        sim_loss = tf.reduce_mean(tf.square(z_a - z_b))\n",
    "        std_z_a = tf.math.reduce_std(z_a, axis=0)\n",
    "        std_z_b = tf.math.reduce_std(z_b, axis=0)\n",
    "        std_loss = tf.reduce_mean(tf.nn.relu(1 - std_z_a)) + tf.reduce_mean(tf.nn.relu(1 - std_z_b))\n",
    "        z_a -= tf.reduce_mean(z_a, axis=0)\n",
    "        z_b -= tf.reduce_mean(z_b, axis=0)\n",
    "        cov_z_a = tf.matmul(z_a, z_a, transpose_a=True) / (tf.cast(tf.shape(z_a)[0], tf.float32) - 1.0)\n",
    "        cov_z_b = tf.matmul(z_b, z_b, transpose_a=True) / (tf.cast(tf.shape(z_b)[0], tf.float32) - 1.0)\n",
    "        cov_loss = tf.reduce_sum(tf.square(self.off_diagonal(cov_z_a))) + tf.reduce_sum(tf.square(self.off_diagonal(cov_z_b)))\n",
    "        return self.lambda_v * sim_loss + self.mu * std_loss + self.nu * cov_loss\n",
    "\n",
    "    def local_criterion(self, patch_a, patch_b):\n",
    "        feature_loss = feature_based_matching(patch_a, patch_b)\n",
    "        location_loss = location_based_matching(patch_a, patch_b)\n",
    "        return feature_loss + location_loss\n",
    "\n",
    "    def off_diagonal(self, x):\n",
    "        n = tf.shape(x)[0]\n",
    "        return tf.reshape(tf.boolean_mask(x, tf.eye(n, dtype=tf.bool)), [n, -1])\n",
    "\n",
    "# 예제 사용\n",
    "#batch_size = 12\n",
    "#H, W, D = 7, 7, 1024\n",
    "#feature_map_a = tf.random.normal([batch_size, H, W, D])\n",
    "#feature_map_b = tf.random.normal([batch_size, H, W, D])\n",
    "#z_a = keras.layers.GlobalAveragePooling2D()(feature_map_a)\n",
    "#z_b = keras.layers.GlobalAveragePooling2D()(feature_map_b)\n",
    "#loss_fn = VICRegL_Loss()\n",
    "#loss = loss_fn((z_a, feature_map_a), (z_b, feature_map_b))\n",
    "#print(\"Loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f19b4543",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T06:27:07.813364Z",
     "iopub.status.busy": "2024-11-08T06:27:07.813083Z",
     "iopub.status.idle": "2024-11-08T06:27:07.829261Z",
     "shell.execute_reply": "2024-11-08T06:27:07.828406Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.050221,
     "end_time": "2024-11-08T06:27:07.831083",
     "exception": false,
     "start_time": "2024-11-08T06:27:07.780862",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VICRegL(keras.Model):\n",
    "    def __init__(self, feature_extractor, embed_dims, student_model=None, grayscale=None, patch_size=None, multiview=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.student_model = student_model\n",
    "        self.projector = keras.layers.Dense(units=embed_dims, activation=\"gelu\", use_bias=False, name = \"projector\")\n",
    "        if student_model is None:\n",
    "            self.student_model = self.feature_extractor\n",
    "            self.student_projector = self.projector\n",
    "        elif student_model in [\"distil\", \"distillation\", \"distill\", \"Distil\", \"Distillation\", \"Distil\"]:\n",
    "            self.student_model = tf.keras.models.clone_model(feature_extractor)\n",
    "            self.student_model.set_weights(self.feature_extractor.get_weights())\n",
    "\n",
    "            self.student_projector = keras.layers.Dense(units=embed_dims, activation=\"gelu\", use_bias=False, name = \"student_projector\")\n",
    "            self.student_projector.set_weights(self.projector.get_weights())\n",
    "\n",
    "        self.grayscale = grayscale\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        self.loss_tracker = keras.metrics.Mean(\"VICRegL_loss_tracker\")\n",
    "        self.loss_fn = VICRegL_Loss()\n",
    "\n",
    "    def get_env_config(self):\n",
    "        return {\n",
    "            \"feature_extractor_name\": self.feature_extractor.name,\n",
    "            \"SSL_method\": \"VICRegL\",\n",
    "            \"Grayscale\": self.grayscale,\n",
    "        }\n",
    "\n",
    "    def train_step(self, dataset):\n",
    "        image, aug_image = dataset\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            feature_token, feature_seq, weights = self.feature_extractor(image)\n",
    "            feature_token_aug, feature_seq_aug, weights_aug = self.student_model(aug_image)\n",
    "            feature_seq = self.projector(feature_seq)\n",
    "            feature_seq_aug = self.student_projector(feature_seq_aug)\n",
    "\n",
    "            loss = self.loss_fn((tf.stop_gradient(feature_token), tf.stop_gradient(feature_seq)),\n",
    "                                (feature_token_aug, feature_seq_aug))\n",
    "\n",
    "        gradients = tape.gradient(loss, self.student_model.trainable_variables + self.student_projector.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.student_model.trainable_variables + self.student_projector.trainable_variables))\n",
    "\n",
    "        # Update teacher model weights if using distillation\n",
    "        if self.student_model is not None:\n",
    "            lambda_ = 0.999\n",
    "            for f_teacher_part, f_student_part, p_teacher_part, p_student_part in zip(self.feature_extractor.weights, self.student_model.weights, self.projector.weights, self.student_projector.weights):\n",
    "                f_teacher_part.assign(lambda_ * f_teacher_part + (1 - lambda_) * f_student_part)\n",
    "                p_teacher_part.assign(lambda_ * p_teacher_part + (1 - lambda_) * p_student_part)\n",
    "\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"VICRegL_loss\": self.loss_tracker.result()}\n",
    "\n",
    "    def test_step(self, dataset):\n",
    "        image, aug_image = dataset\n",
    "\n",
    "        feature_token, feature_seq, weights = self.feature_extractor(image)\n",
    "        feature_token_aug, feature_seq_aug, weights_aug = self.student_model(aug_image)\n",
    "        feature_seq = self.projector(feature_seq)\n",
    "        feature_seq_aug = self.student_projector(feature_seq_aug)\n",
    "\n",
    "        loss = self.loss_fn((feature_token, feature_seq), (feature_token_aug, feature_seq_aug))\n",
    "\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"VICRegL_loss\": self.loss_tracker.result()}\n",
    "\n",
    "    def call(self, dataset):\n",
    "        return self.test_step(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5875f80",
   "metadata": {
    "papermill": {
     "duration": 0.030901,
     "end_time": "2024-11-08T06:27:07.893012",
     "exception": false,
     "start_time": "2024-11-08T06:27:07.862111",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "> helper functions for Sementaic Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4238efda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T06:27:07.957147Z",
     "iopub.status.busy": "2024-11-08T06:27:07.956844Z",
     "iopub.status.idle": "2024-11-08T06:27:07.968753Z",
     "shell.execute_reply": "2024-11-08T06:27:07.967953Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.046105,
     "end_time": "2024-11-08T06:27:07.970666",
     "exception": false,
     "start_time": "2024-11-08T06:27:07.924561",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# thanks to GPT4o\n",
    "class KMeansLayer(keras.layers.Layer):\n",
    "    def __init__(self, n_clusters, max_iters=100, **kwargs):\n",
    "        super(KMeansLayer, self).__init__(**kwargs)\n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_iters = max_iters\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        embed_dim = input_shape[-1]\n",
    "        self.centroids = self.add_weight(shape=(self.n_clusters, embed_dim),\n",
    "                                         initializer='glorot_uniform',\n",
    "                                         trainable=False,\n",
    "                                         name='centroids')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # Flatten the inputs if they are 3D (batch_size, num_patches, embed_dims)\n",
    "        if len(inputs.shape) == 3:\n",
    "            batch_size, num_patches, embed_dims = inputs.shape\n",
    "            inputs_reshaped = tf.reshape(inputs, [-1, embed_dims])\n",
    "        else:\n",
    "            inputs_reshaped = inputs\n",
    "        \n",
    "        # Initialize centroids\n",
    "        centroids = tf.identity(self.centroids)\n",
    "\n",
    "        # K-means clustering\n",
    "        for i in range(self.max_iters):\n",
    "            # Compute distances and assign clusters\n",
    "            distances = tf.reduce_sum(tf.square(tf.expand_dims(inputs_reshaped, axis=1) - tf.expand_dims(centroids, axis=0)), axis=2)\n",
    "            cluster_assignments = tf.argmin(distances, axis=1)\n",
    "\n",
    "            # Update centroids\n",
    "            for j in range(self.n_clusters):\n",
    "                mask = tf.equal(cluster_assignments, j)\n",
    "                mask = tf.cast(mask, tf.float32)\n",
    "                count = tf.reduce_sum(mask)\n",
    "                count = tf.maximum(count, 1.0)  # Avoid division by zero\n",
    "                new_centroid = tf.reduce_sum(inputs_reshaped * tf.expand_dims(mask, axis=1), axis=0) / count\n",
    "                centroids = tf.tensor_scatter_nd_update(centroids, [[j]], [new_centroid])\n",
    "\n",
    "        self.centroids.assign(centroids)\n",
    "\n",
    "        # Reshape cluster assignments back to original shape if necessary\n",
    "        if len(inputs.shape) == 3:\n",
    "            cluster_assignments = tf.reshape(cluster_assignments, [-1, num_patches])\n",
    "        \n",
    "        return cluster_assignments\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(KMeansLayer, self).get_config()\n",
    "        config.update({\n",
    "            \"n_clusters\": self.n_clusters,\n",
    "            \"max_iters\": self.max_iters,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be48bd4e",
   "metadata": {
    "papermill": {
     "duration": 0.030965,
     "end_time": "2024-11-08T06:27:08.032478",
     "exception": false,
     "start_time": "2024-11-08T06:27:08.001513",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "> Unsupervised Segmentation, differentiable!\n",
    "\n",
    "- high pass filter\n",
    "- UnSupSeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3f2301f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T06:27:08.095814Z",
     "iopub.status.busy": "2024-11-08T06:27:08.095529Z",
     "iopub.status.idle": "2024-11-08T06:27:08.106393Z",
     "shell.execute_reply": "2024-11-08T06:27:08.105501Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.044718,
     "end_time": "2024-11-08T06:27:08.108231",
     "exception": false,
     "start_time": "2024-11-08T06:27:08.063513",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HighPassFilterLayer(layers.Layer):\n",
    "    def __init__(self, r=30, **kwargs):\n",
    "        super(HighPassFilterLayer, self).__init__(**kwargs)\n",
    "        self.r = r  # Radius for the low-frequency block to suppress\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a high-pass filter mask\n",
    "        rows, cols = input_shape[1], input_shape[2]\n",
    "        crow, ccol = rows // 2, cols // 2\n",
    "        mask = np.ones((rows, cols), dtype=np.float32)\n",
    "        mask[crow-self.r:crow+self.r, ccol-self.r:ccol+self.r] = 0\n",
    "        self.mask = tf.convert_to_tensor(mask, dtype=tf.complex64)\n",
    "        self.mask = tf.expand_dims(tf.expand_dims(self.mask, axis=0), axis=0)  # Add batch and channel dimensions\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Get the shape of the input image\n",
    "        batch_size, rows, cols, channels = tf.shape(inputs)[0], tf.shape(inputs)[1], tf.shape(inputs)[2], tf.shape(inputs)[3]\n",
    "\n",
    "        # Convert image to frequency domain\n",
    "        inputs = tf.cast(inputs, tf.complex64)\n",
    "        freq_domain = tf.signal.fft2d(tf.signal.fftshift(tf.transpose(inputs, perm=[0, 3, 1, 2])))\n",
    "\n",
    "        # Apply the mask to each image in the batch\n",
    "        mask = tf.tile(self.mask, [batch_size, channels, 1, 1])  # Repeat mask for each image and channel in the batch\n",
    "        filtered_freq_domain = freq_domain * mask\n",
    "\n",
    "        # Convert back to spatial domain\n",
    "        high_freq_image = tf.signal.ifft2d(filtered_freq_domain)\n",
    "        high_freq_image = tf.signal.ifftshift(tf.abs(high_freq_image))\n",
    "        high_freq_image = tf.transpose(high_freq_image, perm=[0, 2, 3, 1])  # Reshape back to original\n",
    "\n",
    "        return tf.cast(high_freq_image, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e63eaaf0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T06:27:08.174136Z",
     "iopub.status.busy": "2024-11-08T06:27:08.173811Z",
     "iopub.status.idle": "2024-11-08T06:27:08.205716Z",
     "shell.execute_reply": "2024-11-08T06:27:08.204814Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.068096,
     "end_time": "2024-11-08T06:27:08.207563",
     "exception": false,
     "start_time": "2024-11-08T06:27:08.139467",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MixedUnsupSeg(keras.Model):\n",
    "    def __init__(self, feature_extractor, q = 10, mu = 1.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.q = q\n",
    "        self.mu = mu\n",
    "        self.cluster_layer = layers.Dense(q, activation=None)\n",
    "        self.norm_layer = layers.BatchNormalization()\n",
    "        self.vic_loss = VICRegLoss()\n",
    "        self.barlow_loss = BarlowLoss()\n",
    "    def get_env_config(self):\n",
    "        return {\"feature_extractor_name\" : self.feature_extractor.name,\n",
    "               \"SSL_method\" : \"MixedUnsupSeg\",\n",
    "                \"Q(n_cluster)\" : self.q,\n",
    "                \"Mu(continuity loss weight)\" : self.mu,\n",
    "               }\n",
    "\n",
    "    def call(self, inputs):\n",
    "        high_freq_image = HighPassFilterLayer()(inputs)\n",
    "        token_highfreq, features_highfreq, w_ = self.feature_extractor(high_freq_image)\n",
    "        token_original, features_original, w_ = self.feature_extractor(inputs)\n",
    "        \n",
    "        features = ops.concatenate([features_original, features_highfreq],\n",
    "                                  axis = -1)\n",
    "        \n",
    "        responses = self.cluster_layer(features)\n",
    "        normalized_responses = self.norm_layer(responses)\n",
    "        cluster_labels = tf.argmax(normalized_responses, axis=-1)\n",
    "        return normalized_responses, cluster_labels\n",
    "\n",
    "    def compute_seg_loss(self, normalized_responses, cluster_labels):\n",
    "        feature_similarity_loss = tf.reduce_mean(\n",
    "            tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits=normalized_responses, labels=cluster_labels\n",
    "            )\n",
    "        )\n",
    "        spatial_continuity_loss = self.compute_spatial_continuity_loss(normalized_responses)\n",
    "        total_loss = feature_similarity_loss + self.mu * spatial_continuity_loss\n",
    "        return total_loss\n",
    "\n",
    "    def compute_spatial_continuity_loss(self, responses):\n",
    "        batch_size, n_patches, dims = ops.shape(responses)\n",
    "        p = tf.sqrt(tf.cast(n_patches, tf.float32))\n",
    "        p = ops.cast(p, \"int32\")\n",
    "        responses = ops.reshape(responses, [-1, p, p, dims])\n",
    "        diff_x = tf.reduce_sum(tf.abs(responses[:, 1:, :, :] - responses[:, :-1, :, :]))\n",
    "        diff_y = tf.reduce_sum(tf.abs(responses[:, :, 1:, :] - responses[:, :, :-1, :]))\n",
    "        continuity_loss = diff_x + diff_y\n",
    "        return continuity_loss\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        # input dataset : masked MIM dataset\n",
    "        mask_indices, mask_image, image, original_image = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            normalized_responses, cluster_labels = self(original_image, training=True)\n",
    "            patchwise_loss = self.compute_seg_loss(normalized_responses, cluster_labels)\n",
    "            \n",
    "            cls_mask, _, _ = self.feature_extractor(mask_image)\n",
    "            cls_image, _, _ = self.feature_extractor(image)\n",
    "            cls_original, _, _ = self.feature_extractor(original_image)\n",
    "            \n",
    "            vic_loss = ops.mean([self.vic_loss(cls_mask, cls_image), self.vic_loss(cls_mask, cls_original),\n",
    "                                self.vic_loss(cls_image, cls_original),\n",
    "                                self.vic_loss(cls_image, cls_mask), self.vic_loss(cls_original, cls_mask),\n",
    "                                self.vic_loss(cls_original, cls_image),\n",
    "                                ]\n",
    "                               )\n",
    "            barlow_loss = ops.mean([self.barlow_loss(cls_mask, cls_image), self.barlow_loss(cls_mask, cls_original),\n",
    "                                self.barlow_loss(cls_image, cls_original),\n",
    "                                   self.barlow_loss(cls_image, cls_mask), self.barlow_loss(cls_original, cls_mask),\n",
    "                                self.barlow_loss(cls_original, cls_image)\n",
    "                                   ]\n",
    "                                  )\n",
    "            loss = 0.01*patchwise_loss + vic_loss + barlow_loss\n",
    "            \n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        return {\"loss\": loss,\n",
    "               \"UnsupSegLoss\" : patchwise_loss,\n",
    "               \"Tokenwise_VIC_loss\" : vic_loss,\n",
    "               \"Tokenwise_Barlow_loss\" : barlow_loss\n",
    "               }\n",
    "    def test_step(self, data):\n",
    "        # input dataset : masked MIM dataset\n",
    "        mask_indices, mask_image, image, original_image = data\n",
    "        if True:\n",
    "            normalized_responses, cluster_labels = self(original_image, training=False)\n",
    "            patchwise_loss = self.compute_seg_loss(normalized_responses, cluster_labels)\n",
    "            \n",
    "            cls_mask, _, _ = self.feature_extractor(mask_image)\n",
    "            cls_image, _, _ = self.feature_extractor(image)\n",
    "            cls_original, _, _ = self.feature_extractor(original_image)\n",
    "            \n",
    "            vic_loss = ops.mean([self.vic_loss(cls_mask, cls_image), self.vic_loss(cls_mask, cls_original),\n",
    "                                self.vic_loss(cls_image, cls_original),\n",
    "                                self.vic_loss(cls_image, cls_mask), self.vic_loss(cls_original, cls_mask),\n",
    "                                self.vic_loss(cls_original, cls_image),\n",
    "                                ]\n",
    "                               )\n",
    "            barlow_loss = ops.mean([self.barlow_loss(cls_mask, cls_image), self.barlow_loss(cls_mask, cls_original),\n",
    "                                self.barlow_loss(cls_image, cls_original),\n",
    "                                   self.barlow_loss(cls_image, cls_mask), self.barlow_loss(cls_original, cls_mask),\n",
    "                                self.barlow_loss(cls_original, cls_image)\n",
    "                                   ]\n",
    "                                  )\n",
    "            loss = 0.01*patchwise_loss + vic_loss + barlow_loss\n",
    "        return {\"loss\": loss,\n",
    "               \"UnsupSegLoss\" : patchwise_loss,\n",
    "               \"Tokenwise_VIC_loss\" : vic_loss,\n",
    "               \"Tokenwise_Barlow_loss\" : barlow_loss\n",
    "               }\n",
    "    def get_segments(self, image):\n",
    "        image = ops.cast(image, \"float32\")\n",
    "        if len(ops.shape(image)) == 3:\n",
    "            image = image[tf.newaxis, ...]\n",
    "        normalized_responses, cluster_labels = self(image)\n",
    "        res = ops.shape(image)[1]\n",
    "        \n",
    "        n_patches = tf.shape(cluster_labels)[1]\n",
    "        patch_size = int(tf.sqrt(float(n_patches)))\n",
    "        batch_size = ops.shape(cluster_labels)[0]\n",
    "        cluster_labels = tf.reshape(cluster_labels, (batch_size, patch_size, patch_size))\n",
    "        colors = plt.cm.get_cmap('viridis', self.q)\n",
    "        heatmap = colors(cluster_labels / (self.q - 1))\n",
    "        heatmap = heatmap[..., :3]\n",
    "        \n",
    "        heatmap = [keras.utils.array_to_img(h) for h in heatmap]\n",
    "        heatmap = [h.resize((res,res)) for h in heatmap]\n",
    "        heatmap = [keras.utils.img_to_array(h) for h in heatmap]\n",
    "        heatmap = tf.convert_to_tensor(heatmap)\n",
    "        \n",
    "        heatmap = (heatmap-ops.min(heatmap))/(ops.max(heatmap) - ops.min(heatmap) + 1e-4)\n",
    "        heatmap *= 255.0\n",
    "        image = (image - ops.min(image)) / (ops.max(image) - ops.min(image) + 1e-4)\n",
    "        image *= 255.0\n",
    "        \n",
    "        sup_image = image * 0.5 + heatmap * 0.5\n",
    "        return ops.cast(heatmap, \"uint8\"), ops.cast(sup_image, \"uint8\")\n",
    "\n",
    "## Instantiate and compile the model\n",
    "#encoder_model = get_metaformer(\"gMLP\", res = 256, grayscale = False, \n",
    "#                               att_depth = 6, att_heads = 8, \n",
    "#                               att_dims = 8 * 64, \n",
    "#                               embed_dims = 8 * 64, patch_size = 32,\n",
    "#                              register_tokens = 4,\n",
    "#                               pretrained_encoder = None, return_patches = True\n",
    "#                              )\n",
    "#unsupervised_model = UnsupervisedSegmentation(encoder_model, 10, 0.1)\n",
    "#unsupervised_model.compile(optimizer=keras.optimizers.Adam())\n",
    "## Prepare dummy dataset (replace this with actual image data)\n",
    "#train_images = tf.random.normal([100,256,256,3])\n",
    "#train_dataset = tf.data.Dataset.from_tensor_slices(train_images).batch(25)\n",
    "## Train the model\n",
    "#history = unsupervised_model.fit(train_dataset, epochs=5, steps_per_epoch = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8430ef7f",
   "metadata": {
    "papermill": {
     "duration": 0.031527,
     "end_time": "2024-11-08T06:27:08.271135",
     "exception": false,
     "start_time": "2024-11-08T06:27:08.239608",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# QNCLR\n",
    "- onenote \"New ssl model\" 참조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d328abcf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T06:27:08.336361Z",
     "iopub.status.busy": "2024-11-08T06:27:08.335626Z",
     "iopub.status.idle": "2024-11-08T06:27:08.343013Z",
     "shell.execute_reply": "2024-11-08T06:27:08.342071Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.042767,
     "end_time": "2024-11-08T06:27:08.344919",
     "exception": false,
     "start_time": "2024-11-08T06:27:08.302152",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_last_layers_by_resolution(model, compression_rate = [8,16,32]):\n",
    "    input_shape = model.input_shape[1] \n",
    "    target_resolutions = [input_shape//c for c in compression_rate]\n",
    "    layers_by_resolution = {res: [] for res in target_resolutions}\n",
    "    last_layers = {}\n",
    "\n",
    "    for layer in model.layers:\n",
    "        if len(ops.shape(layer.output)) == 4:\n",
    "            output_resolution = ops.shape(layer.output)[1]  # 출력 특성 맵의 해상도\n",
    "            \n",
    "            if output_resolution in target_resolutions:\n",
    "                layers_by_resolution[output_resolution].append(layer.name)\n",
    "\n",
    "    for resolution in target_resolutions:\n",
    "        if layers_by_resolution[resolution]:\n",
    "            last_layers[resolution] = layers_by_resolution[resolution][-1]\n",
    "\n",
    "    return list(last_layers.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ab118ac1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T06:27:08.410471Z",
     "iopub.status.busy": "2024-11-08T06:27:08.410170Z",
     "iopub.status.idle": "2024-11-08T06:27:08.426424Z",
     "shell.execute_reply": "2024-11-08T06:27:08.425544Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.052217,
     "end_time": "2024-11-08T06:27:08.428313",
     "exception": false,
     "start_time": "2024-11-08T06:27:08.376096",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_encdec_model(encoder, res, q_size = 9, compression_rate = [8,16,32],\n",
    "                    att_heads = 8, att_dims = 512,\n",
    "                    encoder_trainable = False):\n",
    "    \n",
    "    att_depth = len(compression_rate) ; compressed_layers = get_last_layers_by_resolution(encoder, compression_rate)\n",
    "    \n",
    "    encoder.trainable = encoder_trainable\n",
    "    decomposed_encoder = keras.Model(encoder.input,\n",
    "                                    [encoder.get_layer(l).output for l in compressed_layers])\n",
    "    model_name = f\"{encoder.name}_EncoderDecoder_withQformer{q_size}\"\n",
    "    image_input = Input([res,res,3], name = \"ImageInputRGB\")\n",
    "    compressed_layers_tensors = decomposed_encoder(image_input) \n",
    "    final_feature_map = compressed_layers_tensors[-1]\n",
    "    batch_size, w, h, dims = ops.shape(final_feature_map)\n",
    "    upsampling_loops = ops.cast(np.log2(res/w), \"int32\")\n",
    "    \n",
    "    q_token = ops.zeros_like(keras.layers.GlobalAveragePooling2D(keepdims = True)(final_feature_map)) #batch, 1, dims\n",
    "    q_token = q_token[:, 0, ...]\n",
    "    q_token = Dense(units = att_dims, activation = \"gelu\", name = \"QueryProjection\")(q_token)\n",
    "    q_token = ops.tile(q_token, [1, q_size, 1]) #batch, q_size, dims\n",
    "    \n",
    "    for idx, layer in enumerate(compressed_layers_tensors):\n",
    "        mid_tensor = layer\n",
    "        _, w_, h_, dims_ = ops.shape(mid_tensor)\n",
    "        mid_tensor = ops.reshape(mid_tensor, [-1, w_*h_, dims_])\n",
    "        print(q_token.shape, mid_tensor.shape)\n",
    "        mid_tensor = Dense(units = att_dims, activation = 'gelu', name = f\"MiddleTensorProjection{idx}\")(mid_tensor)\n",
    "        q_token, qformer_attention_weight = MultiHeadAttention(att_heads, att_dims, name = f\"MHA_CrossResolution{idx}\")(query = q_token, \n",
    "                                                                                                          #key = mid_tensor, \n",
    "                                                                                                          value = mid_tensor,\n",
    "                                                                                                          return_attention_scores = True)\n",
    "        \n",
    "    \n",
    "    rep_vector = keras.layers.GlobalAveragePooling1D(name = \"Representation_vector\")(q_token)\n",
    "\n",
    "    ##decoder phase\n",
    "    for idx in range(upsampling_loops):\n",
    "        # conv2transpose -> crossattention bw q_token\n",
    "        if idx == 0:\n",
    "            target_dim = 128\n",
    "        elif idx == 1:\n",
    "            target_dim = 64\n",
    "        else:\n",
    "            target_dim = 32\n",
    "        final_feature_map = keras.layers.Conv2DTranspose(target_dim, 2, 2, name = f\"MiniDecoder_Upsample{idx}\")(final_feature_map)\n",
    "        q_token = keras.layers.Dense(units = target_dim, name = f\"DownsampleQ{idx}\")(q_token)\n",
    "        \n",
    "        if idx in [0,1]:\n",
    "            _, w_, h_, dims_ = ops.shape(final_feature_map) ; final_feature_map = ops.reshape(final_feature_map, [-1, w_*h_, dims_])\n",
    "            print(\"Decoder :\", final_feature_map.shape, q_token.shape)\n",
    "            fmap_ = MultiHeadAttention(4, 64, name = f\"MiniDecoder_CrossMHA{idx}\")(final_feature_map, q_token)\n",
    "            final_feature_map = ops.reshape(final_feature_map, [-1, w_, h_, dims_])\n",
    "        else:\n",
    "            pass\n",
    "        final_feature_map = keras.layers.LayerNormalization(name = f\"MiniDecoder_preLN{idx}\")(final_feature_map)\n",
    "        \n",
    "    final_feature_map = Dense(units = 3, activation = \"sigmoid\", name = \"RGBRegressor\")(final_feature_map)\n",
    "    final_feature_map = 255.0 * final_feature_map\n",
    "    model = keras.Model(image_input, [rep_vector, final_feature_map, qformer_attention_weight],\n",
    "                       name = model_name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b1682e34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T06:27:08.497889Z",
     "iopub.status.busy": "2024-11-08T06:27:08.497533Z",
     "iopub.status.idle": "2024-11-08T06:27:08.539659Z",
     "shell.execute_reply": "2024-11-08T06:27:08.538582Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.082642,
     "end_time": "2024-11-08T06:27:08.542237",
     "exception": false,
     "start_time": "2024-11-08T06:27:08.459595",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# original image, aug image, masked image in\n",
    "# f_origin, f_aug, f_mim out / fmap_origin, fmap_aug, fmap_mim out\n",
    "# vic_loss = vic(f_origin, f_aug) + vic(f_origin, f_mim)\n",
    "# contrastive_loss = nclr(f_origin, f_aug) + nclr(f_origin, f_mim) \n",
    "# mim_loss = mim_loss(original_image, fmap_origin) + mim_loss(original_image, fmap_mim)\n",
    "\n",
    "class QNCLR(keras.Model): #Neighbor Contrastive LeaRning + Q-former\n",
    "    def __init__(self, feature_extractor, embed_dims,\n",
    "                 subtype = \"nnclr\", #nnclr, snclr\n",
    "                 q_size = 2**15, t = 0.1,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dims = embed_dims\n",
    "        self.subtype = subtype #subtype에 따라 nearest neighbor 전략 or soft neighbor 전략 달라짐.\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.t = t\n",
    "        self.q_size = q_size\n",
    "        self.feature_q = keras.Variable(\n",
    "            keras.utils.normalize(\n",
    "                keras.random.normal(shape=(self.q_size, self.embed_dims)), #q_size, embed_dims shape matrix : FIFO로 update해야 함\n",
    "                axis=1,\n",
    "                order=2,\n",
    "            ),\n",
    "            trainable=False, dtype = \"float32\"\n",
    "        )\n",
    "        self.compute_vic = VicReg()\n",
    "\n",
    "    def get_env_config(self):\n",
    "        return {\"feature_extractor_name\" : self.feature_extractor.name,\n",
    "               \"use_mim\" : True,\n",
    "               \"SSL_method\" : f\"Q_NCLR_{self.subtype}\",\n",
    "               \"Queue size\" : self.q_size,\n",
    "               \"temperature\" : self.t,\n",
    "                \"MIM mix\" : self.use_mim\n",
    "               }\n",
    "    def call(self, images, training = False):\n",
    "        mask_sequence, mask_image, image, original_image = images\n",
    "        images = [original_image, image, mask_image]\n",
    "        batch_size = ops.shape(images[0])[0] ; n_augs = len(images)\n",
    "        images = keras.ops.concatenate(images, axis = 0)\n",
    "        print(\"model call for test, total input image shape :\", ops.shape(images))\n",
    "        # images : batch_size * n_augs, res, res, c\n",
    "        if (isinstance(self.feature_extractor, ViT_rollout)) or (len(self.feature_extractor.outputs) == 3):\n",
    "            cls_tokens, patches, attention_weights = self.feature_extractor(images)\n",
    "        elif len(self.feature_extractor.outputs) == 2:\n",
    "            cls_tokens, attention_weights = self.feature_extractor(images)\n",
    "\n",
    "        if training or isinstance(images, list):\n",
    "            cls_tokens = ops.split(cls_tokens, n_augs, axis = 0) #[f_original, f_aug, f_mim]\n",
    "            patches = ops.split(patches, n_augs, axis = 0) #recon_image_original, recon_image_aug, recon_image_mim\n",
    "            try:\n",
    "                attention_weights = ops.split(attention_weights, n_augs, axis = 0)\n",
    "            except:\n",
    "                attention_weights = attention_weights[\"final_layer_weight\"]\n",
    "                attention_weights = ops.split(attention_weights, n_augs, axis = 0)\n",
    "            \n",
    "        if patches is not None:\n",
    "            return cls_tokens, patches, attention_weights\n",
    "        else:\n",
    "            return cls_tokens, attention_weights\n",
    "        \n",
    "    def get_neighbor(self, projections): \n",
    "        p = projections\n",
    "        cos_sim = ops.matmul(p, ops.transpose(self.feature_q)\n",
    "                            ) # batch, q_size\n",
    "        if self.subtype in ['nnclr', \"NNCLR\", 'hard']:\n",
    "            support_f = ops.take(self.feature_q,\n",
    "                                 ops.argmax(cos_sim, axis = 1), # <- q_size 개 중 가장 similarity가 높은 batch개만 고려\n",
    "                                 axis = 0)\n",
    "            return p + ops.stop_gradient(support_f-p) # <- NNCLR: support set 중 가장 similarity가 높은 support data만 고려\n",
    "        elif self.subtype in ['snclr', \"SNCLR\", 'soft']:\n",
    "            # softmax with temperature + 가중평균\n",
    "            w = ops.softmax(cos_sim/self.t) # batch, q size\n",
    "            support_f = ops.matmul(w, self.feature_q) ; del w #batch, embed_dims\n",
    "            return p + ops.stop_gradient(support_f - p)\n",
    "            \n",
    "    #########loss 구현부터######\n",
    "    # vic_loss = vic(f_origin, f_aug) + vic(f_origin, f_mim)\n",
    "    # contrastive_loss = nclr(f_origin, f_aug) + nclr(f_origin, f_mim) \n",
    "    # mim_loss = mim_loss(original_image, fmap_origin) + mim_loss(original_image, fmap_mim)\n",
    "    def compute_mim_loss(self, original_image, mim_recon):\n",
    "        loss = keras.losses.Huber(reduction = None)(original_image, mim_recon)\n",
    "        return ops.mean(loss)\n",
    "    \n",
    "    def compute_vic_loss(self, projections):\n",
    "        #projections : f_original, f_aug, f_mim\n",
    "        f_original, f_aug, f_mim = projections\n",
    "        vic_loss = ops.sum(self.compute_vic(f_original, f_aug) + self.compute_vic(f_original, f_mim))\n",
    "        return ops.mean(vic_loss)\n",
    "\n",
    "    def compute_nclr_loss(self, projections, training = True):\n",
    "        f_original, f_aug, f_mim = projections\n",
    "        f_original, f_aug, f_mim = ops.normalize(f_original), ops.normalize(f_aug), ops.normalize(f_mim)\n",
    "        # 각각 batch, embed_dims shape tensor\n",
    "        f_original_n, f_aug_n, f_mim_n = self.get_neighbor(f_original), self.get_neighbor(f_aug), self.get_neighbor(f_mim)\n",
    "        sim_matrix_1_1 = ops.matmul(f_original, ops.transpose(f_aug_n)) / self.t\n",
    "        sim_matrix_1_2 = ops.matmul(f_original_n, ops.transpose(f_aug)) / self.t\n",
    "        sim_matrix_2_1 = ops.matmul(f_original, ops.transpose(f_mim_n)) / self.t\n",
    "        sim_matrix_2_2 = ops.matmul(f_original_n, ops.transpose(f_mim)) / self.t\n",
    "        batch_size = ops.shape(f_original)[0]\n",
    "        pseudo_label = ops.arange(ops.shape(f_original)[0])\n",
    "        \n",
    "        loss_1 = (keras.losses.sparse_categorical_crossentropy(pseudo_label, sim_matrix_1_1, from_logits = True) + \n",
    "                 keras.losses.sparse_categorical_crossentropy(pseudo_label, ops.transpose(sim_matrix_1_1), from_logits = True) + \n",
    "                 keras.losses.sparse_categorical_crossentropy(pseudo_label, sim_matrix_1_2, from_logits = True) + \n",
    "                 keras.losses.sparse_categorical_crossentropy(pseudo_label, ops.transpose(sim_matrix_1_2), from_logits = True))\n",
    "        \n",
    "        loss_2 = (keras.losses.sparse_categorical_crossentropy(pseudo_label, sim_matrix_2_1, from_logits = True) + \n",
    "                 keras.losses.sparse_categorical_crossentropy(pseudo_label, ops.transpose(sim_matrix_2_1), from_logits = True) + \n",
    "                 keras.losses.sparse_categorical_crossentropy(pseudo_label, sim_matrix_2_2, from_logits = True) + \n",
    "                 keras.losses.sparse_categorical_crossentropy(pseudo_label, ops.transpose(sim_matrix_2_2), from_logits = True))\n",
    "        if training:\n",
    "            self.feature_q.assign(\n",
    "                ops.concatenate([f_original, \n",
    "                                 self.feature_q[:-batch_size ]\n",
    "                                ], axis=0)\n",
    "            )\n",
    "        return ops.mean(loss_1 + loss_2)\n",
    "    \n",
    "    def train_step(self, dataset, training = True):\n",
    "        mask_sequence, mask_image, image, original_image = dataset\n",
    "        with tf.GradientTape() as tape:\n",
    "            cls_tokens, patches, attention_weights = self(dataset, training = training)\n",
    "            recon_image_original, recon_image_aug, recon_image_mim = patches\n",
    "            vic_loss = self.compute_vic_loss(cls_tokens)\n",
    "            nclr_loss = self.compute_nclr_loss(cls_tokens)\n",
    "            mim_loss = self.compute_mim_loss(original_image, recon_image_mim)\n",
    "            loss = nclr_loss + 0.1 * vic_loss + mim_loss * 0.1\n",
    "            #attention_weight_loss, entropy = attention_loss(attention_weights[0])\n",
    "            #loss += attention_weight_loss\n",
    "        dims = ops.shape(attention_weights[0])[-1]\n",
    "        w_transpose = ops.reshape(attention_weights[0], [-1, dims])\n",
    "        entropy = ops.mean(-ops.sum(w_transpose * ops.log(w_transpose + 1e-5), axis=-1))\n",
    "            \n",
    "        grads = tape.gradient(loss, \n",
    "                             self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, \n",
    "                                          self.trainable_weights)\n",
    "                                      )\n",
    "        \n",
    "        return {\"Total_loss\" : loss, \"MIM_loss\" : mim_loss,\n",
    "                #\"Attention_weight_loss\" : attention_weight_loss,\n",
    "                \"Attention_weight_entropy\" : entropy,\n",
    "               \"NCLR_loss\" : nclr_loss, \"VIC_loss\" : vic_loss\n",
    "               }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636b4a4e",
   "metadata": {
    "papermill": {
     "duration": 0.031383,
     "end_time": "2024-11-08T06:27:08.611454",
     "exception": false,
     "start_time": "2024-11-08T06:27:08.580071",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "----------------\n",
    "# Multimodal Contrastive method\n",
    "- CLIP\n",
    "- SigLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ae7fc4b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T06:27:08.685972Z",
     "iopub.status.busy": "2024-11-08T06:27:08.685583Z",
     "iopub.status.idle": "2024-11-08T06:27:08.713101Z",
     "shell.execute_reply": "2024-11-08T06:27:08.712067Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.072094,
     "end_time": "2024-11-08T06:27:08.715451",
     "exception": false,
     "start_time": "2024-11-08T06:27:08.643357",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CLIP(keras.Model): #original CLIP + CLIP surgery\n",
    "    def __init__(self, image_encoder, text_encoder,\n",
    "                embed_dims, pool_heads = 8, t = 2.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.image_encoder = image_encoder\n",
    "        self.text_encoder = text_encoder\n",
    "        self.embed_dims = embed_dims\n",
    "        self.t = t\n",
    "        self.mlp_image = keras.layers.Dense(units = embed_dims)\n",
    "        self.mlp_text = keras.layers.Dense(units = embed_dims)\n",
    "        self.pe_fn = keras_nlp.layers.RotaryEmbedding(name = \"RotatoryPE\")\n",
    "        self.image_pooler = AttentionPooling(pool_heads, embed_dims, name = \"ImageAttentionPooler\")\n",
    "        self.text_pooler = AttentionPooling(pool_heads, embed_dims, name = \"TextAttentionPooler\")\n",
    "        self.loss_tracker = keras.metrics.Mean(name = \"CLIP_loss\")\n",
    "    def get_env_config(self):\n",
    "        return {\"Image encoder name\": self.image_encoder.name,\n",
    "               \"Text encoder name\" : self.text_encoder.name,\n",
    "               \"embed_dims\" : self.embed_dims,\n",
    "               \"temperature\" : self.t,\n",
    "               \"SSL_method\" : \"CLIP_with_Attentional_Pooling\"}\n",
    "    def get_clip_loss(self, image_vector, text_vector):\n",
    "        batch_size = ops.shape(image_vector)[0]\n",
    "        image_vector = keras.utils.normalize(image_vector, axis = -1, order = 2)\n",
    "        text_vector = keras.utils.normalize(text_vector, axis = -1, order = 2)\n",
    "        \n",
    "        cor_mat = tf.einsum(\"ab, cb->ac\", image_vector, text_vector) / self.t\n",
    "        cor_mat = ops.reshape(cor_mat, [batch_size, batch_size])\n",
    "        pseudo_label = ops.zeros_like(cor_mat)\n",
    "        diags = tf.linalg.diag_part(pseudo_label) + 1.0\n",
    "        pseudo_label = tf.linalg.set_diag(pseudo_label, diags)\n",
    "        pseudo_label = pseudo_label - ops.ones_like(pseudo_label)\n",
    "        loss = 0.5*(ops.mean(keras.losses.CategoricalFocalCrossentropy(reduction = None, label_smoothing = 0.05)(pseudo_label, cor_mat)) + \n",
    "                    ops.mean(keras.losses.CategoricalFocalCrossentropy(reduction = None, label_smoothing = 0.05)(pseudo_label, ops.transpose(cor_mat)))\n",
    "                   )\n",
    "        return loss\n",
    "        \n",
    "    def train_step(self, dataset):\n",
    "        image, text = dataset\n",
    "        with tf.GradientTape() as tape: \n",
    "            image_feature, text_feature = self.image_encoder(image, training = True), self.text_encoder(text, training = True)\n",
    "            batch_size, w, h, dims = ops.shape(image_feature) ; batch_size = ops.shape(image_feature)[0]\n",
    "            \n",
    "            image_feature = ops.reshape(image_feature, [batch_size, w*h, dims])\n",
    "            image_feature = self.pe_fn(image_feature)\n",
    "            text_vector = keras.layers.GlobalAveragePooling1D()(text_feature)\n",
    "            if len(ops.shape(image_feature)) == 3:\n",
    "                image_vector = keras.layers.GlobalAveragePooling1D()(image_feature)\n",
    "            elif len(ops.shape(image_feature)) == 4:\n",
    "                image_vector = keras.layers.GlobalAveragePooling2D()(image_feature)\n",
    "\n",
    "            image_vector, text_vector = self.mlp_image(image_vector, training = True), self.mlp_text(text_vector, training = True)\n",
    "            image_vector, text_vector = self.image_pooler([image_vector, image_feature], training = True)[0], self.text_pooler([text_vector, text_feature], training = True)[0]\n",
    "            loss = self.get_clip_loss(image_vector, text_vector)\n",
    "        \n",
    "        encoder_weights = self.image_encoder.trainable_weights + self.text_encoder.trainable_weights\n",
    "        mlp_weights = self.mlp_image.trainable_weights + self.mlp_text.trainable_weights\n",
    "        pool_weights = self.image_pooler.trainable_weights + self.text_pooler.trainable_weights\n",
    "        trainable_weights = encoder_weights + mlp_weights + pool_weights + self.pe_fn.trainable_weights\n",
    "        \n",
    "        grads = tape.gradient(loss, trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, trainable_weights))\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {self.loss_tracker.name : self.loss_tracker.result()}\n",
    "    \n",
    "    def test_step(self, dataset):\n",
    "        image, text = dataset\n",
    "        image_feature, text_feature = self.image_encoder(image, training = False), self.text_encoder(text, training = False)\n",
    "        batch_size, w, h, dims = ops.shape(image_feature) ; batch_size = ops.shape(image_feature)[0]\n",
    "            \n",
    "        image_feature = ops.reshape(image_feature, [batch_size, w*h, dims])\n",
    "        image_feature = self.pe_fn(image_feature)\n",
    "        \n",
    "        text_vector = keras.layers.GlobalAveragePooling1D()(text_feature)\n",
    "        if len(ops.shape(image_feature)) == 3:\n",
    "            image_vector = keras.layers.GlobalAveragePooling1D()(image_feature)\n",
    "        elif len(ops.shape(image_feature)) == 4:\n",
    "            image_vector = keras.layers.GlobalAveragePooling2D()(image_feature)\n",
    "\n",
    "        image_vector, text_vector = self.mlp_image(image_vector, training = False), self.mlp_text(text_vector, training = False)\n",
    "        image_vector, text_vector = self.image_pooler([image_vector, image_feature], training = False)[0], self.text_pooler([text_vector, text_feature], training = False)[0]\n",
    "        loss = self.get_clip_loss(image_vector, text_vector)\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {self.loss_tracker.name : self.loss_tracker.result()}\n",
    "    def call(self, dataset):\n",
    "        return self.test_step(dataset)\n",
    "    def get_full_model(self):\n",
    "        inputs = self.image_encoder.inputs\n",
    "        feature = self.image_encoder.output\n",
    "        if len(ops.shape(feature)) == 4:\n",
    "            batch_size, w, h, dims = ops.shape(feature)\n",
    "            batch_size = ops.shape(feature)[0]\n",
    "            feature = ops.reshape(feature, [-1, w*h, dims])\n",
    "        feature = self.pe_fn(feature)\n",
    "        image_vector = keras.layers.GlobalAveragePooling1D()(feature)\n",
    "        z_image = self.mlp_image(image_vector)\n",
    "        outputs = self.image_pooler([z_image, feature])\n",
    "        return keras.Model(inputs, outputs,\n",
    "                          name = f\"FullModel_{self.image_encoder.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3109059d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T06:27:08.788768Z",
     "iopub.status.busy": "2024-11-08T06:27:08.788349Z",
     "iopub.status.idle": "2024-11-08T06:27:08.821735Z",
     "shell.execute_reply": "2024-11-08T06:27:08.820658Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.069512,
     "end_time": "2024-11-08T06:27:08.824752",
     "exception": false,
     "start_time": "2024-11-08T06:27:08.755240",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SigLIP(keras.Model):\n",
    "    def __init__(self, image_encoder, text_encoder,\n",
    "                embed_dims, pool_heads = 8, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.image_encoder = image_encoder\n",
    "        self.text_encoder = text_encoder\n",
    "        self.pe_fn = keras_nlp.layers.RotaryEmbedding(name = \"RotatoryPE\")\n",
    "        self.embed_dims = embed_dims\n",
    "        self.mlp_image = keras.layers.Dense(units = embed_dims)\n",
    "        self.mlp_text = keras.layers.Dense(units = embed_dims)\n",
    "        self.image_pooler = AttentionPooling(pool_heads, embed_dims, name = \"ImageAttentionPooler\")\n",
    "        self.text_pooler = AttentionPooling(pool_heads, embed_dims, name = \"TextAttentionPooler\")\n",
    "        self.loss_tracker = keras.metrics.Mean(name = \"SigLIP_loss\")\n",
    "        \n",
    "        self.t = tf.Variable(1.0, trainable = True, dtype = \"float32\")\n",
    "        self.b = tf.Variable(0.0, trainable = True, dtype = \"float32\")\n",
    "    def get_env_config(self):\n",
    "        return {\"Image encoder name\": self.image_encoder.name,\n",
    "               \"Text encoder name\" : self.text_encoder.name,\n",
    "               \"embed_dims\" : self.embed_dims,\n",
    "               \"SSL_method\" : \"SigLIP\"}\n",
    "    \n",
    "    def compute_loss(self, image_vector, text_vector):\n",
    "        batch_size = ops.shape(image_vector)[0]\n",
    "        z_img = keras.utils.normalize(image_vector, axis = -1, order = 2)\n",
    "        z_text = keras.utils.normalize(text_vector, axis = -1, order = 2)\n",
    "        z_img = tf.cast(z_img, tf.float32)\n",
    "        z_text = tf.cast(z_text, tf.float32)\n",
    "        cor_mat = ops.dot(z_img, ops.transpose(z_text)) * ops.exp(self.t) + self.b\n",
    "        cor_mat = ops.reshape(cor_mat, [batch_size, batch_size])\n",
    "        \n",
    "        pseudo_label = ops.zeros_like(cor_mat)\n",
    "        diags = tf.linalg.diag_part(pseudo_label) + 2.0\n",
    "        pseudo_label = tf.linalg.set_diag(pseudo_label, diags)\n",
    "        pseudo_label = pseudo_label - ops.ones_like(pseudo_label)\n",
    "        \n",
    "        loss = -ops.mean(ops.log_sigmoid(ops.multiply(cor_mat, pseudo_label)))\n",
    "        return loss\n",
    "        \n",
    "    def train_step(self, dataset):\n",
    "        image, text = dataset\n",
    "        with tf.GradientTape() as tape: \n",
    "            image_feature, text_feature = self.image_encoder(image, training = True), self.text_encoder(text, training = True)\n",
    "            batch_size, w, h, dims = ops.shape(image_feature) ; batch_size = ops.shape(image_feature)[0]\n",
    "            \n",
    "            image_feature = ops.reshape(image_feature, [batch_size, w*h, dims])\n",
    "            image_feature = self.pe_fn(image_feature)\n",
    "            \n",
    "            text_vector = keras.layers.GlobalAveragePooling1D()(text_feature)\n",
    "            if len(ops.shape(image_feature)) == 3:\n",
    "                image_vector = keras.layers.GlobalAveragePooling1D()(image_feature)\n",
    "            elif len(ops.shape(image_feature)) == 4:\n",
    "                image_vector = keras.layers.GlobalAveragePooling2D()(image_feature)\n",
    "\n",
    "            image_vector, text_vector = self.mlp_image(image_vector, training = True), self.mlp_text(text_vector, training = True)\n",
    "            image_vector, text_vector = self.image_pooler([image_vector, image_feature], training = True)[0], self.text_pooler([text_vector, text_feature], training = True)[0]\n",
    "            loss = 0.5*(self.compute_loss(image_vector, text_vector) + self.compute_loss(text_vector, image_vector))\n",
    "        \n",
    "        encoder_weights = self.image_encoder.trainable_weights + self.text_encoder.trainable_weights\n",
    "        mlp_weights = self.mlp_image.trainable_weights + self.mlp_text.trainable_weights\n",
    "        pool_weights = self.image_pooler.trainable_weights + self.text_pooler.trainable_weights\n",
    "        trainable_weights = encoder_weights + mlp_weights + pool_weights + [self.t, self.b] + self.pe_fn.trainable_weights\n",
    "        \n",
    "        grads = tape.gradient(loss, trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, trainable_weights))\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {self.loss_tracker.name : self.loss_tracker.result()}\n",
    "    \n",
    "    def test_step(self, dataset):\n",
    "        image, text = dataset\n",
    "        image_feature, text_feature = self.image_encoder(image, training = False), self.text_encoder(text, training = False)\n",
    "        batch_size, w, h, dims = ops.shape(image_feature) ; batch_size = ops.shape(image_feature)[0]\n",
    "            \n",
    "        image_feature = ops.reshape(image_feature, [batch_size, w*h, dims])\n",
    "        image_feature = self.pe_fn(image_feature)\n",
    "        text_vector = keras.layers.GlobalAveragePooling1D()(text_feature)\n",
    "        if len(ops.shape(image_feature)) == 3:\n",
    "            image_vector = keras.layers.GlobalAveragePooling1D()(image_feature)\n",
    "        elif len(ops.shape(image_feature)) == 4:\n",
    "            image_vector = keras.layers.GlobalAveragePooling2D()(image_feature)\n",
    "\n",
    "        image_vector, text_vector = self.mlp_image(image_vector, training = False), self.mlp_text(text_vector, training = False)\n",
    "        image_vector, text_vector = self.image_pooler([image_vector, image_feature], training = False)[0], self.text_pooler([text_vector, text_feature], training = False)[0]\n",
    "        loss = 0.5*(self.compute_loss(image_vector, text_vector) + self.compute_loss(text_vector, image_vector))\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {self.loss_tracker.name : self.loss_tracker.result()}\n",
    "    def call(self, dataset):\n",
    "        return self.test_step(dataset)\n",
    "    def get_full_model(self):\n",
    "        inputs = self.image_encoder.inputs\n",
    "        feature = self.image_encoder.output\n",
    "        if len(ops.shape(feature)) == 4:\n",
    "            batch_size, w, h, dims = ops.shape(feature)\n",
    "            batch_size = ops.shape(feature)[0]\n",
    "            feature = ops.reshape(feature, [-1, w*h, dims])\n",
    "        feature = self.pe_fn(feature)\n",
    "        \n",
    "        image_vector = keras.layers.GlobalAveragePooling1D()(feature)\n",
    "        z_image = self.mlp_image(image_vector)\n",
    "        outputs = self.image_pooler([z_image, feature])\n",
    "        return keras.Model(inputs, outputs,\n",
    "                          name = f\"FullModel_{self.image_encoder.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e3ad860f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T06:27:08.895294Z",
     "iopub.status.busy": "2024-11-08T06:27:08.894784Z",
     "iopub.status.idle": "2024-11-08T06:27:08.940660Z",
     "shell.execute_reply": "2024-11-08T06:27:08.939626Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.08172,
     "end_time": "2024-11-08T06:27:08.942831",
     "exception": false,
     "start_time": "2024-11-08T06:27:08.861111",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SPARC(keras.Model): \n",
    "    #Reference : https://arxiv.org/abs/2401.09865 \"Improving fine-grained understanding in image-text pre-training\"\n",
    "    def __init__(self, image_encoder, text_encoder,\n",
    "                embed_dims, pool_heads = 8, preprocessor = None,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.image_encoder = image_encoder\n",
    "        self.text_encoder = text_encoder\n",
    "        self.preprocessor = preprocessor\n",
    "        self.pe_fn = keras_nlp.layers.RotaryEmbedding(name = \"RotatoryPE\")\n",
    "        self.embed_dims = embed_dims\n",
    "        self.mlp_image = keras.layers.Dense(units = embed_dims)\n",
    "        self.mlp_text = keras.layers.Dense(units = embed_dims)\n",
    "        self.image_pooler = AttentionPooling(pool_heads, embed_dims, name = \"ImageAttentionPooler\")\n",
    "        self.text_pooler = AttentionPooling(pool_heads, embed_dims, name = \"TextAttentionPooler\")\n",
    "        self.ce_fn = keras.losses.CategoricalCrossentropy(from_logits=True, reduction = None)\n",
    "        self.loss_tracker = keras.metrics.Mean(name = \"SPARC_loss\")\n",
    "        self.global_loss_tracker = keras.metrics.Mean(name = \"SPARC_global_loss\")\n",
    "        self.local_loss_tracker = keras.metrics.Mean(name = \"SPARC_local_loss\")\n",
    "        \n",
    "        self.t = tf.Variable(1.0, trainable = True, dtype = \"float32\")\n",
    "        self.b = tf.Variable(0.0, trainable = True, dtype = \"float32\")\n",
    "        self.threshold = tf.Variable(0.5, trainable = True, dtype = \"float32\")\n",
    "        \n",
    "    def get_env_config(self):\n",
    "        return {\"Image encoder name\": self.image_encoder.name,\n",
    "               \"Text encoder name\" : self.text_encoder.name,\n",
    "               \"embed_dims\" : self.embed_dims,\n",
    "               \"SSL_method\" : \"SPARC\"}\n",
    "    \n",
    "    def compute_global_loss(self, image_vector, text_vector): #<- SigLIP\n",
    "        batch_size = ops.shape(image_vector)[0]\n",
    "        z_img = keras.utils.normalize(image_vector, axis = -1, order = 2)\n",
    "        z_text = keras.utils.normalize(text_vector, axis = -1, order = 2)\n",
    "        z_img = tf.cast(z_img, tf.float32)\n",
    "        z_text = tf.cast(z_text, tf.float32)\n",
    "        cor_mat = ops.dot(z_img, ops.transpose(z_text)) * ops.exp(self.t) + self.b\n",
    "        cor_mat = ops.reshape(cor_mat, [batch_size, batch_size])\n",
    "        \n",
    "        pseudo_label = ops.zeros_like(cor_mat)\n",
    "        diags = tf.linalg.diag_part(pseudo_label) + 2.0\n",
    "        pseudo_label = tf.linalg.set_diag(pseudo_label, diags)\n",
    "        pseudo_label = pseudo_label - ops.ones_like(pseudo_label)\n",
    "        \n",
    "        loss = -ops.mean(ops.log_sigmoid(ops.multiply(cor_mat, pseudo_label)))\n",
    "        loss = tf.cast(loss, tf.float32)\n",
    "        return loss\n",
    "    \n",
    "    def text_compare(self, text_label, text_logit, mask = None):\n",
    "        loss = self.ce_fn(text_label, text_logit)\n",
    "        loss = tf.cast(loss, tf.float32)\n",
    "        if mask == None:\n",
    "            return ops.mean(loss)\n",
    "        else:\n",
    "            loss *= tf.cast(mask, tf.float32)\n",
    "            loss = (ops.sum(loss))/(ops.sum(tf.cast(mask, tf.float32)) + 1e-4)\n",
    "            return loss\n",
    "        \n",
    "    def compute_loss(self, image_sequence, text_sequence, mask = None):\n",
    "        batch_size = ops.shape(image_sequence)[0]\n",
    "        _, token_len, dim_ = ops.shape(text_sequence)\n",
    "        if len(ops.shape(image_sequence)) == 3:\n",
    "            _, image_len, dim_ = ops.shape(image_sequence)\n",
    "        elif len(ops.shape(image_sequence)) == 4:\n",
    "            _, w, h, dim_ = ops.shape(image_sequence)\n",
    "            image_sequence = ops.reshape(image_sequence, [batch_size, w*h, dim_])\n",
    "            image_len = w*h\n",
    "            \n",
    "        image_sequence, text_sequence = self.mlp_image(image_sequence), self.mlp_text(text_sequence) \n",
    "        image_sequence = self.pe_fn(image_sequence)\n",
    "        # [batch, image_len, embed_dims] / [batch, token_len, embed_dims], respectively.\n",
    "        \n",
    "        z_image, att_weight = self.image_pooler([keras.layers.GlobalAveragePooling1D()(image_sequence),\n",
    "                                                     image_sequence])\n",
    "        z_text, att_weight_ = self.text_pooler([keras.layers.GlobalAveragePooling1D()(text_sequence),\n",
    "                                                     text_sequence])\n",
    "        #1. global alignment loss\n",
    "        global_loss = 0.5*(self.compute_global_loss(z_image, z_text) + self.compute_global_loss(z_text, z_image))\n",
    "        #2. Fine Grained local loss\n",
    "        \n",
    "        #a. get similarity matrix b/w text sequence and image sequence\n",
    "        sim_matrix = ops.einsum(\"atd, aid -> ati\", text_sequence, image_sequence) #batch, token_len, image_len\n",
    "        sim_matrix = (sim_matrix - ops.min(sim_matrix, axis = -1, keepdims = True)) / (1e-4 + ops.max(sim_matrix, axis = -1, keepdims = True) - ops.min(sim_matrix, axis = -1, keepdims = True))\n",
    "        sim_matrix = tf.cast(sim_matrix, tf.float32)\n",
    "        \n",
    "        sim_matrix = ops.clip(sim_matrix, self.threshold, 1e4)\n",
    "        attended_text_sequence = ops.einsum(\"ati, aid -> atd\", sim_matrix, image_sequence)\n",
    "        \n",
    "        text_logit = ops.einsum(\"atd, aqd -> atq\", \n",
    "                                keras.utils.normalize(text_sequence, axis = -1, order = 2), \n",
    "                                keras.utils.normalize(attended_text_sequence, axis = -1, order = 2))/self.t\n",
    "        \n",
    "        pseudo_text_label = ops.tile(ops.expand_dims(ops.eye(token_len), axis = 0),\n",
    "                                     [batch_size,1,1])\n",
    "        local_loss = self.text_compare(pseudo_text_label, text_logit, mask)\n",
    "        loss = 0.5*global_loss + 1.0*local_loss\n",
    "        return loss, global_loss, local_loss\n",
    "        \n",
    "    def train_step(self, dataset):\n",
    "        image, text = dataset\n",
    "        if self.preprocessor != None:\n",
    "            text_mask = self.preprocessor(text)[\"padding_mask\"]\n",
    "            text_mask = tf.cast(text_mask, dtype = tf.int32)\n",
    "        else:\n",
    "            text_mask = None\n",
    "        with tf.GradientTape() as tape: \n",
    "            image_feature, text_feature = self.image_encoder(image, training = True), self.text_encoder(text, training = True)\n",
    "            loss, global_loss, local_loss = self.compute_loss(image_feature, text_feature, text_mask)\n",
    "        \n",
    "        encoder_weights = self.image_encoder.trainable_weights + self.text_encoder.trainable_weights\n",
    "        mlp_weights = self.mlp_image.trainable_weights + self.mlp_text.trainable_weights\n",
    "        pool_weights = self.image_pooler.trainable_weights + self.text_pooler.trainable_weights\n",
    "        trainable_weights = encoder_weights + mlp_weights + pool_weights + [self.t, self.b] + self.pe_fn.trainable_weights\n",
    "        \n",
    "        grads = tape.gradient(loss, trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, trainable_weights))\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.global_loss_tracker.update_state(global_loss)\n",
    "        self.local_loss_tracker.update_state(local_loss)\n",
    "        return {self.loss_tracker.name : self.loss_tracker.result(),\n",
    "               self.global_loss_tracker.name : self.global_loss_tracker.result(),\n",
    "               self.local_loss_tracker.name : self.local_loss_tracker.result()\n",
    "               }\n",
    "    \n",
    "    def test_step(self, dataset):\n",
    "        image, text = dataset\n",
    "        if self.preprocessor != None:\n",
    "            text_mask = self.preprocessor(text)[\"padding_mask\"]\n",
    "            text_mask = tf.cast(text_mask, tf.int32)\n",
    "        else:\n",
    "            text_mask = None\n",
    "            \n",
    "        image_feature, text_feature = self.image_encoder(image, training = False), self.text_encoder(text, training = False)\n",
    "        loss, global_loss, local_loss = self.compute_loss(image_feature, text_feature, text_mask)\n",
    "        \n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.global_loss_tracker.update_state(global_loss)\n",
    "        self.local_loss_tracker.update_state(local_loss)\n",
    "        return {self.loss_tracker.name : self.loss_tracker.result(),\n",
    "               self.global_loss_tracker.name : self.global_loss_tracker.result(),\n",
    "               self.local_loss_tracker.name : self.local_loss_tracker.result()\n",
    "               }\n",
    "    \n",
    "    def call(self, dataset):\n",
    "        return self.test_step(dataset)\n",
    "    def get_full_model(self):\n",
    "        inputs = self.image_encoder.inputs\n",
    "        feature = self.image_encoder.output\n",
    "        feature = self.mlp_image(feature)\n",
    "        if len(ops.shape(feature)) == 4:\n",
    "            batch_size, w, h, dims = ops.shape(feature)\n",
    "            batch_size = ops.shape(feature)[0]\n",
    "            feature = ops.reshape(feature, [-1, w*h, dims])\n",
    "        feature = self.pe_fn(feature)\n",
    "        \n",
    "        image_vector = keras.layers.GlobalAveragePooling1D()(feature)\n",
    "        outputs = self.image_pooler([image_vector, feature])\n",
    "        return keras.Model(inputs, outputs,\n",
    "                          name = f\"FullModel_{self.image_encoder.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7faa649f",
   "metadata": {
    "papermill": {
     "duration": 0.031414,
     "end_time": "2024-11-08T06:27:09.008497",
     "exception": false,
     "start_time": "2024-11-08T06:27:08.977083",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "eb0516f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T06:27:09.073371Z",
     "iopub.status.busy": "2024-11-08T06:27:09.072865Z",
     "iopub.status.idle": "2024-11-08T06:27:09.095893Z",
     "shell.execute_reply": "2024-11-08T06:27:09.094963Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.058123,
     "end_time": "2024-11-08T06:27:09.097949",
     "exception": false,
     "start_time": "2024-11-08T06:27:09.039826",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def att_visualize(model, images, res, thresholding = True,\n",
    "                 impose_alpha = 0.5):\n",
    "    # Use jet colormap to colorize heatmap\n",
    "    try:\n",
    "        model = model.get_full_model()\n",
    "    except Exception as e:\n",
    "        print(\"Using Raw model with Att pooling\",\"\\n\",\"Possible error:\",\"\\n\",e)\n",
    "        pass\n",
    "    try:\n",
    "        outputs = model.predict_on_batch(images)\n",
    "        att_weights = outputs[-1]\n",
    "        if isinstance(att_weights, dict):\n",
    "            att_weights = att_weights[\"final_layer_weight\"]\n",
    "            att_weights_ = att_weights[:, :, 0, 1:]\n",
    "        else:\n",
    "            att_weights_ = att_weights[:, :, 0, :] #batch, heads, cls_token, w*h\n",
    "        _, heads, token_length = ops.shape(att_weights_) ; batch_size = ops.shape(att_weights_)[0]\n",
    "        token_length = tf.cast(token_length, tf.float32)\n",
    "        \n",
    "        w = tf.cast(tf.math.sqrt(token_length), tf.int32)\n",
    "        print(att_weights_.shape, w, heads)\n",
    "        heatmap = tf.reshape(att_weights_, [batch_size, heads, w, w])\n",
    "        M = tf.reduce_max(heatmap, axis = [2, 3], keepdims = True)\n",
    "        m = tf.reduce_min(heatmap, axis = [2, 3], keepdims = True)\n",
    "        heatmap = (heatmap - m) / (M-m + 1e-5)\n",
    "        if thresholding:\n",
    "            threshold = ops.median(heatmap, [2,3], keepdims = True)\n",
    "            heatmap = ops.where(heatmap < threshold, 0.0, heatmap)\n",
    "        else:\n",
    "            pass\n",
    "        heatmap = ops.reshape(heatmap, [-1, w, w])\n",
    "        heatmap = np.array(255.0*heatmap).astype(\"uint8\")\n",
    "        # Use jet colormap to colorize heatmap\n",
    "        cmap = mpl.colormaps[\"jet\"]\n",
    "        # Use RGB values of the colormap\n",
    "        _colors = cmap(np.arange(256))[:, :3]\n",
    "        heatmap = _colors[heatmap]\n",
    "        \n",
    "        heatmap = [keras.utils.array_to_img(h) for h in heatmap]\n",
    "        heatmap = [h.resize((res,res)) for h in heatmap]\n",
    "        heatmap = [keras.utils.img_to_array(h) for h in heatmap]\n",
    "        heatmap = np.array(heatmap)\n",
    "        heatmap = ops.reshape(heatmap, [batch_size, heads, res, res, 3])\n",
    "        merged_heatmap = ops.mean(heatmap, axis = 1) #batch, res, res, 3\n",
    "        merged_heatmap = (merged_heatmap - ops.min(merged_heatmap)) / (ops.max(merged_heatmap) - ops.min(merged_heatmap))\n",
    "        merged_heatmap *= 255.0\n",
    "        if ops.shape(images)[-1] == 1:\n",
    "            images = tf.image.grayscale_to_rgb(images)\n",
    "        imposed = ((1-impose_alpha)*tf.cast(images[:, tf.newaxis, ...], tf.float32)) + (impose_alpha*tf.cast(heatmap, tf.float32))\n",
    "        merged_imposed = impose_alpha * ops.cast(merged_heatmap, 'float32') + (1-impose_alpha) * ops.cast(images, 'float32')\n",
    "        imposed = tf.cast(imposed, tf.uint8)\n",
    "        merged_imposed = ops.cast(merged_imposed, \"uint8\")\n",
    "        tf.keras.backend.clear_session()\n",
    "        return imposed, merged_imposed\n",
    "    except Exception as e:\n",
    "        print(\"Error raised during visualization\",\"\\n\",e)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ee5517cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T06:27:09.162082Z",
     "iopub.status.busy": "2024-11-08T06:27:09.161681Z",
     "iopub.status.idle": "2024-11-08T06:27:09.183776Z",
     "shell.execute_reply": "2024-11-08T06:27:09.182958Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.056735,
     "end_time": "2024-11-08T06:27:09.185755",
     "exception": false,
     "start_time": "2024-11-08T06:27:09.129020",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def q_visualize(model, images, res, merge_type = \"mean\", \n",
    "                thresholding = True,\n",
    "                 impose_alpha = 0.5):\n",
    "    # Use jet colormap to colorize heatmap\n",
    "    try:\n",
    "        model = model.get_full_model()\n",
    "    except Exception as e:\n",
    "        print(\"Using Raw model with Att pooling\",\"\\n\",\"Possible error:\",\"\\n\",e)\n",
    "        pass\n",
    "    if True:\n",
    "        outputs = model.predict_on_batch(images)\n",
    "        att_weights = outputs[-1] #batch, heads, n_q, n_len\n",
    "        att_weights = (att_weights - ops.min(att_weights, axis = -1, keepdims = True)) / (ops.max(att_weights, axis = -1, keepdims = True) - ops.min(att_weights, axis = -1, keepdims = True) + 1e-4)\n",
    "        att_weights *= 255.0\n",
    "        if merge_type in [\"mean\", 'average']:\n",
    "            att_weights_ = ops.mean(att_weights, axis = 1)\n",
    "        elif merge_type == \"max\":\n",
    "            att_weights_ = ops.max(att_weights, axis = 1)\n",
    "        elif merge_type == \"min\":\n",
    "            att_weights_ = ops.min(att_weights, axis = 1)\n",
    "        else:\n",
    "            raise \"Merge type should be one of the following: mean, average, max, min\"\n",
    "        batch_size, n_q, token_length = ops.shape(att_weights_)\n",
    "        token_length = tf.cast(token_length, tf.float32)\n",
    "        \n",
    "        w = tf.cast(tf.math.sqrt(token_length), tf.int32)\n",
    "        #print(att_weights_.shape, w, heads)\n",
    "        heatmap = tf.reshape(att_weights_, [batch_size, n_q, w, w])\n",
    "        if thresholding:\n",
    "            threshold = ops.median(heatmap, [2,3], keepdims = True)\n",
    "            heatmap = ops.where(heatmap < threshold, 0.0, heatmap)\n",
    "        else:\n",
    "            pass\n",
    "        heatmap = ops.reshape(heatmap, [-1, w, w])\n",
    "        heatmap = np.array(heatmap).astype(\"uint8\")\n",
    "        # Use jet colormap to colorize heatmap\n",
    "        cmap = mpl.colormaps[\"jet\"]\n",
    "        # Use RGB values of the colormap\n",
    "        _colors = cmap(np.arange(256))[:, :3]\n",
    "        heatmap = _colors[heatmap]\n",
    "        \n",
    "        heatmap = [keras.utils.array_to_img(h) for h in heatmap]\n",
    "        heatmap = [h.resize((res,res)) for h in heatmap]\n",
    "        heatmap = [keras.utils.img_to_array(h) for h in heatmap]\n",
    "        heatmap = np.array(heatmap)\n",
    "        heatmap = ops.reshape(heatmap, [batch_size, n_q, res, res, 3])\n",
    "        merged_heatmap = ops.mean(heatmap, axis = 1) #batch, res, res, 3\n",
    "        merged_heatmap = (merged_heatmap - ops.min(merged_heatmap)) / (ops.max(merged_heatmap) - ops.min(merged_heatmap))\n",
    "        merged_heatmap *= 255.0\n",
    "        if ops.shape(images)[-1] == 1:\n",
    "            images = tf.image.grayscale_to_rgb(images)\n",
    "        imposed = ((1-impose_alpha)*tf.cast(images[:, tf.newaxis, ...], tf.float32)) + (impose_alpha*tf.cast(heatmap, tf.float32))\n",
    "        merged_imposed = impose_alpha * ops.cast(merged_heatmap, 'float32') + (1-impose_alpha) * ops.cast(images, 'float32')\n",
    "        imposed = tf.cast(imposed, tf.uint8)\n",
    "        merged_imposed = ops.cast(merged_imposed, \"uint8\")\n",
    "        \n",
    "        tf.keras.backend.clear_session()\n",
    "        return imposed, merged_imposed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1676b6b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T06:27:09.250184Z",
     "iopub.status.busy": "2024-11-08T06:27:09.249439Z",
     "iopub.status.idle": "2024-11-08T06:27:09.262410Z",
     "shell.execute_reply": "2024-11-08T06:27:09.261566Z"
    },
    "papermill": {
     "duration": 0.047438,
     "end_time": "2024-11-08T06:27:09.264416",
     "exception": false,
     "start_time": "2024-11-08T06:27:09.216978",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pca_patch_viz(model, image, alpha = 0.5):\n",
    "    if len(ops.shape(image)) == 3:\n",
    "        image = image[tf.newaxis, ...]\n",
    "    res = image.shape[1]\n",
    "    patches = model(image)[-2]\n",
    "    batch, n_patches, n_dim = patches.shape ; patch_res = ops.cast(ops.sqrt(ops.cast(n_patches, \"float32\")), \"int32\")\n",
    "    patches = ops.reshape(patches, [-1, n_dim])\n",
    "    pca = PCA(3)\n",
    "    patches_rgb = pca.fit_transform(patches, [-1, n_dim])\n",
    "    patches_rgb = ops.reshape(patches_rgb, [batch, patch_res,patch_res, 3])\n",
    "    #patches_rgb = keras.layers.Activation(\"relu\")(patches_rgb)\n",
    "    patches_rgb = (patches_rgb - ops.min(patches_rgb, axis = [1,2,3], keepdims = True)) / (ops.max(patches_rgb, axis = [1,2,3], keepdims = True) - ops.min(patches_rgb, axis = [1,2,3], keepdims = True))\n",
    "    heatmap = patches_rgb * 255.0 ; del patches_rgb, patches\n",
    "    \n",
    "    heatmap = [keras.utils.array_to_img(h) for h in heatmap]\n",
    "    heatmap = [h.resize((res,res)) for h in heatmap]\n",
    "    heatmap = [keras.utils.img_to_array(h) for h in heatmap] #resized heatmap\n",
    "    heatmap = tf.convert_to_tensor(heatmap)\n",
    "    imposed_image = (1-alpha) * ops.cast(image, \"float32\") + (alpha) * ops.cast(heatmap, \"float32\")\n",
    "    return heatmap, imposed_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae3fe3a",
   "metadata": {
    "papermill": {
     "duration": 0.031569,
     "end_time": "2024-11-08T06:27:09.327354",
     "exception": false,
     "start_time": "2024-11-08T06:27:09.295785",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "> attention weight merging and max-out\n",
    "- reference code in [here](https://www.kaggle.com/code/financekim/vit-visualize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ed7072fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T06:27:09.392377Z",
     "iopub.status.busy": "2024-11-08T06:27:09.391930Z",
     "iopub.status.idle": "2024-11-08T06:27:09.408868Z",
     "shell.execute_reply": "2024-11-08T06:27:09.407964Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.052386,
     "end_time": "2024-11-08T06:27:09.411289",
     "exception": false,
     "start_time": "2024-11-08T06:27:09.358903",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def att_visualize_merged(model, images, res, impose_alpha = 0.5, merge_type = \"max\", dropout_rate = 0.75):\n",
    "    # weight merging -> upscaling -> imposing\n",
    "    attention_weight = model(images)[-1]\n",
    "    if isinstance(attention_weight, dict):\n",
    "        attention_weight = attention_weight[\"final_layer_weight\"]\n",
    "        attention_weight = attention_weight[:, :, 0:1, 1:] #batch, heads, n_patches\n",
    "    if merge_type == 'max':\n",
    "        attention_heads_fused = ops.max(attention_weight, axis = 1)\n",
    "    elif merge_type == \"min\":\n",
    "        attention_heads_fused = ops.min(attention_weight, axis = 1)\n",
    "    elif merge_type in [\"mean\", \"average\"]:\n",
    "        attention_heads_fused = ops.mean(attention_weight, axis = 1)\n",
    "    flat = ops.reshape(attention_heads_fused, [attention_heads_fused.shape[0], -1])\n",
    "    _, indices = ops.top_k(-flat, k = int(dropout_rate * flat.shape[-1]), sorted = False)\n",
    "\n",
    "    mask = tf.ones_like(flat)\n",
    "    batch_size = ops.shape(flat)[0]\n",
    "    batch_indices = tf.repeat(tf.range(batch_size), repeats=indices.shape[1])\n",
    "    indices = tf.stack([batch_indices, tf.reshape(indices, [-1])], axis=1)\n",
    "\n",
    "    mask = tf.tensor_scatter_nd_update(mask, indices, tf.zeros(tf.shape(indices)[0], dtype=flat.dtype))\n",
    "    flat *= mask \n",
    "    attention_heads_fused = ops.reshape(flat, attention_heads_fused.shape)\n",
    "    cls_weight = attention_heads_fused/ops.sum(attention_heads_fused, axis = -1, keepdims = True)\n",
    "    \n",
    "    batch, _, token_len = cls_weight.shape\n",
    "    w = ops.sqrt(ops.cast(token_len, 'float32'))\n",
    "    att_w = ops.reshape(cls_weight, [batch,w,w])\n",
    "    att_w = (att_w - ops.min(att_w, axis = (1,2), keepdims = True))/(ops.max(att_w, axis = (1,2), keepdims = True) - ops.min(att_w, axis = (1,2), keepdims = True) + 1e-4)\n",
    "    colored = plt.cm.jet(att_w)\n",
    "    rgb_att_w = ops.cast(colored[:,:,:,:3]*255,\n",
    "                  \"uint8\")\n",
    "    heatmap = [keras.utils.array_to_img(h) for h in rgb_att_w]\n",
    "    heatmap = [h.resize((res,res)) for h in heatmap]\n",
    "    heatmap = [keras.utils.img_to_array(h) for h in heatmap]\n",
    "    heatmap = [ops.cast(h, \"uint8\") for h in heatmap]\n",
    "    \n",
    "    imposed = (1-impose_alpha)*ops.cast(images, \"float32\") + impose_alpha*ops.cast(heatmap, \"float32\")\n",
    "    imposed = ops.cast(imposed, \"uint8\")\n",
    "    return heatmap, imposed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a8e4000f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T06:27:09.479047Z",
     "iopub.status.busy": "2024-11-08T06:27:09.478448Z",
     "iopub.status.idle": "2024-11-08T06:27:09.501131Z",
     "shell.execute_reply": "2024-11-08T06:27:09.500266Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.057177,
     "end_time": "2024-11-08T06:27:09.503074",
     "exception": false,
     "start_time": "2024-11-08T06:27:09.445897",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def attention_weight_rollout_(attentions, dropout_rate = 0.75, merge_type = \"max\"):\n",
    "    # attentions : LIST of attention weights\n",
    "    # dropout_rate : top-k ratio\n",
    "    # merge_type : fusion types\n",
    "    result = ops.eye(ops.shape(attentions[0])[-1]\n",
    "                    )\n",
    "    n_layers = len(attentions)\n",
    "    batch, heads, _, seq_ = ops.shape(attentions[0])\n",
    "    n_patches = seq_ -1 ; patch_size = ops.cast(ops.sqrt(ops.cast(n_patches, \"float32\")),\n",
    "                                               \"int32\")\n",
    "                                      \n",
    "    for attention in attentions: #attention shape : batch, heads, 1+n_patches, 1+n_patches\n",
    "        # 1. Fuse\n",
    "        if merge_type == \"max\":\n",
    "            attention_heads_fused  = ops.max(attention, axis = 1)\n",
    "        elif merge_type == \"min\":\n",
    "            attention_heads_fused  = ops.min(attention, axis = 1)\n",
    "        elif merge_type == \"mean\":\n",
    "            attention_heads_fused  = ops.mean(attention, axis = 1)\n",
    "        else:\n",
    "            raise \"Attention head fusion type Not supported\"\n",
    "        #2. discard lower top-ks\n",
    "        flat = ops.reshape(attention_heads_fused, [batch, -1]) #batch, att_len\n",
    "        mask = tf.ones_like(flat)\n",
    "        _, indices = ops.top_k(-flat, k = int(dropout_rate * flat.shape[-1]), sorted = False)\n",
    "        target_indices = []\n",
    "        for j, indices_per_batch in enumerate(indices):\n",
    "            top_indices = indices_per_batch[indices_per_batch != 0]\n",
    "            batch_indices = tf.repeat(j, repeats = top_indices.shape[-1])\n",
    "            top_indices = tf.stack([batch_indices, tf.reshape(top_indices, [-1])], axis=1)\n",
    "            target_indices.append(top_indices)\n",
    "        target_indices = ops.concatenate(target_indices, axis = 0)\n",
    "        mask = tf.tensor_scatter_nd_update(mask, target_indices, \n",
    "                                           tf.zeros(tf.shape(target_indices)[0], dtype=flat.dtype))\n",
    "        flat *= mask\n",
    "        attention_heads_fused = ops.reshape(flat, attention_heads_fused.shape)\n",
    "        #3. add skip connection and cumulative product\n",
    "        I = ops.eye(attention_heads_fused.shape[-1])\n",
    "        a = (attention_heads_fused + I) / 2\n",
    "        a = a/ops.sum(a, axis = -1, keepdims = True)\n",
    "        result = ops.matmul(a, result)\n",
    "    cls_token_weight = result[:, 0, 1:]\n",
    "    cls_token_weight = ops.reshape(cls_token_weight, [batch, patch_size, patch_size])\n",
    "    return result, cls_token_weight\n",
    "\n",
    "def get_attention_rollout(model, images, impose_alpha = 0.5, dropout_rate = 0.75, merge_type = \"max\"):\n",
    "    if len(ops.shape(images)) == 3 :\n",
    "        images = images[1, ...]\n",
    "    batch_size, res, _, _ = ops.shape(images)\n",
    "    attentions = model.predict_on_batch(images)[-1][\"total_weight\"]\n",
    "    total_weight, att_w = attention_weight_rollout_(attentions, dropout_rate = dropout_rate, merge_type = merge_type)\n",
    "    \n",
    "    att_w = (att_w - ops.min(att_w, axis = (1,2), keepdims = True))/(ops.max(att_w, axis = (1,2), keepdims = True) - ops.min(att_w, axis = (1,2), keepdims = True) + 1e-4)\n",
    "    colored = plt.cm.jet(att_w)\n",
    "    rgb_att_w = ops.cast(colored[:,:,:,:3]*255,\n",
    "                  \"uint8\")\n",
    "    heatmap = [keras.utils.array_to_img(h) for h in rgb_att_w]\n",
    "    heatmap = [h.resize((res,res)) for h in heatmap]\n",
    "    heatmap = [keras.utils.img_to_array(h) for h in heatmap]\n",
    "    heatmap = [ops.cast(h, \"uint8\") for h in heatmap]\n",
    "    \n",
    "    imposed = (1-impose_alpha)*ops.cast(images, \"float32\") + impose_alpha*ops.cast(heatmap, \"float32\")\n",
    "    imposed = ops.cast(imposed, \"uint8\")\n",
    "    return heatmap, imposed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6b669bb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T06:27:09.567767Z",
     "iopub.status.busy": "2024-11-08T06:27:09.567462Z",
     "iopub.status.idle": "2024-11-08T06:27:09.637442Z",
     "shell.execute_reply": "2024-11-08T06:27:09.636532Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.104511,
     "end_time": "2024-11-08T06:27:09.639630",
     "exception": false,
     "start_time": "2024-11-08T06:27:09.535119",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.manifold import TSNE\n",
    "def feature_visualize(model, images):\n",
    "    if len(ops.shape(images)) == 3:\n",
    "        images = images[tf.newaxis, ...]\n",
    "\n",
    "    f_fn = model.feature_extractor\n",
    "    if isinstance(f_fn, ViT_rollout) or (len(f_fn.outputs) == 3):\n",
    "        representation_vectors, patches, attention_weights = f_fn(images)\n",
    "    elif len(f_fn.outputs) == 2:\n",
    "        representation_vectors, attention_weights = f_fn(images)\n",
    "    \n",
    "    batch_size, embed_dims = ops.shape(representation_vectors)\n",
    "    \n",
    "    p = np.minimum(5, batch_size//2)\n",
    "    tsne_fn = TSNE(n_components=2, learning_rate='auto', init='random', perplexity=p)\n",
    "    embed_v = tsne_fn.fit_transform(representation_vectors)\n",
    "    return embed_v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d45a1c1",
   "metadata": {
    "papermill": {
     "duration": 0.031257,
     "end_time": "2024-11-08T06:27:09.702800",
     "exception": false,
     "start_time": "2024-11-08T06:27:09.671543",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Test - drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "39e3dd81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T06:27:09.767745Z",
     "iopub.status.busy": "2024-11-08T06:27:09.767000Z",
     "iopub.status.idle": "2024-11-08T06:27:09.773455Z",
     "shell.execute_reply": "2024-11-08T06:27:09.772594Z"
    },
    "papermill": {
     "duration": 0.040889,
     "end_time": "2024-11-08T06:27:09.775330",
     "exception": false,
     "start_time": "2024-11-08T06:27:09.734441",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_drive = 0\n",
    "mim = 0\n",
    "other = 0\n",
    "if test_drive:\n",
    "    heads = 8\n",
    "    res = 384\n",
    "    embed_dims = 768\n",
    "    batch_size = 8\n",
    "    grayscale = False\n",
    "    if grayscale:\n",
    "        att_depth = 4\n",
    "        patch_size = 24\n",
    "    else:\n",
    "        att_depth = 1\n",
    "        patch_size = 32\n",
    "        q_size = 4\n",
    "    mode = \"attention\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a2ca1b4f",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-11-08T06:27:09.840581Z",
     "iopub.status.busy": "2024-11-08T06:27:09.840310Z",
     "iopub.status.idle": "2024-11-08T06:27:09.849809Z",
     "shell.execute_reply": "2024-11-08T06:27:09.848979Z"
    },
    "papermill": {
     "duration": 0.043214,
     "end_time": "2024-11-08T06:27:09.851610",
     "exception": false,
     "start_time": "2024-11-08T06:27:09.808396",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if test_drive:\n",
    "    gc_configs = get_gcvit_configs(res, 64, \"GC_ViT_xxtiny\")\n",
    "    gc_configs[\"level_depth\"] = [1,1,2,2]\n",
    "    print(gc_configs)\n",
    "    dataset = tfds.load(\"voc\", \n",
    "                        split = 'train'\n",
    "                       )\n",
    "    data_size = len(dataset)\n",
    "    print(f\"dataset size : {data_size}\")\n",
    "    def map_fn(dataset):\n",
    "        image = dataset[\"image\"]\n",
    "        image = tf.image.rgb_to_grayscale(image)\n",
    "        image = tf.image.resize_with_pad(image, res, res, antialias = True)\n",
    "        image = tf.cast(image, tf.uint8)\n",
    "        return image\n",
    "    #def fix_batch_size(dataset, batch_size):\n",
    "    #    return dataset.apply(\n",
    "    #        tf.data.experimental.assert_cardinality(batch_size)\n",
    "    #    ) #<- batch size 고정, 훈련 안정화!\n",
    "    train_ds_ = (dataset\n",
    "             .map(map_fn)\n",
    "             .batch(batch_size, drop_remainder=True)\n",
    "             .prefetch(tf.data.AUTOTUNE))\n",
    "    \n",
    "    mask_map_fn = get_masking_fn(grayscale = grayscale, masking_rate = 0.5, patch_size = patch_size)\n",
    "    \n",
    "    train_ds_masked = train_ds_.unbatch().map(mask_map_fn).batch(batch_size, drop_remainder = True).repeat().prefetch(tf.data.AUTOTUNE)\n",
    "    val_ds = train_ds_masked.unbatch().batch(32, drop_remainder = True).prefetch(tf.data.AUTOTUNE)\n",
    "    for data in val_ds.take(1):\n",
    "        sample_images = data[-1]\n",
    "    ssl_fn = get_map_fn(res, \"supervised\", \"ssl\",2, grayscale = grayscale)\n",
    "    \n",
    "    #train_ds = train_ds_.unbatch().map(ssl_fn).repeat().batch(batch_size, drop_remainder = True).prefetch(tf.data.AUTOTUNE)\n",
    "    #val_ds = train_ds.unbatch().batch(32, drop_remainder = True).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9432390a",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-11-08T06:27:09.914963Z",
     "iopub.status.busy": "2024-11-08T06:27:09.914642Z",
     "iopub.status.idle": "2024-11-08T06:27:09.934640Z",
     "shell.execute_reply": "2024-11-08T06:27:09.933816Z"
    },
    "papermill": {
     "duration": 0.053902,
     "end_time": "2024-11-08T06:27:09.936569",
     "exception": false,
     "start_time": "2024-11-08T06:27:09.882667",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if test_drive and mim:\n",
    "    tf.keras.backend.clear_session()\n",
    "    for data in train_ds_masked.take(1):\n",
    "        mask_sequence, mask_image, image, original_image = data\n",
    "        mask_sets = data\n",
    "    for index in range(3):\n",
    "        print(mask_sequence[index])\n",
    "        fig, axes = plt.subplots(1,3, figsize=(15,5))\n",
    "        axes = axes.flatten()\n",
    "        axes[0].imshow(tf.cast(mask_image[index], \"uint8\"))\n",
    "        axes[1].imshow(tf.cast(image[index], \"uint8\"))\n",
    "        axes[2].imshow(tf.cast(original_image[index], \"uint8\"))\n",
    "        plt.show()\n",
    "    #resnet = keras_cv.models.ResNetV2Backbone(stackwise_filters=[64, 128, 256, 512], stackwise_blocks=[2, 2, 2, 2], stackwise_strides=[1, 2, 2, 2], include_rescaling = True, input_shape = [res,res,3])\n",
    "    conv_tiny = keras.applications.ConvNeXtTiny(input_shape = [res,res,3], include_top = False, weights = None)\n",
    "    \n",
    "    for layer in conv_tiny.layers:\n",
    "        layer.dtype_policy = keras.mixed_precision.Policy('mixed_float16')\n",
    "    vit_model = get_metaformer(mode, res, grayscale = grayscale, \n",
    "                               att_depth = 4, att_heads = heads, \n",
    "                               att_dims = heads * 64, \n",
    "                               embed_dims = embed_dims, patch_size = patch_size,\n",
    "                              register_tokens = 0,\n",
    "                               pretrained_encoder = conv_tiny, \n",
    "                               return_patches = True\n",
    "                              )\n",
    "    #vit_model = ViT_rollout(res = res, att_depth = 8, att_heads = heads, att_dims = heads * 32, embed_dims = embed_dims, patch_size = patch_size, name = \"ViT\")\n",
    "    #conv_base = get_encdec_model(conv_tiny, res = res, q_size = q_size,\n",
    "    #                             compression_rate = [8,16,32], att_dims = embed_dims, att_heads=heads,\n",
    "    #                            #encoder_trainable = True\n",
    "    #                            )\n",
    "    #ssl = MixedMIM(vit_model, grayscale = grayscale, patch_size = patch_size) #SimMIM or MixedMIM\n",
    "    vit_model.summary()\n",
    "    ssl = DINO_MIM(vit_model, vit_model)\n",
    "    \n",
    "    #ssl = NCLR(vit_model, \n",
    "    #            embed_dims = embed_dims, \n",
    "    #            use_mim = True, \n",
    "    #            subtype = \"nnclr\", t = 0.1,\n",
    "    #            patch_size = patch_size\n",
    "    #           ) #nnclr, snclr\n",
    "    ssl.compile(optimizer = keras.optimizers.AdamW(learning_rate = 0.0005*batch_size/256, \n",
    "                                                   #clipnorm = 0.5,\n",
    "                                                  #use_ema = True,\n",
    "                                                  #gradient_accumulation_steps = 16\n",
    "                                                  ),\n",
    "                jit_compile = False)\n",
    "    print(ssl(data))\n",
    "    \n",
    "    result = ssl.fit(train_ds_masked, epochs = 1, steps_per_epoch = 100)\n",
    "    print(result)\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        imgs, merged_imgs = att_visualize(ssl.get_full_model(res=res), data[-1], res = res,\n",
    "                            thresholding = 1)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        imgs, merged_imgs = att_visualize(ssl.feature_extractor, data[-1], res = res,\n",
    "                            thresholding = 1)\n",
    "    \n",
    "    \n",
    "    #imgs, merged_imgs = q_visualize(ssl.feature_extractor, data[-1], res = res, thresholding = 0, merge_type = 'mean')\n",
    "    for row in range(5):\n",
    "        fig, axes = plt.subplots(1, heads, figsize = (8*heads,8))\n",
    "        axes = axes.flatten()\n",
    "        for col in range(heads):\n",
    "            axes[col].imshow(imgs[row, col, ...])\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    fig,axes = plt.subplots(1, 5, figsize = (30,6))\n",
    "    axes = axes.flatten()\n",
    "    for ax, merge in zip(axes, merged_imgs[:5]):\n",
    "        ax.imshow(merge)\n",
    "        ax.set_title(\"Naive mean merged\")\n",
    "    plt.show()\n",
    "    \n",
    "    ############################\n",
    "    \n",
    "    heatmap, merged_weight_imgs = att_visualize_merged(ssl.feature_extractor, data[-1], res = res, merge_type = 'mean')\n",
    "    fig,axes = plt.subplots(1, 5, figsize = (30,6))\n",
    "    axes = axes.flatten()\n",
    "    for ax, merge in zip(axes, merged_weight_imgs[:5]):\n",
    "        ax.imshow(merge)\n",
    "        ax.set_title(\"MaxOut merged\")\n",
    "    plt.show()\n",
    "    \n",
    "    heatmap, imposed_image = pca_patch_viz(ssl.feature_extractor, data[-1])\n",
    "    for i in range(batch_size):\n",
    "        fig, axes = plt.subplots(1,3, figsize = (18,5))\n",
    "        axes = axes.flatten()\n",
    "        axes[0].imshow(ops.cast(heatmap[i], \"uint8\"))\n",
    "        axes[1].imshow(ops.cast(imposed_image[i], \"uint8\"))\n",
    "        axes[2].imshow(ops.cast(original_image[i], \"uint8\"))\n",
    "        plt.show()\n",
    "    #vit\n",
    "    #_, rollout_images = get_attention_rollout(ssl.feature_extractor, data[-1])\n",
    "    #fig,axes = plt.subplots(1, 5, figsize = (30,6))\n",
    "    #axes = axes.flatten()\n",
    "    #for ax, merge in zip(axes, rollout_images[:5]):\n",
    "    #    ax.imshow(merge)\n",
    "    #    ax.set_title(\"Rollout merged\")\n",
    "    #plt.show()\n",
    "    \n",
    "    #feature vectors\n",
    "    embed_v = feature_visualize(ssl, sample_images)\n",
    "    plt.scatter(embed_v[..., 0], embed_v[...,1])\n",
    "    plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2873b705",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-11-08T06:27:10.000197Z",
     "iopub.status.busy": "2024-11-08T06:27:09.999890Z",
     "iopub.status.idle": "2024-11-08T06:27:10.012146Z",
     "shell.execute_reply": "2024-11-08T06:27:10.011307Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.046159,
     "end_time": "2024-11-08T06:27:10.014057",
     "exception": false,
     "start_time": "2024-11-08T06:27:09.967898",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if test_drive and other:\n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    vit_model = get_metaformer(mode, res, grayscale = grayscale, \n",
    "                               att_depth = att_depth, att_heads = heads, \n",
    "                               att_dims = heads * 64, \n",
    "                               embed_dims = heads * 64, patch_size = patch_size,\n",
    "                              register_tokens = 4,\n",
    "                              #pretrained_encoder = keras.applications.EfficientNetV2B1(input_shape = [res,res,3], include_top = False),\n",
    "                              pretrained_encoder = None, \n",
    "                               return_patches = True\n",
    "                              )\n",
    "    vit_model.summary()\n",
    "    ssl = MixedUnsupSeg(feature_extractor = vit_model, q = 5)\n",
    "    \n",
    "    ssl.compile(optimizer = keras.optimizers.AdamW(learning_rate = 1e-4, clipnorm = 1.0,\n",
    "                                                  #use_ema = True,\n",
    "                                                  #gradient_accumulation_steps = 16\n",
    "                                                  ),\n",
    "                jit_compile = False)\n",
    "    \n",
    "    print(ssl(sets[0]))\n",
    "    ssl.summary()\n",
    "    result = ssl.fit(train_ds_masked, epochs = 1, steps_per_epoch = 100)\n",
    "    print(result)\n",
    "    try:\n",
    "        imgs = att_visualize(ssl.get_full_model(res=res), sets[0], res = res,\n",
    "                            thresholding = 1)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        imgs = att_visualize(ssl.feature_extractor, sets[0], res = res,\n",
    "                            thresholding = 1)\n",
    "    for row in range(12):\n",
    "        fig, axes = plt.subplots(1, heads, figsize = (5*heads,5))\n",
    "        axes = axes.flatten()\n",
    "        for col in range(heads):\n",
    "            axes[col].imshow(imgs[row, col, ...])\n",
    "    plt.show()\n",
    "    \n",
    "    try:\n",
    "        heatmap, imposed_images = ssl.get_segments(sets[0])\n",
    "        i = 0\n",
    "        for original_img, imposed_img in zip(sets[0], imposed_images):\n",
    "            fig,axes = plt.subplots(1, 2, figsize = (10,5))\n",
    "            axes = axes.flatten()\n",
    "            axes[0].imshow(ops.cast(original_img, \"uint8\"))\n",
    "            axes[1].imshow(ops.cast(imposed_img, \"uint8\"))\n",
    "            i += 1\n",
    "            if i >= 5:\n",
    "                break\n",
    "    except:\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5051531,
     "sourceId": 8471595,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 36.481147,
   "end_time": "2024-11-08T06:27:12.916751",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-08T06:26:36.435604",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
