{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd1e956e",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-07-05T17:50:59.295447Z",
     "iopub.status.busy": "2024-07-05T17:50:59.295075Z",
     "iopub.status.idle": "2024-07-05T17:51:19.905891Z",
     "shell.execute_reply": "2024-07-05T17:51:19.905021Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 20.641443,
     "end_time": "2024-07-05T17:51:19.908278",
     "exception": false,
     "start_time": "2024-07-05T17:50:59.266835",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 17:51:01.585798: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-05 17:51:01.585919: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-05 17:51:01.691674: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirements loaded, keras : v3.3.3, Tensorflow : v2.15.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "seed = 2024\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ML tools \n",
    "\n",
    "import tensorflow as tf\n",
    "import keras #; keras.config.set_dtype_policy(\"mixed_float16\")\n",
    "import keras_nlp\n",
    "import keras_cv\n",
    "from keras import ops\n",
    "\n",
    "keras.utils.set_random_seed(seed)\n",
    "\n",
    "import cv2\n",
    "import tensorflow_io as tfio\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from keras import Input, Model, layers\n",
    "from keras.models import load_model\n",
    "from keras.layers import Layer\n",
    "from keras.layers import Conv2D, DepthwiseConv2D, Dense, Activation, BatchNormalization, LayerNormalization, MultiHeadAttention, Embedding, Subtract, Add, Multiply, GlobalAveragePooling2D, GlobalAveragePooling1D, LayerNormalization\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.applications import *\n",
    "import os, sys\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "from tqdm.notebook import tqdm\n",
    "print(f\"Requirements loaded, keras : v{keras.__version__}, Tensorflow : v{tf.__version__}\")\n",
    "\n",
    "#import external packages\n",
    "sys.path.append(\"/kaggle/input/kimm-keras-image-model-repository\")\n",
    "#sys.setrecursionlimit(10**7)\n",
    "import kimm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56ddd32",
   "metadata": {
    "papermill": {
     "duration": 0.026972,
     "end_time": "2024-07-05T17:51:19.961769",
     "exception": false,
     "start_time": "2024-07-05T17:51:19.934797",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataloader setting\n",
    "- original dataset(input as tf.ds) : image only or image/label paired dataset\n",
    "- target dataset(output as tf.ds) : (image1, image2), (image1, image2, image3), (image1, image2, label)\n",
    "    - Also, implement the mixing function : merges 2 homogenous dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cc10c94",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-07-05T17:51:20.016132Z",
     "iopub.status.busy": "2024-07-05T17:51:20.015538Z",
     "iopub.status.idle": "2024-07-05T17:51:20.020738Z",
     "shell.execute_reply": "2024-07-05T17:51:20.019892Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.035136,
     "end_time": "2024-07-05T17:51:20.022739",
     "exception": false,
     "start_time": "2024-07-05T17:51:19.987603",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "kimm_list = kimm.list_models()\n",
    "def available_models():\n",
    "    return {\"models_from_kimm\": kimm_list,\n",
    "           'models_from_keras' : [\"effnet\", \"effnet_small\", \"effnet_base\",\n",
    "                                 \"convnext\", \"convnext_small\", \"convnext_base\",\n",
    "                                 \"mlpmixer_patch_depth_dims\", \"convmixer_patch_depth_dims\"]\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cac35c04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T17:51:20.076058Z",
     "iopub.status.busy": "2024-07-05T17:51:20.075734Z",
     "iopub.status.idle": "2024-07-05T17:51:20.693183Z",
     "shell.execute_reply": "2024-07-05T17:51:20.692221Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.646779,
     "end_time": "2024-07-05T17:51:20.695511",
     "exception": false,
     "start_time": "2024-07-05T17:51:20.048732",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandAug Component in this SSL module :  ['random_contrast', 'random_brightness', 'random_shear', 'random_shear_1', 'random_translation', 'random_translation_1', 'KerasCVGridMasking']\n"
     ]
    }
   ],
   "source": [
    "basic_aug = keras.Sequential([keras.layers.RandomFlip(), keras.layers.RandomRotation(factor = 0.3),\n",
    "                             keras.layers.GaussianDropout(0.1)])\n",
    "aug_layers = keras_cv.layers.RandAugment.get_standard_policy(\n",
    "    value_range=(0, 255), magnitude=0.2, magnitude_stddev=0.1\n",
    ")\n",
    "\n",
    "aug_layers.pop(0); aug_layers.pop(0) ; aug_layers.pop(0); aug_layers.pop(0)\n",
    "aug_layers = aug_layers + [keras_cv.layers.GridMask(name = \"KerasCVGridMasking\")]\n",
    "print(\"RandAug Component in this SSL module : \",[layer.name for layer in aug_layers])\n",
    "randaug = keras_cv.layers.RandomAugmentationPipeline(\n",
    "    layers=aug_layers, augmentations_per_image=1, seed = seed, auto_vectorize = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "183ef7af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T17:51:20.750296Z",
     "iopub.status.busy": "2024-07-05T17:51:20.749943Z",
     "iopub.status.idle": "2024-07-05T17:51:20.756894Z",
     "shell.execute_reply": "2024-07-05T17:51:20.755982Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.036579,
     "end_time": "2024-07-05T17:51:20.758748",
     "exception": false,
     "start_time": "2024-07-05T17:51:20.722169",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_other_augs_(res):\n",
    "    \n",
    "    \n",
    "    crop_resize_global = keras.Sequential([keras.layers.RandomCrop(int(0.95*res), int(0.95*res)),\n",
    "                                        keras.layers.Resizing(res, res)\n",
    "                                       ])\n",
    "    crop_resize_local = keras.Sequential([keras.layers.RandomCrop(int(0.5*res), int(0.5*res)),\n",
    "                                        keras.layers.Resizing(res, res)\n",
    "                                       ])\n",
    "    crop_resize_medium = keras.Sequential([keras.layers.RandomCrop(int(0.75*res), int(0.75*res)),\n",
    "                                        keras.layers.Resizing(res, res)\n",
    "                                       ])\n",
    "\n",
    "    return crop_resize_global, crop_resize_medium, crop_resize_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e407f07d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T17:51:20.813293Z",
     "iopub.status.busy": "2024-07-05T17:51:20.812485Z",
     "iopub.status.idle": "2024-07-05T17:51:20.817382Z",
     "shell.execute_reply": "2024-07-05T17:51:20.816447Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.034221,
     "end_time": "2024-07-05T17:51:20.819417",
     "exception": false,
     "start_time": "2024-07-05T17:51:20.785196",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def random_color_change(images):\n",
    "    if tf.random.uniform(shape = (), minval = 1, maxval = 11, dtype = \"int32\") <= 1:\n",
    "        return 255.0 - images\n",
    "    else:\n",
    "        return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f5b61c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T17:51:20.873563Z",
     "iopub.status.busy": "2024-07-05T17:51:20.872743Z",
     "iopub.status.idle": "2024-07-05T17:51:20.885605Z",
     "shell.execute_reply": "2024-07-05T17:51:20.884719Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.042276,
     "end_time": "2024-07-05T17:51:20.887639",
     "exception": false,
     "start_time": "2024-07-05T17:51:20.845363",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_map_fn(res, input_type = None, output_type = None, n_view = 2, grayscale = True):\n",
    "    # input_type as \"supervised\", \"with_label\", \"with label\" / OR / \"unsupervised\", \"without label\", \"without_label\", \"image_only\", \"image\"\n",
    "    # output_ytpe as \"ssl\", \"ssl_with_label\" / if ssl, image output : image1, image2, ..., image_n_view\n",
    "    assert n_view >= 2, \"Augmented View number must be >= 2\"\n",
    "    assert input_type in [\"supervised\", \"with_label\", \"with label\", \"unsupervised\", \"without label\", \"without_label\", \"image_only\", \"image\"], 'Pick one of input_type : \"supervised\", \"with_label\", \"with label\",\\n \"unsupervised\", \"without label\", \"without_label\", \"image_only\", \"image\"'\n",
    "    assert output_type in [\"ssl\", \"ssl_with_label\"], 'Pick one of output_type : \"ssl\", \"ssl_with_label\"'\n",
    "    crop_resize_global, crop_resize_medium, crop_resize_local = get_other_augs_(res)\n",
    "    def map_fn(image, label = None):\n",
    "        if grayscale:\n",
    "            try:\n",
    "                image = tf.image.rgb_to_grayscale(image)\n",
    "            except:\n",
    "                pass\n",
    "        else:\n",
    "            try:\n",
    "                image = tf.image.grayscale_to_rgb(image)\n",
    "            except:\n",
    "                pass\n",
    "        if input_type in [\"supervised\", \"with_label\", \"with label\"]:\n",
    "            image, label = image, label\n",
    "        elif input_type in [\"unsupervised\", \"without label\", \"without_label\", \"image_only\", \"image\"]:\n",
    "            image = image\n",
    "            label = None\n",
    "        batch_size = ops.shape(image)[0]\n",
    "        \n",
    "        aug_ = basic_aug(image)\n",
    "        aug_ = randaug(aug_)\n",
    "        global_image = crop_resize_global(aug_)\n",
    "        global_image = random_color_change(global_image)\n",
    "        if n_view == 2:\n",
    "            if output_type == \"ssl\":\n",
    "                return (image, global_image)\n",
    "            elif output_type == \"ssl_with_label\":\n",
    "                return (image, global_image, label)\n",
    "        elif n_view == 3:\n",
    "            medium_image = crop_resize_medium(aug_)\n",
    "            medium_image = random_color_change(medium_image)\n",
    "            if output_type == \"ssl\":\n",
    "                return (image, global_image, medium_image)\n",
    "            elif output_type == \"ssl_with_label\":\n",
    "                return (image, global_image, medium_image, label)\n",
    "        elif n_view > 3:\n",
    "            medium_image = crop_resize_medium(aug_)\n",
    "            medium_image = random_color_change(medium_image)\n",
    "            local_images = [random_color_change(crop_resize_local(image)) for _ in range(n_view - 3)]\n",
    "            local_images = tuple(local_images)\n",
    "            outputs = (image, global_image, medium_image) + local_images\n",
    "            if output_type == \"ssl\":\n",
    "                return outputs\n",
    "            elif output_type == \"ssl_with_label\":\n",
    "                return outputs+(label)\n",
    "    return map_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "240fa566",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T17:51:20.941640Z",
     "iopub.status.busy": "2024-07-05T17:51:20.941292Z",
     "iopub.status.idle": "2024-07-05T17:51:20.953679Z",
     "shell.execute_reply": "2024-07-05T17:51:20.952771Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.041929,
     "end_time": "2024-07-05T17:51:20.955698",
     "exception": false,
     "start_time": "2024-07-05T17:51:20.913769",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MaskLayer(keras.layers.Layer):\n",
    "    def __init__(self, masking_rate, update_value, **kwargs):\n",
    "        super(MaskLayer, self).__init__(**kwargs)\n",
    "        self.masking_rate = masking_rate\n",
    "        self.update_value = update_value\n",
    "\n",
    "    def call(self, patches):\n",
    "        sequence, masked_patches = self.generate_mask(patches, self.masking_rate, self.update_value)\n",
    "        return [sequence, masked_patches]\n",
    "\n",
    "    def generate_mask(self, patches, masking_rate, update_value):\n",
    "        batch_size = tf.shape(patches)[0]\n",
    "        length = tf.shape(patches)[1]\n",
    "        dims = tf.shape(patches)[2]\n",
    "        num_ones = tf.cast(tf.cast(masking_rate, tf.float32) * tf.cast(length, tf.float32), \n",
    "                           tf.int32)\n",
    "        \n",
    "        # Ensure num_ones does not exceed length\n",
    "        num_ones = tf.minimum(num_ones, length)\n",
    "        \n",
    "        # Initialize the sequence with zeros\n",
    "        sequence = tf.zeros((batch_size, length), dtype=tf.int32)\n",
    "\n",
    "        def mask_single_batch(batch_idx):\n",
    "            # Generate random indices for the ones\n",
    "            indices = tf.random.shuffle(tf.range(length))[:num_ones]\n",
    "            scatter_indices = tf.stack([tf.ones([num_ones], dtype=tf.int32) * batch_idx, indices], axis=1)\n",
    "            return scatter_indices\n",
    "\n",
    "        # Apply the mask generation function to each batch using tf.map_fn\n",
    "        all_indices = tf.map_fn(mask_single_batch, tf.range(batch_size), dtype=tf.int32)\n",
    "\n",
    "        # Reshape to apply the updates\n",
    "        all_indices = tf.reshape(all_indices, [-1, 2])\n",
    "\n",
    "        # Update the sequence with ones at the specified indices\n",
    "        sequence = tf.tensor_scatter_nd_update(sequence, all_indices, tf.ones([tf.shape(all_indices)[0]], dtype=tf.int32))\n",
    "\n",
    "        mask_indices = tf.where(sequence == 1)\n",
    "\n",
    "        # Ensure update_value is correctly shaped for high-dimensional replacement\n",
    "        updates_value = tf.tile(tf.expand_dims(update_value, 0), [tf.shape(mask_indices)[0], 1])\n",
    "        \n",
    "        masked_patches_shape = tf.concat([tf.shape(patches)[:2], [dims]], axis=0)\n",
    "        masked_patches = tf.tensor_scatter_nd_update(patches, mask_indices, updates_value)\n",
    "\n",
    "        return tf.cast(sequence, \"float32\"), masked_patches\n",
    "# Test the function with Keras input\n",
    "#inputs = keras.Input(shape=(None, 128))\n",
    "#update_value = tf.Variable(99*tf.ones([128]), trainable=True)\n",
    "#MaskLayer(masking_rate=0.5, update_value=update_value)(tf.random.normal([2,64,128]))\n",
    "#sequence, masked_patches = mask_layer(inputs)\n",
    "#model = keras.Model(inputs=inputs, outputs=[sequence, masked_patches])\n",
    "#model(tf.random.normal([2,64,128]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e21fad2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T17:51:21.009364Z",
     "iopub.status.busy": "2024-07-05T17:51:21.008751Z",
     "iopub.status.idle": "2024-07-05T17:51:21.024659Z",
     "shell.execute_reply": "2024-07-05T17:51:21.023791Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.04503,
     "end_time": "2024-07-05T17:51:21.026711",
     "exception": false,
     "start_time": "2024-07-05T17:51:20.981681",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PatchMask(keras.layers.Layer):\n",
    "    def __init__(self, mode, ratio, patch_size = 16,\n",
    "                 fill_mode = 'constant', fill_value = 0.0, \n",
    "                 output_mode = \"sequence\", #or image\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        assert mode in ['patch', \"grid\"], \"Mode should be one of the following : patch, grid\"\n",
    "        assert (ratio < 1) and (ratio > 0), \"Ratio should be 0~1\"\n",
    "        assert fill_mode in ['constant', 'gaussian', 'gaussian_noise'], \"fill_mode should be one of the following : 'constant', 'gaussian', 'gaussian_noise' \"\n",
    "        assert output_mode in [\"image\", 'sequence'], \"output_mode should be one of the following : 'image', 'sequence' \"\n",
    "        self.mode = mode #patch, grid\n",
    "        self.ratio = ratio\n",
    "        self.fill_mode = fill_mode\n",
    "        self.fill_value = fill_value\n",
    "        self.patch_size = patch_size\n",
    "        self.output_mode = output_mode\n",
    "    def build(self, input_shape):\n",
    "        batch_size, h, w, dims = input_shape\n",
    "        self.res = h\n",
    "        self.original_channels = dims\n",
    "        self.seq_len = int(h//self.patch_size) * int(w//self.patch_size)\n",
    "        \n",
    "    def call(self, image, training = True):\n",
    "        batch_size = tf.shape(image)[0]\n",
    "        image = ops.cast(image, \"float32\")\n",
    "        if training == False:\n",
    "            if self.output_mode == \"sequence\":\n",
    "                return ops.image.extract_patches(image, self.patch_size)\n",
    "            else:\n",
    "                return image\n",
    "        ################################################\n",
    "        if self.mode in [\"grid\", 'Grid', \"GridMask\", \"gridmask\"]:\n",
    "            image = keras_cv.layers.GridMask(ratio_factor = self.ratio,\n",
    "                                            fill_mode = self.fill_mode,\n",
    "                                            fill_value = self.fill_value)\n",
    "            patches = ops.image.extract_patches(image, self.patch_size)\n",
    "        else:\n",
    "            image /= 255.0\n",
    "            patches = ops.image.extract_patches(image, self.patch_size)\n",
    "            _, w_, h_, dims_ = ops.shape(patches)\n",
    "            patches = ops.reshape(patches, [-1, w_*h_, dims_])\n",
    "            if self.fill_mode == \"constant\":\n",
    "                update_value = tf.zeros((dims_,), dtype = \"float32\") + float(self.fill_value)\n",
    "            elif self.fill_mode in [\"gaussian\", \"gaussian_noise\"]:\n",
    "                update_value = tf.random.normal((dims_,))\n",
    "            mask_ids, patches = MaskLayer(masking_rate = self.ratio, update_value = update_value)(patches)\n",
    "            patches = patches * 255.0\n",
    "            patches = ops.cast(patches, \"uint8\")\n",
    "        if self.output_mode == \"sequence\":\n",
    "            return mask_ids, patches\n",
    "        else:\n",
    "            patches = ops.reshape(patches, [-1, w_, h_, self.patch_size, self.patch_size,\n",
    "                                           self.original_channels])\n",
    "            patches = ops.transpose(patches, [0, 1, 3, 2, 4, 5])\n",
    "            patches = ops.reshape(patches, [-1, self.res, self.res, self.original_channels])\n",
    "            return mask_ids, patches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e66966c",
   "metadata": {
    "papermill": {
     "duration": 0.025776,
     "end_time": "2024-07-05T17:51:21.078948",
     "exception": false,
     "start_time": "2024-07-05T17:51:21.053172",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac4f059c",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-07-05T17:51:21.132718Z",
     "iopub.status.busy": "2024-07-05T17:51:21.131974Z",
     "iopub.status.idle": "2024-07-05T17:51:21.141890Z",
     "shell.execute_reply": "2024-07-05T17:51:21.140836Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.039054,
     "end_time": "2024-07-05T17:51:21.143846",
     "exception": false,
     "start_time": "2024-07-05T17:51:21.104792",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_flops(model, model_inputs) -> float:\n",
    "        \"\"\"\n",
    "        Calculate FLOPS [GFLOPs] for a tf.keras.Model or tf.keras.Sequential model\n",
    "        in inference mode. It uses tf.compat.v1.profiler under the hood.\n",
    "        Code reference : https://github.com/tensorflow/tensorflow/issues/32809\n",
    "        \"\"\"\n",
    "        # if not hasattr(model, \"model\"):\n",
    "        #     raise wandb.Error(\"self.model must be set before using this method.\")\n",
    "        \n",
    "        if not isinstance(\n",
    "            model, (tf.keras.models.Sequential, tf.keras.models.Model, keras.models.Model, keras.Sequential, keras.Model)\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                \"Calculating FLOPS is only supported for \"\n",
    "                \"`tf.keras.Model` and `tf.keras.Sequential` instances.\"\n",
    "            )\n",
    "\n",
    "        from tensorflow.python.framework.convert_to_constants import (\n",
    "            convert_variables_to_constants_v2_as_graph,\n",
    "        )\n",
    "\n",
    "        # Compute FLOPs for one sample\n",
    "        batch_size = 1\n",
    "        inputs = [\n",
    "            tf.TensorSpec([batch_size] + inp.shape[1:], inp.dtype)\n",
    "            for inp in model_inputs\n",
    "        ]\n",
    "\n",
    "        # convert tf.keras model into frozen graph to count FLOPs about operations used at inference\n",
    "        real_model = tf.function(model).get_concrete_function(inputs)\n",
    "        frozen_func, _ = convert_variables_to_constants_v2_as_graph(real_model)\n",
    "\n",
    "        # Calculate FLOPs with tf.profiler\n",
    "        run_meta = tf.compat.v1.RunMetadata()\n",
    "        opts = (\n",
    "            tf.compat.v1.profiler.ProfileOptionBuilder(\n",
    "                tf.compat.v1.profiler.ProfileOptionBuilder().float_operation()\n",
    "            )\n",
    "            .with_empty_output()\n",
    "            .build()\n",
    "        )\n",
    "\n",
    "        flops = tf.compat.v1.profiler.profile(\n",
    "            graph=frozen_func.graph, run_meta=run_meta, cmd=\"scope\", options=opts\n",
    "        )\n",
    "\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "\n",
    "        # convert to GFLOPs\n",
    "        return (flops.total_float_ops / 1e9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0159a5",
   "metadata": {
    "papermill": {
     "duration": 0.025304,
     "end_time": "2024-07-05T17:51:21.194888",
     "exception": false,
     "start_time": "2024-07-05T17:51:21.169584",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "> custom losses\n",
    "- keras_cv에서 simclr loss 지원\n",
    "- Barlow Twins가 배경 vs 전체 사물의 구별에 효과적(empirical) -> 다른 SSL과 같이 쓰자\n",
    "    - awesome code reference in [keras.io](https://keras.io/examples/vision/barlow_twins/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8fc1030",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-07-05T17:51:21.255210Z",
     "iopub.status.busy": "2024-07-05T17:51:21.254635Z",
     "iopub.status.idle": "2024-07-05T17:51:21.268228Z",
     "shell.execute_reply": "2024-07-05T17:51:21.267318Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.049705,
     "end_time": "2024-07-05T17:51:21.270153",
     "exception": false,
     "start_time": "2024-07-05T17:51:21.220448",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#helper functions\n",
    "# For Barlow Twins\n",
    "def get_cor_matrix(z1, z2, e = 1e-5):\n",
    "    #z1, z2 = [batch, embed_dims] shape tensor\n",
    "    # 각각 Normalize한 뒤 내적 -> dims by dims correlation matrix\n",
    "    z1_mean, z1_std = ops.mean(z1, axis = 0), ops.std(z1, axis = 0)\n",
    "    z2_mean, z2_std = ops.mean(z2, axis = 0), ops.std(z2, axis = 0)\n",
    "    z1 = (z1 - z1_mean) / (z1_std + e)\n",
    "    z2 = (z2 - z2_mean) / (z2_std + e)\n",
    "    bs = tf.cast(ops.shape(z1)[0], tf.float32)\n",
    "    matrix = (ops.transpose(z1)@z2) / bs\n",
    "    matrix = tf.cast(matrix, tf.float32)\n",
    "    return matrix\n",
    "\n",
    "# For VICreg\n",
    "def invariance_loss(za, zb): #invariance\n",
    "    l2_distances = keras.losses.MeanSquaredError(reduction = None)(za, zb)\n",
    "    return ops.cast(l2_distances, \"float32\")\n",
    "\n",
    "def variance(za, e = 1e-4, gamma = 5.0):\n",
    "    mu, var = tf.nn.moments(za, axes = 0)\n",
    "    mu = tf.cast(mu, tf.float32)\n",
    "    var = tf.cast(var, tf.float32)\n",
    "    var += tf.cast(e, tf.float32)\n",
    "    \n",
    "    s_xe_hinge = gamma - tf.math.sqrt(var)\n",
    "    s_val = tf.math.maximum(0.0, s_xe_hinge)\n",
    "    return ops.cast(s_val, \"float32\")\n",
    "\n",
    "def covariance(za, e = 1e-4, testing = False):\n",
    "    mu, var = tf.nn.moments(za, axes = 0) ; bs = tf.cast(ops.shape(za)[0], tf.float32) ; embed_dims = tf.cast(ops.shape(za)[-1], tf.float32)\n",
    "    mu = tf.cast(mu, tf.float32)\n",
    "    var = tf.cast(var, tf.float32)\n",
    "    div_ = ops.maximum(bs-1, 1)\n",
    "    \n",
    "    za -= mu #batch, dims\n",
    "    cov_mat = tf.transpose(za)@za/div_\n",
    "    lower_tri = cov_mat - tf.linalg.band_part(cov_mat, 0, -1)\n",
    "    upper_tri = cov_mat - tf.linalg.band_part(cov_mat, -1, 0)\n",
    "    off_diag = lower_tri + upper_tri\n",
    "    off_diag = ops.sum(tf.math.square(off_diag))/embed_dims\n",
    "    off_diag = ops.cast(off_diag, \"float32\")\n",
    "    if testing : \n",
    "        print(f\"Covariacne matrix shape : {ops.shape(cov_mat)}\")\n",
    "    return off_diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81db976b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T17:51:21.324475Z",
     "iopub.status.busy": "2024-07-05T17:51:21.323650Z",
     "iopub.status.idle": "2024-07-05T17:51:21.331903Z",
     "shell.execute_reply": "2024-07-05T17:51:21.330957Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.037352,
     "end_time": "2024-07-05T17:51:21.333991",
     "exception": false,
     "start_time": "2024-07-05T17:51:21.296639",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BarlowLoss(keras.losses.Loss):\n",
    "    def __init__(self, diag = 0.6, off_diag = 0.4):\n",
    "        super().__init__()\n",
    "        self.diag = diag\n",
    "        self.off_diag = off_diag\n",
    "        \n",
    "    def compute_loss(self, correlation_matrix):\n",
    "        diag_component = tf.linalg.diag_part(correlation_matrix)\n",
    "        zero_diag = ops.zeros(correlation_matrix.shape[-1])\n",
    "        off_diag_matrix = tf.linalg.set_diag(correlation_matrix, zero_diag)\n",
    "        \n",
    "        diag_loss = tf.pow(diag_component-1, 2) * self.diag\n",
    "        off_diag_loss = tf.pow(off_diag_matrix, 2) * self.off_diag\n",
    "        \n",
    "        loss = ops.mean(diag_loss) + ops.mean(off_diag_loss)\n",
    "        loss = tf.cast(loss, tf.float32)\n",
    "        return loss\n",
    "    def call(self, z_a, z_b):\n",
    "        cor_matrix = get_cor_matrix(z_a, z_b)\n",
    "        return self.compute_loss(cor_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62b24a72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T17:51:21.388281Z",
     "iopub.status.busy": "2024-07-05T17:51:21.387584Z",
     "iopub.status.idle": "2024-07-05T17:51:21.395560Z",
     "shell.execute_reply": "2024-07-05T17:51:21.394712Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.037294,
     "end_time": "2024-07-05T17:51:21.397423",
     "exception": false,
     "start_time": "2024-07-05T17:51:21.360129",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VICRegLoss(keras.losses.Loss):\n",
    "    def __init__(self, var_coef = 20, invar_coef = 20, cov_coef = 1, gamma = 2.5):\n",
    "        super().__init__()\n",
    "        self.var_coef = var_coef\n",
    "        self.invar_coef = invar_coef\n",
    "        self.cov_coef = cov_coef\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def call(self, rep_a, rep_b):\n",
    "        invar_loss = invariance_loss(rep_a, rep_b) * self.invar_coef\n",
    "        invar_loss = ops.mean(invar_loss)\n",
    "        \n",
    "        variance_a = variance(rep_a, gamma = self.gamma)\n",
    "        variance_a = ops.mean(variance_a)\n",
    "        \n",
    "        covariance_a = covariance(rep_a)\n",
    "        \n",
    "        variance_b = variance(rep_b, gamma = self.gamma)\n",
    "        variance_b = ops.mean(variance_b)\n",
    "        \n",
    "        covariance_b = covariance(rep_b)\n",
    "        #Variance loss -> variance가 toward gamma\n",
    "        # covariance -> covariance가 toward zero\n",
    "        var_loss = (variance_a + variance_b) * self.var_coef\n",
    "        covar_loss = (covariance_a + covariance_b) * self.cov_coef\n",
    "        loss = invar_loss + var_loss + covar_loss\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9de43fa",
   "metadata": {
    "papermill": {
     "duration": 0.025597,
     "end_time": "2024-07-05T17:51:21.449853",
     "exception": false,
     "start_time": "2024-07-05T17:51:21.424256",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# CREATING NEW BACKBONES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "369edaa5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T17:51:21.503672Z",
     "iopub.status.busy": "2024-07-05T17:51:21.502841Z",
     "iopub.status.idle": "2024-07-05T17:51:21.522805Z",
     "shell.execute_reply": "2024-07-05T17:51:21.522054Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.048799,
     "end_time": "2024-07-05T17:51:21.524646",
     "exception": false,
     "start_time": "2024-07-05T17:51:21.475847",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TRBlock(keras.layers.Layer): #ViT Transformer block with register token, attention weight return\n",
    "    def __init__(self, att_depth, att_heads, att_dims,\n",
    "                 embed_dims = None,\n",
    "                 return_att_weight = True,\n",
    "                n_register_token = 4, dropout_rate = 0.25,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.att_depth = att_depth\n",
    "        self.att_dims = att_dims\n",
    "        self.return_weight = return_att_weight\n",
    "        self.n_reg = n_register_token\n",
    "        if embed_dims is None:\n",
    "            self.embed_dims = self.att_dims\n",
    "        else:\n",
    "            self.embed_dims = embed_dims\n",
    "            \n",
    "        self.ln_before_set = [keras.layers.LayerNormalization(name = f'LN_before_{i+1}') for i in range(self.att_depth)]\n",
    "        self.ln_after_set = [keras.layers.LayerNormalization(name = f'LN_after_{i+1}') for i in range(self.att_depth)]\n",
    "        self.mha_set = [keras.layers.MultiHeadAttention(att_heads, att_dims, name = f\"MHA_{i+1}\", use_bias = False) for i in range(self.att_depth)]\n",
    "        self.att_dropout = keras.layers.Dropout(dropout_rate)\n",
    "        self.proj_dropout = keras.layers.Dropout(dropout_rate)\n",
    "    def build(self, input_shape):\n",
    "        #query, key, value\n",
    "        if len(input_shape) == 3:\n",
    "            query_shape, key_shape, value_shape = input_shape[0], input_shape[1], input_shape[2]\n",
    "        elif len(input_shape) == 2:\n",
    "            query_shape, key_shape = input_shape[0], input_shape[1]\n",
    "            value_shape = key_shape\n",
    "        batch_size = query_shape[0]\n",
    "        self.query_length = query_shape[1]\n",
    "        self.embed_dims = query_shape[2]\n",
    "        self.embed_dim = self.embed_dims\n",
    "        self.dense_set = [Dense(units = self.embed_dims, name = f\"Dense_{i+1}\", activation = \"gelu\") for i in range(self.att_depth)]\n",
    "        self.first_embedding = Dense(units = self.embed_dims, activation = 'gelu')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if len(inputs) == 2:\n",
    "            query = inputs[0]\n",
    "            key = inputs[1]\n",
    "            value = key\n",
    "        elif len(inputs) == 3:\n",
    "            query, key, value = inputs\n",
    "        tok_ = keras.layers.GlobalAveragePooling1D()(query)\n",
    "        tok_ = tok_[:, tf.newaxis, :]\n",
    "        if self.n_reg > 0:\n",
    "            self.register_tokens = ops.cast(ops.ones_like(tok_), \"float32\")\n",
    "            self.register_tokens = ops.tile(self.register_tokens, [1, self.n_reg, 1])\n",
    "        else:\n",
    "            self.register_tokens = None\n",
    "        self.cls_token = ops.cast(ops.ones_like(tok_), \"float32\")\n",
    "        \n",
    "        \n",
    "        if self.register_tokens is not None:\n",
    "            encoded_patches = ops.concatenate([query, \n",
    "                                              self.cls_token,\n",
    "                                              self.register_tokens], axis = 1)\n",
    "        else:\n",
    "            encoded_patches = ops.concatenate([query, \n",
    "                                              self.cls_token], axis = 1)\n",
    "        encoded_patches = self.first_embedding(encoded_patches)\n",
    "        \n",
    "        for idx in range(self.att_depth):\n",
    "            x0 = self.ln_before_set[idx](encoded_patches)\n",
    "            x1_ = self.mha_set[idx](query = encoded_patches, key = key, value = value,\n",
    "                                    return_attention_scores = self.return_weight)\n",
    "\n",
    "            if self.return_weight:\n",
    "                x1, att_weights = x1_\n",
    "                del x1_\n",
    "            else:\n",
    "                x1 = x1_\n",
    "                del x1_\n",
    "            x1 = self.att_dropout(x1)\n",
    "            x2 = x0 + x1\n",
    "            x3 = self.ln_after_set[idx](x2)\n",
    "            x4 = self.dense_set[idx](x3)\n",
    "            x4 = self.proj_dropout(x4)\n",
    "            encoded_patches = x2+x4\n",
    "        patches, cls_token = encoded_patches[:, :self.query_length, :], encoded_patches[:, self.query_length, :]\n",
    "        del encoded_patches\n",
    "        if self.return_weight:\n",
    "            return cls_token, patches, att_weights[:, :, self.query_length:self.query_length+1, :self.query_length]\n",
    "        else:\n",
    "            return cls_token, patches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c337ffa4",
   "metadata": {
    "papermill": {
     "duration": 0.026824,
     "end_time": "2024-07-05T17:51:21.577564",
     "exception": false,
     "start_time": "2024-07-05T17:51:21.550740",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Brand-New TransformerEncoder block\n",
    "- input tensor : patches (PE를 거치지 않은)\n",
    "    - [batch_size, sequence_length, c] shape tensor. 각각 q, k 혹은 q, k, v\n",
    "- ViT에서 제시된 Transformer Encoder 블록을 유지하되, Self-Attention 자리에:\n",
    "    - simple MHA\n",
    "    - gated MLP 및 gaMLP(-> PE 거치지 않고 attention-like op 수행)\n",
    "    - MLP mixer\n",
    "    - ConvMixer\n",
    "    - Focal Modulation\n",
    "    - 들이 들어갈 수 있게 함. \n",
    "        - Ensemble : 모든 경우의 수의 가중 합\n",
    "- Code reference: [MLP and FNet](https://keras.io/examples/vision/mlp_image_classification/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f826522",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T17:51:21.675958Z",
     "iopub.status.busy": "2024-07-05T17:51:21.675149Z",
     "iopub.status.idle": "2024-07-05T17:51:21.692874Z",
     "shell.execute_reply": "2024-07-05T17:51:21.691964Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.046818,
     "end_time": "2024-07-05T17:51:21.694746",
     "exception": false,
     "start_time": "2024-07-05T17:51:21.647928",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DeformableConv2D(keras.layers.Layer):\n",
    "    def __init__(self, filters, kernel_size, groups = None, **kwargs):\n",
    "        super(DeformableConv2D, self).__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv = keras.layers.Conv2D(filters, kernel_size, padding='same', use_bias = True, groups = groups,\n",
    "                                       kernel_regularizer = keras.regularizers.L1L2(l1=0.01, l2=0.01))\n",
    "        self.offset_conv = keras.layers.Conv2D(2 * kernel_size * kernel_size, kernel_size, padding='same', use_bias=False,\n",
    "                                              kernel_regularizer = keras.regularizers.L1L2(l1=0.01, l2=0.01))\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        c = ops.shape(inputs)[-1]\n",
    "        offsets = self.offset_conv(inputs)\n",
    "        grid_y, grid_x = tf.meshgrid(tf.range(tf.shape(inputs)[1]), tf.range(tf.shape(inputs)[2]))\n",
    "        grid = tf.stack((grid_y, grid_x), axis=-1)  # shape (H, W, 2)\n",
    "        grid = tf.expand_dims(grid, axis=0)  # shape (1, H, W, 2)\n",
    "        grid = tf.tile(grid, [tf.shape(inputs)[0], 1, 1, 1])  # shape (N, H, W, 2)\n",
    "        \n",
    "        offsets = tf.reshape(offsets, [tf.shape(inputs)[0], tf.shape(inputs)[1], tf.shape(inputs)[2], -1, 2])\n",
    "        grid = tf.expand_dims(grid, axis=3)\n",
    "        grid = tf.cast(grid, \"float32\")\n",
    "        \n",
    "        offsets = offsets + grid\n",
    "        offsets = tf.cast(offsets, \"float32\")\n",
    "        \n",
    "        x_offset = tf.clip_by_value(offsets[..., 1], 0.0, \n",
    "                            tf.cast(tf.shape(inputs)[2] - 1, \"float32\")\n",
    "                           )\n",
    "        y_offset = tf.clip_by_value(offsets[..., 0], 0.0, \n",
    "                            tf.cast(tf.shape(inputs)[1] - 1, \"float32\")\n",
    "                           )\n",
    "        x_offset, y_offset = tf.cast(x_offset, \"int32\"), tf.cast(y_offset, \"int32\")\n",
    "        \n",
    "        batch_indices = tf.tile(tf.range(tf.shape(inputs)[0])[:, tf.newaxis, tf.newaxis, tf.newaxis], [1, tf.shape(inputs)[1], tf.shape(inputs)[2], self.kernel_size * self.kernel_size])\n",
    "        batch_indices = tf.reshape(batch_indices, [-1])\n",
    "        y_offset = tf.reshape(y_offset, [-1])\n",
    "        x_offset = tf.reshape(x_offset, [-1])\n",
    "        \n",
    "        indices = tf.stack([batch_indices, y_offset, x_offset], axis=-1)\n",
    "        sampled_features = tf.gather_nd(inputs, indices)\n",
    "        sampled_features = tf.reshape(sampled_features, \n",
    "                                      [tf.shape(inputs)[0], tf.shape(inputs)[1], tf.shape(inputs)[2], self.kernel_size * self.kernel_size, -1])\n",
    "        \n",
    "        sampled_features = tf.transpose(sampled_features, [0, 1, 2, 4, 3])\n",
    "        sampled_features = tf.reshape(sampled_features, [tf.shape(inputs)[0], tf.shape(inputs)[1], tf.shape(inputs)[2], c*self.kernel_size * self.kernel_size])\n",
    "        output = self.conv(sampled_features)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c0b4c9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T17:51:21.749290Z",
     "iopub.status.busy": "2024-07-05T17:51:21.748668Z",
     "iopub.status.idle": "2024-07-05T17:51:21.761156Z",
     "shell.execute_reply": "2024-07-05T17:51:21.760286Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.042087,
     "end_time": "2024-07-05T17:51:21.763001",
     "exception": false,
     "start_time": "2024-07-05T17:51:21.720914",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FNetLayer(layers.Layer):\n",
    "    def __init__(self, embedding_dim, dropout_rate, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "    def call(self, inputs):\n",
    "        # Apply fourier transformations.\n",
    "        real_part = inputs\n",
    "        im_part = keras.ops.zeros_like(inputs)\n",
    "        x = keras.ops.fft2((real_part, im_part))[0]\n",
    "        return x\n",
    "\n",
    "class gDense(layers.Layer): # 원본 논문에서, channel proj, Activation ~ SGU 및 그 이후 Channel proj까지 구현\n",
    "    def __init__(self, embed_dims, dropout_rate = 0.25, use_attention = False,\n",
    "                 *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.embed_dims = embed_dims\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.gelu = keras.layers.Activation('gelu')\n",
    "        self.channel_proj_first = Dense(units = embed_dims, use_bias = False)\n",
    "        self.use_attention = use_attention\n",
    "        self.ln = keras.layers.LayerNormalization(name = \"LN_in_SGU\")        \n",
    "    def build(self, input_shape):\n",
    "        batch_size, self.n_patches, c = input_shape\n",
    "        self.channel_proj_second = Dense(units = c, use_bias = False)\n",
    "        self.spatial_proj = Dense(units = self.n_patches, bias_initializer = keras.initializers.Ones(), \n",
    "                                  kernel_initializer = keras.initializers.Zeros())\n",
    "        if self.use_attention:\n",
    "            self.tiny_attn = MultiHeadAttention(4,64,dropout = self.dropout_rate, use_bias = False, output_shape = self.embed_dims//2)\n",
    "    def call(self, inputs):\n",
    "        x = self.channel_proj_first(inputs)\n",
    "        x = self.gelu(x)\n",
    "        #Spatial Gating unit\n",
    "        u, v = keras.ops.split(x, indices_or_sections=2, axis=2)\n",
    "        v_proj = self.ln(v)\n",
    "        v_proj = keras.ops.transpose(v_proj, (0,2,1))\n",
    "        v_proj = self.spatial_proj(v_proj)\n",
    "        v_proj = keras.ops.transpose(v_proj, (0,2,1))\n",
    "        if self.use_attention:\n",
    "            attn_ = self.tiny_attn(query = v, key = v, value = v)\n",
    "            v_proj += attn_\n",
    "        x = u*v_proj\n",
    "        x = self.channel_proj_second(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81a71139",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T17:51:21.816936Z",
     "iopub.status.busy": "2024-07-05T17:51:21.816288Z",
     "iopub.status.idle": "2024-07-05T17:51:21.831995Z",
     "shell.execute_reply": "2024-07-05T17:51:21.831136Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.044796,
     "end_time": "2024-07-05T17:51:21.834070",
     "exception": false,
     "start_time": "2024-07-05T17:51:21.789274",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Focal modulation implementation -> From keras, https://keras.io/examples/vision/focal_modulation_network/#focal-modulation-layer\n",
    "class FocalModulationLayer(layers.Layer):\n",
    "    \"\"\"The Focal Modulation layer includes query projection & context aggregation.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Projection dimension.\n",
    "        focal_window (int): Window size for focal modulation.\n",
    "        focal_level (int): The current focal level.\n",
    "        focal_factor (int): Factor of focal modulation.\n",
    "        proj_drop_rate (float): Rate of dropout.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        focal_window: int,\n",
    "        focal_level: int,\n",
    "        focal_factor: int = 2,\n",
    "        proj_drop_rate: float = 0.0,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dim = dim\n",
    "        self.focal_window = focal_window\n",
    "        self.focal_level = focal_level\n",
    "        self.focal_factor = focal_factor\n",
    "        self.proj_drop_rate = proj_drop_rate\n",
    "\n",
    "        # Project the input feature into a new feature space using a\n",
    "        # linear layer. Note the `units` used. We will be projecting the input\n",
    "        # feature all at once and split the projection into query, context,\n",
    "        # and gates.\n",
    "        self.initial_proj = layers.Dense(\n",
    "            units=(2 * self.dim) + (self.focal_level + 1),\n",
    "            use_bias=True,\n",
    "        )\n",
    "        self.focal_layers = list()\n",
    "        self.kernel_sizes = list()\n",
    "        for idx in range(self.focal_level):\n",
    "            kernel_size = (self.focal_factor * idx) + self.focal_window\n",
    "            depth_gelu_block = keras.Sequential(\n",
    "                [\n",
    "                    layers.ZeroPadding2D(padding=(kernel_size // 2, kernel_size // 2)),\n",
    "                    layers.Conv2D(\n",
    "                        filters=self.dim,\n",
    "                        kernel_size=kernel_size,\n",
    "                        activation=keras.activations.gelu,\n",
    "                        groups=self.dim,\n",
    "                        use_bias=False,\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "            self.focal_layers.append(depth_gelu_block)\n",
    "            self.kernel_sizes.append(kernel_size)\n",
    "        self.activation = keras.activations.gelu\n",
    "        self.gap = layers.GlobalAveragePooling2D(keepdims=True)\n",
    "        self.modulator_proj = layers.Conv2D(\n",
    "            filters=self.dim,\n",
    "            kernel_size=(1, 1),\n",
    "            use_bias=True,\n",
    "        )\n",
    "        self.proj = layers.Dense(units=self.dim)\n",
    "        self.proj_drop = layers.Dropout(self.proj_drop_rate)\n",
    "\n",
    "    def call(self, x: tf.Tensor, training = True) -> tf.Tensor:\n",
    "        \"\"\"Forward pass of the layer.\n",
    "\n",
    "        Args:\n",
    "            x: Tensor of shape (B, H, W, C)\n",
    "        \"\"\"\n",
    "        # Apply the linear projecion to the input feature map\n",
    "        x_proj = self.initial_proj(x)\n",
    "\n",
    "        # Split the projected x into query, context and gates\n",
    "        query, context, self.gates = tf.split(\n",
    "            value=x_proj,\n",
    "            num_or_size_splits=[self.dim, self.dim, self.focal_level + 1],\n",
    "            axis=-1,\n",
    "        )\n",
    "\n",
    "        # Context aggregation\n",
    "        context = self.focal_layers[0](context)\n",
    "        context_all = context * self.gates[..., 0:1]\n",
    "        for idx in range(1, self.focal_level):\n",
    "            context = self.focal_layers[idx](context)\n",
    "            context_all += context * self.gates[..., idx : idx + 1]\n",
    "\n",
    "        # Build the global context\n",
    "        context_global = self.activation(self.gap(context))\n",
    "        context_all += context_global * self.gates[..., self.focal_level :]\n",
    "\n",
    "        # Focal Modulation\n",
    "        self.modulator = self.modulator_proj(context_all)\n",
    "        x_output = query * self.modulator\n",
    "\n",
    "        # Project the output and apply dropout\n",
    "        x_output = self.proj(x_output)\n",
    "        x_output = self.proj_drop(x_output)\n",
    "\n",
    "        return x_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d29bb570",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T17:51:21.888539Z",
     "iopub.status.busy": "2024-07-05T17:51:21.887771Z",
     "iopub.status.idle": "2024-07-05T17:51:21.909230Z",
     "shell.execute_reply": "2024-07-05T17:51:21.908546Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.050709,
     "end_time": "2024-07-05T17:51:21.911222",
     "exception": false,
     "start_time": "2024-07-05T17:51:21.860513",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# input, delta, A, B, C, D로 SSM을 구현\n",
    "def _ssm(x, delta, A, B, C, D):\n",
    "    \"\"\"\n",
    "    c : input tensor x의 channel (paper notation = n)\n",
    "    d : 모델 내부에서 projection dimension (attention dims 비슷한 역할)\n",
    "    \n",
    "    x : input tensor, [batch, length, c]\n",
    "    delta : 이산화를 위한 parameter\n",
    "    A, B, C, D : SSM에 쓰이는 Tensors\n",
    "    \n",
    "    delta : [batch, length, d]\n",
    "    A : [d, c]\n",
    "    B : [batch, length, d]\n",
    "    input : X, output : X와 같은 shape의 tensor\n",
    "    \"\"\"\n",
    "    #1. discretize\n",
    "    dA = tf.einsum(\"bld, dc -> bldc\", delta, A) #외적\n",
    "    dBu = tf.einsum(\"bld, bld, blc -> bldc\", delta, x, B) #B_bar x inputs(x)\n",
    "    #2. pre-calcuate the kernel\n",
    "    dA_cumulateSum = tf.pad(dA[:, 1:], [[0, 0], [1, 1], [0, 0], [0, 0]])[:, 1:, :, :] # <- Zero padding\n",
    "    dA_cumulateSum = tf.reverse(dA_cumulateSum, axis=[1]) # <- later token 가중치는 이전 가중치들의 총합이라 뒤집어 계산하는게 편함\n",
    "    dA_cumulateSum = tf.math.cumsum(A, axis = 1)\n",
    "    dA_cumulateSum = tf.exp(dA_cumulateSum) #A_bar = exp(delta * A)\n",
    "    dA_cumulateSum = tf.reverse(dA_cumulateSum, axis = [1])\n",
    "    \n",
    "    hidden = dBu * dA_cumulateSum\n",
    "    hidden = tf.math.cumsum(hidden, axis = 1)/(dA_cumulateSum + 1e-5)\n",
    "    y = tf.einsum(\"bldc, blc -> bld\", hidden, C)\n",
    "    return y + x*D\n",
    "\n",
    "class MambaBlock(keras.layers.Layer):\n",
    "    def __init__(self, hidden_dims, projection_expand_factor = 2,\n",
    "                kernel_size = 5, conv_use_bias = True, dense_use_bias = False,\n",
    "                **kwargs) : \n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden_dims = hidden_dims\n",
    "        assert isinstance(projection_expand_factor, int), 'projection_expand_factor should be integer' \n",
    "        self.proj_exp_factor = projection_expand_factor\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv_use_bias = conv_use_bias\n",
    "        self.dense_use_bias = dense_use_bias\n",
    "        \n",
    "        self.model_internal_dim = self.hidden_dims * self.proj_exp_factor\n",
    "        self.conv1d = keras.layers.Conv1D(filters = self.model_internal_dim,\n",
    "                                         use_bias = conv_use_bias,\n",
    "                                         kernel_size = kernel_size,\n",
    "                                         groups = self.model_internal_dim,\n",
    "                                         data_format = \"channels_first\", #ssm 구현 파트에서 tensor axis transpose가 일어난다!!\n",
    "                                         padding = \"causal\")\n",
    "        # Mamba에서는 A만 input indep. 학습대상, B, C, D, delta는 input dependent\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        batch_size, seq_len, d_in = input_shape\n",
    "        self.seq_len = seq_len\n",
    "        self.d_in = d_in\n",
    "        self.in_proj = keras.layers.Dense(units = 2*self.model_internal_dim, \n",
    "                                         input_shape = (d_in,),\n",
    "                                         use_bias = False)\n",
    "        self.dt_rank = int(d_in // 16)\n",
    "        # 특정 시점의 input tensor를 받아 delta, B, C 계산\n",
    "        self.x_proj = keras.layers.Dense(units = self.dt_rank + 2*self.hidden_dims, \n",
    "                                         use_bias = False)\n",
    "        # delta tensor를 받아 model internal dim으로 embed.\n",
    "        self.dt_proj = keras.layers.Dense(self.model_internal_dim,\n",
    "                                         input_shape = (self.dt_rank,),\n",
    "                                         use_bias = True)\n",
    "        ### A, D setting\n",
    "        self.A = self.add_weight(shape = (self.model_internal_dim, self.hidden_dims),\n",
    "                                 initializer = \"glorot_normal\",\n",
    "                                 trainable = True,\n",
    "                                 dtype = \"float32\")\n",
    "        self.D = self.add_weight(shape = (self.model_internal_dim,),\n",
    "                                 initializer = \"ones\",\n",
    "                                 trainable = True,\n",
    "                                 dtype = \"float32\")\n",
    "        self.out_proj = keras.layers.Dense(units = self.d_in, input_shape = (self.model_internal_dim,),\n",
    "                                          use_bias = self.dense_use_bias)\n",
    "    def compute_ssm(self, x):\n",
    "        #input x : [batch, seq_len, internal_dim] shape tensor\n",
    "        \n",
    "        #1. discretize delta, A~D\n",
    "        #A shape : d_in, n == model_internal_dim, hidden dim\n",
    "        A = -ops.exp(self.A)\n",
    "        D = ops.cast(self.D, \"float32\")\n",
    "        x_dbl = self.x_proj(x) #[batch, dt_rank + 2*hid_dim] shape tensor\n",
    "        (delta, B, C) = tf.split(x_dbl, num_or_size_splits = [self.dt_rank, self.hidden_dims, self.hidden_dims],\n",
    "                                axis = -1)\n",
    "        delta = self.dt_proj(delta)\n",
    "        delta = keras.activations.softplus(delta)\n",
    "        ssm_output = _ssm(x, delta, A, B, C, D)\n",
    "        return ssm_output\n",
    "    def call(self, x):\n",
    "        #x = [batch_size, seq_len, embed_dim] shape tensor\n",
    "        try:\n",
    "            x_with_shortcut = self.in_proj(x)\n",
    "            (x, shortcut) = tf.split(x_with_shortcut, num_or_size_splits = [self.model_internal_dim, self.model_internal_dim],\n",
    "                                    axis = -1)\n",
    "            x = ops.transpose(x, [0, 2, 1])\n",
    "            x = self.conv1d(x)\n",
    "            x = ops.transpose(x, [0, 2, 1])\n",
    "            x = keras.activations.swish(x)\n",
    "            x_ssm = self.compute_ssm(x)\n",
    "            x_ssm *= keras.activations.swish(shortcut)\n",
    "            output = self.out_proj(x_ssm)\n",
    "            return output\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf666dbd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T17:51:21.964683Z",
     "iopub.status.busy": "2024-07-05T17:51:21.964318Z",
     "iopub.status.idle": "2024-07-05T17:51:21.984022Z",
     "shell.execute_reply": "2024-07-05T17:51:21.983144Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.048763,
     "end_time": "2024-07-05T17:51:21.985899",
     "exception": false,
     "start_time": "2024-07-05T17:51:21.937136",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MetaEncoder(keras.layers.Layer):\n",
    "    def __init__(self, operation_type = 'attention', att_dims = 256, att_heads = 8, dropout_rate = 0.2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        assert operation_type in [\"attention\", \"gMLP\", 'gaMLP', 'mlpmixer', \"fnet\",\n",
    "                                  'focal_modulation', \"mamba\", \"Mamba\", \"S6\"]\n",
    "        self.op_type = operation_type\n",
    "        self.att_dims = att_dims\n",
    "        self.att_heads = att_heads\n",
    "        \n",
    "        self.layernorm1 = keras.layers.LayerNormalization()\n",
    "        self.layernorm2 = keras.layers.LayerNormalization()\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "    def build(self, input_shape): #image input\n",
    "        if len(input_shape) == 3:\n",
    "            q_shape, k_shape, v_shape = input_shape\n",
    "        elif len(input_shape) == 2:\n",
    "            q_shape, k_shape = input_shape\n",
    "            v_shape = k_shape\n",
    "        elif len(input_shape) == 1:\n",
    "            q_shape = input_shape[0]\n",
    "            v_shape, k_shape = q_shape, q_shape\n",
    "        \n",
    "        batch_size, self.n_patches, self.embed_dims = q_shape\n",
    "        self.res_ = tf.cast(tf.cast(self.n_patches, \"float32\")**0.5,\n",
    "                      \"int32\")\n",
    "        self.proj = Dense(units = self.embed_dims, activation = \"gelu\")\n",
    "        if self.op_type == 'attention':\n",
    "            self.op_fn = keras.layers.MultiHeadAttention(self.att_heads, self.att_dims, \n",
    "                                                        dropout = self.dropout_rate, use_bias = False)\n",
    "        elif self.op_type == \"mlpmixer\":\n",
    "            self.op_fn = keras.Sequential([keras.layers.Permute((2,1)),\n",
    "                                           keras.layers.Dense(units = self.n_patches, use_bias = False),\n",
    "                                           keras.layers.Activation(\"gelu\"),\n",
    "                                           keras.layers.Dropout(self.dropout_rate),\n",
    "                                           keras.layers.Dense(units = self.n_patches, use_bias = False),\n",
    "                                          keras.layers.Permute((2,1))]\n",
    "                                         )\n",
    "        elif self.op_type == \"gMLP\":\n",
    "            self.op_fn = gDense(embed_dims = 2*self.embed_dims, \n",
    "                                use_attention = False)\n",
    "        elif self.op_type == \"gaMLP\":\n",
    "            self.op_fn = gDense(embed_dims = 2*self.embed_dims, \n",
    "                                use_attention = True)\n",
    "        elif self.op_type == 'fnet':\n",
    "            self.op_fn = FNetLayer(self.embed_dims, self.dropout_rate)\n",
    "        elif self.op_type == \"focal_modulation\":\n",
    "            self.op_fn = FocalModulationLayer(self.embed_dims, \n",
    "                                              focal_window = 3, \n",
    "                                              focal_level = 3,\n",
    "                                             proj_drop_rate = self.dropout_rate)\n",
    "        elif self.op_type in [\"mamba\", \"Mamba\", \"S6\"] :\n",
    "            self.op_fn = MambaBlock(self.att_dims, name = \"ForwardMamba\")\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        if len(inputs) == 3:\n",
    "            q, k, v = inputs\n",
    "        elif len(inputs) == 2:\n",
    "            q, k = inputs\n",
    "            v = k\n",
    "        elif len(inputs) == 1:\n",
    "            q = inputs[0]\n",
    "            k = q\n",
    "            v = q\n",
    "        shortcut = q\n",
    "        q = self.layernorm1(q)\n",
    "        if self.op_type == \"attention\":\n",
    "            q, att_weight = self.op_fn(query = q, key = k, value = v, return_attention_scores = True)\n",
    "        else:\n",
    "            if self.op_type == \"focal_modulation\":\n",
    "                q = ops.reshape(q, [-1, self.res_, self.res_, self.embed_dims])\n",
    "                q = self.op_fn(q)\n",
    "                q = ops.reshape(q, [-1, self.n_patches, self.embed_dims])\n",
    "            elif self.op_type in [\"mamba\", \"Mamba\", \"S6\"]:\n",
    "                q = 0.5*(self.op_fn(q) + self.op_fn(tf.reverse(q, axis = [1])\n",
    "                                                   )\n",
    "                        )\n",
    "            else:\n",
    "                q = self.op_fn(q)\n",
    "        q = q + shortcut\n",
    "        q2 = q\n",
    "        q = self.layernorm2(q)\n",
    "        q = self.proj(q)\n",
    "        q += q2 ; del q2\n",
    "        return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e9eecbb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T17:51:22.039779Z",
     "iopub.status.busy": "2024-07-05T17:51:22.039152Z",
     "iopub.status.idle": "2024-07-05T17:51:22.058297Z",
     "shell.execute_reply": "2024-07-05T17:51:22.057400Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.048193,
     "end_time": "2024-07-05T17:51:22.060299",
     "exception": false,
     "start_time": "2024-07-05T17:51:22.012106",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AttentionPooling(keras.layers.Layer):\n",
    "    def __init__(self, attention_heads, attention_dims = None, bias = False, scale = None, \n",
    "                 dropout_rate = 0.05, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.n_heads = attention_heads\n",
    "        self.n_dims = attention_dims\n",
    "        self.bias = bias\n",
    "        self.scale = scale\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # query, key\n",
    "        query_dims = input_shape[0][-1]\n",
    "        key_length = input_shape[1][1]\n",
    "        \n",
    "        if self.n_dims == None:\n",
    "            embed_dims = query_dims\n",
    "        else:\n",
    "            embed_dims = self.n_dims\n",
    "        self.embed_dims = embed_dims\n",
    "        self.per_head_dims = embed_dims//self.n_heads\n",
    "        \n",
    "        self.scale = self.scale if self.scale != None else embed_dims**-0.5\n",
    "        \n",
    "        self.query_embed_fn = keras.layers.Dense(units = embed_dims, use_bias = self.bias,\n",
    "                                               name = \"Q_Embedding_Dense_layer\")\n",
    "        self.key_embed_fn = keras.layers.Dense(units = embed_dims, use_bias = self.bias,\n",
    "                                               name = \"K_Embedding_Dense_layer\")\n",
    "        self.value_embed_fn = keras.layers.Dense(units = embed_dims, use_bias = self.bias,\n",
    "                                               name = \"V_Embedding_Dense_layer\")\n",
    "        \n",
    "        self.softmax = keras.layers.Activation(\"softmax\", name = \"AttentionWeightSoftmax\")\n",
    "        self.proj = keras.layers.Dense(units = query_dims, use_bias = self.bias, \n",
    "                                      name = \"ProjectToOriginalDimension\")\n",
    "        self.att_dropout = keras.layers.Dropout(self.dropout_rate)\n",
    "        self.proj_dropout = keras.layers.Dropout(self.dropout_rate)\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, inputs, **kwargs):\n",
    "        if len(inputs) == 2:\n",
    "            q, k = inputs\n",
    "            value_ = False\n",
    "        elif len(inputs) == 3:\n",
    "            q, k, v = inputs\n",
    "            value_ = True\n",
    "            \n",
    "        if len(ops.shape(q)) == 2:\n",
    "            q = q[:, tf.newaxis, :]\n",
    "        if len(ops.shape(k)) == 4:\n",
    "            b_, w_, h_, dims_ = ops.shape(k)\n",
    "            k = ops.reshape(k, [b_, w_*h_, dims_])\n",
    "        if (value_) and (len(ops.shape(v))) == 4:\n",
    "            b_, w_, h_, dims_ = ops.shape(v)\n",
    "            v = ops.reshape(v, [b_, w_*h_, dims_])\n",
    "            \n",
    "        batch_size, query_length, q_dims = ops.shape(q)\n",
    "        _, key_length, k_dms = ops.shape(k)\n",
    "        \n",
    "        query = self.query_embed_fn(q) * self.scale #batch, 1(or, query length), q_dims\n",
    "        query = ops.reshape(query, [batch_size, query_length, self.n_heads, self.per_head_dims])\n",
    "        \n",
    "        key = self.key_embed_fn(k)\n",
    "        key = ops.reshape(key, [batch_size, key_length, self.n_heads, self.per_head_dims])\n",
    "        \n",
    "        if value_:\n",
    "            value = self.value_embed_fn(v)#각각 batch, token_length, heads, per_head_dims\n",
    "        else:\n",
    "            value = self.value_embed_fn(k)\n",
    "        value = ops.reshape(value, [batch_size, key_length, self.n_heads, self.per_head_dims])\n",
    "        attention_score = keras.ops.einsum(\"abhd, achd -> ahbc\",\n",
    "                                          query, key) #b = query length, c = key length\n",
    "        attention_weight = self.softmax(attention_score)\n",
    "        attention_weight = self.att_dropout(attention_weight)\n",
    "        self.attention_weight = attention_weight \n",
    "        attended_output = keras.ops.einsum(\"ahbc, achd -> abhd\",\n",
    "                                          attention_weight, value)\n",
    "        attended_output = keras.ops.reshape(attended_output, \n",
    "                                           [batch_size, query_length, self.n_heads*self.per_head_dims]\n",
    "                                           )\n",
    "        attended_output = self.proj(attended_output)\n",
    "        attended_output = self.proj_dropout(attended_output)\n",
    "        return attended_output, attention_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2cc7c9ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T17:51:22.115022Z",
     "iopub.status.busy": "2024-07-05T17:51:22.114047Z",
     "iopub.status.idle": "2024-07-05T17:51:22.124281Z",
     "shell.execute_reply": "2024-07-05T17:51:22.123384Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.039487,
     "end_time": "2024-07-05T17:51:22.126298",
     "exception": false,
     "start_time": "2024-07-05T17:51:22.086811",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ImagePatchEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, patch_size, embed_dim, groups, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = embed_dim\n",
    "        #self.conv = DeformableConv2D(filters = self.embed_dim, kernel_size = 4, groups = groups)\n",
    "        self.conv = keras.layers.Conv2D(filters = self.embed_dim, kernel_size = 3, \n",
    "                                        groups = groups, activation = \"gelu\", padding = 'same',\n",
    "                                       kernel_regularizer = keras.regularizers.L1L2(l1=0.01, l2=0.01)\n",
    "                                       )\n",
    "    def build(self, input_shape): #image input\n",
    "        batch_size, h, w, c = input_shape\n",
    "        self.n_patches = (h//self.patch_size) * (w//self.patch_size)\n",
    "        self.pe_coefficient = c**-0.5\n",
    "        self.position_embedding= keras.layers.Embedding(\n",
    "            input_dim=self.n_patches, output_dim=self.embed_dim\n",
    "        )\n",
    "    def call(self, image):\n",
    "        \n",
    "        positions = ops.expand_dims(\n",
    "            ops.arange(start=0, stop=self.n_patches, step=1), axis=0\n",
    "        )\n",
    "\n",
    "        image = ops.image.extract_patches(image, size= self.patch_size, padding = 'same')\n",
    "        image = self.conv(image)\n",
    "        batch, w, h, dims = ops.shape(image)\n",
    "        image = ops.reshape(image, [-1, w*h, dims])\n",
    "        image += self.pe_coefficient * (self.position_embedding(positions))\n",
    "       \n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a328342",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T17:51:22.180550Z",
     "iopub.status.busy": "2024-07-05T17:51:22.179663Z",
     "iopub.status.idle": "2024-07-05T17:51:22.187455Z",
     "shell.execute_reply": "2024-07-05T17:51:22.186527Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.036769,
     "end_time": "2024-07-05T17:51:22.189355",
     "exception": false,
     "start_time": "2024-07-05T17:51:22.152586",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PatchEncoder(keras.layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = keras.layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = keras.layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = ops.expand_dims(\n",
    "            ops.arange(start=0, stop=self.num_patches, step=1), axis=0\n",
    "        )\n",
    "        projected_patches = self.projection(patch)\n",
    "        encoded = projected_patches + self.position_embedding(positions)\n",
    "        return encoded\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"num_patches\": self.num_patches})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4156642",
   "metadata": {
    "papermill": {
     "duration": 0.025502,
     "end_time": "2024-07-05T17:51:22.240918",
     "exception": false,
     "start_time": "2024-07-05T17:51:22.215416",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# General Context Vision Transformer implementation\n",
    "- Reference : [another kaggle notebook, identical implementation](https://www.kaggle.com/code/hskimjjys/general-context-vit-script/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6bd71d2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T17:51:22.304818Z",
     "iopub.status.busy": "2024-07-05T17:51:22.304461Z",
     "iopub.status.idle": "2024-07-05T17:51:22.393973Z",
     "shell.execute_reply": "2024-07-05T17:51:22.392993Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.121586,
     "end_time": "2024-07-05T17:51:22.396152",
     "exception": false,
     "start_time": "2024-07-05T17:51:22.274566",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SE(keras.layers.Layer):\n",
    "    def __init__(self, output_dim = None, squeeze_rate = 0.25, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.output_dim = output_dim\n",
    "        self.rate = squeeze_rate\n",
    "    def build(self, input_shape) : #batch_size, h, w, dims\n",
    "        if self.output_dim == None:\n",
    "            self.output_dim = input_shape[-1]\n",
    "        else:\n",
    "            pass\n",
    "        self.avg_pool = keras.layers.GlobalAveragePooling2D(keepdims = True, name = \"AvgPooling\")\n",
    "        self.mlps = keras.Sequential([keras.layers.Dense(units = int(self.rate * self.output_dim),\n",
    "                                                        use_bias = False, name = \"Dense1\"),\n",
    "                                      keras.layers.Activation(\"gelu\", name = \"GeluAct\"),\n",
    "                                      keras.layers.Dense(units = self.output_dim, use_bias = False, name = \"Dense2\"),\n",
    "                                      keras.layers.Activation(\"sigmoid\", name = \"Excitation_Sigmoid\")\n",
    "                                     ])\n",
    "        #super().build(input_shape)\n",
    "    def call(self, inputs, **kwargs):\n",
    "        pooled = self.avg_pool(inputs)\n",
    "        weights = self.mlps(pooled)\n",
    "        return inputs * weights\n",
    "    \n",
    "class DownSampler(keras.layers.Layer):\n",
    "    def __init__(self, keepdims = False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.keepdims = keepdims\n",
    "    def build(self, input_shape):\n",
    "        embed_dims = input_shape[-1]\n",
    "        out_dim = embed_dims if self.keepdims else 2*embed_dims\n",
    "        self.fused_mbconv = keras.Sequential([keras.layers.DepthwiseConv2D(kernel_size = 3, padding = 'same', use_bias = False, name = \"DWConv\"),\n",
    "                                             keras.layers.Activation(\"gelu\", name = 'GeluAct'),\n",
    "                                             SE(name = \"SqueezeAndExcitation2D\"),\n",
    "                                             keras.layers.Conv2D(filters = embed_dims, kernel_size = 1, padding = 'same', use_bias = False, name = \"PointWiseConv\")],\n",
    "                                            name = \"Fused_MBConvLayer\")\n",
    "        self.down_conv = keras.layers.Conv2D(filters = out_dim, kernel_size = 3, strides = 2, padding = 'same', use_bias = False, name = \"DownConvolution\")\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon = 1e-5, name = 'LayerNorm1')\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon = 1e-5, name = 'LayerNorm2')\n",
    "    def call(self, inputs, **kwargs):\n",
    "        x = self.layernorm1(inputs)\n",
    "        x += self.fused_mbconv(inputs)\n",
    "        x = self.down_conv(x)\n",
    "        return self.layernorm2(x)\n",
    "    \n",
    "class MLP(keras.layers.Layer):\n",
    "    def __init__(self, middle_dim = None, output_dim = None,\n",
    "                activation = 'gelu', dropout = 0.2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.middle_dim = middle_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.activation = activation\n",
    "        self.dropout_rate = dropout\n",
    "    def build(self, input_shape):\n",
    "        self.input_dims = input_shape[-1]\n",
    "        self.middle_dim = int(1.5*self.input_dims) if self.middle_dim == None else self.middle_dim\n",
    "        self.output_dim = self.input_dims if self.output_dim == None else self.output_dim\n",
    "        self.mlp1 = keras.layers.Dense(units = self.middle_dim, name = \"FirstMLP\")\n",
    "        self.act = keras.layers.Activation(self.activation, name = \"MiddleActivation\")\n",
    "        self.mlp2 = keras.layers.Dense(units = self.output_dim, name = \"SecondMLP\")\n",
    "        self.drop1 = keras.layers.Dropout(self.dropout_rate, name = \"Dropout1\")\n",
    "        self.drop2 = keras.layers.Dropout(self.dropout_rate, name = \"Dropout2\")\n",
    "    def call(self, inputs, **kwargs):\n",
    "        x = self.mlp1(inputs)\n",
    "        x = self.act(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.mlp2(x)\n",
    "        x = self.drop2(x)\n",
    "        return x\n",
    "    \n",
    "class PatchEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, patching_type = \"conv\", #conv or tokenlearner\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.patching_type = patching_type\n",
    "    def build(self, input_shape):\n",
    "        if (self.patching_type == \"tokenlearner\") or (self.patching_type == \"token_learner\"):\n",
    "            self.proj = keras.layers.Conv2D(self.embed_dim, kernel_size = 3, strides = 2, padding = 'same', name = \"projection_conv\") #Overlapping patches\n",
    "            #token learner implementation\n",
    "            batch_size, w, h, filters = input_shape\n",
    "            n_tokens = (w//4) * (h//4) ; self.resized_w, self.resized_h = int(w//4), int(h/4)\n",
    "            self.input_seq_flatten = keras.layers.Reshape([1, -1, self.embed_dim], name = \"image_to_sequence_reshape\")\n",
    "            self.layer_norm = keras.layers.LayerNormalization(epsilon = 1e-5)\n",
    "            self.attention_ops = keras.Sequential([keras.layers.Conv2D(n_tokens, kernel_size = 3, activation = \"gelu\", use_bias = False, padding = 'same'),\n",
    "                                                  keras.layers.Conv2D(n_tokens, kernel_size = 3, activation = \"gelu\", use_bias = False, padding = 'same'),\n",
    "                                                  keras.layers.Conv2D(n_tokens, kernel_size = 3, activation = \"sigmoid\", use_bias = False, padding = 'same'),\n",
    "                                                  keras.layers.Reshape([-1, n_tokens]), #batch_size, HW, n_tokens\n",
    "                                                  keras.layers.Permute([2,1])], #batch_size, n_tokens, HW\n",
    "                                                 name = \"Conv_for_attention_weight\")\n",
    "        else:\n",
    "            self.proj = keras.layers.Conv2D(self.embed_dim, kernel_size = 3, strides = 2, padding = 'same') #Overlapping patches\n",
    "            self.down_sample = DownSampler(keepdims = True, name = \"DownSampler_after_projection\")\n",
    "    def call(self, inputs, **kwargs):\n",
    "        if (self.patching_type == \"tokenlearner\") or (self.patching_type == \"token_learner\"):\n",
    "            #token learner implementation\n",
    "            norm_input = self.layer_norm(inputs)\n",
    "            proj_inputs = self.proj(norm_input)\n",
    "            seq_inputs = self.input_seq_flatten(proj_inputs) #batch, 1, HW, embed_dims\n",
    "            att_weights = self.attention_ops(proj_inputs) #batch, n_tokens, HW\n",
    "            att_weights = ops.expand_dims(att_weights, axis = -1) #batch, n_tokens, HW, 1\n",
    "            attended = att_weights * seq_inputs #batch, n_tokens, HW, embed_dims\n",
    "            attended = ops.mean(attended, axis = 2) #batch, n_tokens, embed_dims\n",
    "            #reshape to 2D array\n",
    "            attended = ops.reshape(attended, [-1, self.resized_w, self.resized_h, self.embed_dim]\n",
    "                                  )\n",
    "            return attended\n",
    "        else:\n",
    "            x = self.proj(inputs)\n",
    "            x = self.down_sample(x)\n",
    "            return x\n",
    "        \n",
    "class FeatExtract(keras.layers.Layer):\n",
    "    def __init__(self, keepdims = False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.keepdims = keepdims\n",
    "    def build(self, input_shape):\n",
    "        batch_size, H, W, embed_dims = input_shape\n",
    "        self.fused_mbconv = keras.Sequential([keras.layers.DepthwiseConv2D(kernel_size = 3, padding = 'same', use_bias = False, name = \"DWConv\"),\n",
    "                                             keras.layers.Activation(\"gelu\", name = 'GeluAct'),\n",
    "                                             SE(name = \"SqueezeAndExcitation2D\"),\n",
    "                                             keras.layers.Conv2D(filters = embed_dims, kernel_size = 1, padding = 'same', use_bias = False, name = \"PointWiseConv\")],\n",
    "                                            name = \"Fused_MBConvLayer\")\n",
    "        if self.keepdims == False:\n",
    "            self.pool = keras.layers.MaxPooling2D(name = \"FeatExtractMaxPool2D\")\n",
    "    def call(self, inputs):\n",
    "        x = inputs + self.fused_mbconv(inputs)\n",
    "        if self.keepdims == False:\n",
    "            return self.pool(x)\n",
    "        return x\n",
    "    \n",
    "class GlobalQueryGenerator(keras.layers.Layer):\n",
    "    def __init__(self, keepdims = False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.keepdims = keepdims #Keepdims는 여기서 0과 1로 이루어진 list도 될 수 있다 -> FeatExtract layer를 keepdims의 원소 갯수만큼 repeat!\n",
    "    def build(self, input_shape):\n",
    "        self.q_generator = keras.Sequential([FeatExtract(keepdims = keepdim, name = f\"FeatureExtraction_{idx+1}\") for idx, keepdim in enumerate(self.keepdims)])\n",
    "    def call(self, inputs):\n",
    "        return self.q_generator(inputs)\n",
    "    \n",
    "class WindowAttention(keras.layers.Layer):\n",
    "    def __init__(self, window_size, n_heads, global_query, #제공된다면 global, 아니라면 local mHSA -> 0 or 1\n",
    "                qkv_bias = True, qk_scale = None,\n",
    "                dropout_rate = 0.05, return_attention_weights = False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.window_size = (window_size, window_size)\n",
    "        self.n_heads = n_heads\n",
    "        self.global_query = global_query\n",
    "        self.bias = qkv_bias\n",
    "        self.scale = qk_scale\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.return_attention_weights = return_attention_weights\n",
    "    def build(self, input_shape) :\n",
    "        #input = [query, key, value]\n",
    "        embed_dims = input_shape[0][-1]\n",
    "        head_dims = embed_dims//self.n_heads\n",
    "        self.scale = self.scale if self.scale != None else embed_dims**-0.5\n",
    "        self.qkv_size = 3-int(self.global_query)\n",
    "        self.qkv_embed_fn = keras.layers.Dense(units = embed_dims * self.qkv_size, use_bias = self.bias,\n",
    "                                 name = \"QKV_Embedding_Dense_layer\")\n",
    "        self.softmax = keras.layers.Activation(\"softmax\", name = \"AttentionWeightSoftmax\") #for attention weight computation\n",
    "        self.proj = keras.layers.Dense(units = embed_dims, use_bias = self.bias, name = \"Projection\")\n",
    "        self.attention_dropout = keras.layers.Dropout(self.dropout_rate)\n",
    "        self.projection_dropout = keras.layers.Dropout(self.dropout_rate)\n",
    "        self.relative_position_bias_table = self.add_weight(\n",
    "            name=\"relative_position_bias_table\",\n",
    "            shape=[\n",
    "                (2 * self.window_size[0] - 1) * (2 * self.window_size[1] - 1),\n",
    "                self.n_heads,\n",
    "            ],\n",
    "            initializer=keras.initializers.TruncatedNormal(stddev=0.02),\n",
    "            trainable=True,\n",
    "            dtype=self.dtype,\n",
    "        ) #<- learnable weight of relational position. 위 window size = 4의 예시에서, 임의의 두 지점 간 거리의 경우의 수는 총 49개 -> 이에 해당하는 weight tensor를 만듬.\n",
    "        super().build(input_shape)\n",
    "    def get_relative_position_index(self): #<- window 내 2 지점 간 거리의 index matrix.\n",
    "        coords_h = ops.arange(self.window_size[0])\n",
    "        coords_w = ops.arange(self.window_size[1])\n",
    "        coords = ops.stack(ops.meshgrid(coords_h, coords_w, indexing=\"ij\"), axis=0)\n",
    "        coords_flatten = ops.reshape(coords, [2, -1])\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
    "        relative_coords = ops.transpose(relative_coords, axes=[1, 2, 0])\n",
    "        relative_coords_xx = relative_coords[:, :, 0] + self.window_size[0] - 1\n",
    "        relative_coords_yy = relative_coords[:, :, 1] + self.window_size[1] - 1\n",
    "        relative_coords_xx = relative_coords_xx * (2 * self.window_size[1] - 1)\n",
    "        relative_position_index = relative_coords_xx + relative_coords_yy\n",
    "        return relative_position_index\n",
    "    def call(self, inputs, **kwargs):\n",
    "        #input : key(=value), global query OR key only\n",
    "        # input component shape : batch*n_windows, h_window*w_window, embed_dims -> level/block 설계 시 repeat 처리 후 attention에 feed\n",
    "        if self.global_query :\n",
    "            inputs, q_global = inputs\n",
    "            batch_size = ops.shape(q_global)[0]\n",
    "        else:\n",
    "            inputs = inputs[0]\n",
    "        batch_, token_length, embed_dims = ops.shape(inputs) #global query는 query generator에 의해 token_length 개 만큼의 token으로 전체 이미지/feature map을 압축한 상태\n",
    "        \n",
    "        qkv = self.qkv_embed_fn(inputs) #batch*n_windows, h_w * w_w, qkv_size * embed_dims\n",
    "        \n",
    "        qkv = ops.reshape(qkv, [-1, token_length, self.qkv_size, self.n_heads, embed_dims//self.n_heads])\n",
    "        qkv = ops.transpose(qkv, [2, 0, 3, 1, 4]) #qkv_size, batch_, n_heads, token_length, C\n",
    "        \n",
    "        #QKV 분리\n",
    "        if self.global_query:\n",
    "            k, v = ops.split(qkv, 2, axis = 0) #각각 batch_, n_heads, token_length, C\n",
    "            #repeat the global query tensor\n",
    "            # batch_size, n_query_tokens, dims -> batch_(=batch * n_windows), n_query_tokens, dims\n",
    "            q_global = ops.repeat(q_global, batch_//batch_size, axis = 0) #->batch_, n_query_tokens, dims\n",
    "            q = ops.reshape(q_global, [batch_, token_length, self.n_heads, embed_dims//self.n_heads])\n",
    "            q = ops.transpose(q, [0, 2, 1, 3]\n",
    "                             )\n",
    "        else:\n",
    "            q, k, v = ops.split(qkv, 3, axis = 0)\n",
    "            q = ops.squeeze(q, axis = 0)\n",
    "        k = ops.squeeze(k, axis = 0)\n",
    "        v = ops.squeeze(v, axis = 0)\n",
    "        \n",
    "        q *= self.scale #batch_, n_heads, token_length, dimension_per_heads(=C)\n",
    "        attention_score = q@ops.transpose(k, [0, 1, 3, 2]) #batch_, n_heads, token_length, token_length\n",
    "        \n",
    "        #positional encoding(bias) 계산 -> attention score에 더해 주기\n",
    "        # Code from original keras homepage\n",
    "        relative_position_bias = ops.take(\n",
    "            self.relative_position_bias_table,\n",
    "            ops.reshape(self.get_relative_position_index(), [-1]),\n",
    "        )\n",
    "        relative_position_bias = ops.reshape(\n",
    "            relative_position_bias,\n",
    "            [\n",
    "                self.window_size[0] * self.window_size[1],\n",
    "                self.window_size[0] * self.window_size[1],\n",
    "                -1,\n",
    "            ],\n",
    "        )\n",
    "        relative_position_bias = ops.transpose(relative_position_bias, axes=[2, 0, 1])\n",
    "        attention_score += relative_position_bias[None,]\n",
    "        attention_weight = self.softmax(attention_score)\n",
    "        attention_weight = self.attention_dropout(attention_weight) #batch_, n_heads, token_length, token_length\n",
    "        #value tensor shape : batch_, n_heads, token_length, dimension_per_heads(=C)\n",
    "        attended_output = attention_weight@v\n",
    "        attended_output = ops.transpose(attended_output, [0, 2, 1, 3])\n",
    "        attended_output = ops.reshape(attended_output, [batch_, token_length, embed_dims])\n",
    "        attended_output = self.projection_dropout(self.proj(attended_output))\n",
    "        self.attention_weight = attention_weight\n",
    "        if self.return_attention_weights:\n",
    "            return attended_output, attention_weight\n",
    "        else:\n",
    "            return attended_output\n",
    "        \n",
    "        \n",
    "class Block(keras.layers.Layer):\n",
    "    def __init__(self, #이하는 Window Attention configurations\n",
    "                 window_size, num_heads, global_query, \n",
    "                 qkv_bias = True, qk_scale = None, dropout_rate = 0.05, \n",
    "                 # 이하는 MLP module의 configuration\n",
    "                 mlp_ratio = 4.0, layer_scale = None, return_attention_weights = False,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.window_size = window_size\n",
    "        self.n_heads = num_heads\n",
    "        self.global_query = global_query\n",
    "        self.qkv_bias = qkv_bias\n",
    "        self.qk_scale = qk_scale\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        self.layer_scale = layer_scale\n",
    "        self.return_attention_weights = return_attention_weights\n",
    "    def build(self, input_shape):\n",
    "        #input tensor : list of key/query or key only\n",
    "        # each tensor is batch_size, w, h, channel dims shape tensor\n",
    "        batch_size, H, W, dims = input_shape[0]\n",
    "        self.norm1 = keras.layers.LayerNormalization(epsilon = 1e-5)\n",
    "        self.norm2 = keras.layers.LayerNormalization(epsilon = 1e-5)\n",
    "        self.window_attention = WindowAttention(window_size = self.window_size,\n",
    "                                               n_heads = self.n_heads,\n",
    "                                               global_query = self.global_query,\n",
    "                                               qkv_bias = self.qkv_bias,\n",
    "                                               qk_scale = self.qk_scale,\n",
    "                                               dropout_rate = self.dropout_rate,\n",
    "                                                return_attention_weights = self.return_attention_weights)\n",
    "        self.mlps = MLP(middle_dim = int(self.mlp_ratio * dims), dropout = self.dropout_rate)\n",
    "        if self.layer_scale != None:\n",
    "            self.gamma1 = self.add_weight(shape = [dims], name = \"Gamma1\", trainable = True,\n",
    "                                         initializer = keras.initializer.Constant(self.layer_scale), dtype = self.dtype)\n",
    "            self.gamma2 = self.add_weight(shape = [dims], name = \"Gamma2\", trainable = True,\n",
    "                                         initializer = keras.initializer.Constant(self.layer_scale), dtype = self.dtype)\n",
    "        else:\n",
    "            self.gamma1, self.gamma2 = 1.0, 1.0\n",
    "        self.n_windows = int(H//self.window_size) * int(W//self.window_size)\n",
    "        \n",
    "    #input feature map을 일정 크기의 window로 partition을 만들어주는 함수 및\n",
    "    # 그 partition을 받아 원래의 feature map으로 돌려주는 함수를 만들자\n",
    "    def window_partition(self, inputs): #feature map -> multiple windows\n",
    "        batch_size, H, W, dims = ops.shape(inputs)\n",
    "        h, w = H//self.window_size, W//self.window_size\n",
    "        inputs = ops.reshape(inputs, [batch_size, \n",
    "                                      h, self.window_size,\n",
    "                                     w, self.window_size, \n",
    "                                     dims])\n",
    "        inputs = ops.transpose(inputs, [0,#batch_size\n",
    "                                        1,3, #h, w\n",
    "                                        2,4, #winsize, winsize\n",
    "                                        5])\n",
    "        return ops.reshape(inputs, [-1, self.window_size, self.window_size, dims]) #batch_size*n_windows, window_size, window_size, dims\n",
    "        \n",
    "    def window_reverse(self, inputs, H, W, dims): #window partition -> original feature map\n",
    "        x = ops.reshape(inputs, [-1, H//self.window_size, W//self.window_size, self.window_size, self.window_size, dims])\n",
    "        x = ops.transpose(x, [0, 1, 3, 2, 4, 5])\n",
    "        return ops.reshape(x, [-1, H, W, dims])\n",
    "    \n",
    "    def call(self, inputs, **kwargs):\n",
    "        if self.global_query:\n",
    "            inputs, global_query = inputs\n",
    "        else:\n",
    "            inputs = inputs[0]\n",
    "        batch_size, H, W, dims = ops.shape(inputs)\n",
    "        x = self.norm1(inputs)\n",
    "        x = self.window_partition(x) \n",
    "        x = ops.reshape(x, [-1, self.window_size*self.window_size, dims])\n",
    "        if self.global_query:\n",
    "            outputs_ = self.window_attention([x, global_query]\n",
    "                                     )\n",
    "        else:\n",
    "            outputs_ = self.window_attention([x])\n",
    "        if self.return_attention_weights:\n",
    "            x, attention_weight = outputs_\n",
    "        else:\n",
    "            x = outputs_\n",
    "        x = self.window_reverse(x, H, W, dims)\n",
    "        x = inputs + self.gamma1*x\n",
    "        x += self.gamma2*(self.mlps(self.norm2(x)))\n",
    "        if self.return_attention_weights:\n",
    "            return x, attention_weight\n",
    "        else:\n",
    "            return x\n",
    "    \n",
    "class Level(keras.layers.Layer):\n",
    "    def __init__(self, \n",
    "                depth, #<- Block repetition depth\n",
    "                num_heads, window_size, keepdims, #downsampler 및 block의 hyperparameter\n",
    "                downsample = True, mlp_ratio = 4.0,\n",
    "                qkv_bias = True, qk_scale = None,\n",
    "                dropout = 0.05, layer_scale = None, return_attention_weights = True,\n",
    "                **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.depth = depth\n",
    "        self.n_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.keepdims = keepdims\n",
    "        self.downsample = downsample\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        self.qkv_bias = qkv_bias\n",
    "        self.qk_scale = qk_scale\n",
    "        self.dropout_rate = dropout\n",
    "        self.layer_scale = layer_scale\n",
    "        self.return_attention_weights = return_attention_weights\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        #input tensor : feature map / patches\n",
    "        batch_size, H, W, dims = input_shape\n",
    "        self.blocks = [Block(window_size = self.window_size, num_heads = self.n_heads, global_query = bool(idx%2), \n",
    "                             qkv_bias = self.qkv_bias, qk_scale = self.qk_scale, dropout_rate = self.dropout_rate, \n",
    "                             mlp_ratio = self.mlp_ratio, layer_scale = self.layer_scale, return_attention_weights = self.return_attention_weights,\n",
    "                             name = f\"GCViTBlock{idx+1}\") for idx in range(self.depth)]\n",
    "        self.downsampler = DownSampler(name = \"Downsampler\")\n",
    "        self.query_generator = GlobalQueryGenerator(keepdims = self.keepdims, name = \"GlobalQueryGenerator\")\n",
    "        \n",
    "    def call(self, inputs, **kwargs):\n",
    "        patches = inputs\n",
    "        global_query = self.query_generator(inputs)\n",
    "        for idx, block in enumerate(self.blocks):\n",
    "            if idx % 2 :\n",
    "                outputs_ = block([patches, global_query])\n",
    "            else:\n",
    "                outputs_ = block([patches])\n",
    "            if self.return_attention_weights:\n",
    "                patches, attention_weights = outputs_\n",
    "            else:\n",
    "                patches = outputs_\n",
    "        if self.downsample == False:\n",
    "            return patches\n",
    "        else:\n",
    "            return self.downsampler(patches)\n",
    "def get_gcvit_configs(res, initial_embedding_dims, name = None):\n",
    "    return {'res' : res,\n",
    "            'embed_dims' : initial_embedding_dims,\n",
    "            \"patch_embedding_type\" : \"conv\", #conv or tokenlearner\n",
    "            \"level_depth\" : [2,4,6,8],\n",
    "            \"level_heads\" : [2,4,8,16],\n",
    "            \"level_keepdims\" : [[0,0,0],\n",
    "                                   [0,0],\n",
    "                                   [1], \n",
    "                                    [1]\n",
    "                                   ], #3번째 level부터는 window attention == global attention\n",
    "            \"level_window_size\" : [res//32, res//32, res//16, res//32],\n",
    "            \"model_name\" : f\"GCViT_res{res}\" if name == None else name\n",
    "                }\n",
    "def get_gcvit(configs):\n",
    "    res = configs[\"res\"]\n",
    "    inputs = Input([res,res,3], name = \"ImageInput\")\n",
    "    patcher = PatchEmbedding(embed_dim = configs['embed_dims'], patching_type = configs[\"patch_embedding_type\"],\n",
    "                             name = \"PatchEmbedding\")\n",
    "    patches = patcher(inputs)\n",
    "    \n",
    "    for idx, (depth, heads, keepdims, window_size) in enumerate(zip(configs[\"level_depth\"], configs[\"level_heads\"], configs[\"level_keepdims\"], configs[\"level_window_size\"])):\n",
    "        if idx == len(configs['level_depth'])-1:\n",
    "            downsample = False\n",
    "        else:\n",
    "            downsample = True\n",
    "        level = Level(depth = depth, num_heads = heads, window_size = window_size, keepdims = keepdims, downsample = downsample,\n",
    "                      name = f\"GCViT_Lv{idx+1}_downsample_{downsample}\")\n",
    "        patches = level(patches)\n",
    "    model = keras.Model(inputs, patches,\n",
    "                       name = configs[\"model_name\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3781bc19",
   "metadata": {
    "papermill": {
     "duration": 0.025717,
     "end_time": "2024-07-05T17:51:22.448305",
     "exception": false,
     "start_time": "2024-07-05T17:51:22.422588",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a29ab6",
   "metadata": {
    "papermill": {
     "duration": 0.025589,
     "end_time": "2024-07-05T17:51:22.499965",
     "exception": false,
     "start_time": "2024-07-05T17:51:22.474376",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# MLP-mixer\n",
    "- for lower resource demand!\n",
    "- Code Reference : [Keras.io](https://keras.io/examples/vision/mlp_image_classification/#the-mlpmixer-model)\n",
    "\n",
    "![](https://velog.velcdn.com/images/minkyu4506/post/0237ee55-74eb-4836-952c-6bd33694aecc/image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dad96d40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T17:51:22.554197Z",
     "iopub.status.busy": "2024-07-05T17:51:22.553359Z",
     "iopub.status.idle": "2024-07-05T17:51:22.562563Z",
     "shell.execute_reply": "2024-07-05T17:51:22.561672Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.038519,
     "end_time": "2024-07-05T17:51:22.564472",
     "exception": false,
     "start_time": "2024-07-05T17:51:22.525953",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NaivePatchesExtraction(keras.layers.Layer): #untrainable layer\n",
    "    def __init__(self, patch_size, output_type = \"seq\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.patch_size = patch_size\n",
    "        self.output_type = output_type\n",
    "        assert output_type in [\"seq\", \"sequence\", \"Sequence\", \"3D\", \"conv\", \"feature_map\", \"2D\"]\n",
    "    def call(self, x):\n",
    "        if len(ops.shape(x)) == 3:\n",
    "            x = ops.expand_dims(x, 0)\n",
    "        patches = keras.ops.image.extract_patches(x, self.patch_size)\n",
    "        batch_size = keras.ops.shape(patches)[0]\n",
    "        num_patches = keras.ops.shape(patches)[1] * keras.ops.shape(patches)[2]\n",
    "        patch_dim = keras.ops.shape(patches)[3]\n",
    "        if self.output_type in [\"seq\", \"sequence\", \"Sequence\", \"3D\"] : \n",
    "            out = keras.ops.reshape(patches, (batch_size, num_patches, patch_dim))\n",
    "        elif self.output_type in [\"conv\", \"feature_map\", \"2D\"]:\n",
    "            out = keras.ops.reshape(patches, (batch_size, keras.ops.shape(patches)[1], keras.ops.shape(patches)[2],\n",
    "                                             patch_dim))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0b4a0319",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T17:51:22.618494Z",
     "iopub.status.busy": "2024-07-05T17:51:22.618155Z",
     "iopub.status.idle": "2024-07-05T17:51:22.631624Z",
     "shell.execute_reply": "2024-07-05T17:51:22.630776Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.042151,
     "end_time": "2024-07-05T17:51:22.633568",
     "exception": false,
     "start_time": "2024-07-05T17:51:22.591417",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLPMixer(keras.layers.Layer): #untrainable layer\n",
    "    def __init__(self, units = None, dropout_rate = 0.2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.layernorm1 = keras.layers.LayerNormalization()\n",
    "        self.layernorm2 = keras.layers.LayerNormalization()\n",
    "        self.drop_rate = dropout_rate\n",
    "        self.units = units\n",
    "        \n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        #batch, seq_len, dims\n",
    "        batch_size = input_shape[0]\n",
    "        n_tokens = input_shape[1]\n",
    "        channels = input_shape[2]\n",
    "        embed_dim_middle = channels if self.units is None else self.units\n",
    "        \n",
    "        self.mlp1 = keras.Sequential([keras.layers.Dense(units = n_tokens, use_bias = False),\n",
    "                                     keras.layers.Activation(\"gelu\"),\n",
    "                                     keras.layers.Dropout(self.drop_rate),\n",
    "                                     keras.layers.Dense(units = n_tokens, use_bias = False),],\n",
    "                                    name = \"MLP1_TokenWiseMixing\") #output : batch, embedding_dims(channels), n_tokens\n",
    "        \n",
    "        self.mlp2 = keras.Sequential([keras.layers.Dense(units = embed_dim_middle, use_bias = False),\n",
    "                                     keras.layers.Activation(\"gelu\"),\n",
    "                                     keras.layers.Dropout(self.drop_rate),\n",
    "                                     keras.layers.Dense(units = channels, use_bias = False),],\n",
    "                                    name = \"MLP2_ChannelWiseMixing\")\n",
    "    def enable_lora(self, rank):\n",
    "        print(\"Use only in pretrained model!\\n\\n\")\n",
    "        if rank == 0:\n",
    "            print(\"lora disabled\\n\\n\")\n",
    "            pass\n",
    "        else:\n",
    "            self.layernorm1.trainable = False\n",
    "            self.layernorm2.trainable = False\n",
    "            for layer in self.mlp1.layers:\n",
    "                if isinstance(layer, keras.layers.Dense):\n",
    "                    layer.enable_lora(rank)\n",
    "                else:\n",
    "                    layer.trainable = False\n",
    "            for layer in self.mlp2.layers:\n",
    "                if isinstance(layer, keras.layers.Dense):\n",
    "                    layer.enable_lora(rank)\n",
    "                else:\n",
    "                    layer.trainable = False\n",
    "    def call(self, x):\n",
    "        # x : patches, [batch, n_patches, embed_dims]\n",
    "        normed_patch = self.layernorm1(x)\n",
    "        normed_patch = ops.transpose(normed_patch, [0,2,1]) #batch, embed_dims, n_patches\n",
    "        normed_patch = self.mlp1(normed_patch)\n",
    "        normed_patch = ops.transpose(normed_patch, [0,2,1]) #batch, n_patches, embed_dims\n",
    "        normed_patch += x\n",
    "        normed_patch2 = self.layernorm2(normed_patch)\n",
    "        normed_patch2 = self.mlp2(normed_patch2)\n",
    "        normed_patch2 += normed_patch\n",
    "        return normed_patch2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e6d63f99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T17:51:22.687213Z",
     "iopub.status.busy": "2024-07-05T17:51:22.686437Z",
     "iopub.status.idle": "2024-07-05T17:51:22.693948Z",
     "shell.execute_reply": "2024-07-05T17:51:22.693062Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.036644,
     "end_time": "2024-07-05T17:51:22.695996",
     "exception": false,
     "start_time": "2024-07-05T17:51:22.659352",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_mlp_mixer(res, name = \"mlpmixer_16_4_768\", mask = False):\n",
    "    _, patch_size, depth, embed_dims = name.split(\"_\")\n",
    "    patch_size = int(patch_size)\n",
    "    depth = int(depth)\n",
    "    embed_dims = int(embed_dims)\n",
    "    \n",
    "    model_name = f\"Res{res}_{name}\"\n",
    "    inputs = Input([res,res,3], name = \"ImageInput\")\n",
    "    scaled_input = inputs/255.0\n",
    "    patches = NaivePatchesExtraction(patch_size, name = \"PatchExtraction\")(scaled_input)\n",
    "    patches_embedding = PatchEncoder(num_patches = ops.shape(patches)[1], projection_dim = embed_dims)(patches)\n",
    "    if mask:\n",
    "        patches_embedding *= mask\n",
    "    for idx in range(depth):\n",
    "        patches_embedding = MLPMixer(name = f\"MLPMixer_{idx+1}\")(patches_embedding)\n",
    "    model = Model(inputs, patches_embedding,\n",
    "                 name = model_name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb67c7ea",
   "metadata": {
    "papermill": {
     "duration": 0.02637,
     "end_time": "2024-07-05T17:51:22.748392",
     "exception": false,
     "start_time": "2024-07-05T17:51:22.722022",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ConvMixer\n",
    "- Dense layer inside MLPmixer --> Conv2D\n",
    "- Code Reference : [Keras.io](https://keras.io/examples/vision/convmixer/)\n",
    "\n",
    "![](https://i.imgur.com/yF8actg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7655907d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T17:51:22.803035Z",
     "iopub.status.busy": "2024-07-05T17:51:22.802205Z",
     "iopub.status.idle": "2024-07-05T17:51:22.814591Z",
     "shell.execute_reply": "2024-07-05T17:51:22.813683Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.042093,
     "end_time": "2024-07-05T17:51:22.816648",
     "exception": false,
     "start_time": "2024-07-05T17:51:22.774555",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConvMixer(keras.layers.Layer): #untrainable layer\n",
    "    def __init__(self, filters = None, kernel_size = 5, dropout_rate = 0.2, output_type = \"2D\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.layernorm1 = keras.layers.LayerNormalization()\n",
    "        self.layernorm2 = keras.layers.LayerNormalization()\n",
    "        self.drop_rate = dropout_rate ; self.drop = keras.layers.Dropout(self.drop_rate)\n",
    "        \n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.output_type = output_type\n",
    "        assert output_type in [\"seq\", \"sequence\", \"Sequence\", \"3D\", \"conv\", \"feature_map\", \"2D\"]\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        #batch, seq_len, dims\n",
    "        batch_size = input_shape[0]\n",
    "        h = input_shape[1]\n",
    "        w = input_shape[2]\n",
    "        channels = input_shape[3]\n",
    "        c = channels if self.filters is None else self.filters\n",
    "        \n",
    "        self.depthconv = keras.layers.DepthwiseConv2D(kernel_size = self.kernel_size, padding = \"SAME\", use_bias = False)\n",
    "        self.pointconv = keras.layers.Conv2D(filters = c, kernel_size = 1, padding = 'SAME', use_bias = False)\n",
    "        self.gelu = keras.layers.Activation('gelu')\n",
    "\n",
    "    def call(self, x):\n",
    "        # x : feature_map, [batch, h,w, embed_dims]\n",
    "        fmap = self.depthconv(x)\n",
    "        fmap = self.gelu(fmap)\n",
    "        fmap = self.layernorm1(fmap) + x\n",
    "        fmap = self.pointconv(fmap)\n",
    "        fmap = self.gelu(fmap)\n",
    "        fmap = self.layernorm2(fmap)\n",
    "        \n",
    "        batch_size = ops.shape(fmap)[0]\n",
    "        h = ops.shape(fmap)[1]\n",
    "        w = ops.shape(fmap)[2]\n",
    "        dims_ = ops.shape(fmap)[3]\n",
    "        \n",
    "        if self.output_type in [\"seq\", \"sequence\", \"Sequence\", \"3D\"] : \n",
    "            fmap = keras.ops.reshape(fmap, (batch_size, h*w, dims_))\n",
    "            return fmap\n",
    "        elif self.output_type in [\"conv\", \"feature_map\", \"2D\"]:\n",
    "            return fmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fbc47f8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T17:51:22.870726Z",
     "iopub.status.busy": "2024-07-05T17:51:22.869913Z",
     "iopub.status.idle": "2024-07-05T17:51:22.878909Z",
     "shell.execute_reply": "2024-07-05T17:51:22.877984Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.038025,
     "end_time": "2024-07-05T17:51:22.880762",
     "exception": false,
     "start_time": "2024-07-05T17:51:22.842737",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_conv_mixer(res, name = \"convmixer_16_4_768\", mask = False):\n",
    "    _, patch_size, depth, embed_dims = name.split(\"_\")\n",
    "    patch_size = int(patch_size)\n",
    "    depth = int(depth)\n",
    "    embed_dims = int(embed_dims)\n",
    "    \n",
    "    model_name = f\"Res{res}_{name}\"\n",
    "    inputs = Input([res,res,3], name = \"ImageInput\")\n",
    "    scaled_input = inputs/255.0\n",
    "    \n",
    "    patches = NaivePatchesExtraction(patch_size, name = \"PatchExtraction\")(scaled_input)\n",
    "    patches_embedding = PatchEncoder(num_patches = ops.shape(patches)[1], projection_dim = embed_dims)(patches)\n",
    "    batch, n_patches, dims = ops.shape(patches_embedding) ; res_ = int(ops.sqrt(ops.cast(n_patches, \"float32\")))\n",
    "    patches_embedding = ops.reshape(patches_embedding, (-1, res_, res_, dims))\n",
    "    if mask:\n",
    "        patches_embedding *= mask\n",
    "    for idx in range(depth):\n",
    "        if idx == depth-1:\n",
    "            types = \"seq\"\n",
    "        else:\n",
    "            types = \"2D\"\n",
    "        patches_embedding = ConvMixer(name = f\"ConvMixer_{idx+1}\", output_type = types)(patches_embedding)\n",
    "    model = Model(inputs, patches_embedding,\n",
    "                 name = model_name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cb8796",
   "metadata": {
    "papermill": {
     "duration": 0.02583,
     "end_time": "2024-07-05T17:51:22.932665",
     "exception": false,
     "start_time": "2024-07-05T17:51:22.906835",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Get final Feature extractor\n",
    "- according to [recent study](https://arxiv.org/abs/2309.16588), add cls token and register tokens \"after\" the patches!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f07b7770",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T17:51:22.985207Z",
     "iopub.status.busy": "2024-07-05T17:51:22.984552Z",
     "iopub.status.idle": "2024-07-05T17:51:23.002094Z",
     "shell.execute_reply": "2024-07-05T17:51:23.001209Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.046036,
     "end_time": "2024-07-05T17:51:23.004084",
     "exception": false,
     "start_time": "2024-07-05T17:51:22.958048",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_metaformer(meta_type, res, att_depth, att_heads, att_dims, \n",
    "                   patch_size = 16, embed_dims = 1024, grayscale = False, register_tokens = 0,\n",
    "                   pretrained_encoder = None, return_patches = False,\n",
    "                  ):\n",
    "    assert isinstance(register_tokens, int), \"register_tokens should be integer\"\n",
    "    name = f\"Metaformer_res{res}_type_{meta_type}\"\n",
    "    if grayscale:\n",
    "        inputs = Input([res,res,1], name = \"Input_images_grayscale\")\n",
    "    else:\n",
    "        inputs = Input([res,res,3], name = \"Input_images\")\n",
    "    if pretrained_encoder is None:\n",
    "        scaled_inputs = keras.layers.Rescaling(scale=1./127.5, offset=-1, name = \"RescalingImage\")(inputs)\n",
    "        patches = ImagePatchEmbedding(patch_size = patch_size, embed_dim = embed_dims, groups = att_heads//2,\n",
    "                                      name = \"ImagePatchingWithLearnablePE\")(scaled_inputs)\n",
    "    else:\n",
    "        assert grayscale is False, \"To use pretrained network, input images must have 3 channels\"\n",
    "        patches = pretrained_encoder(inputs)\n",
    "        _, w_, h_, dims_ = ops.shape(patches)\n",
    "        patches = ops.reshape(patches, [-1, w_*h_, dims_])\n",
    "        patches = keras.layers.Dense(embed_dims, name = \"EmbeddingMLP\", activation = \"gelu\")(patches)\n",
    "    \n",
    "    n_patches = ops.shape(patches)[1]\n",
    "    cls_token_ = keras.layers.GlobalAveragePooling1D(keepdims = True)(patches)\n",
    "    cls_token_ = ops.ones_like(cls_token_, dtype = \"float32\")\n",
    "    if register_tokens > 0:\n",
    "        reg_tokens = ops.concatenate([ops.ones_like(cls_token_, dtype = \"float32\") for _ in range(register_tokens)],\n",
    "                                    axis = 1)\n",
    "        patches_ = ops.concatenate([patches, cls_token_, reg_tokens], axis = 1)\n",
    "    else:\n",
    "        patches_ = ops.concatenate([patches, cls_token_], axis = 1)\n",
    "    for idx in range(att_depth):\n",
    "        patches_ = MetaEncoder(operation_type = meta_type, att_dims = att_dims, att_heads = att_heads,\n",
    "                                 name = f\"MetaEncoder_{meta_type}_{idx+1}\")([patches_])    \n",
    "    patches = patches_[:, :n_patches, :]\n",
    "    cls_token = patches_[:, n_patches:n_patches+1, :] ; del patches_\n",
    "    if len(ops.shape(cls_token)) == 2:\n",
    "        cls_token = cls_token[:, tf.newaxis, :]\n",
    "    total_patches = ops.concatenate([cls_token, patches], axis = 1)\n",
    "    attention_mask = ops.ones([1, n_patches + 1, n_patches + 1])\n",
    "    attention_mask = tf.linalg.set_diag(attention_mask, \n",
    "                                       ops.zeros([1, n_patches + 1]))\n",
    "    patches_, weights_ = MultiHeadAttention(att_heads, att_dims, use_bias = False, name = \"AttentionPooling\", dropout = 0.4)(query = total_patches, key = total_patches, \n",
    "                                                                                            value = total_patches,\n",
    "                                                                                           return_attention_scores = True,\n",
    "                                                                                           attention_mask = attention_mask\n",
    "                                                                                                                            )\n",
    "    patches = patches[:, -n_patches:, :]\n",
    "    cls_token = patches[:, 0, :]\n",
    "    attention_weight = weights_[:, :, 0, -n_patches:]\n",
    "    attention_weight = attention_weight[:,:, tf.newaxis, :]\n",
    "    cls_token = keras.layers.Identity(name = \"Representation_vector\")(cls_token)\n",
    "    attention_weight = keras.layers.Identity(name = \"Attention_weight\")(attention_weight)\n",
    "    \n",
    "    if return_patches:\n",
    "        return keras.Model(inputs, [cls_token, patches, attention_weight],\n",
    "                          name = name)\n",
    "    else:\n",
    "        return keras.Model(inputs, [cls_token, attention_weight],\n",
    "                          name = name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ac534a73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T17:51:23.057686Z",
     "iopub.status.busy": "2024-07-05T17:51:23.057295Z",
     "iopub.status.idle": "2024-07-05T17:51:23.083028Z",
     "shell.execute_reply": "2024-07-05T17:51:23.082128Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.054936,
     "end_time": "2024-07-05T17:51:23.084981",
     "exception": false,
     "start_time": "2024-07-05T17:51:23.030045",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_feature_extractor(conv_base, #if None, Vanilla ViT\n",
    "                         embed_dims, res, pe_type = \"learnable\",\n",
    "                          patch_size = 16,\n",
    "                        att_depth = 4, att_heads = 16, mask = False, \n",
    "                          return_patches = False) : \n",
    "    inputs = Input([res,res,3], name = \"Input_images\")\n",
    "    batch_size = ops.shape(inputs)[0]\n",
    "    if conv_base in [None, 'vit', \"ViT\"] : #Vanilla Vision Transformer\n",
    "        scaled_inputs = inputs/255.0\n",
    "        patches = ops.image.extract_patches(scaled_inputs,\n",
    "                                           size = patch_size,\n",
    "                                           padding = 'same')\n",
    "        \n",
    "        _, w, h, dims = ops.shape(patches) ; n_patches = w*h \n",
    "        \n",
    "        patches = ops.reshape(patches, [-1, w*h, dims])\n",
    "        if pe_type in ['rotary', 'rotation', 'rotatory', 'roformer']:\n",
    "            patches = keras_nlp.layers.RotaryEmbedding(name = \"RotaryPositionalEmbedding\")(patches)\n",
    "            patches = Dense(units = embed_dims, activation = \"gelu\", name = \"EmbeddingAfterRPE\")(patches)\n",
    "        elif pe_type in [\"learnable\", 'absolute']:\n",
    "            patches = PatchEncoder(num_patches = w*h, projection_dim = embed_dims)(patches)\n",
    "        elif pe_type == None:\n",
    "            pass\n",
    "        \n",
    "        if mask:\n",
    "            patches *= mask\n",
    "        \n",
    "        learned_token, patches, attention_score = TRBlock(att_depth = att_depth, \n",
    "                                                          att_dims = embed_dims, \n",
    "                                                          att_heads = att_heads)([patches, patches])\n",
    "        \n",
    "        learned_token = keras.layers.Identity(name = \"feature_vector\")(learned_token)\n",
    "        patches = keras.layers.Identity(name = \"encoded_patches\")(patches)\n",
    "        attention_score = keras.layers.Identity(name = \"attention_weight\")(attention_score) \n",
    "        model_name = f\"ViT_depth{att_depth}_dims{embed_dims}_heads{att_heads}_patch{patch_size}\"\n",
    "    else:\n",
    "        feature_map = conv_base(inputs)\n",
    "        dims = ops.shape(feature_map)[-1] ; batch_size = ops.shape(feature_map)[0]\n",
    "        if len(ops.shape(feature_map)) == 4:\n",
    "            _, w, h, dims = ops.shape(feature_map)\n",
    "            feature_map = ops.reshape(feature_map, [-1, w*h, dims])\n",
    "        n_patches = ops.shape(feature_map)[1]\n",
    "        \n",
    "        if pe_type in ['rotary', 'rotation', 'rotatory', 'roformer']:\n",
    "            feature_map = keras_nlp.layers.RotaryEmbedding(name = \"RotaryPositionalEmbedding\")(feature_map)\n",
    "            feature_map = Dense(units = embed_dims, name = \"FeatureEmbedding\", activation = \"gelu\")(feature_map)\n",
    "        elif pe_type in [\"learnable\", 'absolute']:\n",
    "            feature_map = PatchEncoder(num_patches = n_patches, projection_dim = embed_dims)(feature_map)\n",
    "        elif pe_type == None:\n",
    "            feature_map = Dense(units = embed_dims, name = \"FeatureEmbedding\", activation = \"gelu\")(feature_map)\n",
    "        learned_token = keras.layers.GlobalAveragePooling1D()(feature_map)\n",
    "        learned_token = learned_token[:, tf.newaxis, :]\n",
    "        learned_token, attention_score = AttentionPooling(attention_heads = att_heads, \n",
    "                                                          attention_dims = att_heads*32)([learned_token, feature_map])\n",
    "        learned_token = ops.squeeze(learned_token, axis = 1)\n",
    "        \n",
    "        learned_token = keras.layers.Identity(name = \"feature_vector\")(learned_token)\n",
    "        patches = keras.layers.Identity(name = \"encoded_patches\")(feature_map)\n",
    "        attention_score = keras.layers.Identity(name = \"attention_weight\")(attention_score) \n",
    "        \n",
    "        model_name = f\"{conv_base.name}_depth{att_depth}_dims{embed_dims}_heads{att_heads}\"\n",
    "    if return_patches:\n",
    "        \n",
    "        model = Model(inputs,\n",
    "                     [learned_token, patches, \n",
    "                      attention_score],\n",
    "                     name = model_name + \"_withPatches\")\n",
    "    else:\n",
    "        model = Model(inputs, [learned_token, attention_score],\n",
    "                      name = model_name)\n",
    "    return model\n",
    "\n",
    "def get_full_model(conv_base_name, res, embed_dims = 1280, patch_size = 16, pe_type = 'rotary',\n",
    "                   att_depth = 4, att_heads = 8,\n",
    "                  extra_configs = None, mask = False,\n",
    "                   return_patches = False):\n",
    "    if isinstance(conv_base_name, keras.Model):\n",
    "        conv_base = conv_base_name\n",
    "        #pe_type = \"learnable\"\n",
    "    elif conv_base_name in [\"effnet\", 'EfficientNet']:\n",
    "        conv_base = keras.applications.EfficientNetV2B1(input_shape = [res,res,3],\n",
    "                                                       include_top = False)\n",
    "    elif conv_base_name in [\"effnet_small\", \"EfficientNetSmall\"]:\n",
    "        conv_base = keras.applications.EfficientNetV2S(input_shape = [res,res,3],\n",
    "                                                       include_top = False)\n",
    "    elif conv_base_name in [\"effnet_base\", \"EfficientNetBase\"]:\n",
    "        conv_base = keras.applications.EfficientNetV2M(input_shape = [res,res,3],\n",
    "                                                       include_top = False)\n",
    "    elif conv_base_name in [\"convnext\", 'ConvNeXt']:\n",
    "        conv_base = keras.applications.ConvNeXtTiny(input_shape = [res,res,3],\n",
    "                                                       include_top = False)\n",
    "    elif conv_base_name in [\"convnext_small\", 'ConvNeXtSmall']:\n",
    "        conv_base = keras.applications.ConvNeXtSmall(input_shape = [res,res,3],\n",
    "                                                       include_top = False)\n",
    "    elif conv_base_name in [\"convnext_base\", 'ConvNeXtBase']:\n",
    "        conv_base = keras.applications.ConvNeXtBase(input_shape = [res,res,3],\n",
    "                                                       include_top = False)\n",
    "    elif isinstance(conv_base_name, dict):\n",
    "        conv_base = get_gcvit(conv_base_name)\n",
    "    elif conv_base_name.split(\"_\")[0] in [\"MLPMixer\", \"mlpmixer\", \"MLP\", \"mlp\"]:\n",
    "        conv_base = get_mlp_mixer(res = res, name = conv_base_name, mask = mask)\n",
    "        pe_type = None\n",
    "    elif conv_base_name.split(\"_\")[0] in [\"ConvMixer\", \"convmixer\", \"Conv\", \"conv\"]:\n",
    "        conv_base = get_conv_mixer(res = res, name = conv_base_name, mask = mask)\n",
    "        pe_type = None\n",
    "    else:\n",
    "        conv_base = None\n",
    "    return get_feature_extractor(conv_base, pe_type = pe_type, res = res, patch_size = patch_size, embed_dims = embed_dims, att_depth = att_depth, att_heads = att_heads,\n",
    "                                return_patches = return_patches, mask = mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eece9b0",
   "metadata": {
    "papermill": {
     "duration": 0.025733,
     "end_time": "2024-07-05T17:51:23.136850",
     "exception": false,
     "start_time": "2024-07-05T17:51:23.111117",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---------\n",
    "# Information-maximization method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5db8cbf2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T17:51:23.191523Z",
     "iopub.status.busy": "2024-07-05T17:51:23.191127Z",
     "iopub.status.idle": "2024-07-05T17:51:23.240198Z",
     "shell.execute_reply": "2024-07-05T17:51:23.239379Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.078806,
     "end_time": "2024-07-05T17:51:23.242256",
     "exception": false,
     "start_time": "2024-07-05T17:51:23.163450",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BarlowModel(keras.Model):\n",
    "    def __init__(self, feature_extractor, embed_dims, probe = False, multiview = False,\n",
    "                 probe_heads = None, probe_activation = \"sigmoid\",\n",
    "                 diag = 0.6, off_diag = 0.4, \n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dims = embed_dims\n",
    "        self.multiview = multiview\n",
    "        self.feature_extractor = feature_extractor #weight sharing for VICreg, Barlow Twins\n",
    "        self.projector_a = self.get_projector()\n",
    "        self.projector_b = self.get_projector()\n",
    "        self.diag = diag ; self.off_diag = off_diag\n",
    "        self.loss_tracker = tf.keras.metrics.Mean(\"Barlow_loss_tracker\")\n",
    "        self.linear_probing = probe\n",
    "        if probe:\n",
    "            self.probe_loss_tracker = tf.keras.metrics.Mean(\"Probe_loss_tracker\")\n",
    "            self.probe_metrics = [\n",
    "                                  keras.metrics.F1Score(average=\"micro\", threshold=None, name=\"probe_micro_f1_score\"),\n",
    "                                 keras.metrics.TruePositives(name = \"probe_TP\"),\n",
    "                                 keras.metrics.TrueNegatives(name = \"probe_TN\"),\n",
    "                                 keras.metrics.FalsePositives(name = \"probe_FP\"),\n",
    "                                 keras.metrics.FalseNegatives(name = \"probe_FN\")]\n",
    "            \n",
    "            self.probe_categories = probe_heads\n",
    "            self.probe_act = probe_activation\n",
    "            self.linear_probe = keras.layers.Dense(units = probe_heads, activation = self.probe_act, dtype = \"float32\")\n",
    "            if self.probe_act == \"sigmoid\":\n",
    "                self.probe_loss_fn = keras.losses.BinaryCrossentropy(reduction = None, label_smoothing = 0.1)\n",
    "                self.probe_metrics.append(keras.metrics.BinaryAccuracy(name=\"probe_accuracy\", threshold=0.5))\n",
    "            elif self.probe_act == \"softmax\":\n",
    "                self.probe_loss_fn = keras.losses.CategoricalCrossentropy(reduction = None, label_smoothing = 0.1)\n",
    "                self.probe_metrics.append(keras.metrics.CategoricalAccuracy(name=\"probe_accuracy\", threshold=0.5))\n",
    "    def compile(self, optimizer, probe_optimizer = None, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "        self.optimizer = optimizer\n",
    "        if self.linear_probing:\n",
    "            self.probe_optimizer = probe_optimizer\n",
    "        \n",
    "    def get_config(self):\n",
    "        return {\"feature_extractor_name\" : self.feature_extractor.name,\n",
    "               \"embed_dims\" : self.embed_dims,\n",
    "                \"Multiview(>2)\" : self.multiview,\n",
    "               'diag_part_coefficient' : self.diag,\n",
    "               'off_diag_coefficient' : self.off_diag,\n",
    "               \"SSL_method\" : \"Barlow_Twins\",\n",
    "               \"Linear Probe\" : self.linear_probing,\n",
    "               \"N_Categories\" : self.probe_categories if self.linear_probing else 0,\n",
    "               \"Probe Activation\" : self.probe_act if self.linear_probing else \"NA\"}\n",
    "    \n",
    "    def get_projector(self):\n",
    "        \n",
    "        model = keras.Sequential([Dense(units = self.embed_dims),\n",
    "                                 Dense(units = self.embed_dims),\n",
    "                                 Dense(units = self.embed_dims, dtype = \"float32\"),\n",
    "                                 keras.layers.Lambda(lambda x : ops.normalize(x))]\n",
    "                                )\n",
    "        return model\n",
    "    \n",
    "    def compute_loss(self, correlation_matrix):\n",
    "        diag_component = tf.linalg.diag_part(correlation_matrix)\n",
    "        zero_diag = tf.zeros(correlation_matrix.shape[-1])\n",
    "        off_diag_matrix = tf.linalg.set_diag(correlation_matrix, zero_diag)\n",
    "        \n",
    "        diag_loss = tf.pow(diag_component-1, 2) * self.diag\n",
    "        off_diag_loss = tf.pow(off_diag_matrix, 2) * self.off_diag\n",
    "        loss = tf.reduce_mean(diag_loss) + tf.reduce_mean(off_diag_loss)\n",
    "        loss = tf.cast(loss, tf.float32)\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    def train_step(self, dataset):\n",
    "        \n",
    "        if self.linear_probing:\n",
    "            data, aug_data, labels = dataset\n",
    "        else:\n",
    "            if self.multiview:\n",
    "                data = dataset[0]\n",
    "                aug_data = dataset[1:] ; n_augs = len(aug_data)\n",
    "                aug_data = ops.concatenate(aug_data, axis = 0)\n",
    "            else:\n",
    "                data, aug_data = dataset\n",
    "        ## 1. SSL encoder loss ##\n",
    "        with tf.GradientTape() as tape:\n",
    "            if len(self.feature_extractor.outputs) == 2 :\n",
    "                feature_seq, weights = self.feature_extractor(data)\n",
    "                feature_seq_aug, weights_aug = self.feature_extractor(aug_data)\n",
    "            elif len(self.feature_extractor.outputs) == 1:\n",
    "                feature_seq = self.feature_extractor(data)\n",
    "                feature_seq_aug = self.feature_extractor(aug_data)\n",
    "                \n",
    "            if len(ops.shape(feature_seq)) == 3: \n",
    "                rep_vector = keras.layers.GlobalAveragePooling1D()(feature_seq)\n",
    "                rep_vector_aug = keras.layers.GlobalAveragePooling1D()(feature_seq_aug)\n",
    "            elif len(ops.shape(feature_seq)) == 4:\n",
    "                rep_vector = keras.layers.GlobalAveragePooling2D()(feature_seq)\n",
    "                rep_vector_aug = keras.layers.GlobalAveragePooling2D()(feature_seq_aug)\n",
    "            elif len(ops.shape(feature_seq)) == 2:\n",
    "                rep_vector = feature_seq\n",
    "                rep_vector_aug = feature_seq_aug\n",
    "            original_rep_vector = rep_vector #for Linear probing\n",
    "            \n",
    "            rep_vector = self.projector_a(rep_vector)\n",
    "            rep_vector_aug = self.projector_b(rep_vector_aug)\n",
    "            if self.multiview:\n",
    "                rep_vector_aug_set = ops.split(rep_vector_aug, n_augs, 0)\n",
    "                loss = []\n",
    "                for idx in range(n_augs):\n",
    "                    correlation_matrix = get_cor_matrix(rep_vector, rep_vector_aug_set[idx])\n",
    "                    loss.append(self.compute_loss(correlation_matrix)\n",
    "                               )\n",
    "                loss = ops.mean(loss)\n",
    "            else:\n",
    "                correlation_matrix = get_cor_matrix(rep_vector, rep_vector_aug)\n",
    "                loss = self.compute_loss(correlation_matrix)\n",
    "    \n",
    "\n",
    "        gradients = tape.gradient(loss, \n",
    "                                 (self.feature_extractor.trainable_variables + self.projector_a.trainable_variables + self.projector_b.trainable_variables))\n",
    "        self.optimizer.apply_gradients(zip(gradients, \n",
    "                                          (self.feature_extractor.trainable_variables + self.projector_a.trainable_variables + self.projector_b.trainable_variables)\n",
    "                                          ))\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        output_dict = {'Barlow_loss' : self.loss_tracker.result()}\n",
    "        if self.linear_probing:\n",
    "            ##2. linear probing and calculate metrics##\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                if len(self.feature_extractor.outputs) == 2 :\n",
    "                    feature_seq, weights = self.feature_extractor(data, training = False)\n",
    "                elif len(self.feature_extractor.outputs) == 1:\n",
    "                    feature_seq = self.feature_extractor(data, training = False)\n",
    "                if len(ops.shape(feature_seq)) == 3: \n",
    "                    original_rep_vector = keras.layers.GlobalAveragePooling1D()(feature_seq)\n",
    "                elif len(ops.shape(feature_seq)) == 4:\n",
    "                    original_rep_vector = keras.layers.GlobalAveragePooling2D()(feature_seq)\n",
    "                elif len(ops.shape(feature_seq)) == 2:\n",
    "                    rep_vector = feature_seq\n",
    "        \n",
    "                class_logits = self.linear_probe(original_rep_vector, training = True)\n",
    "                class_probe_loss = self.probe_loss_fn(y_true = labels, y_pred = class_logits)\n",
    "                class_probe_loss = ops.mean(class_probe_loss)\n",
    "            gradients = tape.gradient(class_probe_loss, \n",
    "                                            self.linear_probe.trainable_weights)\n",
    "            self.probe_optimizer.apply_gradients(zip(gradients, self.linear_probe.trainable_weights))\n",
    "            self.probe_loss_tracker.update_state(class_probe_loss)\n",
    "            output_dict.update({\"probe_classification_loss\" : self.probe_loss_tracker.result()})\n",
    "            #metrics\n",
    "            for comp in self.probe_metrics:\n",
    "                comp.update_state(labels, class_logits)\n",
    "                \n",
    "            probe_metric_dict = {comp.name :  comp.result() for comp in self.probe_metrics}\n",
    "            output_dict.update(probe_metric_dict)\n",
    "        else:\n",
    "            pass\n",
    "        del tape\n",
    "        return output_dict\n",
    "    def test_step(self, dataset):\n",
    "        if self.linear_probing:\n",
    "            data, aug_data, labels = dataset\n",
    "        else:\n",
    "            if self.multiview:\n",
    "                data = dataset[0]\n",
    "                aug_data = dataset[1:] ; n_augs = len(aug_data)\n",
    "                aug_data = ops.concatenate(aug_data, axis = 0)\n",
    "            else:\n",
    "                data, aug_data = dataset\n",
    "            \n",
    "        if len(self.feature_extractor.outputs) == 2 :\n",
    "            feature_seq, weights = self.feature_extractor(data)\n",
    "            feature_seq_aug, weights_aug = self.feature_extractor(aug_data)\n",
    "        elif len(self.feature_extractor.outputs) == 1:\n",
    "            feature_seq = self.feature_extractor(data)\n",
    "            feature_seq_aug = self.feature_extractor(aug_data)\n",
    "        if len(ops.shape(feature_seq)) == 3: \n",
    "            rep_vector = keras.layers.GlobalAveragePooling1D()(feature_seq)\n",
    "            rep_vector_aug = keras.layers.GlobalAveragePooling1D()(feature_seq_aug)\n",
    "        elif len(ops.shape(feature_seq)) == 4:\n",
    "            rep_vector = keras.layers.GlobalAveragePooling2D()(feature_seq)\n",
    "            rep_vector_aug = keras.layers.GlobalAveragePooling2D()(feature_seq_aug)\n",
    "        elif len(ops.shape(feature_seq)) == 2:\n",
    "            rep_vector = feature_seq\n",
    "            rep_vector_aug = feature_seq_aug\n",
    "        \n",
    "        rep_vector = self.projector_a(rep_vector)\n",
    "        rep_vector_aug = self.projector_b(rep_vector_aug)\n",
    "        \n",
    "        if self.multiview:\n",
    "            rep_vector_aug_set = ops.split(rep_vector_aug, n_augs, 0)\n",
    "            loss = []\n",
    "            for idx in range(n_augs):\n",
    "                correlation_matrix = get_cor_matrix(rep_vector, rep_vector_aug_set[idx])\n",
    "                loss.append(self.compute_loss(correlation_matrix)\n",
    "                            )\n",
    "            loss = ops.mean(loss)\n",
    "        else:\n",
    "            correlation_matrix = get_cor_matrix(rep_vector, rep_vector_aug)\n",
    "            loss = self.compute_loss(correlation_matrix)\n",
    "        \n",
    "        self.loss_tracker.update_state(loss)\n",
    "        output_dict = {'Barlow_loss' : self.loss_tracker.result()}\n",
    "        \n",
    "        if self.linear_probing:\n",
    "            ##2. linear probing and calculate metrics##\n",
    "            if len(self.feature_extractor.outputs) == 2 :\n",
    "                feature_seq, weights = self.feature_extractor(data, training = False)\n",
    "            elif len(self.feature_extractor.outputs) == 1:\n",
    "                feature_seq = self.feature_extractor(data, training = False)\n",
    "            if len(ops.shape(feature_seq)) == 3: \n",
    "                original_rep_vector = keras.layers.GlobalAveragePooling1D()(feature_seq)\n",
    "            elif len(ops.shape(feature_seq)) == 4:\n",
    "                original_rep_vector = keras.layers.GlobalAveragePooling2D()(feature_seq)\n",
    "            elif len(ops.shape(feature_seq)) == 2:\n",
    "                original_rep_vector = feature_seq\n",
    "\n",
    "            class_logits = self.linear_probe(original_rep_vector, training = False)\n",
    "            class_probe_loss = self.probe_loss_fn(y_true = labels, y_pred = class_logits)\n",
    "            class_probe_loss = ops.mean(class_probe_loss)\n",
    "            self.probe_loss_tracker.update_state(class_probe_loss)\n",
    "            output_dict.update({\"probe_classification_loss\" : self.probe_loss_tracker.result()})\n",
    "            #metrics\n",
    "            for comp in self.probe_metrics:\n",
    "                comp.update_state(labels, class_logits)\n",
    "                \n",
    "            probe_metric_dict = {comp.name :  comp.result() for comp in self.probe_metrics}\n",
    "            output_dict.update(probe_metric_dict)\n",
    "        else:\n",
    "            pass\n",
    "        return output_dict\n",
    "    def get_classifier_model(self):\n",
    "        self.feature_extractor.trainable = True\n",
    "        if self.linear_probing == False:\n",
    "            return self.feature_extractor\n",
    "        self.linear_probe.trainable = True\n",
    "        inputs = self.feature_extractor.input\n",
    "        outputs = self.feature_extractor.outputs\n",
    "        if len(self.feature_extractor.outputs) == 2 :\n",
    "            feature_map, att_weights = outputs\n",
    "        elif len(self.feature_extractor.outputs) == 1:\n",
    "            feature_map = outputs[0]\n",
    "        \n",
    "        if len(ops.shape(feature_map)) == 3: \n",
    "            rep_vector = keras.layers.GlobalAveragePooling1D()(feature_map)\n",
    "        elif len(ops.shape(feature_map)) == 4:\n",
    "            rep_vector = keras.layers.GlobalAveragePooling2D()(feature_map)\n",
    "        elif len(ops.shape(feature_map)) == 2:\n",
    "            rep_vector = feature_map\n",
    "            \n",
    "        class_logits = self.linear_probe(rep_vector)\n",
    "        classifier_model = keras.Model(inputs, class_logits,\n",
    "                                      name = f\"Barlow_pretrained_{self.feature_extractor.name}\")\n",
    "        return classifier_model\n",
    "    def call(self, dataset):\n",
    "        return self.test_step(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4236f797",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T17:51:23.296307Z",
     "iopub.status.busy": "2024-07-05T17:51:23.295969Z",
     "iopub.status.idle": "2024-07-05T17:51:23.349613Z",
     "shell.execute_reply": "2024-07-05T17:51:23.348867Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.083071,
     "end_time": "2024-07-05T17:51:23.351869",
     "exception": false,
     "start_time": "2024-07-05T17:51:23.268798",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VICRegModel(keras.Model):\n",
    "    def __init__(self, feature_extractor, embed_dims, multiview = False,\n",
    "                 probe = False, \n",
    "                 probe_heads = None, probe_activation = \"sigmoid\",\n",
    "                 variance_coeff = 20, invariance_coeff = 20, covariance_coeff = 1, \n",
    "                 variance_gamma = 5.0,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dims = embed_dims\n",
    "        self.multiview = multiview\n",
    "        self.feature_extractor = feature_extractor #weight sharing for VICreg, Barlow Twins\n",
    "        self.projector_a = self.get_projector()\n",
    "        self.projector_b = self.get_projector()\n",
    "        self.var_coef = variance_coeff ; self.invar_coef = invariance_coeff ; self.cov_coef = covariance_coeff\n",
    "        self.gamma = variance_gamma\n",
    "        \n",
    "        self.loss_tracker = tf.keras.metrics.Mean(\"VIC_loss_tracker\")\n",
    "        self.invar_loss_tracker = tf.keras.metrics.Mean(\"Invariance_loss_tracker\")\n",
    "        self.var_loss_tracker = tf.keras.metrics.Mean(\"Variance_loss_tracker\")\n",
    "        self.covar_loss_tracker = tf.keras.metrics.Mean(\"Covariance_loss_tracker\")\n",
    "        self.linear_probing = probe\n",
    "        if probe:\n",
    "            self.probe_loss_tracker = tf.keras.metrics.Mean(\"Probe_loss_tracker\")\n",
    "            self.probe_metrics = [keras.metrics.Accuracy(name=\"probe_accuracy\"),\n",
    "                                  keras.metrics.F1Score(average=\"micro\", threshold=None, name=\"probe_micro_f1_score\"),\n",
    "                                 keras.metrics.TruePositives(name = \"probe_TP\"),\n",
    "                                 keras.metrics.TrueNegatives(name = \"probe_TN\"),\n",
    "                                 keras.metrics.FalsePositives(name = \"probe_FP\"),\n",
    "                                 keras.metrics.FalseNegatives(name = \"probe_FN\")]\n",
    "            \n",
    "            self.probe_categories = probe_heads\n",
    "            self.probe_act = probe_activation\n",
    "            self.linear_probe = keras.layers.Dense(units = probe_heads, activation = self.probe_act)\n",
    "            if self.probe_act == \"sigmoid\":\n",
    "                self.probe_loss_fn = keras.losses.BinaryCrossentropy(reduction = None, label_smoothing = 0.1)\n",
    "            elif self.probe_act == \"softmax\":\n",
    "                self.probe_loss_fn = keras.losses.CategoricalCrossentropy(reduction = None, label_smoothing = 0.1)\n",
    "    def compile(self, optimizer, probe_optimizer = None, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "        self.optimizer = optimizer\n",
    "        if self.linear_probing:\n",
    "            self.probe_optimizer = probe_optimizer     \n",
    "    def get_config(self):\n",
    "        return {\"feature_extractor_name\" : self.feature_extractor.name,\n",
    "               \"embed_dims\" : self.embed_dims,\n",
    "                 \"Multiview(>2)\" : self.multiview,\n",
    "               'Variance_coefficient' : self.var_coef,\n",
    "               'Invariance_coefficient' : self.invar_coef,\n",
    "               \"Covariance_coefficient\" : self.cov_coef,\n",
    "                \"Variance_gamma\" : self.gamma,\n",
    "               \"SSL_method\" : \"VICReg\",\n",
    "               \"Linear Probe\" : self.linear_probing,\n",
    "               \"N_Categories\" : self.probe_categories if self.linear_probing else 0,\n",
    "               \"Probe Activation\" : self.probe_act if self.linear_probing else \"NA\"}\n",
    "    def get_projector(self):\n",
    "        model = keras.Sequential([Dense(units = self.embed_dims),\n",
    "                                  keras.layers.Activation(\"gelu\"),\n",
    "                                  keras.layers.Dropout(0.25),\n",
    "                                 Dense(units = self.embed_dims,dtype = \"float32\"),\n",
    "                                 keras.layers.Lambda(lambda x : ops.normalize(x))]\n",
    "                                )\n",
    "        return model\n",
    "    \n",
    "    def compute_loss(self, rep_a, rep_b):\n",
    "        invar_loss = invariance_loss(rep_a, rep_b) * self.invar_coef\n",
    "        invar_loss = ops.mean(invar_loss)\n",
    "        \n",
    "        variance_a = variance(rep_a, gamma = self.gamma)\n",
    "        variance_a = ops.mean(variance_a)\n",
    "        \n",
    "        covariance_a = covariance(rep_a)\n",
    "        \n",
    "        variance_b = variance(rep_b, gamma = self.gamma)\n",
    "        variance_b = ops.mean(variance_b)\n",
    "        \n",
    "        covariance_b = covariance(rep_b)\n",
    "        #Variance loss -> variance가 toward gamma\n",
    "        # covariance -> covariance가 toward zero\n",
    "        var_loss = (variance_a + variance_b) * self.var_coef\n",
    "        covar_loss = (covariance_a + covariance_b) * self.cov_coef\n",
    "        loss = invar_loss + var_loss + covar_loss\n",
    "        return loss, invar_loss, var_loss, covar_loss\n",
    "        \n",
    "    def train_step(self, dataset):\n",
    "        if self.linear_probing:\n",
    "            data, aug_data, labels = dataset\n",
    "        else:\n",
    "            if self.multiview:\n",
    "                data = dataset[0]\n",
    "                aug_data = dataset[1:] ; n_augs = len(aug_data)\n",
    "                aug_data = ops.concatenate(aug_data, axis = 0)\n",
    "            else:\n",
    "                data, aug_data = dataset\n",
    "            \n",
    "        with tf.GradientTape() as tape:\n",
    "            \n",
    "            if len(self.feature_extractor.outputs) == 2 :\n",
    "                feature_seq, weights = self.feature_extractor(data)\n",
    "                feature_seq_aug, weights_aug = self.feature_extractor(aug_data)\n",
    "            elif len(self.feature_extractor.outputs) == 1:\n",
    "                feature_seq = self.feature_extractor(data)\n",
    "                feature_seq_aug = self.feature_extractor(aug_data)\n",
    "            \n",
    "            if len(ops.shape(feature_seq)) == 3: \n",
    "                rep_vector = keras.layers.GlobalAveragePooling1D()(feature_seq)\n",
    "                rep_vector_aug = keras.layers.GlobalAveragePooling1D()(feature_seq_aug)\n",
    "            elif len(ops.shape(feature_seq)) == 4:\n",
    "                rep_vector = keras.layers.GlobalAveragePooling2D()(feature_seq)\n",
    "                rep_vector_aug = keras.layers.GlobalAveragePooling2D()(feature_seq_aug)\n",
    "            elif len(ops.shape(feature_seq)) == 2:\n",
    "                rep_vector = feature_seq\n",
    "                rep_vector_aug = feature_seq_aug\n",
    "            \n",
    "            \n",
    "            rep_vector = self.projector_a(rep_vector)\n",
    "            rep_vector_aug = self.projector_b(rep_vector_aug)\n",
    "            if self.multiview:\n",
    "                loss, invar_loss, var_loss, covar_loss = 0.0, 0.0, 0.0, 0.0\n",
    "                rep_vector_aug_set = ops.split(rep_vector_aug, n_augs, 0)\n",
    "                for idx in range(n_augs):\n",
    "                    loss_, invar_loss_, var_loss_, covar_loss_ = self.compute_loss(rep_vector, rep_vector_aug_set[idx])\n",
    "                    loss += loss_\n",
    "                    invar_loss += invar_loss_\n",
    "                    var_loss += var_loss_\n",
    "                    covar_loss += covar_loss_\n",
    "                loss /= n_augs\n",
    "                invar_loss /= n_augs\n",
    "                var_loss /= n_augs\n",
    "                covar_loss /= n_augs\n",
    "                \n",
    "            else:\n",
    "                loss, invar_loss, var_loss, covar_loss = self.compute_loss(rep_vector, rep_vector_aug)\n",
    "        \n",
    "        feature_extractor_var = self.feature_extractor.trainable_variables\n",
    "        proj_a_var = self.projector_a.trainable_variables\n",
    "        proj_b_var = self.projector_b.trainable_variables\n",
    "        \n",
    "        trainable_variables = (feature_extractor_var + proj_a_var + proj_b_var)\n",
    "        gradients = tape.gradient(loss, trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "        \n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.invar_loss_tracker.update_state(invar_loss)\n",
    "        self.var_loss_tracker.update_state(var_loss)\n",
    "        self.covar_loss_tracker.update_state(covar_loss)\n",
    "        output_dict = {'loss' : self.loss_tracker.result(),\n",
    "                        \"invariance_loss\" : self.invar_loss_tracker.result(),\n",
    "                        'variance_loss' : self.var_loss_tracker.result(),\n",
    "                        'covariance_loss' : self.covar_loss_tracker.result()\n",
    "                       }\n",
    "        \n",
    "        if self.linear_probing:\n",
    "            ##2. linear probing and calculate metrics##\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                if len(self.feature_extractor.outputs) == 2 :\n",
    "                    feature_seq, weights = self.feature_extractor(data, training = False)\n",
    "                elif len(self.feature_extractor.outputs) == 1:\n",
    "                    feature_seq = self.feature_extractor(data, training = False)\n",
    "                if len(ops.shape(feature_seq)) == 3: \n",
    "                    original_rep_vector = keras.layers.GlobalAveragePooling1D()(feature_seq)\n",
    "                elif len(ops.shape(feature_seq)) == 4:\n",
    "                    original_rep_vector = keras.layers.GlobalAveragePooling2D()(feature_seq)\n",
    "                elif len(ops.shape(feature_seq)) == 2:\n",
    "                    original_rep_vector = feature_seq\n",
    "                    \n",
    "                class_logits = self.linear_probe(original_rep_vector, training = True)\n",
    "                class_probe_loss = self.probe_loss_fn(y_true = labels, y_pred = class_logits)\n",
    "                class_probe_loss = ops.mean(class_probe_loss)\n",
    "            gradients = tape.gradient(class_probe_loss, \n",
    "                                            self.linear_probe.trainable_weights)\n",
    "            self.probe_optimizer.apply_gradients(zip(gradients, self.linear_probe.trainable_weights))\n",
    "            self.probe_loss_tracker.update_state(class_probe_loss)\n",
    "            output_dict.update({\"probe_classification_loss\" : self.probe_loss_tracker.result()})\n",
    "            #metrics\n",
    "            for comp in self.probe_metrics:\n",
    "                comp.update_state(labels, class_logits)\n",
    "                \n",
    "            probe_metric_dict = {comp.name :  comp.result() for comp in self.probe_metrics}\n",
    "            output_dict.update(probe_metric_dict)\n",
    "        else:\n",
    "            pass\n",
    "        del tape\n",
    "        return output_dict\n",
    "    def test_step(self, dataset):\n",
    "        if self.linear_probing:\n",
    "            data, aug_data, labels = dataset\n",
    "        else:\n",
    "            if self.multiview:\n",
    "                data = dataset[0]\n",
    "                aug_data = dataset[1:] ; n_augs = len(aug_data)\n",
    "                aug_data = ops.concatenate(aug_data, axis = 0)\n",
    "            else:\n",
    "                data, aug_data = dataset\n",
    "        \n",
    "        if len(self.feature_extractor.outputs) == 2 :\n",
    "            feature_seq, weights = self.feature_extractor(data)\n",
    "            feature_seq_aug, weights_aug = self.feature_extractor(aug_data)\n",
    "        elif len(self.feature_extractor.outputs) == 1:\n",
    "            feature_seq = self.feature_extractor(data)\n",
    "            feature_seq_aug = self.feature_extractor(aug_data)\n",
    "        \n",
    "        if len(ops.shape(feature_seq)) == 3: \n",
    "            rep_vector = keras.layers.GlobalAveragePooling1D()(feature_seq)\n",
    "            rep_vector_aug = keras.layers.GlobalAveragePooling1D()(feature_seq_aug)\n",
    "        elif len(ops.shape(feature_seq)) == 4:\n",
    "            rep_vector = keras.layers.GlobalAveragePooling2D()(feature_seq)\n",
    "            rep_vector_aug = keras.layers.GlobalAveragePooling2D()(feature_seq_aug)\n",
    "        elif len(ops.shape(feature_seq)) == 2:\n",
    "            rep_vector = feature_seq\n",
    "            rep_vector_aug = feature_seq_aug\n",
    "            \n",
    "        original_rep_vector = rep_vector\n",
    "        \n",
    "        rep_vector = self.projector_a(rep_vector, training = False)\n",
    "        rep_vector_aug = self.projector_b(rep_vector_aug, training = False)\n",
    "        if self.multiview:\n",
    "            loss, invar_loss, var_loss, covar_loss = [],[],[],[]\n",
    "            rep_vector_aug_set = ops.split(rep_vector_aug, n_augs, 0)\n",
    "            for idx in range(n_augs):\n",
    "                loss_, invar_loss_, var_loss_, covar_loss_ = self.compute_loss(rep_vector, rep_vector_aug_set[idx])\n",
    "                loss.append(loss_)\n",
    "                invar_loss.append(invar_loss_)\n",
    "                var_loss.append(var_loss_)\n",
    "                covar_loss.append(covar_loss_)\n",
    "            loss = ops.mean(loss)\n",
    "            invar_loss = ops.mean(invar_loss)\n",
    "            var_loss = ops.mean(var_loss)\n",
    "            covar_loss = ops.mean(covar_loss)    \n",
    "        else:\n",
    "            loss, invar_loss, var_loss, covar_loss = self.compute_loss(rep_vector, rep_vector_aug)\n",
    "        \n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.invar_loss_tracker.update_state(invar_loss)\n",
    "        self.var_loss_tracker.update_state(var_loss)\n",
    "        self.covar_loss_tracker.update_state(covar_loss)\n",
    "        \n",
    "        output_dict =  {'loss' : self.loss_tracker.result(),\n",
    "                \"invariance_loss\" : self.invar_loss_tracker.result(),\n",
    "                'variance_loss' : self.var_loss_tracker.result(),\n",
    "                'covariance_loss' : self.covar_loss_tracker.result()\n",
    "               }\n",
    "        if self.linear_probing:\n",
    "            ##2. linear probing and calculate metrics##\n",
    "            self.feature_extractor.trainable = False\n",
    "            class_logits = self.linear_probe(original_rep_vector, training = False)\n",
    "            class_probe_loss = self.probe_loss_fn(y_true = labels, y_pred = class_logits)\n",
    "            class_probe_loss = ops.mean(class_probe_loss)\n",
    "            self.probe_loss_tracker.update_state(class_probe_loss)\n",
    "            output_dict.update({\"probe_classification_loss\" : self.probe_loss_tracker.result()})\n",
    "            #metrics\n",
    "            for comp in self.probe_metrics:\n",
    "                comp.update_state(labels, class_logits)\n",
    "                \n",
    "            probe_metric_dict = {comp.name :  comp.result() for comp in self.probe_metrics}\n",
    "            output_dict.update(probe_metric_dict)\n",
    "            self.feature_extractor.trainable = True\n",
    "        return output_dict\n",
    "    def get_classifier_model(self):\n",
    "        self.feature_extractor.trainable = True\n",
    "        if self.linear_probing == False:\n",
    "            return self.feature_extractor\n",
    "        self.linear_probe.trainable = True\n",
    "        inputs = self.feature_extractor.input\n",
    "        outputs = self.feature_extractor.outputs\n",
    "        if len(self.feature_extractor.outputs) == 2 :\n",
    "            feature_map, att_weights = outputs\n",
    "        elif len(self.feature_extractor.outputs) == 1:\n",
    "            feature_map = outputs[0]\n",
    "        \n",
    "        if len(ops.shape(feature_map)) == 3: \n",
    "            rep_vector = keras.layers.GlobalAveragePooling1D()(feature_map)\n",
    "        elif len(ops.shape(feature_map)) == 4:\n",
    "            rep_vector = keras.layers.GlobalAveragePooling2D()(feature_map)\n",
    "        elif len(ops.shape(feature_seq)) == 2:\n",
    "            rep_vector = feature_map\n",
    "            \n",
    "        class_logits = self.linear_probe(rep_vector)\n",
    "        classifier_model = keras.Model(inputs, class_logits,\n",
    "                                      name = f\"SSL_pretrained_{self.feature_extractor.name}\")\n",
    "        return classifier_model\n",
    "    def call(self, dataset):\n",
    "        return self.test_step(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2958779b",
   "metadata": {
    "papermill": {
     "duration": 0.025676,
     "end_time": "2024-07-05T17:51:23.403540",
     "exception": false,
     "start_time": "2024-07-05T17:51:23.377864",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "-------------\n",
    "# Contrastive-based method\n",
    "- SimSiam\n",
    "- SimCLR\n",
    "- SwAV\n",
    "- DINO\n",
    "- MoCo and moco-based learnings:\n",
    "    - [Dense Contrastive learning](https://arxiv.org/abs/2011.09157v2) -> DCL\n",
    "    - [Nearest-Neighbor Contrastive Learning of Visual Representations](https://arxiv.org/abs/2104.14548v2) -> NNCLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0cfc4380",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T17:51:23.457036Z",
     "iopub.status.busy": "2024-07-05T17:51:23.456687Z",
     "iopub.status.idle": "2024-07-05T17:51:23.491145Z",
     "shell.execute_reply": "2024-07-05T17:51:23.490226Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.063842,
     "end_time": "2024-07-05T17:51:23.493044",
     "exception": false,
     "start_time": "2024-07-05T17:51:23.429202",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SimSiam(keras.Model):\n",
    "    def __init__(self, feature_extractor, embed_dims = 256, multiview = False,\n",
    "                 probe = False, probe_heads = None, probe_activation = \"sigmoid\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.feature_extractor = feature_extractor #attentive pooling을 거친 encoder로 가정 : rep_vector, attention_Weight를 output으로\n",
    "        self.embed_dims = embed_dims\n",
    "        self.multiview = multiview\n",
    "        \n",
    "        self.train_type = 'SimSiam'\n",
    "        self.loss_tracker = keras.metrics.Mean(name = f\"{self.train_type}_loss\")\n",
    "        self.probe = probe ; self.probe_heads = probe_heads\n",
    "        self.probe_activation = probe_activation\n",
    "        \n",
    "        print(\"Train with Gradient Accumulation is recommended!\")\n",
    "        \n",
    "        self.embed_dims = embed_dims\n",
    "        self.predictor = keras.Sequential([keras.layers.Dense(units = self.embed_dims,\n",
    "                                                            use_bias = False,\n",
    "                                                            kernel_regularizer = keras.regularizers.l2(5e-4),\n",
    "                                                            ),\n",
    "                                          keras.layers.Activation(\"gelu\"),\n",
    "                                          keras.layers.BatchNormalization(),\n",
    "                                          keras.layers.Dense(units = self.embed_dims),\n",
    "                                          ], name = f\"{self.train_type}_predictor\")\n",
    "        if self.probe:\n",
    "            self.linear_probe = Dense(units = self.probe_heads, \n",
    "                                     activation = self.probe_activation)\n",
    "            self.probe_loss_tracker = tf.keras.metrics.Mean(\"Probe_loss_tracker\")\n",
    "            self.probe_metrics = [keras.metrics.Accuracy(name=\"probe_accuracy\"),\n",
    "                                  keras.metrics.F1Score(average=\"micro\", threshold=None, name=\"probe_micro_f1_score\"),\n",
    "                                 keras.metrics.TruePositives(name = \"probe_TP\"),\n",
    "                                 keras.metrics.TrueNegatives(name = \"probe_TN\"),\n",
    "                                 keras.metrics.FalsePositives(name = \"probe_FP\"),\n",
    "                                 keras.metrics.FalseNegatives(name = \"probe_FN\")]\n",
    "            if self.probe_activation == \"sigmoid\":\n",
    "                self.probe_loss_fn = keras.losses.BinaryCrossentropy(reduction = None, label_smoothing = 0.1)\n",
    "            elif self.probe_activation == \"softmax\":\n",
    "                self.probe_loss_fn = keras.losses.CategoricalCrossentropy(reduction = None, label_smoothing = 0.1)\n",
    "    def get_config(self):\n",
    "        return {\"feature_extractor_name\" : self.feature_extractor.name,\n",
    "               \"embed_dims\" : self.embed_dims,\n",
    "                \"Multiview(>2)\" : self.multiview,\n",
    "               \"SSL_method\" : \"SimSiam\",\n",
    "               \"Linear Probe\" : self.probe,\n",
    "               \"N_Categories\" : self.probe_categories if self.probe else 0,\n",
    "               \"Probe Activation\" : self.probe_act if self.probe else \"NA\"}\n",
    "    def compile(self, optimizer, probe_optimizer = None, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "        self.optimizer = optimizer\n",
    "        if self.probe:\n",
    "            self.probe_optimizer = probe_optimizer \n",
    "    def compute_loss(self, p, z, epsilon = 1e-5):\n",
    "        # stop gradient is essential in SimSiam structure\n",
    "        # p, z is representation vectors : batch_size, embed_dims\n",
    "        \n",
    "        z = ops.stop_gradient(p)\n",
    "        z = keras.utils.normalize(z, axis = -1, order = 2)\n",
    "        p = keras.utils.normalize(p, axis = -1, order = 2)\n",
    "        cos_sim = ops.mean(ops.sum(z*p, axis = -1)\n",
    "                          ) #batchwise mean\n",
    "        return -cos_sim\n",
    "\n",
    "    def train_step(self, dataset):\n",
    "        if self.probe:\n",
    "            img1, img2, labels = dataset\n",
    "        else:\n",
    "            if self.multiview:\n",
    "                img1 = dataset[0]\n",
    "                img2 = dataset[1:] ; n_augs = len(img2)\n",
    "                img2 = ops.concatenate(img2, axis = 0)\n",
    "            else:\n",
    "                img1, img2 = dataset\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            if len(self.feature_extractor.outputs) == 2:\n",
    "                z1, weight1 = self.feature_extractor(img1, training = True)\n",
    "                z2, weight2 = self.feature_extractor(img2, training = True)\n",
    "            elif len(self.feature_extractor.outputs) == 1:\n",
    "                z1 = self.feature_extractor(img1, training = True)\n",
    "                z2 = self.feature_extractor(img2, training = True)\n",
    "            v1, v2 = self.predictor(z1, training = True), self.predictor(z2, training = True)\n",
    "            \n",
    "            if self.multiview:\n",
    "                v2 = ops.split(v2, n_augs, 0)\n",
    "                loss = []\n",
    "                for idx in range(n_augs):\n",
    "                    loss.append(self.compute_loss(v1, v2[idx])\n",
    "                               )\n",
    "                loss = ops.mean(loss)\n",
    "            else:\n",
    "                loss = 0.5*(self.compute_loss(v1, v2) + self.compute_loss(v2, v1))\n",
    "        trainable_params = self.feature_extractor.trainable_variables + self.predictor.trainable_variables\n",
    "        grads = tape.gradient(loss, trainable_params)\n",
    "        self.optimizer.apply_gradients(zip(grads, trainable_params))\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        output_dict = {self.loss_tracker.name : self.loss_tracker.result()}\n",
    "        \n",
    "        if self.probe:\n",
    "            with tf.GradientTape() as tape:\n",
    "                class_logits = self.linear_probe(z1)\n",
    "                probe_loss = self.probe_loss_fn(y_true = labels, y_pred = class_logits)\n",
    "                probe_loss = ops.mean(probe_loss)\n",
    "            grads = tape.gradient(probe_loss, self.linear_probe.trainable_weights)\n",
    "            self.probe_optimizer.apply_gradients(zip(grads, self.linear_probe.trainable_weights))\n",
    "            self.probe_loss_tracker.update_state(probe_loss)\n",
    "            output_dict.update({\"probe_classification_loss\" : self.probe_loss_tracker.result()})\n",
    "            #metrics\n",
    "            for comp in self.probe_metrics:\n",
    "                comp.update_state(labels, class_logits)\n",
    "                \n",
    "            probe_metric_dict = {comp.name :  comp.result() for comp in self.probe_metrics}\n",
    "            output_dict.update(probe_metric_dict)\n",
    "        del tape\n",
    "        return output_dict\n",
    "    \n",
    "    def test_step(self, dataset):\n",
    "        if self.probe:\n",
    "            img1, img2, labels = dataset\n",
    "        else:\n",
    "            if self.multiview:\n",
    "                img1 = dataset[0]\n",
    "                img2 = dataset[1:] ; n_augs = len(img2)\n",
    "                img2 = ops.concatenate(img2, axis = 0)\n",
    "            else:\n",
    "                img1, img2 = dataset\n",
    "        \n",
    "        if len(self.feature_extractor.outputs) == 2:\n",
    "            z1, weight1 = self.feature_extractor(img1, training = False)\n",
    "            z2, weight2 = self.feature_extractor(img2, training = False)\n",
    "        elif len(self.feature_extractor.outputs) == 1:\n",
    "            z1 = self.feature_extractor(img1, training = False)\n",
    "            z2 = self.feature_extractor(img2, training = False)\n",
    "        v1, v2 = self.predictor(z1, training = False), self.predictor(z2, training = False)\n",
    "        if self.multiview:\n",
    "            v2 = ops.split(v2, n_augs, 0)\n",
    "            loss = []\n",
    "            for idx in range(n_augs):\n",
    "                loss.append(self.compute_loss(v1, v2[idx])\n",
    "                            )\n",
    "            loss = ops.mean(loss)\n",
    "        else:\n",
    "            loss = 0.5*(self.compute_loss(v1, v2) + self.compute_loss(v2, v1))\n",
    "        \n",
    "        self.loss_tracker.update_state(loss)\n",
    "        output_dict = {self.loss_tracker.name : self.loss_tracker.result()}\n",
    "        \n",
    "        if self.probe:\n",
    "            class_logits = self.linear_probe(z1)\n",
    "            probe_loss = self.probe_loss_fn(y_true = labels, y_pred = class_logits)\n",
    "            probe_loss = ops.mean(probe_loss)\n",
    "            self.probe_loss_tracker.update_state(probe_loss)\n",
    "            output_dict.update({\"probe_classification_loss\" : self.probe_loss_tracker.result()})\n",
    "            for comp in self.probe_metrics:\n",
    "                comp.update_state(labels, class_logits)\n",
    "                \n",
    "            probe_metric_dict = {comp.name :  comp.result() for comp in self.probe_metrics}\n",
    "            output_dict.update(probe_metric_dict)\n",
    "        \n",
    "        return output_dict\n",
    "    \n",
    "    def call(self, dataset):\n",
    "        return self.test_step(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0cc0cb3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T17:51:23.547583Z",
     "iopub.status.busy": "2024-07-05T17:51:23.546775Z",
     "iopub.status.idle": "2024-07-05T17:51:23.571083Z",
     "shell.execute_reply": "2024-07-05T17:51:23.570381Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.053641,
     "end_time": "2024-07-05T17:51:23.573065",
     "exception": false,
     "start_time": "2024-07-05T17:51:23.519424",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SimCLR(keras.Model):\n",
    "    def __init__(self, feature_extractor, embed_dims = 256, multiview = False, probe = False,\n",
    "                 temperature = 0.1,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.feature_extractor = feature_extractor #attentive pooling을 거친 encoder로 가정 : rep_vector, attention_Weight를 output으로\n",
    "        self.embed_dims = embed_dims\n",
    "        self.multiview = multiview\n",
    "        self.t = temperature\n",
    "        self.train_type = 'SimCLR'\n",
    "        print(\"Train with Gradient Accumulation is recommended!\")\n",
    "        \n",
    "        self.embed_dims = embed_dims\n",
    "        self.predictor = keras.Sequential([keras.layers.Dense(units = self.embed_dims,\n",
    "                                                            use_bias = False,\n",
    "                                                            ),\n",
    "                                          keras.layers.Activation(\"gelu\"),\n",
    "                                          keras.layers.LayerNormalization(),\n",
    "                                          keras.layers.Dense(units = self.embed_dims),\n",
    "                                          keras.layers.Activation(\"gelu\"),\n",
    "                                          keras.layers.LayerNormalization(),\n",
    "                                          keras.layers.Dense(units = self.embed_dims),\n",
    "                                           keras.layers.Lambda(lambda x : ops.normalize(x))\n",
    "                                          ], name = f\"{self.train_type}_predictor\")\n",
    "        self.probe = False\n",
    "        \n",
    "        self.loss_tracker = keras.metrics.Mean(name = f\"{self.train_type}_loss\")\n",
    "        \n",
    "    def get_config(self):\n",
    "        return {\"feature_extractor_name\" : self.feature_extractor.name,\n",
    "               \"embed_dims\" : self.embed_dims,\n",
    "                \"Multiview(>2)\" : self.multiview,\n",
    "               \"SSL_method\" : \"SimCLR\",\n",
    "               \"Linear Probe\" : self.probe,\n",
    "               \"N_Categories\" : self.probe_categories if self.probe else 0,\n",
    "               \"Probe Activation\" : self.probe_act if self.probe else \"NA\"}\n",
    "    def compile(self, optimizer, probe_optimizer = None, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "        self.optimizer = optimizer\n",
    "        if self.probe:\n",
    "            self.probe_optimizer = probe_optimizer \n",
    "\n",
    "    def train_step(self, dataset):\n",
    "        if self.probe:\n",
    "            img1, img2, labels = dataset\n",
    "        else:\n",
    "            if self.multiview:\n",
    "                img1 = dataset[0]\n",
    "                img2 = dataset[1:] ; n_augs = len(img2)\n",
    "                img2 = ops.concatenate(img2, axis = 0)\n",
    "            else:\n",
    "                img1, img2 = dataset\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            if len(self.feature_extractor.outputs) == 2:\n",
    "                z1, weight1 = self.feature_extractor(img1, training = True)\n",
    "                z2, weight2 = self.feature_extractor(img2, training = True)\n",
    "            elif len(self.feature_extractor.outputs) == 1:\n",
    "                z1 = self.feature_extractor(img1, training = True)\n",
    "                z2 = self.feature_extractor(img2, training = True)\n",
    "            v1, v2 = self.predictor(z1, training = True), self.predictor(z2, training = True)\n",
    "            \n",
    "            if self.multiview:\n",
    "                v2 = ops.split(v2, n_augs, 0)\n",
    "                loss = []\n",
    "                for idx in range(n_augs):\n",
    "                    loss.append(keras_cv.losses.SimCLRLoss(temperature = 0.1, reduction = None)(v1, v2[idx]))\n",
    "                    \n",
    "                loss = ops.mean(loss)\n",
    "                \n",
    "            else:\n",
    "                loss = keras_cv.losses.SimCLRLoss(temperature = 0.1)(v1, v2)\n",
    "        trainable_params = self.feature_extractor.trainable_variables + self.predictor.trainable_variables\n",
    "        grads = tape.gradient(loss, trainable_params)\n",
    "        self.optimizer.apply_gradients(zip(grads, trainable_params))\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        output_dict = {self.loss_tracker.name : self.loss_tracker.result(),\n",
    "                      }\n",
    "        return output_dict\n",
    "    \n",
    "    def test_step(self, dataset):\n",
    "        if self.probe:\n",
    "            img1, img2, labels = dataset\n",
    "        else:\n",
    "            if self.multiview:\n",
    "                img1 = dataset[0]\n",
    "                img2 = dataset[1:] ; n_augs = len(img2)\n",
    "                img2 = ops.concatenate(img2, axis = 0)\n",
    "            else:\n",
    "                img1, img2 = dataset\n",
    "        \n",
    "        if len(self.feature_extractor.outputs) == 2:\n",
    "            z1, weight1 = self.feature_extractor(img1, training = True)\n",
    "            z2, weight2 = self.feature_extractor(img2, training = True)\n",
    "        elif len(self.feature_extractor.outputs) == 1:\n",
    "            z1 = self.feature_extractor(img1, training = True)\n",
    "            z2 = self.feature_extractor(img2, training = True)\n",
    "        v1, v2 = self.predictor(z1, training = False), self.predictor(z2, training = False)\n",
    "            \n",
    "        if self.multiview:\n",
    "            v2 = ops.split(v2, n_augs, 0)\n",
    "            loss = []\n",
    "            for idx in range(n_augs):\n",
    "                loss.append(keras_cv.losses.SimCLRLoss(temperature = 0.1, reduction = None)(v1, v2[idx]))        \n",
    "            loss = ops.mean(loss)\n",
    "            \n",
    "        else:\n",
    "            loss = keras_cv.losses.SimCLRLoss(temperature = 0.1)(v1, v2)\n",
    "        \n",
    "        self.loss_tracker.update_state(loss)\n",
    "        output_dict = {self.loss_tracker.name : self.loss_tracker.result(),\n",
    "                      }\n",
    "        return output_dict\n",
    "    \n",
    "    def call(self, dataset):\n",
    "        return self.test_step(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1b81a175",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T17:51:23.628182Z",
     "iopub.status.busy": "2024-07-05T17:51:23.627818Z",
     "iopub.status.idle": "2024-07-05T17:51:23.653796Z",
     "shell.execute_reply": "2024-07-05T17:51:23.653031Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.055143,
     "end_time": "2024-07-05T17:51:23.655730",
     "exception": false,
     "start_time": "2024-07-05T17:51:23.600587",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Moco(keras.Model):\n",
    "    def __init__(self, feature_extractor, embed_dims, q_size = 2**13, pool_heads = 8, t = 0.07,\n",
    "                 momentum_coefficient = 0.999,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.momentum_encoder = feature_extractor\n",
    "        self.momentum_encoder.set_weights(self.feature_extractor.get_weights())\n",
    "        self.m = momentum_coefficient\n",
    "        self.pool_heads = pool_heads\n",
    "        self.t = t\n",
    "        self.q_size = q_size\n",
    "        self.embed_dims = embed_dims\n",
    "        self.feature_queue = keras.Variable(\n",
    "            keras.utils.normalize(\n",
    "                keras.random.normal(shape=(self.q_size, self.embed_dims)),\n",
    "                axis=1,\n",
    "                order=2,\n",
    "            ),\n",
    "            trainable=False, dtype = \"float32\"\n",
    "        )\n",
    "        self.projector = keras.Sequential([keras.layers.Dense(units = embed_dims),\n",
    "                                           keras.layers.Activation(\"relu\"),\n",
    "                                          keras.layers.Dense(units = embed_dims, dtype = \"float32\"),\n",
    "                                          ])\n",
    "        self.momentum_projector = keras.models.clone_model(self.projector)\n",
    "        \n",
    "        self.ce_loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
    "        self.loss_tracker = keras.metrics.Mean(name = \"MoCo_loss\")\n",
    "    def get_config(self):\n",
    "        return {\"feature_extractor_name\" : self.feature_extractor.name,\n",
    "               \"embed_dims\" : self.embed_dims,\n",
    "               \"SSL_method\" : \"MoCo\",\n",
    "               \"Queue size\" : self.q_size,\n",
    "               \"temperature\" : self.t,\n",
    "               \"Momentum\" : self.m}\n",
    "    def compute_loss(self, z_a, z_b, training = True): #Info-NCE loss in the original paper\n",
    "        z_b = ops.stop_gradient(z_b)\n",
    "        z_a, z_b = keras.utils.normalize(z_a, axis = -1, order = 2), keras.utils.normalize(z_b, axis = -1, order = 2)\n",
    "        z_a, z_b = ops.cast(z_a, \"float32\"), ops.cast(z_b, \"float32\")\n",
    "        \n",
    "        pos_sim_ = ops.diagonal(z_a@ops.transpose(z_b)) ; pseudolabel = ops.zeros_like(pos_sim_)\n",
    "        pos_pair_similarity = ops.expand_dims(pos_sim_, axis = -1) ; del pos_sim_\n",
    "        \n",
    "        neg_pair_similarity = z_a@ops.cast(ops.transpose(self.feature_queue), \"float32\")\n",
    "        logits = ops.concatenate([pos_pair_similarity, neg_pair_similarity], axis = 1)\n",
    "        logits = ops.exp(logits/self.t)\n",
    "        logits = logits / (ops.sum(logits, axis = -1, keepdims = True) + 1e-8)\n",
    "        loss = self.ce_loss_fn(y_true = pseudolabel, y_pred = logits)\n",
    "        \n",
    "        if training:\n",
    "            self.feature_queue.assign(\n",
    "                ops.concatenate([z_a, ops.cast(self.feature_queue[:-ops.shape(z_a)[0] ],\n",
    "                                              \"float32\")\n",
    "                                ], axis=0)\n",
    "            )\n",
    "\n",
    "        return loss\n",
    "    def train_step(self, dataset):\n",
    "        img1, img2 = dataset\n",
    "        with tf.GradientTape() as tape:\n",
    "            if len(self.feature_extractor.outputs) == 2:\n",
    "                z1, weight1 = self.feature_extractor(img1)\n",
    "                z2, weight2 = self.momentum_encoder(img2)\n",
    "            elif len(self.feature_extractor.outputs) == 1:\n",
    "                z1 = self.feature_extractor(img1)\n",
    "                z2 = self.momentum_encoder(img2)\n",
    "            v1, v2 = self.projector(z1), self.momentum_projector(z2)\n",
    "            moco_loss = self.compute_loss(v1, v2)\n",
    "        grads = tape.gradient(moco_loss, \n",
    "                             self.feature_extractor.trainable_weights + self.projector.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, \n",
    "                                          self.feature_extractor.trainable_weights + self.projector.trainable_weights)\n",
    "                                      )\n",
    "        self.loss_tracker.update_state(moco_loss)\n",
    "        \n",
    "        #momentum update\n",
    "        for weight, m_weight in zip(self.feature_extractor.weights, self.momentum_encoder.weights):\n",
    "            m_weight.assign(self.m * m_weight + (1-self.m)*weight)\n",
    "        for weight, m_weight in zip(self.projector.weights, self.momentum_projector.weights):\n",
    "            m_weight.assign(self.m * m_weight + (1-self.m)*weight)\n",
    "        return {self.loss_tracker.name : self.loss_tracker.result()}\n",
    "    def test_step(self, dataset):\n",
    "        img1, img2 = dataset\n",
    "        if len(self.feature_extractor.outputs) == 2:\n",
    "            z1, weight1 = self.feature_extractor(img1)\n",
    "            z2, weight2 = self.momentum_encoder(img2)\n",
    "        elif len(self.feature_extractor.outputs) == 1:\n",
    "            z1 = self.feature_extractor(img1)\n",
    "            z2 = self.momentum_encoder(img2)\n",
    "        v1, v2 = self.projector(z1), self.momentum_projector(z2)\n",
    "        moco_loss = self.compute_loss(v1, v2, training = False)\n",
    "        self.loss_tracker.update_state(moco_loss)\n",
    "        return {self.loss_tracker.name : self.loss_tracker.result()}\n",
    "    def call(self, dataset):\n",
    "        img1, img2 = dataset\n",
    "        if len(self.feature_extractor.outputs) == 2:\n",
    "            z1, weight1 = self.feature_extractor(img1)\n",
    "            z2, weight2 = self.momentum_encoder(img2)\n",
    "        elif len(self.feature_extractor.outputs) == 1:\n",
    "            z1 = self.feature_extractor(img1)\n",
    "            z2 = self.momentum_encoder(img2)\n",
    "        v1, v2 = self.projector(z1), self.momentum_projector(z2)\n",
    "        moco_loss = self.compute_loss(v1, v2, training = False)\n",
    "        self.loss_tracker.update_state(moco_loss)\n",
    "        return {self.loss_tracker.name : self.loss_tracker.result()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2704504d",
   "metadata": {
    "papermill": {
     "duration": 0.02553,
     "end_time": "2024-07-05T17:51:23.707066",
     "exception": false,
     "start_time": "2024-07-05T17:51:23.681536",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f450c5a",
   "metadata": {
    "papermill": {
     "duration": 0.02607,
     "end_time": "2024-07-05T17:51:23.758527",
     "exception": false,
     "start_time": "2024-07-05T17:51:23.732457",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Clustering & Distillation\n",
    "- SwAV, DINO\n",
    "\n",
    "> DINO architecture:\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:1400/1*huuMgEbBryxXUufW33uhvQ.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d6a38e9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T17:51:23.812606Z",
     "iopub.status.busy": "2024-07-05T17:51:23.812236Z",
     "iopub.status.idle": "2024-07-05T17:51:23.869245Z",
     "shell.execute_reply": "2024-07-05T17:51:23.868315Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.087334,
     "end_time": "2024-07-05T17:51:23.871577",
     "exception": false,
     "start_time": "2024-07-05T17:51:23.784243",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DINO(keras.Model): #g : feature extractor + predictor\n",
    "    def __init__(self, feature_extractor, apply_simclr = False, apply_barlow = False,\n",
    "                 student_model = None,\n",
    "                 embed_dims = 1024, multiview = False, probe = False,\n",
    "                 teacher_t = 0.1, student_t = 0.1,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.feature_extractor = feature_extractor #output as feature vector and attention weight\n",
    "        if student_model is None:\n",
    "            self.student_extractor = tf.keras.models.clone_model(feature_extractor)\n",
    "        else:\n",
    "            self.student_extractor = student_model\n",
    "            \n",
    "        for f_t, f_p in zip(self.feature_extractor.weights, self.student_extractor.weights):\n",
    "            f_t.assign(f_p) \n",
    "            \n",
    "        self.embed_dims = embed_dims\n",
    "        self.multiview = multiview\n",
    "        self.teacher_t = teacher_t\n",
    "        self.student_t = student_t\n",
    "        self.c = tf.Variable(keras.random.normal(shape = (embed_dims,),\n",
    "                                      dtype = tf.float32)\n",
    "                            )\n",
    "        self.train_type = 'DINO'\n",
    "        self.embed_dims = embed_dims\n",
    "        self.teacher_predictor = keras.Sequential([keras.layers.Dense(units = self.embed_dims, use_bias = False),\n",
    "                                          keras.layers.Dense(units = self.embed_dims),\n",
    "                                          keras.layers.Dense(units = self.embed_dims),\n",
    "                                            keras.layers.Lambda(lambda x : ops.normalize(x))\n",
    "                                          ], name = f\"Teacher_{self.train_type}_predictor\")\n",
    "        self.student_predictor = keras.Sequential([keras.layers.Dense(units = self.embed_dims, use_bias = False),\n",
    "                                          keras.layers.Dense(units = self.embed_dims),\n",
    "                                          keras.layers.Dense(units = self.embed_dims),\n",
    "                                            keras.layers.Lambda(lambda x : ops.normalize(x))\n",
    "                                          ], name = f\"Student_{self.train_type}_predictor\")\n",
    "        for s_t, s_p in zip(self.teacher_predictor.weights, self.student_predictor.weights):\n",
    "            s_t.assign(s_p)\n",
    "        \n",
    "        self.probe = False\n",
    "        self.apply_simclr = apply_simclr\n",
    "        self.apply_barlow = apply_barlow\n",
    "        self.loss_tracker = keras.metrics.Mean(name = \"Total_loss\")\n",
    "        self.dino_tracker = keras.metrics.Mean(name = f\"{self.train_type}_loss\")\n",
    "        self.simclr_tracker = keras.metrics.Mean(name = \"SimCLR_loss\")\n",
    "        self.barlow_tracker = keras.metrics.Mean(name = \"Barlow_loss\")\n",
    "    def get_config(self):\n",
    "        return {\"feature_extractor_name\" : self.feature_extractor.name,\n",
    "               \"embed_dims\" : self.embed_dims,\n",
    "                \"Multiview(>2)\" : self.multiview,\n",
    "               \"SSL_method\" : \"DINOv1\",\n",
    "               \"Linear Probe\" : self.probe,\n",
    "               \"N_Categories\" : self.probe_categories if self.probe else 0,\n",
    "               \"Probe Activation\" : self.probe_act if self.probe else \"NA\",\n",
    "               \"Apply SimCLR\" : self.apply_simclr,\n",
    "               \"Apply Barlow\" : self.apply_barlow}\n",
    "    def compile(self, optimizer, probe_optimizer = None, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "        self.optimizer = optimizer\n",
    "        if self.probe:\n",
    "            self.probe_optimizer = probe_optimizer \n",
    "    def compute_h(self, t, s):\n",
    "        # t, s : embedding vector of teacher/student network (feature extractor + MLPs)\n",
    "        # C: centering coefficient, updated as EMA (teacher output의 평균)\n",
    "        c = self.c\n",
    "        t = tf.stop_gradient(t)\n",
    "        s = ops.softmax(s/self.student_t, \n",
    "                        axis = -1)\n",
    "        t = ops.softmax(((t - c)/self.teacher_t),\n",
    "                        axis = -1)\n",
    "        loss = -ops.mean(t*ops.log(s + 1e-5))\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    def train_step(self, dataset):\n",
    "        view_indices = list(range(len(dataset)))\n",
    "        if self.probe:\n",
    "            global_view, local_view, labels = dataset\n",
    "            n_global, n_local = 1, 1\n",
    "            total_view = ops.concatenate(dataset, axis = 0)\n",
    "        else:\n",
    "            if self.multiview:\n",
    "                global_view = dataset[0:2]\n",
    "                \n",
    "                n_global = len(global_view)\n",
    "                n_local = len(dataset) - n_global\n",
    "                \n",
    "                global_view = ops.concatenate(global_view, axis = 0)\n",
    "                total_view = ops.concatenate(dataset, axis = 0) #global_1, global_2, locals\n",
    "            else:\n",
    "                global_view, local_view = dataset #x1, x2\n",
    "                n_global, n_local = 1, 1\n",
    "                total_view = ops.concatenate(dataset, axis = 0)\n",
    "        # teacher -> global views, student -> global and local views\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            if self.multiview:\n",
    "                if len(self.feature_extractor.outputs) == 2:\n",
    "                    z_global_teacher, global_weight_teacher = self.feature_extractor(global_view, training = True)\n",
    "                    z_total_student, total_weight_student = self.student_extractor(total_view, training = True)\n",
    "\n",
    "                elif len(self.feature_extractor.outputs) == 1:\n",
    "                    z_global_teacher = self.feature_extractor(global_view, training = True)\n",
    "                    z_total_student = self.student_extractor(total_view, training = True)\n",
    "                    \n",
    "                v_teacher = self.teacher_predictor(z_global_teacher)\n",
    "                v_student = self.student_predictor(z_total_student)\n",
    "                v_teacher_total = v_teacher\n",
    "            else:\n",
    "                if len(self.feature_extractor.outputs) == 2:\n",
    "                    z_total_teacher, _ = self.feature_extractor(total_view, training = True)\n",
    "                    z_total_student, _ = self.student_extractor(total_view, training = True)\n",
    "\n",
    "                elif len(self.feature_extractor.outputs) == 1:\n",
    "                    z_total_teacher = self.feature_extractor(total_view, training = True)\n",
    "                    z_total_student = self.student_extractor(total_view, training = True)\n",
    "                \n",
    "                v_teacher_total = self.teacher_predictor(z_total_teacher) ; del z_total_teacher, _\n",
    "                v_student_total = self.student_predictor(z_total_student) ; del z_total_student\n",
    "                \n",
    "                v_teacher_1, v_teacher_2 = ops.split(v_teacher_total, 2, 0)\n",
    "                v_student_1, v_student_2 = ops.split(v_student_total, 2, 0) ; del v_student_total\n",
    "\n",
    "\n",
    "            if self.multiview:\n",
    "                dino_loss = []\n",
    "                simclr_loss = []\n",
    "                barlow_loss = []\n",
    "                #start from here    \n",
    "                teacher_set = ops.split(v_teacher, n_global, 0)\n",
    "                student_set = ops.split(v_student, (n_global + n_local), 0)\n",
    "                teacher_indices = list(range(n_global))\n",
    "                \n",
    "                for idx in teacher_indices: #0, 1\n",
    "                    t = teacher_set[idx]\n",
    "                    view_indices.remove(idx)\n",
    "                    for j in view_indices: #(0), 1, 2, 3\n",
    "                        s = student_set[j]\n",
    "                        loss_ = self.compute_h(t, s) ; loss_ = ops.clip(loss_, -10.0, 10**4)\n",
    "                        dino_loss.append(loss_)\n",
    "                        simclr_loss_ = keras_cv.losses.SimCLRLoss(temperature = self.teacher_t, \n",
    "                                                                  name = \"simclr_loss_compute\")(tf.stop_gradient(t),\n",
    "                                                                                                s)\n",
    "                        simclr_loss.append(simclr_loss_)\n",
    "                        b_loss_ = BarlowLoss()(tf.stop_gradient(t), \n",
    "                                               s)\n",
    "                        barlow_loss.append(b_loss_)\n",
    "                    view_indices.append(idx)\n",
    "                dino_loss = ops.mean(dino_loss)\n",
    "                simclr_loss = ops.mean(simclr_loss)\n",
    "                barlow_loss = ops.mean(barlow_loss)\n",
    "                loss = dino_loss + (self.apply_simclr * simclr_loss) + (self.apply_barlow * barlow_loss)\n",
    "            else:\n",
    "                dino_loss = 0.5*(self.compute_h(v_teacher_1, v_student_2) + self.compute_h(v_teacher_2, v_student_1))\n",
    "                simclr_loss = 0.5*(keras_cv.losses.SimCLRLoss(temperature = self.teacher_t, name = \"simclr_loss_compute\")(tf.stop_gradient(v_teacher_1), v_student_2) + keras_cv.losses.SimCLRLoss(temperature = self.teacher_t, name = \"simclr_loss_compute\")(tf.stop_gradient(v_teacher_2), v_student_1))\n",
    "                barlow_loss = 0.5*(BarlowLoss()(tf.stop_gradient(v_teacher_1), v_student_2) + BarlowLoss()(tf.stop_gradient(v_teacher_2), v_student_1))\n",
    "                loss = dino_loss + (self.apply_simclr * simclr_loss) + (self.apply_barlow * barlow_loss)\n",
    "        #Track hyperparams and metrics\n",
    "        mean_val = ops.mean(v_teacher_total, axis = 0)\n",
    "        self.c.assign(0.99*self.c + (1-0.99)*mean_val)\n",
    "        \n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.dino_tracker.update_state(dino_loss)\n",
    "        self.simclr_tracker.update_state(simclr_loss)\n",
    "        self.barlow_tracker.update_state(barlow_loss)\n",
    "        \n",
    "        output_dict = {self.loss_tracker.name : self.loss_tracker.result(),\n",
    "                       self.dino_tracker.name : self.dino_tracker.result(),\n",
    "                       self.simclr_tracker.name : self.simclr_tracker.result(),\n",
    "                       self.barlow_tracker.name : self.barlow_tracker.result(),\n",
    "\n",
    "                      }\n",
    "        \n",
    "        #Update student params as backprop\n",
    "        student_params = self.student_extractor.trainable_variables + self.student_predictor.trainable_variables\n",
    "        grads = tape.gradient(loss, student_params)\n",
    "        self.optimizer.apply_gradients(zip(grads, student_params))\n",
    "        #Update teacher params as EMA\n",
    "        #lambda = 0.999\n",
    "        teacher_feature_w, teacher_predictor_w = [],[]\n",
    "        l_ = 0.999\n",
    "        #print(\"teacher EMA\")\n",
    "        for f_teacher_part, f_student_part in zip(self.feature_extractor.weights, self.student_extractor.weights):\n",
    "            f_teacher_part.assign(l_*f_teacher_part + (1-l_)*f_student_part)\n",
    "        #self.feature_extractor.set_weights(teacher_feature_w)\n",
    "        \n",
    "        #print(\"predictor EMA\")\n",
    "        for p_teacher_part, p_student_part in zip(self.teacher_predictor.weights, self.student_predictor.weights):\n",
    "            p_teacher_part.assign(l_*p_teacher_part + (1-l_)*p_student_part)\n",
    "        #self.teacher_predictor.set_weights(teacher_predictor_w)\n",
    "        \n",
    "        return output_dict\n",
    "    \n",
    "    def test_step(self, dataset):\n",
    "        view_indices = list(range(len(dataset)))\n",
    "        if self.probe:\n",
    "            global_view, local_view, labels = dataset\n",
    "            n_global, n_local = 1, 1\n",
    "            total_view = ops.concatenate(dataset, axis = 0)\n",
    "        else:\n",
    "            if self.multiview:\n",
    "                global_view = dataset[0:2]\n",
    "                \n",
    "                n_global = len(global_view)\n",
    "                n_local = len(dataset) - n_global\n",
    "                \n",
    "                global_view = ops.concatenate(global_view, axis = 0)\n",
    "                total_view = ops.concatenate(dataset, axis = 0) #global_1, global_2, locals\n",
    "            else:\n",
    "                global_view, local_view = dataset\n",
    "                n_global, n_local = 1, 1\n",
    "                total_view = ops.concatenate(dataset, axis = 0)\n",
    "        # teacher -> global views, student -> global and local views\n",
    "        if self.multiview:\n",
    "            if len(self.feature_extractor.outputs) == 2:\n",
    "                z_global_teacher, global_weight_teacher = self.feature_extractor(global_view, training = True)\n",
    "                z_total_student, total_weight_student = self.student_extractor(total_view, training = True)\n",
    "\n",
    "            elif len(self.feature_extractor.outputs) == 1:\n",
    "                z_global_teacher = self.feature_extractor(global_view, training = True)\n",
    "                z_total_student = self.student_extractor(total_view, training = True)\n",
    "                    \n",
    "            v_teacher = self.teacher_predictor(z_global_teacher)\n",
    "            v_student = self.student_predictor(z_total_student)\n",
    "            \n",
    "        else:\n",
    "            if len(self.feature_extractor.outputs) == 2:\n",
    "                z_total_teacher, _ = self.feature_extractor(total_view, training = True)\n",
    "                z_total_student, _ = self.student_extractor(total_view, training = True)\n",
    "\n",
    "            elif len(self.feature_extractor.outputs) == 1:\n",
    "                z_total_teacher = self.feature_extractor(total_view, training = True)\n",
    "                z_total_student = self.student_extractor(total_view, training = True)\n",
    "                \n",
    "            v_teacher_total = self.teacher_predictor(z_total_teacher) ; del z_total_teacher, _\n",
    "            v_student_total = self.student_predictor(z_total_student) ; del z_total_student\n",
    "                \n",
    "            v_teacher_1, v_teacher_2 = ops.split(v_teacher_total, 2, 0) ; del v_teacher_total\n",
    "            v_student_1, v_student_2 = ops.split(v_student_total, 2, 0) ; del v_student_total\n",
    "        \n",
    "        if self.multiview:\n",
    "            #start from here \n",
    "            dino_loss = []\n",
    "            simclr_loss = []\n",
    "            barlow_loss = []\n",
    "            teacher_set = ops.split(v_teacher, n_global, 0)\n",
    "            student_set = ops.split(v_student, (n_global + n_local), 0)\n",
    "            teacher_indices = list(range(n_global))\n",
    "                \n",
    "            for idx in teacher_indices:\n",
    "                t = teacher_set[idx]\n",
    "                view_indices.remove(idx)\n",
    "                for j in view_indices:\n",
    "                    s = student_set[j]\n",
    "                    loss_ = self.compute_h(t, s) ; loss_ = ops.clip(loss_, -10.0, 10**4)\n",
    "                    dino_loss.append(loss_)\n",
    "                    simclr_loss_ = keras_cv.losses.SimCLRLoss(temperature = self.teacher_t, name = \"simclr_loss_compute\")(t,s)\n",
    "                    simclr_loss.append(simclr_loss_)\n",
    "                    b_loss_ = BarlowLoss()(t,s)\n",
    "                    barlow_loss.append(b_loss_)\n",
    "                view_indices.append(idx)\n",
    "            dino_loss = ops.mean(dino_loss)\n",
    "            simclr_loss = ops.mean(simclr_loss)\n",
    "            barlow_loss = ops.mean(barlow_loss)\n",
    "            loss = dino_loss + (self.apply_simclr * simclr_loss) + (self.apply_barlow * barlow_loss)\n",
    "        else:\n",
    "            dino_loss = 0.5*(self.compute_h(v_teacher_1, v_student_2) + self.compute_h(v_teacher_2, v_student_1))\n",
    "            simclr_loss = 0.5*(keras_cv.losses.SimCLRLoss(temperature = self.teacher_t, name = \"simclr_loss_compute\")(tf.stop_gradient(v_teacher_1), v_student_2) + keras_cv.losses.SimCLRLoss(temperature = self.teacher_t, name = \"simclr_loss_compute\")(tf.stop_gradient(v_teacher_2), v_student_1))\n",
    "            barlow_loss = 0.5*(BarlowLoss()(tf.stop_gradient(v_teacher_1), v_student_2) + BarlowLoss()(tf.stop_gradient(v_teacher_2), v_student_1))\n",
    "            loss = dino_loss + (self.apply_simclr * simclr_loss) + (self.apply_barlow * barlow_loss)\n",
    "        #Track hyperparams and metrics\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.dino_tracker.update_state(dino_loss)\n",
    "        self.simclr_tracker.update_state(simclr_loss)\n",
    "        self.barlow_tracker.update_state(barlow_loss)\n",
    "        \n",
    "        output_dict = {self.loss_tracker.name : self.loss_tracker.result(),\n",
    "                       self.dino_tracker.name : self.dino_tracker.result(),\n",
    "                       self.simclr_tracker.name : self.simclr_tracker.result(),\n",
    "                       self.barlow_tracker.name : self.barlow_tracker.result(),\n",
    "                      }\n",
    "        \n",
    "        return output_dict\n",
    "    \n",
    "    def call(self, dataset):\n",
    "        return self.test_step(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c30de57",
   "metadata": {
    "papermill": {
     "duration": 0.025665,
     "end_time": "2024-07-05T17:51:23.923547",
     "exception": false,
     "start_time": "2024-07-05T17:51:23.897882",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "> iBOT 및 DINOv2\n",
    "\n",
    "- iBOT architecture: MIM, online tokenizing!\n",
    "![](https://velog.velcdn.com/images%2Frucola-pizza%2Fpost%2F4dc0785f-7072-4d03-8374-5bbccd8391b4%2F%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202022-03-10%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%202.01.16.png)\n",
    "\n",
    "- DINOv2:\n",
    "    - iBOT + Sinkhorn-knopp in teacher softmax + untying projection heads (student, teacher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "256de8b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T17:51:23.977104Z",
     "iopub.status.busy": "2024-07-05T17:51:23.976745Z",
     "iopub.status.idle": "2024-07-05T17:51:23.994049Z",
     "shell.execute_reply": "2024-07-05T17:51:23.993190Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.046608,
     "end_time": "2024-07-05T17:51:23.996007",
     "exception": false,
     "start_time": "2024-07-05T17:51:23.949399",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sinkhorn(feature_vector_logits):\n",
    "    \"\"\"\n",
    "    Applies the Sinkhorn-Knopp algorithm to normalize feature vector logits.\n",
    "    \n",
    "    Args:\n",
    "    - feature_vector_logits (tf.Tensor): The input logits to be normalized.\n",
    "    \n",
    "    Returns:\n",
    "    - tf.Tensor: The doubly stochastic matrix.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Exponentiate the logits\n",
    "    Q = tf.transpose(tf.exp(feature_vector_logits))\n",
    "    \n",
    "    # Normalize the entire matrix\n",
    "    Q /= tf.reduce_sum(Q)\n",
    "    \n",
    "    # Get dimensions K (number of rows) and B (number of columns)\n",
    "    K = tf.shape(Q)[0]\n",
    "    B = tf.shape(Q)[1]\n",
    "    \n",
    "    # Initialize u, r, and c\n",
    "    u = tf.zeros(K, dtype=tf.float32)\n",
    "    r = tf.ones(K, dtype=tf.float32) / (tf.cast(K, tf.float32) + 1e-7)\n",
    "    c = tf.ones(B, dtype=tf.float32) / (tf.cast(B, tf.float32) + 1e-7)\n",
    "    \n",
    "    # Sinkhorn iterations\n",
    "    for _ in range(3):\n",
    "        u = tf.reduce_sum(Q, axis=1)\n",
    "        Q *= tf.expand_dims((r / u), axis=1)\n",
    "        Q *= tf.expand_dims(c / tf.reduce_sum(Q, axis=0), 0)\n",
    "    \n",
    "    # Final normalization\n",
    "    final_quantity = Q / (tf.reduce_sum(Q, axis=0, keepdims=True) + 1e-7)\n",
    "    final_quantity = tf.transpose(final_quantity)\n",
    "    \n",
    "    return final_quantity\n",
    "\n",
    "def compute_h(s, t,\n",
    "              c = 0.0, t_t = 0.05, t_s = 0.05, teacher_softmax = True):\n",
    "    t = tf.stop_gradient(t)\n",
    "    s = ops.softmax(s/t_s , axis = -1)\n",
    "    if teacher_softmax:\n",
    "        t = ops.softmax((  (t - c)/t_t  ),\n",
    "                            axis = -1)\n",
    "    else:\n",
    "        t = sinkhorn((t - c)/t_t)\n",
    "    return -ops.sum(t*ops.log(s + 1e-6) , axis = -1)\n",
    "\n",
    "def compute_cls_loss(cls_a, cls_b, c, t_t, t_s, teacher_softmax):\n",
    "    cls_loss = compute_h(cls_a, cls_b, teacher_softmax = teacher_softmax,\n",
    "                            c = c, t_t = t_t, t_s = t_s)\n",
    "    return cls_loss\n",
    "def compute_mim_loss(m_u, patch_u_s, patch_u_t, c, t_t, t_s, teacher_softmax):\n",
    "    mim_loss_u = m_u * compute_h(patch_u_s, patch_u_t, teacher_softmax=teacher_softmax,\n",
    "                                 c=c, t_t=t_t, t_s=t_s)\n",
    "    # 마스킹된 위치의 손실을 유효한 위치에서만 계산\n",
    "    valid_positions = tf.reduce_sum(m_u, axis=1)\n",
    "    valid_mask = tf.cast(valid_positions > 0, tf.float32)\n",
    "    mim_loss_u = tf.reduce_sum(mim_loss_u, axis=1) / (valid_positions + 1e-6)\n",
    "    return tf.reduce_mean(mim_loss_u * valid_mask)\n",
    "\n",
    "\n",
    "def compute_iBOT_loss(cls_v_student, cls_v_teacher,\n",
    "                      cls_u_student, cls_u_teacher,\n",
    "                      c_cls, t_t_cls, t_s_cls,\n",
    "                      \n",
    "                      m_u, patch_u_s, patch_u_t,\n",
    "                      m_v, patch_v_s, patch_v_t,\n",
    "                      c_mim, t_t_mim, t_s_mim,\n",
    "                      teacher_softmax):\n",
    "    \"\"\"Compute one-pair cls, mim loss\"\"\"\n",
    "    cls_v_loss = compute_cls_loss(cls_a = cls_v_student, cls_b = cls_v_teacher, \n",
    "                                  c = c_cls, t_t = t_t_cls, t_s = t_s_cls, teacher_softmax = teacher_softmax)\n",
    "    cls_u_loss = compute_cls_loss(cls_a = cls_u_student, cls_b = cls_u_teacher, \n",
    "                                  c = c_cls, t_t = t_t_cls, t_s = t_s_cls, teacher_softmax = teacher_softmax)\n",
    "    cls_loss = ops.mean(cls_v_loss + cls_u_loss)\n",
    "    \n",
    "    mim_u_loss = compute_mim_loss(m_u, patch_u_s, patch_u_t, c_mim, t_t_mim, t_s_mim, teacher_softmax)\n",
    "    mim_v_loss = compute_mim_loss(m_v, patch_v_s, patch_v_t, c_mim, t_t_mim, t_s_mim, teacher_softmax)\n",
    "        \n",
    "    mim_loss = ops.mean(mim_u_loss + mim_v_loss)\n",
    "    return cls_loss, mim_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b6548f",
   "metadata": {
    "papermill": {
     "duration": 0.025219,
     "end_time": "2024-07-05T17:51:24.047108",
     "exception": false,
     "start_time": "2024-07-05T17:51:24.021889",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "> iBOT loss의 경우,\n",
    "\n",
    "- 원본 논문에서 아래와 같은 비교 실험을 진행하였고,\n",
    "- 그 결과 case b가 가장 성능이 높았음\n",
    "    - $x$ : global view\n",
    "    - $y$ : local view\n",
    "    - mask generation : generate_mask function 이용, m_i , masked patches return\n",
    "    - image patching & embedding : ImagePatchEmbedding layer 이용 (att: patch_size, embed_dim)\n",
    "    \n",
    "![](https://ar5iv.labs.arxiv.org/html/2111.07832/assets/x9.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a49e1c9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T17:51:24.100107Z",
     "iopub.status.busy": "2024-07-05T17:51:24.099753Z",
     "iopub.status.idle": "2024-07-05T17:51:24.115136Z",
     "shell.execute_reply": "2024-07-05T17:51:24.114240Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.044407,
     "end_time": "2024-07-05T17:51:24.116987",
     "exception": false,
     "start_time": "2024-07-05T17:51:24.072580",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ibot_brainstorming =0\n",
    "if ibot_brainstorming : \n",
    "    #generate_mask 이용 -> m_i, masked patches\n",
    "    batch_size_ = 16\n",
    "    embed_dims_ = 768\n",
    "    n_patches_ = 256\n",
    "    mask_rate_ = tf.random.uniform(shape = (), minval = 0.5, maxval = 0.8, seed = 1)\n",
    "    mask_token = tf.random.normal([embed_dims_])\n",
    "    masklayer = MaskLayer(masking_rate=0.5, update_value=update_value)\n",
    "    # teacher -> global image only (2)\n",
    "    # student -> global and local views.\n",
    "\n",
    "    teacher_output_1 = [tf.random.normal([batch_size_, embed_dims_]), \n",
    "                     tf.ones([batch_size_, n_patches_, embed_dims_], dtype = tf.float32)]\n",
    "\n",
    "    teacher_output_2 = [tf.random.normal([batch_size_, embed_dims_]), \n",
    "                     tf.ones([batch_size_, n_patches_, embed_dims_], dtype = tf.float32)]\n",
    "\n",
    "    student_output_1, student_output_2, student_output_3, student_output_4 = tf.random.normal([batch_size_, n_patches_, embed_dims_]), tf.random.normal([batch_size_, n_patches_, embed_dims_]), tf.random.normal([batch_size_, n_patches_, embed_dims_]), tf.random.normal([batch_size_, n_patches_, embed_dims_])\n",
    "    m_s1, patch_s1 = MaskLayer(masking_rate=0.5, update_value=update_value)(student_output_1)\n",
    "    m_s2, patch_s2 = MaskLayer(masking_rate=0.5, update_value=update_value)(student_output_2)\n",
    "    m_s3, patch_s3 = MaskLayer(masking_rate=0.5, update_value=update_value)(student_output_3)\n",
    "    m_s4, patch_s4 = MaskLayer(masking_rate=0.5, update_value=update_value)(student_output_4)\n",
    "\n",
    "    ######compute loss####\n",
    "    mim_loss_a = compute_mim_loss(m_s1, patch_s1, teacher_output_2[-1], c = 0.5, t_t = 0.1, t_s = 0.1, teacher_softmax = True)\n",
    "    mim_loss_b = compute_mim_loss(m_s2, patch_s2, teacher_output_1[-1], c = 0.5, t_t = 0.1, t_s = 0.1, teacher_softmax = True)\n",
    "    mim_loss = ops.mean(mim_loss_a + mim_loss_b)\n",
    "\n",
    "    pool = keras.layers.GlobalAveragePooling1D()\n",
    "    cls_s1, cls_s2, cls_s3, cls_s4 = pool(patch_s1), pool(patch_s2), pool(patch_s3), pool(patch_s4)\n",
    "    cls_t1, cls_t2 = teacher_output_1[0], teacher_output_2[0]\n",
    "\n",
    "    cls_loss = []\n",
    "    cls_loss.append(compute_cls_loss(cls_s2, cls_t1, c = 0.5, t_t = 0.1, t_s = 0.1, teacher_softmax = True))\n",
    "    cls_loss.append(compute_cls_loss(cls_s3, cls_t1, c = 0.5, t_t = 0.1, t_s = 0.1, teacher_softmax = True))\n",
    "    cls_loss.append(compute_cls_loss(cls_s4, cls_t1, c = 0.5, t_t = 0.1, t_s = 0.1, teacher_softmax = True))\n",
    "    cls_loss.append(compute_cls_loss(cls_s1, cls_t1, c = 0.5, t_t = 0.1, t_s = 0.1, teacher_softmax = True))\n",
    "    cls_loss.append(compute_cls_loss(cls_s3, cls_t1, c = 0.5, t_t = 0.1, t_s = 0.1, teacher_softmax = True))\n",
    "    cls_loss.append(compute_cls_loss(cls_s4, cls_t1, c = 0.5, t_t = 0.1, t_s = 0.1, teacher_softmax = True))\n",
    "    cls_loss = ops.mean(cls_loss)\n",
    "    loss = mim_loss + cls_loss\n",
    "    print(f\"MIM Loss : {mim_loss}, CLS loss : {cls_loss}, Total loss : {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9400e4b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T17:51:24.171660Z",
     "iopub.status.busy": "2024-07-05T17:51:24.171289Z",
     "iopub.status.idle": "2024-07-05T17:51:24.233919Z",
     "shell.execute_reply": "2024-07-05T17:51:24.232793Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.092701,
     "end_time": "2024-07-05T17:51:24.236065",
     "exception": false,
     "start_time": "2024-07-05T17:51:24.143364",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class iBOT(keras.Model): #g : feature extractor + predictor\n",
    "    def __init__(self, att_depth, att_dims, att_heads, \n",
    "                 patch_size = 16, embed_dims = 1024, multiview = False, probe = False,\n",
    "                 teacher_t = 0.07, student_t = 0.07, apply_simclr = False, grayscale = True,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.grayscale = grayscale\n",
    "        self.att_depth = att_depth\n",
    "        self.att_dims = att_dims\n",
    "        self.att_heads = att_heads\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dims = embed_dims\n",
    "        self.multiview = multiview\n",
    "        self.train_type = 'iBOT'\n",
    "        self.embed_dims = embed_dims\n",
    "        self.probe = False\n",
    "        \n",
    "        # modelling\n",
    "        # image input -> patch embedding -> patch for teacher, masked patch for student -> get rep vector, get embed. patches\n",
    "        \n",
    "        self.patch_embedding_fn = ImagePatchEmbedding(patch_size = self.patch_size, embed_dim = self.att_dims)\n",
    "        self.f_t = TRBlock(att_depth = self.att_depth, att_dims = self.att_dims, att_heads = self.att_heads, name = \"Teacher_Encoder\")\n",
    "        self.f_s = TRBlock(att_depth = self.att_depth, att_dims = self.att_dims, att_heads = self.att_heads, name = \"Student_Encoder\")\n",
    "        self.f_t.set_weights(self.f_s.get_weights())\n",
    "        self.teacher_predictor = keras.Sequential([keras.layers.Dense(units = self.embed_dims,\n",
    "                                                            use_bias = False,\n",
    "                                                            ),\n",
    "                                          keras.layers.Dense(units = self.embed_dims, use_bias = False),\n",
    "                                          keras.layers.Dense(units = self.embed_dims, use_bias = False),\n",
    "                                            ], name = f\"Teacher_{self.train_type}_predictor\")\n",
    "        self.student_predictor = keras.Sequential([keras.layers.Dense(units = self.embed_dims, use_bias = False,),\n",
    "                                          keras.layers.Dense(units = self.embed_dims, use_bias = False),\n",
    "                                          keras.layers.Dense(units = self.embed_dims, use_bias = False),\n",
    "                                          ], name = f\"Student_{self.train_type}_predictor\")\n",
    "        \n",
    "        self.teacher_patch_predictor = keras.Sequential([keras.layers.Dense(units = self.embed_dims,\n",
    "                                                            use_bias = False,\n",
    "                                                            ),\n",
    "                                          keras.layers.Dense(units = self.embed_dims, use_bias = False),\n",
    "                                          keras.layers.Dense(units = self.embed_dims, use_bias = False),\n",
    "                                            ], name = f\"Teacher_Patch_{self.train_type}_predictor\")\n",
    "        self.student_patch_predictor = keras.Sequential([keras.layers.Dense(units = self.embed_dims,\n",
    "                                                            use_bias = False,\n",
    "                                                            ),\n",
    "                                          keras.layers.Dense(units = self.embed_dims, use_bias = False),\n",
    "                                          keras.layers.Dense(units = self.embed_dims, use_bias = False),\n",
    "                                          ], name = f\"Student_Patch_{self.train_type}_predictor\")\n",
    "        \n",
    "        for s_t, s_p in zip(self.teacher_predictor.weights, self.student_predictor.weights):\n",
    "            s_t.assign(s_p)\n",
    "        for s_t, s_p in zip(self.teacher_patch_predictor.weights, self.student_patch_predictor.weights):\n",
    "            s_t.assign(s_p)\n",
    "        # About loss calculation\n",
    "        self.loss_tracker = keras.metrics.Mean(name = \"iBOT_loss\")\n",
    "        self.cls_tracker = keras.metrics.Mean(name = \"CLS_loss\") ; self.mim_tracker = keras.metrics.Mean(name = \"MIM_loss\")\n",
    "        self.apply_simclr = apply_simclr\n",
    "        self.simclr_tracker = keras.metrics.Mean(name = \"SimCLR_loss\")\n",
    "        \n",
    "        self.c_cls = self.add_weight(name='c_cls', shape=(self.embed_dims,), initializer='glorot_uniform', trainable=False)\n",
    "        self.c_mim = self.add_weight(name='c_mim', shape=(self.embed_dims,), initializer='glorot_uniform', trainable=False)\n",
    "        self.mask_token = self.add_weight(name='mask_token', shape=(self.att_dims,), initializer='glorot_uniform', trainable=True)\n",
    "        \n",
    "        \n",
    "        self.teacher_t = teacher_t\n",
    "        self.student_t = student_t\n",
    "    def get_config(self):\n",
    "        return {\"feature_extractor_name\" : f\"ViT_vanilla_depth{self.att_depth}_heads{self.att_heads}_dims{self.att_dims}\",\n",
    "               \"embed_dims\" : self.embed_dims,\n",
    "                \"Multiview(>2)\" : self.multiview,\n",
    "               \"SSL_method\" : \"iBOT\",\n",
    "               \"Linear Probe\" : self.probe,\n",
    "               \"N_Categories\" : self.probe_categories if self.probe else 0,\n",
    "               \"Probe Activation\" : self.probe_act if self.probe else \"NA\",\n",
    "               \"Apply SimCLR\" : bool(self.apply_simclr),\n",
    "               }\n",
    "    def compile(self, optimizer, probe_optimizer = None, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "        self.optimizer = optimizer\n",
    "        if self.probe:\n",
    "            self.probe_optimizer = probe_optimizer \n",
    "\n",
    "        \n",
    "    def train_step(self, dataset):\n",
    "        view_indices = list(range(len(dataset)))\n",
    "        batch_size = ops.shape(dataset[0])[0]\n",
    "        \n",
    "        if self.probe:\n",
    "            global_view, local_view, labels = dataset\n",
    "            n_global, n_local = 1, 1\n",
    "            total_view = ops.concatenate(dataset, axis = 0)\n",
    "        else:\n",
    "            if self.multiview:\n",
    "                n_global = 2\n",
    "                n_local = len(dataset) - n_global\n",
    "                \n",
    "                #global_view = ops.concatenate(global_view, axis = 0)\n",
    "                total_view = ops.concatenate(dataset, axis = 0) #global_1, global_2, locals\n",
    "            else:\n",
    "                global_view, local_view = dataset\n",
    "                n_global, n_local = 1, 1\n",
    "                total_view = ops.concatenate(dataset, axis = 0)\n",
    "        # teacher -> global views, student -> global and local views\n",
    "        teacher_indices = list(range(n_global))\n",
    "        with tf.GradientTape() as tape:\n",
    "            total_view = self.patch_embedding_fn(total_view)\n",
    "            global_view = total_view[:n_global*batch_size, ...] #2B, patch_length, dims\n",
    "            local_view = total_view[n_global*batch_size:, ...] #N, patch_length, dims\n",
    "            m_set, global_view_masked = MaskLayer(masking_rate= 0.6, \n",
    "                                                  update_value = self.mask_token)(global_view)\n",
    "            \n",
    "            global_z_t, global_patches_t, att_weights_t = self.f_t([global_view, global_view])\n",
    "            global_z_masked_s, global_patches_masked_s, _ = self.f_s([global_view_masked, global_view_masked])\n",
    "            local_z_s, local_patches_s, _ = self.f_s([local_view, local_view])\n",
    "            del _, local_patches_s\n",
    "            \n",
    "            global_z_t, global_patches_t = self.teacher_predictor(global_z_t), self.teacher_patch_predictor(global_patches_t)\n",
    "            global_z_masked_s, global_patches_masked_s = self.student_predictor(global_z_masked_s), self.student_patch_predictor(global_patches_masked_s)\n",
    "            local_z_s = self.student_predictor(local_z_s)\n",
    "            \n",
    "            teacher_cls_set = ops.split(global_z_t, n_global, 0) ; del global_z_t\n",
    "            teacher_patches_set = ops.split(global_patches_t, n_global, 0) ; del global_patches_t\n",
    "                \n",
    "            student_global_cls_set = ops.split(global_z_masked_s, n_global, 0) ; del global_z_masked_s\n",
    "            student_global_patches_set = ops.split(global_patches_masked_s, n_global, 0); del global_patches_masked_s\n",
    "            m_set = ops.split(m_set, n_global, 0)\n",
    "            student_local_cls_set = ops.split(local_z_s, n_local, 0) ; del local_z_s\n",
    "            \n",
    "            student_cls_set = student_global_cls_set\n",
    "            student_cls_set.extend(student_local_cls_set)\n",
    "            \n",
    "            ##PAIRWISE LOSS CALCUATION##\n",
    "            total_loss = []\n",
    "            mim_loss = []\n",
    "            cls_loss = []\n",
    "            simclr_loss = []\n",
    "                \n",
    "                #teacher-student combination\n",
    "                #global-global : mim, cls loss\n",
    "                #global-local : cls loss\n",
    "                # cls loss = DINO and SimCLR loss\n",
    "                \n",
    "                #1. patch-patch mim loss\n",
    "            for m_, patch_u_s, patch_u_t in zip(m_set, student_global_patches_set, teacher_patches_set):\n",
    "                mim_loss.append(compute_mim_loss(m_, patch_u_s, patch_u_t, self.c_mim, self.teacher_t, self.student_t, teacher_softmax = True))\n",
    "            mim_loss = ops.mean(mim_loss)\n",
    "                #2. cls-cls DINO, simclr loss\n",
    "            for idx in teacher_indices: #0, 1\n",
    "                t = teacher_cls_set[idx]\n",
    "                view_indices.remove(idx)\n",
    "                for j in view_indices: #(0), 1, 2, 3\n",
    "                    s = student_cls_set[j]\n",
    "                    loss_ = compute_cls_loss(s, t, teacher_softmax = True, c = self.c_cls, t_t = self.teacher_t, t_s = self.student_t)\n",
    "                    loss_ = ops.clip(loss_, -10.0, 10**4)\n",
    "                    cls_loss.append(loss_)\n",
    "                    simclr_loss_ = keras_cv.losses.SimCLRLoss(temperature = self.teacher_t, \n",
    "                                                                  name = \"simclr_loss_compute\")(t,s)\n",
    "                    simclr_loss.append(simclr_loss_)\n",
    "                        \n",
    "                view_indices.append(idx)\n",
    "            cls_loss = ops.mean(cls_loss)\n",
    "            simclr_loss = ops.mean(simclr_loss)\n",
    "            loss = cls_loss + mim_loss + (self.apply_simclr * simclr_loss)\n",
    "            \n",
    "        #Track hyperparams and metrics\n",
    "        mean_cls = ops.mean(ops.concatenate(teacher_cls_set, axis = 0), \n",
    "                            axis = 0)\n",
    "        mean_patch = ops.mean(ops.concatenate(teacher_patches_set, axis = 0), \n",
    "                              axis = (0,1))\n",
    "        self.c_cls.assign(0.99*self.c_cls + (1-0.99)*mean_cls)\n",
    "        self.c_mim.assign(0.99*self.c_mim + (1-0.99)*mean_patch)\n",
    "        \n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.cls_tracker.update_state(cls_loss)\n",
    "        self.mim_tracker.update_state(mim_loss)\n",
    "        self.simclr_tracker.update_state(simclr_loss)\n",
    "        \n",
    "        output_dict = {self.loss_tracker.name : self.loss_tracker.result(),\n",
    "                       self.cls_tracker.name : self.cls_tracker.result(),\n",
    "                       self.simclr_tracker.name : self.simclr_tracker.result(),\n",
    "                       self.mim_tracker.name : self.mim_tracker.result(),\n",
    "                      }\n",
    "        \n",
    "        #Update student params as backprop\n",
    "        student_params = self.patch_embedding_fn.trainable_variables + self.f_s.trainable_variables + self.student_predictor.trainable_variables + self.student_patch_predictor.trainable_variables \n",
    "        student_params.append(self.mask_token)\n",
    "        \n",
    "        \n",
    "        grads = tape.gradient(loss, student_params)\n",
    "        self.optimizer.apply_gradients(zip(grads, student_params))\n",
    "        #Update teacher params as EMA\n",
    "        #lambda = 0.999\n",
    "        teacher_feature_w, teacher_predictor_w = [],[]\n",
    "        l_ = 0.999\n",
    "        #print(\"teacher EMA\")\n",
    "        for f_teacher_part, f_student_part in zip(self.f_t.weights, self.f_s.weights):\n",
    "            f_teacher_part.assign(l_*f_teacher_part + (1-l_)*f_student_part)\n",
    "        #self.feature_extractor.set_weights(teacher_feature_w)\n",
    "        \n",
    "        #print(\"predictor EMA\")\n",
    "        for p_teacher_part, p_student_part in zip(self.teacher_predictor.weights, self.student_predictor.weights):\n",
    "            p_teacher_part.assign(l_*p_teacher_part + (1-l_)*p_student_part)\n",
    "        #self.teacher_predictor.set_weights(teacher_predictor_w)\n",
    "        for p_teacher_part, p_student_part in zip(self.teacher_patch_predictor.weights, self.student_patch_predictor.weights):\n",
    "            p_teacher_part.assign(l_*p_teacher_part + (1-l_)*p_student_part)\n",
    "        return output_dict\n",
    "    \n",
    "    def test_step(self, dataset):\n",
    "        view_indices = list(range(len(dataset)))\n",
    "        batch_size = ops.shape(dataset[0])[0]\n",
    "        \n",
    "        if self.probe:\n",
    "            global_view, local_view, labels = dataset\n",
    "            n_global, n_local = 1, 1\n",
    "            total_view = ops.concatenate(dataset, axis = 0)\n",
    "        else:\n",
    "            if self.multiview:\n",
    "                n_global = 2\n",
    "                n_local = len(dataset) - n_global\n",
    "                \n",
    "                #global_view = ops.concatenate(global_view, axis = 0)\n",
    "                total_view = ops.concatenate(dataset, axis = 0) #global_1, global_2, locals\n",
    "            else:\n",
    "                global_view, local_view = dataset\n",
    "                n_global, n_local = 1, 1\n",
    "                total_view = ops.concatenate(dataset, axis = 0)\n",
    "        # teacher -> global views, student -> global and local views\n",
    "        teacher_indices = list(range(n_global))\n",
    "        if True:\n",
    "            total_view = self.patch_embedding_fn(total_view)\n",
    "            print(\"total view shape : \",ops.shape(total_view))\n",
    "            global_view = total_view[:n_global*batch_size, ...] #2B, patch_length, dims\n",
    "            local_view = total_view[n_global*batch_size:, ...] #N, patch_length, dims\n",
    "            m_set, global_view_masked = MaskLayer(masking_rate = 0.6, \n",
    "                                                  update_value = self.mask_token)(global_view)\n",
    "            #m_set, global_view_masked = generate_mask(global_view, \n",
    "            #                                          tf.random.uniform(shape = (), minval = 0.4, maxval = 0.8), #<- mask probability\n",
    "            #                                          self.mask_token,\n",
    "            #                                         self.batch_size)\n",
    "            \n",
    "            global_z_t, global_patches_t, att_weights_t = self.f_t([global_view, global_view])\n",
    "            global_z_masked_s, global_patches_masked_s, _ = self.f_s([global_view_masked, global_view_masked])\n",
    "            local_z_s, local_patches_s, _ = self.f_s([local_view, local_view])\n",
    "            del _, local_patches_s\n",
    "            \n",
    "            global_z_t, global_patches_t = self.teacher_predictor(global_z_t), self.teacher_patch_predictor(global_patches_t)\n",
    "            global_z_masked_s, global_patches_masked_s = self.student_predictor(global_z_masked_s), self.student_patch_predictor(global_patches_masked_s)\n",
    "            local_z_s = self.student_predictor(local_z_s)\n",
    "            \n",
    "            teacher_cls_set = ops.split(global_z_t, n_global, 0) ; del global_z_t\n",
    "            teacher_patches_set = ops.split(global_patches_t, n_global, 0) ; del global_patches_t\n",
    "                \n",
    "            student_global_cls_set = ops.split(global_z_masked_s, n_global, 0) ; del global_z_masked_s\n",
    "            student_global_patches_set = ops.split(global_patches_masked_s, n_global, 0); del global_patches_masked_s\n",
    "            m_set = ops.split(m_set, n_global, 0)\n",
    "            student_local_cls_set = ops.split(local_z_s, n_local, 0) ; del local_z_s\n",
    "            \n",
    "            student_cls_set = student_global_cls_set\n",
    "            student_cls_set.extend(student_local_cls_set)\n",
    "            \n",
    "            ##PAIRWISE LOSS CALCUATION##\n",
    "            total_loss = []\n",
    "            mim_loss = []\n",
    "            cls_loss = []\n",
    "            simclr_loss = []\n",
    "                \n",
    "                #teacher-student combination\n",
    "                #global-global : mim, cls loss\n",
    "                #global-local : cls loss\n",
    "                # cls loss = DINO and SimCLR loss\n",
    "                \n",
    "                #1. patch-patch mim loss\n",
    "            for m_, patch_u_s, patch_u_t in zip(m_set, student_global_patches_set, teacher_patches_set):\n",
    "                print(ops.shape(m_), ops.shape(patch_u_s), ops.shape(self.c_mim))\n",
    "                mim_loss.append(compute_mim_loss(m_, patch_u_s, patch_u_t, self.c_mim, self.teacher_t, self.student_t, teacher_softmax = True))\n",
    "            mim_loss = ops.mean(mim_loss)\n",
    "                #2. cls-cls DINO, simclr loss\n",
    "            for idx in teacher_indices: #0, 1\n",
    "                t = teacher_cls_set[idx]\n",
    "                view_indices.remove(idx)\n",
    "                for j in view_indices: #(0), 1, 2, 3\n",
    "                    s = student_cls_set[j]\n",
    "                    loss_ = compute_cls_loss(s, t, teacher_softmax = True, c = self.c_cls, t_t = self.teacher_t, t_s = self.student_t)\n",
    "                    loss_ = ops.clip(loss_, -10.0, 10**4)\n",
    "                    cls_loss.append(loss_)\n",
    "                    simclr_loss_ = keras_cv.losses.SimCLRLoss(temperature = self.teacher_t, \n",
    "                                                                  name = \"simclr_loss_compute\")(t,s)\n",
    "                    simclr_loss.append(simclr_loss_)\n",
    "                        \n",
    "                view_indices.append(idx)\n",
    "            cls_loss = ops.mean(cls_loss)\n",
    "            simclr_loss = ops.mean(simclr_loss)\n",
    "            loss = cls_loss + mim_loss + (self.apply_simclr * 0.5 * simclr_loss)\n",
    "            \n",
    "        #Track hyperparams and metrics\n",
    "        mean_cls = ops.mean(ops.concatenate(teacher_cls_set, axis = 0), \n",
    "                            axis = 0)\n",
    "        mean_patch = ops.mean(ops.concatenate(teacher_patches_set, axis = 0), \n",
    "                              axis = (0,1))\n",
    "        self.c_cls.assign(0.99*self.c_cls + (1-0.99)*mean_cls)\n",
    "        self.c_mim.assign(0.99*self.c_mim + (1-0.99)*mean_patch)\n",
    "        \n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.cls_tracker.update_state(cls_loss)\n",
    "        self.mim_tracker.update_state(mim_loss)\n",
    "        self.simclr_tracker.update_state(simclr_loss)\n",
    "        \n",
    "        output_dict = {self.loss_tracker.name : self.loss_tracker.result(),\n",
    "                       self.cls_tracker.name : self.cls_tracker.result(),\n",
    "                       self.simclr_tracker.name : self.simclr_tracker.result(),\n",
    "                       self.mim_tracker.name : self.mim_tracker.result(),\n",
    "                      }\n",
    "        \n",
    "        return output_dict\n",
    "    \n",
    "    def call(self, dataset):\n",
    "        return self.test_step(dataset)\n",
    "    def get_full_model(self, res):\n",
    "        if self.grayscale:\n",
    "            inputs =keras.layers.Input([res,res,1], name = \"ImageInput\") \n",
    "        else:\n",
    "            inputs = keras.layers.Input([res,res,3], name = \"ImageInput\")\n",
    "        patches = self.patch_embedding_fn(inputs)\n",
    "        cls_token, patches, att_weight = self.f_t([patches, patches])\n",
    "        model = Model(inputs, [cls_token, patches, att_weight],\n",
    "                     name = f\"{self.train_type}_ViT\")\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5c5a51dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T17:51:24.290086Z",
     "iopub.status.busy": "2024-07-05T17:51:24.289253Z",
     "iopub.status.idle": "2024-07-05T17:51:24.293793Z",
     "shell.execute_reply": "2024-07-05T17:51:24.292918Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.033459,
     "end_time": "2024-07-05T17:51:24.295696",
     "exception": false,
     "start_time": "2024-07-05T17:51:24.262237",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# simple MIM: https://github.com/taki0112/vit-tensorflow/blob/main/vit_tensorflow/simmim.py 및 ChatGPT history 참고\n",
    "# 위 sample code  살짝 이상, original paper 보고 구현 : https://ar5iv.labs.arxiv.org/html/2111.09886\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf1f5aa",
   "metadata": {
    "papermill": {
     "duration": 0.025618,
     "end_time": "2024-07-05T17:51:24.347175",
     "exception": false,
     "start_time": "2024-07-05T17:51:24.321557",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "------------\n",
    "# MIM, Mixed SSL\n",
    "- MIM:\n",
    "    - SimMIM\n",
    "- inform. based + MIM based + contrastive based + self-distillation?\n",
    "- Unsupervised semantic segmentation + feature-vector SSL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985641a3",
   "metadata": {
    "papermill": {
     "duration": 0.025173,
     "end_time": "2024-07-05T17:51:24.398073",
     "exception": false,
     "start_time": "2024-07-05T17:51:24.372900",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "> SimMIM\n",
    "\n",
    "![](https://kimjy99.github.io/assets/img/simmim/simmim-fig1.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eedecefd",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-07-05T17:51:24.451029Z",
     "iopub.status.busy": "2024-07-05T17:51:24.450412Z",
     "iopub.status.idle": "2024-07-05T17:51:24.463097Z",
     "shell.execute_reply": "2024-07-05T17:51:24.462394Z"
    },
    "papermill": {
     "duration": 0.041591,
     "end_time": "2024-07-05T17:51:24.465067",
     "exception": false,
     "start_time": "2024-07-05T17:51:24.423476",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_masking_fn(grayscale = True, masking_rate = 0.5, update_value = 0.0, patch_size = 32):\n",
    "    \n",
    "    @tf.function\n",
    "    def mask_and_restore(image, \n",
    "                         masking_rate=masking_rate, update_value = update_value, patch_size = patch_size):\n",
    "        image = ops.cast(image, \"float32\") ; original_image = image\n",
    "        c = ops.shape(image)[-1] ; res = ops.shape(image)[-2]\n",
    "        \n",
    "        #random crop\n",
    "        if tf.random.uniform(shape = (), minval = 1, maxval = 11, dtype = \"int32\") <= 5:\n",
    "            rate = tf.random.uniform(shape = (), minval = 0.5, maxval = 0.91, dtype = \"float32\")\n",
    "            target_size = int(rate * tf.cast(res, \"float32\"))\n",
    "            bs = ops.shape(image)[0]\n",
    "            \n",
    "            image = tf.image.random_crop(image, size = (bs, target_size, target_size, c))\n",
    "            image = tf.image.resize(image, [res, res], antialias = True)\n",
    "        ##\n",
    "        \n",
    "        patches = keras.ops.image.extract_patches(image, size = patch_size, padding = 'same')\n",
    "        _, w_, h_, dims = ops.shape(patches)\n",
    "        patches = keras.layers.Reshape([w_*h_, dims])(patches)\n",
    "\n",
    "        update_token = keras.ops.zeros(shape = (dims,), dtype = \"float32\")\n",
    "        sequence, masked_patches = MaskLayer(masking_rate=masking_rate, \n",
    "                                             update_value=update_token)(patches)\n",
    "        masked_patches = ops.reshape(masked_patches, [-1, res//patch_size, res//patch_size, \n",
    "                                                      patch_size, patch_size, c])\n",
    "        masked_patches = ops.transpose(masked_patches, [0,1,3,2,4,5])\n",
    "        masked_patches = ops.reshape(masked_patches, [-1, res, res, c])\n",
    "        \n",
    "        #if grayscale:\n",
    "        #    try:\n",
    "        #        masked_patches = tf.image.rgb_to_grayscale(masked_patches)\n",
    "        #        image = tf.image.rgb_to_grayscale(image)\n",
    "        #    except:\n",
    "        #        pass\n",
    "        #else:\n",
    "        #    try:\n",
    "        #        masked_patches = tf.image.grayscale_to_rgb(masked_patches)\n",
    "        #        image = tf.image.grayscale_to_rgb(image)\n",
    "        #    except:\n",
    "        #        pass\n",
    "        \n",
    "        masked_patches = ops.cast(masked_patches, \"uint8\")\n",
    "        image = ops.cast(image, \"uint8\")\n",
    "        return (sequence, masked_patches, image, original_image)\n",
    "    return mask_and_restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "852dd7ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T17:51:24.518861Z",
     "iopub.status.busy": "2024-07-05T17:51:24.518493Z",
     "iopub.status.idle": "2024-07-05T17:51:24.538219Z",
     "shell.execute_reply": "2024-07-05T17:51:24.537287Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.048958,
     "end_time": "2024-07-05T17:51:24.540120",
     "exception": false,
     "start_time": "2024-07-05T17:51:24.491162",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#sim MIM:\n",
    "#masked image -> patchwise encoder -> original image와 patch별 L1 loss 구하고  mask sequence로 가중합\n",
    "# Loss의 argument로 들어가는 원소 : [batch, n_patches, patch_size, patch_size, 1 or 3]\n",
    "class SimMIM(keras.Model):\n",
    "    def __init__(self, feature_extractor, grayscale, patch_size,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.patch_size = patch_size\n",
    "        self.feature_extractor = feature_extractor #weight sharing for VICreg, Barlow Twins\n",
    "        self.grayscale = grayscale\n",
    "        if grayscale:\n",
    "            self.projector = Dense(units = patch_size**2 , activation = \"sigmoid\", name = \"Grayscale_Regressor\")\n",
    "        else:\n",
    "            self.projector = Dense(units = 3*(patch_size**2) , activation = \"sigmoid\", name = \"RGB_Regressor\")\n",
    "        \n",
    "        self.loss_tracker = keras.metrics.Mean(\"SimMIM_Regression_loss_tracker\")\n",
    "        self.reg_fn = keras.losses.MeanAbsoluteError(reduction = None)\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {\"feature_extractor_name\" : self.feature_extractor.name,\n",
    "               \"SSL_method\" : \"SimMIM\",\n",
    "                \"Grayscale\" : self.grayscale,\n",
    "               }\n",
    "    \n",
    "    def compute_loss(self, original_patches, predicted_patches, mask_indices):\n",
    "        loss = self.reg_fn(y_true = original_patches, y_pred = predicted_patches)\n",
    "        return ops.sum(loss)/ops.sum(mask_indices)\n",
    "        \n",
    "    def train_step(self, dataset):\n",
    "        mask_indices, mask_image, image = dataset\n",
    "        mask_image = ops.cast(mask_image, \"float32\")\n",
    "        image = ops.cast(image, \"float32\")\n",
    "        image =  keras.layers.Rescaling(scale=1./127.5, offset=-1, name = \"RescalingImage\")(image)\n",
    "        original_patches = ops.image.extract_patches(image, self.patch_size, padding = \"same\")\n",
    "        n_patches = ops.shape(original_patches)[1] * ops.shape(original_patches)[2]\n",
    "        channels = ops.shape(original_patches)[-1]\n",
    "        \n",
    "        if self.grayscale:\n",
    "            original_dims = 1\n",
    "        else:\n",
    "            original_dims = 3\n",
    "        \n",
    "        original_patches = ops.reshape(original_patches, [-1, n_patches, self.patch_size, self.patch_size, original_dims ])\n",
    "        ## 1. SSL encoder loss ##\n",
    "        with tf.GradientTape() as tape:\n",
    "            feature_token, feature_seq, weights = self.feature_extractor(mask_image)\n",
    "            feature_seq = self.projector(feature_seq)\n",
    "            feature_seq = ops.reshape(feature_seq, [-1, n_patches, self.patch_size, self.patch_size, original_dims])\n",
    "            loss = self.compute_loss(original_patches, feature_seq, mask_indices)\n",
    "\n",
    "        gradients = tape.gradient(loss, \n",
    "                                 (self.feature_extractor.trainable_variables + self.projector.trainable_variables))\n",
    "        self.optimizer.apply_gradients(zip(gradients, \n",
    "                                          (self.feature_extractor.trainable_variables + self.projector.trainable_variables)\n",
    "                                          ))\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        output_dict = {'SimMIM_loss' : self.loss_tracker.result()}\n",
    "        return output_dict\n",
    "    def test_step(self, dataset):\n",
    "        mask_indices, mask_image, image = dataset\n",
    "        mask_image = ops.cast(mask_image, \"float32\")\n",
    "        image = ops.cast(image, \"float32\")\n",
    "        image =  keras.layers.Rescaling(scale=1./127.5, offset=-1, name = \"RescalingImage\")(image)\n",
    "        \n",
    "        original_patches = ops.image.extract_patches(image, self.patch_size, padding = \"same\")\n",
    "        n_patches = ops.shape(original_patches)[1] * ops.shape(original_patches)[2]\n",
    "        channels = ops.shape(original_patches)[-1]\n",
    "        if self.grayscale:\n",
    "            original_dims = 1\n",
    "        else:\n",
    "            original_dims = 3\n",
    "        original_patches = ops.reshape(original_patches, [-1, n_patches, self.patch_size, self.patch_size, original_dims ])\n",
    "        \n",
    "        if True:\n",
    "            feature_token, feature_seq, weights = self.feature_extractor(mask_image)\n",
    "            feature_seq = self.projector(feature_seq)\n",
    "            feature_seq = ops.reshape(feature_seq, [-1, n_patches, self.patch_size, self.patch_size, original_dims])\n",
    "            loss = self.compute_loss(original_patches, feature_seq, mask_indices)\n",
    "\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        output_dict = {'SimMIM_loss' : self.loss_tracker.result()}\n",
    "        return output_dict\n",
    "    \n",
    "    def call(self, dataset):\n",
    "        return self.test_step(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c7fb5c",
   "metadata": {
    "papermill": {
     "duration": 0.027255,
     "end_time": "2024-07-05T17:51:24.593600",
     "exception": false,
     "start_time": "2024-07-05T17:51:24.566345",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "> Mixed SimMIM\n",
    "- Original SimMIM plus,\n",
    "- masked image (encoded) 및 image (encoded) feature map간 euclidian distance 최소화\n",
    "- representation vector의 VIC loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b3c670b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T17:51:24.647258Z",
     "iopub.status.busy": "2024-07-05T17:51:24.646927Z",
     "iopub.status.idle": "2024-07-05T17:51:24.678205Z",
     "shell.execute_reply": "2024-07-05T17:51:24.677402Z"
    },
    "papermill": {
     "duration": 0.060807,
     "end_time": "2024-07-05T17:51:24.680275",
     "exception": false,
     "start_time": "2024-07-05T17:51:24.619468",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MixedMIM(keras.Model):\n",
    "    def __init__(self, feature_extractor, grayscale, patch_size,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.patch_size = patch_size\n",
    "        self.feature_extractor = feature_extractor #weight sharing for VICreg, Barlow Twins\n",
    "        self.mha = keras.layers.MultiHeadAttention(8,512)\n",
    "        self.grayscale = grayscale\n",
    "        if grayscale:\n",
    "            self.projector = Dense(units = patch_size**2 , activation = \"sigmoid\", name = \"Grayscale_Regressor\")\n",
    "        else:\n",
    "            self.projector = Dense(units = 3*(patch_size**2) , activation = \"sigmoid\", name = \"RGB_Regressor\")\n",
    "        \n",
    "        self.loss_tracker = keras.metrics.Mean(\"MixedMIM_loss_tracker\")\n",
    "        self.sim_loss_tracker = keras.metrics.Mean(\"SimMIM_Regression_loss_tracker\")\n",
    "        self.feature_loss_tracker = keras.metrics.Mean(\"FeatureMap_distance_loss_tracker\")\n",
    "        self.vic_loss_tracker = keras.metrics.Mean(\"vic_loss_tracker\")\n",
    "        self.barlow_loss_tracker = keras.metrics.Mean(\"barlow_loss_tracker\")\n",
    "        \n",
    "        self.reg_fn = keras.losses.Huber(reduction = None)\n",
    "        self.distance_fn = keras.losses.Huber(reduction = None)\n",
    "        self.vic_loss = VICRegLoss()\n",
    "        self.barlow_loss = BarlowLoss()\n",
    "    def get_config(self):\n",
    "        return {\"feature_extractor_name\" : self.feature_extractor.name,\n",
    "               \"SSL_method\" : \"MixedMIM\",\n",
    "                \"Grayscale\" : self.grayscale,\n",
    "               }\n",
    "    \n",
    "    def compute_simmim_loss(self, original_patches, predicted_patches, mask_indices):\n",
    "        loss = self.reg_fn(y_true = original_patches, y_pred = predicted_patches)\n",
    "        return ops.sum(loss)/ops.sum(mask_indices)\n",
    "    \n",
    "    def compute_fmap_loss(self, original_patches, predicted_patches, mask_indices):\n",
    "        loss = self.distance_fn(y_true = original_patches, y_pred = predicted_patches)\n",
    "        return ops.sum(loss)/ops.sum(mask_indices)\n",
    "\n",
    "    \n",
    "    def train_step(self, dataset):\n",
    "        mask_indices, mask_image, image, original_image = dataset\n",
    "        mask_image = ops.cast(mask_image, \"float32\")\n",
    "        image = ops.cast(image, \"float32\")\n",
    "        image =  keras.layers.Rescaling(scale=1./127.5, offset=-1, name = \"RescalingImage\")(image)\n",
    "        original_patches = ops.image.extract_patches(image, self.patch_size, padding = \"same\")\n",
    "        n_patches = ops.shape(original_patches)[1] * ops.shape(original_patches)[2]\n",
    "        channels = ops.shape(original_patches)[-1]\n",
    "        \n",
    "        if self.grayscale:\n",
    "            original_dims = 1\n",
    "        else:\n",
    "            original_dims = 3\n",
    "        \n",
    "        original_patches_rgb = ops.reshape(original_patches, \n",
    "                                       [-1, n_patches, self.patch_size, self.patch_size, \n",
    "                                        original_dims ])\n",
    "        with tf.GradientTape() as tape:\n",
    "            feature_token, feature_seq, weights = self.feature_extractor(image)\n",
    "            feature_token_mask, feature_seq_mask, weights_mask = self.feature_extractor(mask_image)\n",
    "            original_token, original_seq, original_weights = self.feature_extractor(original_image)\n",
    "            \n",
    "            feature_seq_mask_rgb = self.projector(feature_seq_mask)\n",
    "            feature_seq_mask_rgb = ops.reshape(feature_seq_mask_rgb, [-1, n_patches, self.patch_size, self.patch_size, original_dims])\n",
    "            \n",
    "            # RGB regression loss\n",
    "            simmim_loss = self.compute_simmim_loss(original_patches_rgb, feature_seq_mask_rgb, mask_indices)\n",
    "            # Feature map loss\n",
    "            feature_seq_mask = self.mha(query = feature_seq_mask, key = feature_seq, value = feature_seq,\n",
    "                                       return_attention_scores = False,\n",
    "                                       attention_mask = 1-(mask_indices[:, tf.newaxis, :])) #original embedding 중 mask 되지 않은 부분을 context로 삼음.\n",
    "            feature_map_loss = self.compute_fmap_loss(feature_seq, feature_seq_mask, mask_indices)\n",
    "            # VIC loss\n",
    "            vic_loss = 0.5*(self.vic_loss(original_token, feature_token) + self.vic_loss(original_token, feature_token_mask))\n",
    "            barlow_loss = 0.5*(self.barlow_loss(original_token, feature_token) + self.barlow_loss(original_token, feature_token_mask))\n",
    "            loss = simmim_loss/100 + feature_map_loss + vic_loss/50 + barlow_loss\n",
    "\n",
    "        gradients = tape.gradient(loss, \n",
    "                                 (self.feature_extractor.trainable_variables + self.projector.trainable_variables + self.mha.trainable_variables))\n",
    "        self.optimizer.apply_gradients(zip(gradients, \n",
    "                                          (self.feature_extractor.trainable_variables + self.projector.trainable_variables + self.mha.trainable_variables)\n",
    "                                          ))\n",
    "        \n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.sim_loss_tracker.update_state(simmim_loss)\n",
    "        self.feature_loss_tracker.update_state(feature_map_loss)\n",
    "        self.vic_loss_tracker.update_state(vic_loss)\n",
    "        self.barlow_loss_tracker.update_state(barlow_loss)\n",
    "        output_dict = {'MixedMIM_loss' : self.loss_tracker.result(),\n",
    "                      'SimMIM_loss' : self.sim_loss_tracker.result(),\n",
    "                      'Feature_Map_Distance' : self.feature_loss_tracker.result(),\n",
    "                      'CLS_token_VIC_loss' : self.vic_loss_tracker.result(),\n",
    "                       'CLS_token_Barlow_loss' : self.barlow_loss_tracker.result()\n",
    "                      }\n",
    "        return output_dict\n",
    "    \n",
    "    def test_step(self, dataset):\n",
    "        mask_indices, mask_image, image, original_image = dataset\n",
    "        mask_image = ops.cast(mask_image, \"float32\")\n",
    "        image = ops.cast(image, \"float32\")\n",
    "        image =  keras.layers.Rescaling(scale=1./127.5, offset=-1, name = \"RescalingImage\")(image)\n",
    "        original_patches = ops.image.extract_patches(image, self.patch_size, padding = \"same\")\n",
    "        n_patches = ops.shape(original_patches)[1] * ops.shape(original_patches)[2]\n",
    "        channels = ops.shape(original_patches)[-1]\n",
    "        \n",
    "        if self.grayscale:\n",
    "            original_dims = 1\n",
    "        else:\n",
    "            original_dims = 3\n",
    "        \n",
    "        original_patches_rgb = ops.reshape(original_patches, \n",
    "                                       [-1, n_patches, self.patch_size, self.patch_size, \n",
    "                                        original_dims ])\n",
    "        if True:\n",
    "            feature_token, feature_seq, weights = self.feature_extractor(image)\n",
    "            feature_token_mask, feature_seq_mask, weights_mask = self.feature_extractor(mask_image)\n",
    "            original_token, original_seq, original_weights = self.feature_extractor(original_image)\n",
    "            \n",
    "            feature_seq_mask_rgb = self.projector(feature_seq_mask)\n",
    "            feature_seq_mask_rgb = ops.reshape(feature_seq_mask_rgb, [-1, n_patches, self.patch_size, self.patch_size, original_dims])\n",
    "            \n",
    "            # RGB regression loss\n",
    "            simmim_loss = self.compute_simmim_loss(original_patches_rgb, feature_seq_mask_rgb, mask_indices)\n",
    "            # Feature map loss\n",
    "            feature_seq_mask = self.mha(query = feature_seq_mask, key = feature_seq, value = feature_seq,\n",
    "                                       return_attention_scores = False,\n",
    "                                       attention_mask = 1-(mask_indices[:, tf.newaxis, :]))\n",
    "            feature_map_loss = self.compute_fmap_loss(feature_seq, feature_seq_mask, mask_indices)\n",
    "            # VIC loss\n",
    "            vic_loss = 0.5*(self.vic_loss(original_token, feature_token) + self.vic_loss(original_token, feature_token_mask))\n",
    "            barlow_loss = 0.5*(self.barlow_loss(original_token, feature_token) + self.barlow_loss(original_token, feature_token_mask))\n",
    "            loss = simmim_loss/100 + feature_map_loss + vic_loss/50 + barlow_loss\n",
    "\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.sim_loss_tracker.update_state(simmim_loss)\n",
    "        self.feature_loss_tracker.update_state(feature_map_loss)\n",
    "        self.vic_loss_tracker.update_state(vic_loss)\n",
    "        self.barlow_loss_tracker.update_state(barlow_loss)\n",
    "        output_dict = {'MixedMIM_loss' : self.loss_tracker.result(),\n",
    "                      'SimMIM_loss' : self.sim_loss_tracker.result(),\n",
    "                      'Feature_Map_Distance' : self.feature_loss_tracker.result(),\n",
    "                      'CLS_token_VIC_loss' : self.vic_loss_tracker.result(),\n",
    "                       'CLS_token_Barlow_loss' : self.barlow_loss_tracker.result()\n",
    "                      }\n",
    "        return output_dict\n",
    "    \n",
    "    def call(self, dataset):\n",
    "        return self.test_step(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d310c3",
   "metadata": {
    "papermill": {
     "duration": 0.025421,
     "end_time": "2024-07-05T17:51:24.731989",
     "exception": false,
     "start_time": "2024-07-05T17:51:24.706568",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "> DistilMIM\n",
    "\n",
    "- Mixed MIM with self-distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b649b101",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T17:51:24.786351Z",
     "iopub.status.busy": "2024-07-05T17:51:24.785986Z",
     "iopub.status.idle": "2024-07-05T17:51:24.823715Z",
     "shell.execute_reply": "2024-07-05T17:51:24.822859Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.06738,
     "end_time": "2024-07-05T17:51:24.825728",
     "exception": false,
     "start_time": "2024-07-05T17:51:24.758348",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DistilMIM(keras.Model):\n",
    "    def __init__(self, feature_extractor, grayscale, patch_size, lambda_ = 0.99, multiview = False,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.patch_size = patch_size\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.channels = ops.shape(self.feature_extractor.outputs[0])[-1]\n",
    "        self.grayscale = grayscale\n",
    "        if grayscale:\n",
    "            self.projector = Dense(units = patch_size**2 , activation = \"sigmoid\", name = \"Grayscale_Regressor\")\n",
    "        else:\n",
    "            self.projector = Dense(units = 3*(patch_size**2) , activation = \"sigmoid\", name = \"RGB_Regressor\")\n",
    "        \n",
    "        ##student model setting\n",
    "        ##backprop -> student model, EMA -> teacher model (feature extractor, projector)\n",
    "        self.student_extractor = tf.keras.models.clone_model(feature_extractor)\n",
    "        self.student_extractor.set_weights(self.feature_extractor.get_weights())\n",
    "        \n",
    "        self.c_token = self.add_weight(name='center_token', shape=(self.channels,), initializer='glorot_uniform', trainable=False)\n",
    "        self.c_patches = self.add_weight(name='center_patch', shape=(self.channels,), initializer='glorot_uniform', trainable=False)\n",
    "        self.l = lambda_\n",
    "        ##\n",
    "        \n",
    "        self.loss_tracker = keras.metrics.Mean(\"MixedMIM_loss_tracker\")\n",
    "        self.sim_loss_tracker = keras.metrics.Mean(\"SimMIM_Regression_loss_tracker\")\n",
    "        self.feature_loss_tracker = keras.metrics.Mean(\"FeatureMap_H_loss_tracker\")\n",
    "        self.token_loss_tracker = keras.metrics.Mean(\"Token_H_loss_tracker\")\n",
    "        \n",
    "        self.reg_fn = keras.losses.Huber(reduction = None)\n",
    "        \n",
    "    def get_config(self):\n",
    "        return {\"feature_extractor_name\" : self.feature_extractor.name,\n",
    "               \"SSL_method\" : \"DistilMIM\",\n",
    "                \"Grayscale\" : self.grayscale,\n",
    "               }\n",
    "    def compute_h(self, t, s, center, mask = None):\n",
    "        t = tf.stop_gradient(t)\n",
    "        t = (t-center)/0.1 #centering and shapening\n",
    "        t = ops.softmax(t, axis = -1)\n",
    "        s = ops.softmax(s, axis = -1)\n",
    "        ce = -t*ops.log(s + 1e-5)\n",
    "        if mask is not None:\n",
    "            if len(ops.shape(mask)) == 2:\n",
    "                mask = mask[..., tf.newaxis]\n",
    "            ce = ops.multiply(ce, mask)\n",
    "        \n",
    "        if len(ops.shape(t)) == 2:\n",
    "            ce = ops.sum(ce, axis = -1)\n",
    "        elif len(ops.shape(t)) == 3 :\n",
    "            ce = ops.sum(ce, axis = [1, 2])\n",
    "        \n",
    "        if mask is not None:\n",
    "            return ops.sum(ce)/ops.sum(mask)\n",
    "        else:\n",
    "            return ops.mean(ce)\n",
    "        \n",
    "    def compute_simmim_loss(self, original_patches, predicted_patches, mask_indices):\n",
    "        loss = self.reg_fn(y_true = original_patches, y_pred = predicted_patches)\n",
    "        return ops.sum(loss)/ops.sum(mask_indices)\n",
    "    \n",
    "    def train_step(self, dataset):\n",
    "        mask_indices, mask_image, image = dataset\n",
    "        mask_image = ops.cast(mask_image, \"float32\")\n",
    "        image = ops.cast(image, \"float32\")\n",
    "        image =  keras.layers.Rescaling(scale=1./127.5, offset=-1, name = \"RescalingImage\")(image)\n",
    "        original_patches = ops.image.extract_patches(image, self.patch_size, padding = \"same\")\n",
    "        n_patches = ops.shape(original_patches)[1] * ops.shape(original_patches)[2]\n",
    "        channels = ops.shape(original_patches)[-1]\n",
    "        \n",
    "        if self.grayscale:\n",
    "            original_dims = 1\n",
    "        else:\n",
    "            original_dims = 3\n",
    "        \n",
    "        original_patches_rgb = ops.reshape(original_patches, \n",
    "                                       [-1, n_patches, self.patch_size, self.patch_size, \n",
    "                                        original_dims ])\n",
    "        with tf.GradientTape() as tape:\n",
    "            feature_token, feature_seq, weights = self.feature_extractor(image)\n",
    "            feature_token_mask, feature_seq_mask, weights_mask = self.feature_extractor(mask_image)\n",
    "            \n",
    "            feature_token_s, feature_seq_s, weights_s = self.student_extractor(image)\n",
    "            feature_token_mask_s, feature_seq_mask_s, weights_mask_s = self.student_extractor(mask_image)\n",
    "            \n",
    "            # RGB regression loss\n",
    "            feature_seq_mask_rgb = self.projector(feature_seq_mask_s)\n",
    "            feature_seq_mask_rgb = ops.reshape(feature_seq_mask_rgb, [-1, n_patches, self.patch_size, self.patch_size, original_dims])\n",
    "            simmim_loss = self.compute_simmim_loss(original_patches_rgb, feature_seq_mask_rgb, \n",
    "                                                   mask_indices)\n",
    "            # Feature map loss\n",
    "            ##feature_map_loss_1 = self.compute_h(t = feature_seq_mask, s = feature_seq_s, \n",
    "            ##                                 center = self.c_patches, mask = mask_indices)\n",
    "            feature_map_loss = self.compute_h(t = feature_seq, s = feature_seq_mask_s, \n",
    "                                             center = self.c_patches, mask = mask_indices)\n",
    "            \n",
    "            # token loss\n",
    "            token_loss_1 = self.compute_h(t = feature_token, s = feature_token_mask_s,\n",
    "                                       center = self.c_token, mask = None)\n",
    "            token_loss_2 = self.compute_h(t = feature_token_mask, s = feature_token_s,\n",
    "                                       center = self.c_token, mask = None)\n",
    "            token_loss = 0.5*(token_loss_1 + token_loss_2)\n",
    "            \n",
    "            loss = token_loss + feature_map_loss + simmim_loss/50\n",
    "        # A) update student network\n",
    "        gradients = tape.gradient(loss, \n",
    "                                 (self.student_extractor.trainable_variables + self.projector.trainable_variables))\n",
    "        self.optimizer.apply_gradients(zip(gradients, \n",
    "                                          (self.student_extractor.trainable_variables + self.projector.trainable_variables)\n",
    "                                          ))\n",
    "        # B) update center values for centering : c_token, c_patches\n",
    "        mean_cls = ops.mean(ops.concatenate([feature_token, feature_token_mask], axis = 0), axis = 0)\n",
    "        mean_patch = ops.mean(ops.concatenate([feature_seq, feature_seq_mask], axis = 0), axis = (0,1))\n",
    "        self.c_token.assign(self.l*self.c_token + (1.0-self.l)*mean_cls)\n",
    "        self.c_patches.assign(self.l*self.c_patches + (1.0-self.l)*mean_patch)\n",
    "        \n",
    "        #C) update teacher network via EMA\n",
    "        for f_teacher, f_student in zip(self.feature_extractor.weights, self.student_extractor.weights):\n",
    "            f_teacher.assign(self.l*f_teacher + (1-self.l)*f_student)\n",
    "        \n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.sim_loss_tracker.update_state(simmim_loss)\n",
    "        self.feature_loss_tracker.update_state(feature_map_loss)\n",
    "        self.token_loss_tracker.update_state(token_loss)\n",
    "        \n",
    "        output_dict = {'DistilMIM_loss' : self.loss_tracker.result(),\n",
    "                      'SimMIM_loss' : self.sim_loss_tracker.result(),\n",
    "                      'Feature_Map_H' : self.feature_loss_tracker.result(),\n",
    "                      'Token_H' : self.token_loss_tracker.result()\n",
    "                      }\n",
    "        return output_dict\n",
    "    \n",
    "    def test_step(self, dataset):\n",
    "        mask_indices, mask_image, image = dataset\n",
    "        mask_image = ops.cast(mask_image, \"float32\")\n",
    "        image = ops.cast(image, \"float32\")\n",
    "        image =  keras.layers.Rescaling(scale=1./127.5, offset=-1, name = \"RescalingImage\")(image)\n",
    "        original_patches = ops.image.extract_patches(image, self.patch_size, padding = \"same\")\n",
    "        n_patches = ops.shape(original_patches)[1] * ops.shape(original_patches)[2]\n",
    "        channels = ops.shape(original_patches)[-1]\n",
    "        \n",
    "        if self.grayscale:\n",
    "            original_dims = 1\n",
    "        else:\n",
    "            original_dims = 3\n",
    "        \n",
    "        original_patches_rgb = ops.reshape(original_patches, \n",
    "                                       [-1, n_patches, self.patch_size, self.patch_size, \n",
    "                                        original_dims ])\n",
    "        if True:\n",
    "            feature_token, feature_seq, weights = self.feature_extractor(image)\n",
    "            feature_token_mask, feature_seq_mask, weights_mask = self.feature_extractor(mask_image)\n",
    "            \n",
    "            feature_token_s, feature_seq_s, weights_s = self.student_extractor(image)\n",
    "            feature_token_mask_s, feature_seq_mask_s, weights_mask_s = self.student_extractor(mask_image)\n",
    "            \n",
    "            # RGB regression loss\n",
    "            feature_seq_mask_rgb = self.projector(feature_seq_mask_s)\n",
    "            feature_seq_mask_rgb = ops.reshape(feature_seq_mask_rgb, [-1, n_patches, self.patch_size, self.patch_size, original_dims])\n",
    "            simmim_loss = self.compute_simmim_loss(original_patches_rgb, feature_seq_mask_rgb, \n",
    "                                                   mask_indices)\n",
    "            # Feature map loss\n",
    "            ##feature_map_loss_1 = self.compute_h(t = feature_seq_mask, s = feature_seq_s, \n",
    "            ##                                 center = self.c_patches, mask = mask_indices)\n",
    "            feature_map_loss = self.compute_h(t = feature_seq, s = feature_seq_mask_s, \n",
    "                                             center = self.c_patches, mask = mask_indices)\n",
    "            \n",
    "            # token loss\n",
    "            token_loss_1 = self.compute_h(t = feature_token, s = feature_token_mask_s,\n",
    "                                       center = self.c_token, mask = None)\n",
    "            token_loss_2 = self.compute_h(t = feature_token_mask, s = feature_token_s,\n",
    "                                       center = self.c_token, mask = None)\n",
    "            token_loss = 0.5*(token_loss_1 + token_loss_2)\n",
    "            \n",
    "            loss = token_loss + feature_map_loss + simmim_loss/50\n",
    "        \n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.sim_loss_tracker.update_state(simmim_loss)\n",
    "        self.feature_loss_tracker.update_state(feature_map_loss)\n",
    "        self.token_loss_tracker.update_state(token_loss)\n",
    "        \n",
    "        output_dict = {'DistilMIM_loss' : self.loss_tracker.result(),\n",
    "                      'SimMIM_loss' : self.sim_loss_tracker.result(),\n",
    "                      'Feature_Map_H' : self.feature_loss_tracker.result(),\n",
    "                      'Token_H' : self.token_loss_tracker.result()\n",
    "                      }\n",
    "        return output_dict\n",
    "    \n",
    "    def call(self, dataset):\n",
    "        return self.test_step(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701d6626",
   "metadata": {
    "papermill": {
     "duration": 0.024949,
     "end_time": "2024-07-05T17:51:24.876041",
     "exception": false,
     "start_time": "2024-07-05T17:51:24.851092",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "> VICRegL helper functions and loss implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8d79832f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T17:51:24.927931Z",
     "iopub.status.busy": "2024-07-05T17:51:24.927659Z",
     "iopub.status.idle": "2024-07-05T17:51:24.951857Z",
     "shell.execute_reply": "2024-07-05T17:51:24.950995Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.052584,
     "end_time": "2024-07-05T17:51:24.953778",
     "exception": false,
     "start_time": "2024-07-05T17:51:24.901194",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def feature_based_matching(feature_map_a, feature_map_b, k=20):\n",
    "    if len(ops.shape(feature_map_a)) == 4:\n",
    "        batch_size, H, W, D = feature_map_a.shape\n",
    "    elif len(ops.shape(feature_map_a)) == 3 :\n",
    "        batch_size, seq_len, D = feature_map_a.shape\n",
    "        H = tf.sqrt(tf.cast(seq_len, tf.float32))\n",
    "        H = tf.cast(H, tf.int32)\n",
    "        W = H\n",
    "    feature_map_a_flat = tf.reshape(feature_map_a, [-1, H * W, D])\n",
    "    feature_map_b_flat = tf.reshape(feature_map_b, [-1, H * W, D])\n",
    "    dist = tf.norm(feature_map_a_flat[:, :, tf.newaxis, :] - feature_map_b_flat[:, tf.newaxis, :, :], axis=-1)\n",
    "    \n",
    "    matchings = ops.reshape(dist, [-1, H*W*H*W])\n",
    "    matchings = ops.cast(matchings, \"float32\")\n",
    "\n",
    "    # Top-k selection\n",
    "    top_k_values, _ = tf.nn.top_k(-matchings, k=int(k))\n",
    "    top_k_values = -top_k_values\n",
    "    matchings_top_k = tf.reduce_mean(top_k_values, axis=-1)\n",
    "    return matchings_top_k\n",
    "\n",
    "def location_based_matching(feature_map_a, feature_map_b, k=4):\n",
    "    if len(ops.shape(feature_map_a)) == 4:\n",
    "        batch_size, H, W, D = feature_map_a.shape\n",
    "    elif len(ops.shape(feature_map_a)) == 3 :\n",
    "        batch_size, seq_len, D = feature_map_a.shape\n",
    "        H = tf.sqrt(tf.cast(seq_len, tf.float32))\n",
    "        H = tf.cast(H, tf.int32)\n",
    "        W = H\n",
    "    coords = tf.stack(tf.meshgrid(tf.range(H), tf.range(W), indexing='ij'), axis=-1)\n",
    "    coords_flat = tf.cast(tf.reshape(coords, [H * W, 2]), tf.float32)\n",
    "    coord_dist = tf.norm(coords_flat[:, tf.newaxis, :] - coords_flat[tf.newaxis, :, :], axis=-1)\n",
    "    coord_dist = tf.reshape(coord_dist, [H * W, H * W])\n",
    "\n",
    "    feature_map_a_flat = tf.reshape(feature_map_a, [-1, H * W, D])\n",
    "    feature_map_b_flat = tf.reshape(feature_map_b, [-1, H * W, D])\n",
    "\n",
    "    dist = tf.norm(feature_map_a_flat[:, :, tf.newaxis, :] - feature_map_b_flat[:, tf.newaxis, :, :], axis=-1)\n",
    "    combined_dist = dist * tf.expand_dims(coord_dist, axis=0)\n",
    "    \n",
    "    matchings = ops.reshape(dist, [-1, H*W*H*W])\n",
    "    \n",
    "    matchings = ops.cast(matchings, \"float32\")\n",
    "\n",
    "    # Top-k selection\n",
    "    \n",
    "    top_k_values, _ = tf.nn.top_k(-matchings, k=int(k))\n",
    "    top_k_values = -top_k_values\n",
    "    matchings_top_k = tf.reduce_mean(top_k_values, axis=-1)\n",
    "    return matchings_top_k\n",
    "\n",
    "class VICRegL_Loss(keras.losses.Loss):\n",
    "    def __init__(self, alpha=0.75, lambda_v=1.0, mu=1.0, nu=1.0, name=\"VICRegL_Loss\"):\n",
    "        super(VICRegL_Loss, self).__init__(name=name)\n",
    "        self.alpha = alpha\n",
    "        self.lambda_v = lambda_v\n",
    "        self.mu = mu\n",
    "        self.nu = nu\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        z_a, patch_a = y_true\n",
    "        z_b, patch_b = y_pred\n",
    "        local_loss = self.local_criterion(patch_a, patch_b)\n",
    "        global_loss = self.global_criterion(z_a, z_b)\n",
    "        return self.alpha * global_loss + (1 - self.alpha) * local_loss\n",
    "\n",
    "    def global_criterion(self, z_a, z_b):\n",
    "        sim_loss = tf.reduce_mean(tf.square(z_a - z_b))\n",
    "        std_z_a = tf.math.reduce_std(z_a, axis=0)\n",
    "        std_z_b = tf.math.reduce_std(z_b, axis=0)\n",
    "        std_loss = tf.reduce_mean(tf.nn.relu(1 - std_z_a)) + tf.reduce_mean(tf.nn.relu(1 - std_z_b))\n",
    "        z_a -= tf.reduce_mean(z_a, axis=0)\n",
    "        z_b -= tf.reduce_mean(z_b, axis=0)\n",
    "        cov_z_a = tf.matmul(z_a, z_a, transpose_a=True) / (tf.cast(tf.shape(z_a)[0], tf.float32) - 1.0)\n",
    "        cov_z_b = tf.matmul(z_b, z_b, transpose_a=True) / (tf.cast(tf.shape(z_b)[0], tf.float32) - 1.0)\n",
    "        cov_loss = tf.reduce_sum(tf.square(self.off_diagonal(cov_z_a))) + tf.reduce_sum(tf.square(self.off_diagonal(cov_z_b)))\n",
    "        return self.lambda_v * sim_loss + self.mu * std_loss + self.nu * cov_loss\n",
    "\n",
    "    def local_criterion(self, patch_a, patch_b):\n",
    "        feature_loss = feature_based_matching(patch_a, patch_b)\n",
    "        location_loss = location_based_matching(patch_a, patch_b)\n",
    "        return feature_loss + location_loss\n",
    "\n",
    "    def off_diagonal(self, x):\n",
    "        n = tf.shape(x)[0]\n",
    "        return tf.reshape(tf.boolean_mask(x, tf.eye(n, dtype=tf.bool)), [n, -1])\n",
    "\n",
    "# 예제 사용\n",
    "#batch_size = 12\n",
    "#H, W, D = 7, 7, 1024\n",
    "#feature_map_a = tf.random.normal([batch_size, H, W, D])\n",
    "#feature_map_b = tf.random.normal([batch_size, H, W, D])\n",
    "#z_a = keras.layers.GlobalAveragePooling2D()(feature_map_a)\n",
    "#z_b = keras.layers.GlobalAveragePooling2D()(feature_map_b)\n",
    "#loss_fn = VICRegL_Loss()\n",
    "#loss = loss_fn((z_a, feature_map_a), (z_b, feature_map_b))\n",
    "#print(\"Loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e0f8d277",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T17:51:25.009007Z",
     "iopub.status.busy": "2024-07-05T17:51:25.008737Z",
     "iopub.status.idle": "2024-07-05T17:51:25.024745Z",
     "shell.execute_reply": "2024-07-05T17:51:25.023918Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.047426,
     "end_time": "2024-07-05T17:51:25.026623",
     "exception": false,
     "start_time": "2024-07-05T17:51:24.979197",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VICRegL(keras.Model):\n",
    "    def __init__(self, feature_extractor, embed_dims, student_model=None, grayscale=None, patch_size=None, multiview=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.student_model = student_model\n",
    "        self.projector = keras.layers.Dense(units=embed_dims, activation=\"gelu\", use_bias=False, name = \"projector\")\n",
    "        if student_model is None:\n",
    "            self.student_model = self.feature_extractor\n",
    "            self.student_projector = self.projector\n",
    "        elif student_model in [\"distil\", \"distillation\", \"distill\", \"Distil\", \"Distillation\", \"Distil\"]:\n",
    "            self.student_model = tf.keras.models.clone_model(feature_extractor)\n",
    "            self.student_model.set_weights(self.feature_extractor.get_weights())\n",
    "\n",
    "            self.student_projector = keras.layers.Dense(units=embed_dims, activation=\"gelu\", use_bias=False, name = \"student_projector\")\n",
    "            self.student_projector.set_weights(self.projector.get_weights())\n",
    "\n",
    "        self.grayscale = grayscale\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        self.loss_tracker = keras.metrics.Mean(\"VICRegL_loss_tracker\")\n",
    "        self.loss_fn = VICRegL_Loss()\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"feature_extractor_name\": self.feature_extractor.name,\n",
    "            \"SSL_method\": \"VICRegL\",\n",
    "            \"Grayscale\": self.grayscale,\n",
    "        }\n",
    "\n",
    "    def train_step(self, dataset):\n",
    "        image, aug_image = dataset\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            feature_token, feature_seq, weights = self.feature_extractor(image)\n",
    "            feature_token_aug, feature_seq_aug, weights_aug = self.student_model(aug_image)\n",
    "            feature_seq = self.projector(feature_seq)\n",
    "            feature_seq_aug = self.student_projector(feature_seq_aug)\n",
    "\n",
    "            loss = self.loss_fn((tf.stop_gradient(feature_token), tf.stop_gradient(feature_seq)),\n",
    "                                (feature_token_aug, feature_seq_aug))\n",
    "\n",
    "        gradients = tape.gradient(loss, self.student_model.trainable_variables + self.student_projector.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.student_model.trainable_variables + self.student_projector.trainable_variables))\n",
    "\n",
    "        # Update teacher model weights if using distillation\n",
    "        if self.student_model is not None:\n",
    "            lambda_ = 0.999\n",
    "            for f_teacher_part, f_student_part, p_teacher_part, p_student_part in zip(self.feature_extractor.weights, self.student_model.weights, self.projector.weights, self.student_projector.weights):\n",
    "                f_teacher_part.assign(lambda_ * f_teacher_part + (1 - lambda_) * f_student_part)\n",
    "                p_teacher_part.assign(lambda_ * p_teacher_part + (1 - lambda_) * p_student_part)\n",
    "\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"VICRegL_loss\": self.loss_tracker.result()}\n",
    "\n",
    "    def test_step(self, dataset):\n",
    "        image, aug_image = dataset\n",
    "\n",
    "        feature_token, feature_seq, weights = self.feature_extractor(image)\n",
    "        feature_token_aug, feature_seq_aug, weights_aug = self.student_model(aug_image)\n",
    "        feature_seq = self.projector(feature_seq)\n",
    "        feature_seq_aug = self.student_projector(feature_seq_aug)\n",
    "\n",
    "        loss = self.loss_fn((feature_token, feature_seq), (feature_token_aug, feature_seq_aug))\n",
    "\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"VICRegL_loss\": self.loss_tracker.result()}\n",
    "\n",
    "    def call(self, dataset):\n",
    "        return self.test_step(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5de00cf",
   "metadata": {
    "papermill": {
     "duration": 0.025364,
     "end_time": "2024-07-05T17:51:25.077229",
     "exception": false,
     "start_time": "2024-07-05T17:51:25.051865",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "> helper functions for Sementaic Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0fe3a30f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T17:51:25.129165Z",
     "iopub.status.busy": "2024-07-05T17:51:25.128868Z",
     "iopub.status.idle": "2024-07-05T17:51:25.144852Z",
     "shell.execute_reply": "2024-07-05T17:51:25.143977Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.046706,
     "end_time": "2024-07-05T17:51:25.149096",
     "exception": false,
     "start_time": "2024-07-05T17:51:25.102390",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# thanks to GPT4o\n",
    "class KMeansLayer(keras.layers.Layer):\n",
    "    def __init__(self, n_clusters, max_iters=100, **kwargs):\n",
    "        super(KMeansLayer, self).__init__(**kwargs)\n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_iters = max_iters\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        embed_dim = input_shape[-1]\n",
    "        self.centroids = self.add_weight(shape=(self.n_clusters, embed_dim),\n",
    "                                         initializer='glorot_uniform',\n",
    "                                         trainable=False,\n",
    "                                         name='centroids')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # Flatten the inputs if they are 3D (batch_size, num_patches, embed_dims)\n",
    "        if len(inputs.shape) == 3:\n",
    "            batch_size, num_patches, embed_dims = inputs.shape\n",
    "            inputs_reshaped = tf.reshape(inputs, [-1, embed_dims])\n",
    "        else:\n",
    "            inputs_reshaped = inputs\n",
    "        \n",
    "        # Initialize centroids\n",
    "        centroids = tf.identity(self.centroids)\n",
    "\n",
    "        # K-means clustering\n",
    "        for i in range(self.max_iters):\n",
    "            # Compute distances and assign clusters\n",
    "            distances = tf.reduce_sum(tf.square(tf.expand_dims(inputs_reshaped, axis=1) - tf.expand_dims(centroids, axis=0)), axis=2)\n",
    "            cluster_assignments = tf.argmin(distances, axis=1)\n",
    "\n",
    "            # Update centroids\n",
    "            for j in range(self.n_clusters):\n",
    "                mask = tf.equal(cluster_assignments, j)\n",
    "                mask = tf.cast(mask, tf.float32)\n",
    "                count = tf.reduce_sum(mask)\n",
    "                count = tf.maximum(count, 1.0)  # Avoid division by zero\n",
    "                new_centroid = tf.reduce_sum(inputs_reshaped * tf.expand_dims(mask, axis=1), axis=0) / count\n",
    "                centroids = tf.tensor_scatter_nd_update(centroids, [[j]], [new_centroid])\n",
    "\n",
    "        self.centroids.assign(centroids)\n",
    "\n",
    "        # Reshape cluster assignments back to original shape if necessary\n",
    "        if len(inputs.shape) == 3:\n",
    "            cluster_assignments = tf.reshape(cluster_assignments, [-1, num_patches])\n",
    "        \n",
    "        return cluster_assignments\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(KMeansLayer, self).get_config()\n",
    "        config.update({\n",
    "            \"n_clusters\": self.n_clusters,\n",
    "            \"max_iters\": self.max_iters,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f339998",
   "metadata": {
    "papermill": {
     "duration": 0.025378,
     "end_time": "2024-07-05T17:51:25.204004",
     "exception": false,
     "start_time": "2024-07-05T17:51:25.178626",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "> Unsupervised Segmentation, differentiable!\n",
    "\n",
    "- high pass filter\n",
    "- UnSupSeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e39c1588",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T17:51:25.255992Z",
     "iopub.status.busy": "2024-07-05T17:51:25.255685Z",
     "iopub.status.idle": "2024-07-05T17:51:25.266200Z",
     "shell.execute_reply": "2024-07-05T17:51:25.265480Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.0388,
     "end_time": "2024-07-05T17:51:25.268163",
     "exception": false,
     "start_time": "2024-07-05T17:51:25.229363",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HighPassFilterLayer(layers.Layer):\n",
    "    def __init__(self, r=30, **kwargs):\n",
    "        super(HighPassFilterLayer, self).__init__(**kwargs)\n",
    "        self.r = r  # Radius for the low-frequency block to suppress\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a high-pass filter mask\n",
    "        rows, cols = input_shape[1], input_shape[2]\n",
    "        crow, ccol = rows // 2, cols // 2\n",
    "        mask = np.ones((rows, cols), dtype=np.float32)\n",
    "        mask[crow-self.r:crow+self.r, ccol-self.r:ccol+self.r] = 0\n",
    "        self.mask = tf.convert_to_tensor(mask, dtype=tf.complex64)\n",
    "        self.mask = tf.expand_dims(tf.expand_dims(self.mask, axis=0), axis=0)  # Add batch and channel dimensions\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Get the shape of the input image\n",
    "        batch_size, rows, cols, channels = tf.shape(inputs)[0], tf.shape(inputs)[1], tf.shape(inputs)[2], tf.shape(inputs)[3]\n",
    "\n",
    "        # Convert image to frequency domain\n",
    "        inputs = tf.cast(inputs, tf.complex64)\n",
    "        freq_domain = tf.signal.fft2d(tf.signal.fftshift(tf.transpose(inputs, perm=[0, 3, 1, 2])))\n",
    "\n",
    "        # Apply the mask to each image in the batch\n",
    "        mask = tf.tile(self.mask, [batch_size, channels, 1, 1])  # Repeat mask for each image and channel in the batch\n",
    "        filtered_freq_domain = freq_domain * mask\n",
    "\n",
    "        # Convert back to spatial domain\n",
    "        high_freq_image = tf.signal.ifft2d(filtered_freq_domain)\n",
    "        high_freq_image = tf.signal.ifftshift(tf.abs(high_freq_image))\n",
    "        high_freq_image = tf.transpose(high_freq_image, perm=[0, 2, 3, 1])  # Reshape back to original\n",
    "\n",
    "        return tf.cast(high_freq_image, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "68d8c24d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T17:51:25.320500Z",
     "iopub.status.busy": "2024-07-05T17:51:25.320200Z",
     "iopub.status.idle": "2024-07-05T17:51:25.334712Z",
     "shell.execute_reply": "2024-07-05T17:51:25.333916Z"
    },
    "papermill": {
     "duration": 0.043117,
     "end_time": "2024-07-05T17:51:25.336572",
     "exception": false,
     "start_time": "2024-07-05T17:51:25.293455",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class UnsupervisedSegmentation(tf.keras.Model):\n",
    "    def __init__(self, encoder, q, mu):\n",
    "        super(UnsupervisedSegmentation, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.q = q\n",
    "        self.mu = mu\n",
    "        self.cluster_layer = layers.Dense(q, activation=None)\n",
    "        self.norm_layer = layers.BatchNormalization()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        high_freq_image = HighPassFilterLayer()(inputs)\n",
    "        token_highfreq, features_highfreq, w_ = self.encoder(high_freq_image)\n",
    "        token_original, features_original, w_ = self.encoder(inputs)\n",
    "        \n",
    "        features = ops.concatenate([features_original, features_highfreq],\n",
    "                                  axis = -1)\n",
    "        \n",
    "        responses = self.cluster_layer(features)\n",
    "        normalized_responses = self.norm_layer(responses)\n",
    "        cluster_labels = tf.argmax(normalized_responses, axis=-1)\n",
    "        return normalized_responses, cluster_labels\n",
    "\n",
    "    def compute_loss(self, normalized_responses, cluster_labels):\n",
    "        feature_similarity_loss = tf.reduce_mean(\n",
    "            tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits=normalized_responses, labels=cluster_labels\n",
    "            )\n",
    "        )\n",
    "        spatial_continuity_loss = self.compute_spatial_continuity_loss(normalized_responses)\n",
    "        total_loss = feature_similarity_loss + self.mu * spatial_continuity_loss\n",
    "        return total_loss\n",
    "\n",
    "    def compute_spatial_continuity_loss(self, responses):\n",
    "        batch_size, n_patches, dims = ops.shape(responses)\n",
    "        p = tf.sqrt(tf.cast(n_patches, tf.float32))\n",
    "        p = ops.cast(p, \"int32\")\n",
    "        responses = ops.reshape(responses, [-1, p, p, dims])\n",
    "        diff_x = tf.reduce_sum(tf.abs(responses[:, 1:, :, :] - responses[:, :-1, :, :]))\n",
    "        diff_y = tf.reduce_sum(tf.abs(responses[:, :, 1:, :] - responses[:, :, :-1, :]))\n",
    "        continuity_loss = diff_x + diff_y\n",
    "        return continuity_loss\n",
    "\n",
    "    def train_step(self, data):\n",
    "        images = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            normalized_responses, cluster_labels = self(images, training=True)\n",
    "            loss = self.compute_loss(normalized_responses, cluster_labels)\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        return {\"loss\": loss}\n",
    "    def test_step(self, data):\n",
    "        images = data\n",
    "        if True:\n",
    "            normalized_responses, cluster_labels = self(images, training=False)\n",
    "            loss = self.compute_loss(normalized_responses, cluster_labels)\n",
    "        return {\"loss\": loss}\n",
    "    \n",
    "\n",
    "## Instantiate and compile the model\n",
    "#encoder_model = get_metaformer(\"gMLP\", res = 256, grayscale = False, \n",
    "#                               att_depth = 6, att_heads = 8, \n",
    "#                               att_dims = 8 * 64, \n",
    "#                               embed_dims = 8 * 64, patch_size = PATCH_SIZE,\n",
    "#                              register_tokens = 4,\n",
    "#                               pretrained_encoder = None, return_patches = True\n",
    "#                              )\n",
    "#unsupervised_model = UnsupervisedSegmentation(encoder_model, Q, MU)\n",
    "#unsupervised_model.compile(optimizer=keras.optimizers.Adam())\n",
    "## Prepare dummy dataset (replace this with actual image data)\n",
    "#train_images = np.random.rand(100, *IMAGE_SIZE).astype(np.float32)  # 100 dummy images\n",
    "#train_dataset = tf.data.Dataset.from_tensor_slices(train_images).batch(BATCH_SIZE)\n",
    "## Train the model\n",
    "#print(unsupervised_model(train_images[:10]))\n",
    "#history = unsupervised_model.fit(train_dataset, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1982370e",
   "metadata": {
    "papermill": {
     "duration": 0.025045,
     "end_time": "2024-07-05T17:51:25.387041",
     "exception": false,
     "start_time": "2024-07-05T17:51:25.361996",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "----------------\n",
    "# Multimodal Contrastive method\n",
    "- CLIP\n",
    "- SigLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "34bccd21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T17:51:25.439038Z",
     "iopub.status.busy": "2024-07-05T17:51:25.438779Z",
     "iopub.status.idle": "2024-07-05T17:51:25.464667Z",
     "shell.execute_reply": "2024-07-05T17:51:25.463837Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.054365,
     "end_time": "2024-07-05T17:51:25.466553",
     "exception": false,
     "start_time": "2024-07-05T17:51:25.412188",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CLIP(keras.Model): #original CLIP + CLIP surgery\n",
    "    def __init__(self, image_encoder, text_encoder,\n",
    "                embed_dims, pool_heads = 8, t = 2.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.image_encoder = image_encoder\n",
    "        self.text_encoder = text_encoder\n",
    "        self.embed_dims = embed_dims\n",
    "        self.t = t\n",
    "        self.mlp_image = keras.layers.Dense(units = embed_dims)\n",
    "        self.mlp_text = keras.layers.Dense(units = embed_dims)\n",
    "        self.pe_fn = keras_nlp.layers.RotaryEmbedding(name = \"RotatoryPE\")\n",
    "        self.image_pooler = AttentionPooling(pool_heads, embed_dims, name = \"ImageAttentionPooler\")\n",
    "        self.text_pooler = AttentionPooling(pool_heads, embed_dims, name = \"TextAttentionPooler\")\n",
    "        self.loss_tracker = keras.metrics.Mean(name = \"CLIP_loss\")\n",
    "    def get_config(self):\n",
    "        return {\"Image encoder name\": self.image_encoder.name,\n",
    "               \"Text encoder name\" : self.text_encoder.name,\n",
    "               \"embed_dims\" : self.embed_dims,\n",
    "               \"temperature\" : self.t,\n",
    "               \"SSL_method\" : \"CLIP_with_Attentional_Pooling\"}\n",
    "    def get_clip_loss(self, image_vector, text_vector):\n",
    "        batch_size = ops.shape(image_vector)[0]\n",
    "        image_vector = keras.utils.normalize(image_vector, axis = -1, order = 2)\n",
    "        text_vector = keras.utils.normalize(text_vector, axis = -1, order = 2)\n",
    "        \n",
    "        cor_mat = tf.einsum(\"ab, cb->ac\", image_vector, text_vector) / self.t\n",
    "        cor_mat = ops.reshape(cor_mat, [batch_size, batch_size])\n",
    "        pseudo_label = ops.zeros_like(cor_mat)\n",
    "        diags = tf.linalg.diag_part(pseudo_label) + 1.0\n",
    "        pseudo_label = tf.linalg.set_diag(pseudo_label, diags)\n",
    "        pseudo_label = pseudo_label - ops.ones_like(pseudo_label)\n",
    "        loss = 0.5*(ops.mean(keras.losses.CategoricalFocalCrossentropy(reduction = None, label_smoothing = 0.05)(pseudo_label, cor_mat)) + \n",
    "                    ops.mean(keras.losses.CategoricalFocalCrossentropy(reduction = None, label_smoothing = 0.05)(pseudo_label, ops.transpose(cor_mat)))\n",
    "                   )\n",
    "        return loss\n",
    "        \n",
    "    def train_step(self, dataset):\n",
    "        image, text = dataset\n",
    "        with tf.GradientTape() as tape: \n",
    "            image_feature, text_feature = self.image_encoder(image, training = True), self.text_encoder(text, training = True)\n",
    "            batch_size, w, h, dims = ops.shape(image_feature) ; batch_size = ops.shape(image_feature)[0]\n",
    "            \n",
    "            image_feature = ops.reshape(image_feature, [batch_size, w*h, dims])\n",
    "            image_feature = self.pe_fn(image_feature)\n",
    "            text_vector = keras.layers.GlobalAveragePooling1D()(text_feature)\n",
    "            if len(ops.shape(image_feature)) == 3:\n",
    "                image_vector = keras.layers.GlobalAveragePooling1D()(image_feature)\n",
    "            elif len(ops.shape(image_feature)) == 4:\n",
    "                image_vector = keras.layers.GlobalAveragePooling2D()(image_feature)\n",
    "\n",
    "            image_vector, text_vector = self.mlp_image(image_vector, training = True), self.mlp_text(text_vector, training = True)\n",
    "            image_vector, text_vector = self.image_pooler([image_vector, image_feature], training = True)[0], self.text_pooler([text_vector, text_feature], training = True)[0]\n",
    "            loss = self.get_clip_loss(image_vector, text_vector)\n",
    "        \n",
    "        encoder_weights = self.image_encoder.trainable_weights + self.text_encoder.trainable_weights\n",
    "        mlp_weights = self.mlp_image.trainable_weights + self.mlp_text.trainable_weights\n",
    "        pool_weights = self.image_pooler.trainable_weights + self.text_pooler.trainable_weights\n",
    "        trainable_weights = encoder_weights + mlp_weights + pool_weights + self.pe_fn.trainable_weights\n",
    "        \n",
    "        grads = tape.gradient(loss, trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, trainable_weights))\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {self.loss_tracker.name : self.loss_tracker.result()}\n",
    "    \n",
    "    def test_step(self, dataset):\n",
    "        image, text = dataset\n",
    "        image_feature, text_feature = self.image_encoder(image, training = False), self.text_encoder(text, training = False)\n",
    "        batch_size, w, h, dims = ops.shape(image_feature) ; batch_size = ops.shape(image_feature)[0]\n",
    "            \n",
    "        image_feature = ops.reshape(image_feature, [batch_size, w*h, dims])\n",
    "        image_feature = self.pe_fn(image_feature)\n",
    "        \n",
    "        text_vector = keras.layers.GlobalAveragePooling1D()(text_feature)\n",
    "        if len(ops.shape(image_feature)) == 3:\n",
    "            image_vector = keras.layers.GlobalAveragePooling1D()(image_feature)\n",
    "        elif len(ops.shape(image_feature)) == 4:\n",
    "            image_vector = keras.layers.GlobalAveragePooling2D()(image_feature)\n",
    "\n",
    "        image_vector, text_vector = self.mlp_image(image_vector, training = False), self.mlp_text(text_vector, training = False)\n",
    "        image_vector, text_vector = self.image_pooler([image_vector, image_feature], training = False)[0], self.text_pooler([text_vector, text_feature], training = False)[0]\n",
    "        loss = self.get_clip_loss(image_vector, text_vector)\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {self.loss_tracker.name : self.loss_tracker.result()}\n",
    "    def call(self, dataset):\n",
    "        return self.test_step(dataset)\n",
    "    def get_full_model(self):\n",
    "        inputs = self.image_encoder.inputs\n",
    "        feature = self.image_encoder.output\n",
    "        if len(ops.shape(feature)) == 4:\n",
    "            batch_size, w, h, dims = ops.shape(feature)\n",
    "            batch_size = ops.shape(feature)[0]\n",
    "            feature = ops.reshape(feature, [-1, w*h, dims])\n",
    "        feature = self.pe_fn(feature)\n",
    "        image_vector = keras.layers.GlobalAveragePooling1D()(feature)\n",
    "        z_image = self.mlp_image(image_vector)\n",
    "        outputs = self.image_pooler([z_image, feature])\n",
    "        return keras.Model(inputs, outputs,\n",
    "                          name = f\"FullModel_{self.image_encoder.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "96e2ac1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T17:51:25.518352Z",
     "iopub.status.busy": "2024-07-05T17:51:25.518043Z",
     "iopub.status.idle": "2024-07-05T17:51:25.544768Z",
     "shell.execute_reply": "2024-07-05T17:51:25.543942Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.054858,
     "end_time": "2024-07-05T17:51:25.546576",
     "exception": false,
     "start_time": "2024-07-05T17:51:25.491718",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SigLIP(keras.Model):\n",
    "    def __init__(self, image_encoder, text_encoder,\n",
    "                embed_dims, pool_heads = 8, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.image_encoder = image_encoder\n",
    "        self.text_encoder = text_encoder\n",
    "        self.pe_fn = keras_nlp.layers.RotaryEmbedding(name = \"RotatoryPE\")\n",
    "        self.embed_dims = embed_dims\n",
    "        self.mlp_image = keras.layers.Dense(units = embed_dims)\n",
    "        self.mlp_text = keras.layers.Dense(units = embed_dims)\n",
    "        self.image_pooler = AttentionPooling(pool_heads, embed_dims, name = \"ImageAttentionPooler\")\n",
    "        self.text_pooler = AttentionPooling(pool_heads, embed_dims, name = \"TextAttentionPooler\")\n",
    "        self.loss_tracker = keras.metrics.Mean(name = \"SigLIP_loss\")\n",
    "        \n",
    "        self.t = tf.Variable(1.0, trainable = True, dtype = \"float32\")\n",
    "        self.b = tf.Variable(0.0, trainable = True, dtype = \"float32\")\n",
    "    def get_config(self):\n",
    "        return {\"Image encoder name\": self.image_encoder.name,\n",
    "               \"Text encoder name\" : self.text_encoder.name,\n",
    "               \"embed_dims\" : self.embed_dims,\n",
    "               \"SSL_method\" : \"SigLIP\"}\n",
    "    \n",
    "    def compute_loss(self, image_vector, text_vector):\n",
    "        batch_size = ops.shape(image_vector)[0]\n",
    "        z_img = keras.utils.normalize(image_vector, axis = -1, order = 2)\n",
    "        z_text = keras.utils.normalize(text_vector, axis = -1, order = 2)\n",
    "        z_img = tf.cast(z_img, tf.float32)\n",
    "        z_text = tf.cast(z_text, tf.float32)\n",
    "        cor_mat = ops.dot(z_img, ops.transpose(z_text)) * ops.exp(self.t) + self.b\n",
    "        cor_mat = ops.reshape(cor_mat, [batch_size, batch_size])\n",
    "        \n",
    "        pseudo_label = ops.zeros_like(cor_mat)\n",
    "        diags = tf.linalg.diag_part(pseudo_label) + 2.0\n",
    "        pseudo_label = tf.linalg.set_diag(pseudo_label, diags)\n",
    "        pseudo_label = pseudo_label - ops.ones_like(pseudo_label)\n",
    "        \n",
    "        loss = -ops.mean(ops.log_sigmoid(ops.multiply(cor_mat, pseudo_label)))\n",
    "        return loss\n",
    "        \n",
    "    def train_step(self, dataset):\n",
    "        image, text = dataset\n",
    "        with tf.GradientTape() as tape: \n",
    "            image_feature, text_feature = self.image_encoder(image, training = True), self.text_encoder(text, training = True)\n",
    "            batch_size, w, h, dims = ops.shape(image_feature) ; batch_size = ops.shape(image_feature)[0]\n",
    "            \n",
    "            image_feature = ops.reshape(image_feature, [batch_size, w*h, dims])\n",
    "            image_feature = self.pe_fn(image_feature)\n",
    "            \n",
    "            text_vector = keras.layers.GlobalAveragePooling1D()(text_feature)\n",
    "            if len(ops.shape(image_feature)) == 3:\n",
    "                image_vector = keras.layers.GlobalAveragePooling1D()(image_feature)\n",
    "            elif len(ops.shape(image_feature)) == 4:\n",
    "                image_vector = keras.layers.GlobalAveragePooling2D()(image_feature)\n",
    "\n",
    "            image_vector, text_vector = self.mlp_image(image_vector, training = True), self.mlp_text(text_vector, training = True)\n",
    "            image_vector, text_vector = self.image_pooler([image_vector, image_feature], training = True)[0], self.text_pooler([text_vector, text_feature], training = True)[0]\n",
    "            loss = 0.5*(self.compute_loss(image_vector, text_vector) + self.compute_loss(text_vector, image_vector))\n",
    "        \n",
    "        encoder_weights = self.image_encoder.trainable_weights + self.text_encoder.trainable_weights\n",
    "        mlp_weights = self.mlp_image.trainable_weights + self.mlp_text.trainable_weights\n",
    "        pool_weights = self.image_pooler.trainable_weights + self.text_pooler.trainable_weights\n",
    "        trainable_weights = encoder_weights + mlp_weights + pool_weights + [self.t, self.b] + self.pe_fn.trainable_weights\n",
    "        \n",
    "        grads = tape.gradient(loss, trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, trainable_weights))\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {self.loss_tracker.name : self.loss_tracker.result()}\n",
    "    \n",
    "    def test_step(self, dataset):\n",
    "        image, text = dataset\n",
    "        image_feature, text_feature = self.image_encoder(image, training = False), self.text_encoder(text, training = False)\n",
    "        batch_size, w, h, dims = ops.shape(image_feature) ; batch_size = ops.shape(image_feature)[0]\n",
    "            \n",
    "        image_feature = ops.reshape(image_feature, [batch_size, w*h, dims])\n",
    "        image_feature = self.pe_fn(image_feature)\n",
    "        text_vector = keras.layers.GlobalAveragePooling1D()(text_feature)\n",
    "        if len(ops.shape(image_feature)) == 3:\n",
    "            image_vector = keras.layers.GlobalAveragePooling1D()(image_feature)\n",
    "        elif len(ops.shape(image_feature)) == 4:\n",
    "            image_vector = keras.layers.GlobalAveragePooling2D()(image_feature)\n",
    "\n",
    "        image_vector, text_vector = self.mlp_image(image_vector, training = False), self.mlp_text(text_vector, training = False)\n",
    "        image_vector, text_vector = self.image_pooler([image_vector, image_feature], training = False)[0], self.text_pooler([text_vector, text_feature], training = False)[0]\n",
    "        loss = 0.5*(self.compute_loss(image_vector, text_vector) + self.compute_loss(text_vector, image_vector))\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {self.loss_tracker.name : self.loss_tracker.result()}\n",
    "    def call(self, dataset):\n",
    "        return self.test_step(dataset)\n",
    "    def get_full_model(self):\n",
    "        inputs = self.image_encoder.inputs\n",
    "        feature = self.image_encoder.output\n",
    "        if len(ops.shape(feature)) == 4:\n",
    "            batch_size, w, h, dims = ops.shape(feature)\n",
    "            batch_size = ops.shape(feature)[0]\n",
    "            feature = ops.reshape(feature, [-1, w*h, dims])\n",
    "        feature = self.pe_fn(feature)\n",
    "        \n",
    "        image_vector = keras.layers.GlobalAveragePooling1D()(feature)\n",
    "        z_image = self.mlp_image(image_vector)\n",
    "        outputs = self.image_pooler([z_image, feature])\n",
    "        return keras.Model(inputs, outputs,\n",
    "                          name = f\"FullModel_{self.image_encoder.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d375def5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T17:51:25.600298Z",
     "iopub.status.busy": "2024-07-05T17:51:25.599990Z",
     "iopub.status.idle": "2024-07-05T17:51:25.635489Z",
     "shell.execute_reply": "2024-07-05T17:51:25.634638Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.065372,
     "end_time": "2024-07-05T17:51:25.637307",
     "exception": false,
     "start_time": "2024-07-05T17:51:25.571935",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SPARC(keras.Model): \n",
    "    #Reference : https://arxiv.org/abs/2401.09865 \"Improving fine-grained understanding in image-text pre-training\"\n",
    "    def __init__(self, image_encoder, text_encoder,\n",
    "                embed_dims, pool_heads = 8, preprocessor = None,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.image_encoder = image_encoder\n",
    "        self.text_encoder = text_encoder\n",
    "        self.preprocessor = preprocessor\n",
    "        self.pe_fn = keras_nlp.layers.RotaryEmbedding(name = \"RotatoryPE\")\n",
    "        self.embed_dims = embed_dims\n",
    "        self.mlp_image = keras.layers.Dense(units = embed_dims)\n",
    "        self.mlp_text = keras.layers.Dense(units = embed_dims)\n",
    "        self.image_pooler = AttentionPooling(pool_heads, embed_dims, name = \"ImageAttentionPooler\")\n",
    "        self.text_pooler = AttentionPooling(pool_heads, embed_dims, name = \"TextAttentionPooler\")\n",
    "        self.ce_fn = keras.losses.CategoricalCrossentropy(from_logits=True, reduction = None)\n",
    "        self.loss_tracker = keras.metrics.Mean(name = \"SPARC_loss\")\n",
    "        self.global_loss_tracker = keras.metrics.Mean(name = \"SPARC_global_loss\")\n",
    "        self.local_loss_tracker = keras.metrics.Mean(name = \"SPARC_local_loss\")\n",
    "        \n",
    "        self.t = tf.Variable(1.0, trainable = True, dtype = \"float32\")\n",
    "        self.b = tf.Variable(0.0, trainable = True, dtype = \"float32\")\n",
    "        self.threshold = tf.Variable(0.5, trainable = True, dtype = \"float32\")\n",
    "        \n",
    "    def get_config(self):\n",
    "        return {\"Image encoder name\": self.image_encoder.name,\n",
    "               \"Text encoder name\" : self.text_encoder.name,\n",
    "               \"embed_dims\" : self.embed_dims,\n",
    "               \"SSL_method\" : \"SPARC\"}\n",
    "    \n",
    "    def compute_global_loss(self, image_vector, text_vector): #<- SigLIP\n",
    "        batch_size = ops.shape(image_vector)[0]\n",
    "        z_img = keras.utils.normalize(image_vector, axis = -1, order = 2)\n",
    "        z_text = keras.utils.normalize(text_vector, axis = -1, order = 2)\n",
    "        z_img = tf.cast(z_img, tf.float32)\n",
    "        z_text = tf.cast(z_text, tf.float32)\n",
    "        cor_mat = ops.dot(z_img, ops.transpose(z_text)) * ops.exp(self.t) + self.b\n",
    "        cor_mat = ops.reshape(cor_mat, [batch_size, batch_size])\n",
    "        \n",
    "        pseudo_label = ops.zeros_like(cor_mat)\n",
    "        diags = tf.linalg.diag_part(pseudo_label) + 2.0\n",
    "        pseudo_label = tf.linalg.set_diag(pseudo_label, diags)\n",
    "        pseudo_label = pseudo_label - ops.ones_like(pseudo_label)\n",
    "        \n",
    "        loss = -ops.mean(ops.log_sigmoid(ops.multiply(cor_mat, pseudo_label)))\n",
    "        loss = tf.cast(loss, tf.float32)\n",
    "        return loss\n",
    "    \n",
    "    def text_compare(self, text_label, text_logit, mask = None):\n",
    "        loss = self.ce_fn(text_label, text_logit)\n",
    "        loss = tf.cast(loss, tf.float32)\n",
    "        if mask == None:\n",
    "            return ops.mean(loss)\n",
    "        else:\n",
    "            loss *= tf.cast(mask, tf.float32)\n",
    "            loss = (ops.sum(loss))/(ops.sum(tf.cast(mask, tf.float32)) + 1e-4)\n",
    "            return loss\n",
    "        \n",
    "    def compute_loss(self, image_sequence, text_sequence, mask = None):\n",
    "        batch_size = ops.shape(image_sequence)[0]\n",
    "        _, token_len, dim_ = ops.shape(text_sequence)\n",
    "        if len(ops.shape(image_sequence)) == 3:\n",
    "            _, image_len, dim_ = ops.shape(image_sequence)\n",
    "        elif len(ops.shape(image_sequence)) == 4:\n",
    "            _, w, h, dim_ = ops.shape(image_sequence)\n",
    "            image_sequence = ops.reshape(image_sequence, [batch_size, w*h, dim_])\n",
    "            image_len = w*h\n",
    "            \n",
    "        image_sequence, text_sequence = self.mlp_image(image_sequence), self.mlp_text(text_sequence) \n",
    "        image_sequence = self.pe_fn(image_sequence)\n",
    "        # [batch, image_len, embed_dims] / [batch, token_len, embed_dims], respectively.\n",
    "        \n",
    "        z_image, att_weight = self.image_pooler([keras.layers.GlobalAveragePooling1D()(image_sequence),\n",
    "                                                     image_sequence])\n",
    "        z_text, att_weight_ = self.text_pooler([keras.layers.GlobalAveragePooling1D()(text_sequence),\n",
    "                                                     text_sequence])\n",
    "        #1. global alignment loss\n",
    "        global_loss = 0.5*(self.compute_global_loss(z_image, z_text) + self.compute_global_loss(z_text, z_image))\n",
    "        #2. Fine Grained local loss\n",
    "        \n",
    "        #a. get similarity matrix b/w text sequence and image sequence\n",
    "        sim_matrix = ops.einsum(\"atd, aid -> ati\", text_sequence, image_sequence) #batch, token_len, image_len\n",
    "        sim_matrix = (sim_matrix - ops.min(sim_matrix, axis = -1, keepdims = True)) / (1e-4 + ops.max(sim_matrix, axis = -1, keepdims = True) - ops.min(sim_matrix, axis = -1, keepdims = True))\n",
    "        sim_matrix = tf.cast(sim_matrix, tf.float32)\n",
    "        \n",
    "        sim_matrix = ops.clip(sim_matrix, self.threshold, 1e4)\n",
    "        attended_text_sequence = ops.einsum(\"ati, aid -> atd\", sim_matrix, image_sequence)\n",
    "        \n",
    "        text_logit = ops.einsum(\"atd, aqd -> atq\", \n",
    "                                keras.utils.normalize(text_sequence, axis = -1, order = 2), \n",
    "                                keras.utils.normalize(attended_text_sequence, axis = -1, order = 2))/self.t\n",
    "        \n",
    "        pseudo_text_label = ops.tile(ops.expand_dims(ops.eye(token_len), axis = 0),\n",
    "                                     [batch_size,1,1])\n",
    "        local_loss = self.text_compare(pseudo_text_label, text_logit, mask)\n",
    "        loss = 0.5*global_loss + 1.0*local_loss\n",
    "        return loss, global_loss, local_loss\n",
    "        \n",
    "    def train_step(self, dataset):\n",
    "        image, text = dataset\n",
    "        if self.preprocessor != None:\n",
    "            text_mask = self.preprocessor(text)[\"padding_mask\"]\n",
    "            text_mask = tf.cast(text_mask, dtype = tf.int32)\n",
    "        else:\n",
    "            text_mask = None\n",
    "        with tf.GradientTape() as tape: \n",
    "            image_feature, text_feature = self.image_encoder(image, training = True), self.text_encoder(text, training = True)\n",
    "            loss, global_loss, local_loss = self.compute_loss(image_feature, text_feature, text_mask)\n",
    "        \n",
    "        encoder_weights = self.image_encoder.trainable_weights + self.text_encoder.trainable_weights\n",
    "        mlp_weights = self.mlp_image.trainable_weights + self.mlp_text.trainable_weights\n",
    "        pool_weights = self.image_pooler.trainable_weights + self.text_pooler.trainable_weights\n",
    "        trainable_weights = encoder_weights + mlp_weights + pool_weights + [self.t, self.b] + self.pe_fn.trainable_weights\n",
    "        \n",
    "        grads = tape.gradient(loss, trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, trainable_weights))\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.global_loss_tracker.update_state(global_loss)\n",
    "        self.local_loss_tracker.update_state(local_loss)\n",
    "        return {self.loss_tracker.name : self.loss_tracker.result(),\n",
    "               self.global_loss_tracker.name : self.global_loss_tracker.result(),\n",
    "               self.local_loss_tracker.name : self.local_loss_tracker.result()\n",
    "               }\n",
    "    \n",
    "    def test_step(self, dataset):\n",
    "        image, text = dataset\n",
    "        if self.preprocessor != None:\n",
    "            text_mask = self.preprocessor(text)[\"padding_mask\"]\n",
    "            text_mask = tf.cast(text_mask, tf.int32)\n",
    "        else:\n",
    "            text_mask = None\n",
    "            \n",
    "        image_feature, text_feature = self.image_encoder(image, training = False), self.text_encoder(text, training = False)\n",
    "        loss, global_loss, local_loss = self.compute_loss(image_feature, text_feature, text_mask)\n",
    "        \n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.global_loss_tracker.update_state(global_loss)\n",
    "        self.local_loss_tracker.update_state(local_loss)\n",
    "        return {self.loss_tracker.name : self.loss_tracker.result(),\n",
    "               self.global_loss_tracker.name : self.global_loss_tracker.result(),\n",
    "               self.local_loss_tracker.name : self.local_loss_tracker.result()\n",
    "               }\n",
    "    \n",
    "    def call(self, dataset):\n",
    "        return self.test_step(dataset)\n",
    "    def get_full_model(self):\n",
    "        inputs = self.image_encoder.inputs\n",
    "        feature = self.image_encoder.output\n",
    "        feature = self.mlp_image(feature)\n",
    "        if len(ops.shape(feature)) == 4:\n",
    "            batch_size, w, h, dims = ops.shape(feature)\n",
    "            batch_size = ops.shape(feature)[0]\n",
    "            feature = ops.reshape(feature, [-1, w*h, dims])\n",
    "        feature = self.pe_fn(feature)\n",
    "        \n",
    "        image_vector = keras.layers.GlobalAveragePooling1D()(feature)\n",
    "        outputs = self.image_pooler([image_vector, feature])\n",
    "        return keras.Model(inputs, outputs,\n",
    "                          name = f\"FullModel_{self.image_encoder.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8dc991b",
   "metadata": {
    "papermill": {
     "duration": 0.025018,
     "end_time": "2024-07-05T17:51:25.687734",
     "exception": false,
     "start_time": "2024-07-05T17:51:25.662716",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "200cadd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T17:51:25.739480Z",
     "iopub.status.busy": "2024-07-05T17:51:25.739160Z",
     "iopub.status.idle": "2024-07-05T17:51:25.753127Z",
     "shell.execute_reply": "2024-07-05T17:51:25.752287Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.041903,
     "end_time": "2024-07-05T17:51:25.754957",
     "exception": false,
     "start_time": "2024-07-05T17:51:25.713054",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def att_visualize(model, images, res, thresholding = True,\n",
    "                 impose_alpha = 0.5):\n",
    "    # Use jet colormap to colorize heatmap\n",
    "    try:\n",
    "        model = model.get_full_model()\n",
    "    except Exception as e:\n",
    "        print(\"Using Raw model with Att pooling\",\"\\n\",\"Possible error:\",\"\\n\",e)\n",
    "        pass\n",
    "    try:\n",
    "        outputs = model.predict_on_batch(images) \n",
    "        att_weights = outputs[-1]\n",
    "        att_weights_ = att_weights[:, :, 0, :] #batch, heads, cls_token, w*h\n",
    "        _, heads, token_length = ops.shape(att_weights_) ; batch_size = ops.shape(att_weights_)[0]\n",
    "        token_length = tf.cast(token_length, tf.float32)\n",
    "\n",
    "        w = tf.cast(tf.math.sqrt(token_length), tf.int32)\n",
    "        heatmap = tf.reshape(att_weights_, [batch_size, heads, w, w])\n",
    "        M = tf.reduce_max(heatmap, axis = [2, 3], keepdims = True)\n",
    "        m = tf.reduce_min(heatmap, axis = [2, 3], keepdims = True)\n",
    "        heatmap = (heatmap - m) / (M-m + 1e-5)\n",
    "        if thresholding:\n",
    "            threshold = ops.median(heatmap, [2,3], keepdims = True)\n",
    "            heatmap = ops.where(heatmap < threshold, 0.0, heatmap)\n",
    "        else:\n",
    "            pass\n",
    "        heatmap = ops.reshape(heatmap, [-1, w, w])\n",
    "        heatmap = np.array(255.0*heatmap).astype(\"uint8\")\n",
    "        # Use jet colormap to colorize heatmap\n",
    "        cmap = mpl.colormaps[\"jet\"]\n",
    "        # Use RGB values of the colormap\n",
    "        _colors = cmap(np.arange(256))[:, :3]\n",
    "        heatmap = _colors[heatmap]\n",
    "        \n",
    "        heatmap = [keras.utils.array_to_img(h) for h in heatmap]\n",
    "        heatmap = [h.resize((res,res)) for h in heatmap]\n",
    "        heatmap = [keras.utils.img_to_array(h) for h in heatmap]\n",
    "        heatmap = np.array(heatmap)\n",
    "        heatmap = ops.reshape(heatmap, [batch_size, heads, res, res, 3])\n",
    "        try:\n",
    "            images = tf.image.grayscale_to_rgb(images)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "        imposed = ((1-impose_alpha)*tf.cast(images[:, tf.newaxis, ...], tf.float32)) + (impose_alpha*tf.cast(heatmap, tf.float32))\n",
    "        imposed = tf.cast(imposed, tf.uint8)\n",
    "        tf.keras.backend.clear_session()\n",
    "        return imposed\n",
    "    except Exception as e:\n",
    "        print(\"Error raised during visualization\",\"\\n\",e)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee32fc9f",
   "metadata": {
    "papermill": {
     "duration": 0.025301,
     "end_time": "2024-07-05T17:51:25.805939",
     "exception": false,
     "start_time": "2024-07-05T17:51:25.780638",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Test - drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a53151a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T17:51:25.857794Z",
     "iopub.status.busy": "2024-07-05T17:51:25.857511Z",
     "iopub.status.idle": "2024-07-05T17:51:25.862229Z",
     "shell.execute_reply": "2024-07-05T17:51:25.861398Z"
    },
    "papermill": {
     "duration": 0.032974,
     "end_time": "2024-07-05T17:51:25.864112",
     "exception": false,
     "start_time": "2024-07-05T17:51:25.831138",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_drive = 0\n",
    "mim = 0\n",
    "other = 0\n",
    "if test_drive:\n",
    "    heads = 8\n",
    "    res = 384\n",
    "    batch_size = 32\n",
    "    grayscale = True\n",
    "    if grayscale:\n",
    "        att_depth = 1\n",
    "    else:\n",
    "        att_depth = 4\n",
    "    mode = \"gMLP\"\n",
    "    patch_size = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5dd4f613",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-07-05T17:51:25.916028Z",
     "iopub.status.busy": "2024-07-05T17:51:25.915743Z",
     "iopub.status.idle": "2024-07-05T17:51:25.923878Z",
     "shell.execute_reply": "2024-07-05T17:51:25.923038Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.036352,
     "end_time": "2024-07-05T17:51:25.925811",
     "exception": false,
     "start_time": "2024-07-05T17:51:25.889459",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if test_drive:\n",
    "    gc_configs = get_gcvit_configs(res, 64, \"GC_ViT_xxtiny\")\n",
    "    gc_configs[\"level_depth\"] = [1,1,2,2]\n",
    "    print(gc_configs)\n",
    "    dataset = tfds.load(\"beans\", split = 'test')\n",
    "    def map_fn(dataset):\n",
    "        image = dataset[\"image\"]\n",
    "        image = tf.image.rgb_to_grayscale(image)\n",
    "        image = tf.image.resize_with_pad(image, res, res, antialias = True)\n",
    "        image = tf.cast(image, tf.uint8)\n",
    "        return image\n",
    "    train_ds = dataset.map(map_fn).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    mask_map_fn = get_masking_fn(grayscale = grayscale, masking_rate = 0.6, patch_size = patch_size)\n",
    "    \n",
    "    train_ds_masked = train_ds.map(mask_map_fn).prefetch(tf.data.AUTOTUNE).repeat()\n",
    "    \n",
    "    ssl_fn = get_map_fn(res, \"supervised\", \"ssl\",2, grayscale = grayscale)\n",
    "    train_ds = train_ds.unbatch().map(ssl_fn).batch(batch_size).prefetch(tf.data.AUTOTUNE).repeat()\n",
    "    for imgs in train_ds.take(1):\n",
    "        sets = imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "135fbfd6",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-07-05T17:51:25.978365Z",
     "iopub.status.busy": "2024-07-05T17:51:25.978096Z",
     "iopub.status.idle": "2024-07-05T17:51:25.989741Z",
     "shell.execute_reply": "2024-07-05T17:51:25.989042Z"
    },
    "papermill": {
     "duration": 0.040476,
     "end_time": "2024-07-05T17:51:25.991496",
     "exception": false,
     "start_time": "2024-07-05T17:51:25.951020",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if test_drive and mim:\n",
    "    tf.keras.backend.clear_session()\n",
    "    for data in train_ds_masked.take(1):\n",
    "        mask_sequence, mask_image, image, original_image = data\n",
    "        mask_sets = data\n",
    "    for index in range(10):\n",
    "        print(mask_sequence[index])\n",
    "        fig, axes = plt.subplots(1,2, figsize=(10,5))\n",
    "        axes = axes.flatten()\n",
    "        axes[0].imshow(tf.cast(mask_image[index], \"uint8\"))\n",
    "        axes[1].imshow(tf.cast(image[index], \"uint8\"))\n",
    "        plt.show()\n",
    "    \n",
    "    vit_model = get_metaformer(mode, res, grayscale = grayscale, \n",
    "                               att_depth = att_depth, att_heads = heads, \n",
    "                               att_dims = heads * 64, \n",
    "                               embed_dims = heads * 64, patch_size = patch_size,\n",
    "                              register_tokens = 8,\n",
    "                               pretrained_encoder = None, return_patches = True\n",
    "                              )\n",
    "    ssl = MixedMIM(vit_model, grayscale = grayscale, patch_size = patch_size) #SimMIM or MixedMIM\n",
    "    ssl.compile(optimizer = keras.optimizers.AdamW(learning_rate = 1e-4, #clipnorm = 1.0,\n",
    "                                                  #use_ema = True,\n",
    "                                                  #gradient_accumulation_steps = 16\n",
    "                                                  ),\n",
    "                jit_compile = False)\n",
    "    print(ssl(data))\n",
    "    ssl.summary()\n",
    "    result = ssl.fit(train_ds_masked, epochs = 1, steps_per_epoch = 50)\n",
    "    print(result)\n",
    "    \n",
    "    try:\n",
    "        imgs = att_visualize(ssl.get_full_model(res=res), sets[0], res = res,\n",
    "                            thresholding = 1)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        imgs = att_visualize(ssl.feature_extractor, sets[0], res = res,\n",
    "                            thresholding = 1)\n",
    "    for row in range(10):\n",
    "        fig, axes = plt.subplots(1, heads, figsize = (5*heads,5))\n",
    "        axes = axes.flatten()\n",
    "        for col in range(heads):\n",
    "            axes[col].imshow(imgs[row, col, ...])\n",
    "    plt.show()\n",
    "    \n",
    "    token, patches, weight = ssl.feature_extractor(data[2])\n",
    "    print(ops.std(weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8db7619b",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-07-05T17:51:26.043440Z",
     "iopub.status.busy": "2024-07-05T17:51:26.043111Z",
     "iopub.status.idle": "2024-07-05T17:51:26.052987Z",
     "shell.execute_reply": "2024-07-05T17:51:26.052135Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.038171,
     "end_time": "2024-07-05T17:51:26.054904",
     "exception": false,
     "start_time": "2024-07-05T17:51:26.016733",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if test_drive and other:\n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    vit_model = get_metaformer(mode, res, grayscale = grayscale, \n",
    "                               att_depth = att_depth, att_heads = heads, \n",
    "                               att_dims = heads * 64, \n",
    "                               embed_dims = heads * 64, patch_size = patch_size,\n",
    "                              register_tokens = 4,\n",
    "                              #pretrained_encoder = keras.applications.EfficientNetV2B1(input_shape = [res,res,3], include_top = False),\n",
    "                              pretrained_encoder = None, \n",
    "                               return_patches = True\n",
    "                              )\n",
    "    vit_model.summary()\n",
    "    ssl = VICRegL(feature_extractor = vit_model, student_model = \"distil\",\n",
    "               embed_dims = int(1024*1.0), \n",
    "              multiview = False)\n",
    "    \n",
    "    ssl.compile(optimizer = keras.optimizers.AdamW(learning_rate = 1e-4, clipnorm = 1.0,\n",
    "                                                  #use_ema = True,\n",
    "                                                  #gradient_accumulation_steps = 16\n",
    "                                                  ),\n",
    "                jit_compile = False)\n",
    "    \n",
    "    print(ssl(sets))\n",
    "    ssl.summary()\n",
    "    result = ssl.fit(train_ds, epochs = 1, steps_per_epoch = 50)\n",
    "    print(result)\n",
    "    try:\n",
    "        imgs = att_visualize(ssl.get_full_model(res=res), sets[0], res = res,\n",
    "                            thresholding = 1)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        imgs = att_visualize(ssl.feature_extractor, sets[0], res = res,\n",
    "                            thresholding = 1)\n",
    "    for row in range(12):\n",
    "        fig, axes = plt.subplots(1, heads, figsize = (5*heads,5))\n",
    "        axes = axes.flatten()\n",
    "        for col in range(heads):\n",
    "            axes[col].imshow(imgs[row, col, ...])\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5051531,
     "sourceId": 8471595,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 32.993597,
   "end_time": "2024-07-05T17:51:29.770543",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-07-05T17:50:56.776946",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
