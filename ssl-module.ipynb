{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fc3e458",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-05-17T03:32:56.572574Z",
     "iopub.status.busy": "2024-05-17T03:32:56.572129Z",
     "iopub.status.idle": "2024-05-17T03:33:19.736857Z",
     "shell.execute_reply": "2024-05-17T03:33:19.735670Z"
    },
    "papermill": {
     "duration": 23.185896,
     "end_time": "2024-05-17T03:33:19.739433",
     "exception": false,
     "start_time": "2024-05-17T03:32:56.553537",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-17 03:32:59.400101: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-17 03:32:59.400236: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-17 03:32:59.553488: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirements loaded, keras : v3.2.1, Tensorflow : v2.15.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "seed = 2024\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ML tools \n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras_nlp\n",
    "import keras_cv\n",
    "from keras import ops\n",
    "\n",
    "keras.utils.set_random_seed(seed)\n",
    "\n",
    "import cv2\n",
    "import tensorflow_io as tfio\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from keras import Input, Model\n",
    "from keras.models import load_model\n",
    "\n",
    "from keras.layers import Conv2D, DepthwiseConv2D, Dense, Activation, BatchNormalization, LayerNormalization, MultiHeadAttention, Embedding, Subtract, Add, Multiply, GlobalAveragePooling2D, GlobalAveragePooling1D, LayerNormalization\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.applications import *\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "from tqdm.notebook import tqdm\n",
    "print(f\"Requirements loaded, keras : v{keras.__version__}, Tensorflow : v{tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed9d771",
   "metadata": {
    "papermill": {
     "duration": 0.016457,
     "end_time": "2024-05-17T03:33:19.774021",
     "exception": false,
     "start_time": "2024-05-17T03:33:19.757564",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataloader setting\n",
    "- original dataset(input as tf.ds) : image only or image/label paired dataset\n",
    "- target dataset(output as tf.ds) : (image1, image2), (image1, image2, image3), (image1, image2, label)\n",
    "    - Also, implement the mixing function : merges 2 homogenous dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af3aef09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T03:33:19.809229Z",
     "iopub.status.busy": "2024-05-17T03:33:19.808173Z",
     "iopub.status.idle": "2024-05-17T03:33:19.869363Z",
     "shell.execute_reply": "2024-05-17T03:33:19.868284Z"
    },
    "papermill": {
     "duration": 0.082105,
     "end_time": "2024-05-17T03:33:19.872524",
     "exception": false,
     "start_time": "2024-05-17T03:33:19.790419",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "basic_aug = keras.Sequential([keras.layers.RandomFlip(), keras.layers.RandomRotation(factor = 0.25)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec30c620",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T03:33:19.913982Z",
     "iopub.status.busy": "2024-05-17T03:33:19.913542Z",
     "iopub.status.idle": "2024-05-17T03:33:19.960424Z",
     "shell.execute_reply": "2024-05-17T03:33:19.959328Z"
    },
    "papermill": {
     "duration": 0.068213,
     "end_time": "2024-05-17T03:33:19.963020",
     "exception": false,
     "start_time": "2024-05-17T03:33:19.894807",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandAug Component in this SSL module :  ['equalization', 'solarization', 'random_color_degeneration', 'random_contrast', 'random_brightness', 'random_shear', 'random_shear_1', 'random_translation', 'random_translation_1', 'grid_mask']\n"
     ]
    }
   ],
   "source": [
    "aug_layers = keras_cv.layers.RandAugment.get_standard_policy(\n",
    "    value_range=(0, 255), magnitude=0.2, magnitude_stddev=0.15\n",
    ")\n",
    "aug_layers = aug_layers + [keras_cv.layers.GridMask()]\n",
    "aug_layers.pop(0)\n",
    "print(\"RandAug Component in this SSL module : \",[layer.name for layer in aug_layers])\n",
    "randaug = keras_cv.layers.RandomAugmentationPipeline(\n",
    "    layers=aug_layers, augmentations_per_image=3, seed = seed, auto_vectorize = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "217820ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T03:33:19.998638Z",
     "iopub.status.busy": "2024-05-17T03:33:19.997317Z",
     "iopub.status.idle": "2024-05-17T03:33:20.005513Z",
     "shell.execute_reply": "2024-05-17T03:33:20.004577Z"
    },
    "papermill": {
     "duration": 0.028447,
     "end_time": "2024-05-17T03:33:20.008012",
     "exception": false,
     "start_time": "2024-05-17T03:33:19.979565",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_other_augs_(res):\n",
    "\n",
    "    crop_resize_global = keras.Sequential([keras.layers.RandomCrop(int(0.8*res), int(0.8*res)),\n",
    "                                        keras.layers.Resizing(res, res)\n",
    "                                       ])\n",
    "    crop_resize_local = keras.Sequential([keras.layers.RandomCrop(int(0.5*res), int(0.5*res)),\n",
    "                                        keras.layers.Resizing(res, res)\n",
    "                                       ])\n",
    "    crop_resize_medium = keras.Sequential([keras.layers.RandomCrop(int(0.7*res), int(0.7*res)),\n",
    "                                        keras.layers.Resizing(res, res)\n",
    "                                       ])\n",
    "    return crop_resize_global, crop_resize_medium, crop_resize_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0a70e64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T03:33:20.043071Z",
     "iopub.status.busy": "2024-05-17T03:33:20.042646Z",
     "iopub.status.idle": "2024-05-17T03:33:20.055514Z",
     "shell.execute_reply": "2024-05-17T03:33:20.054464Z"
    },
    "papermill": {
     "duration": 0.033223,
     "end_time": "2024-05-17T03:33:20.057806",
     "exception": false,
     "start_time": "2024-05-17T03:33:20.024583",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_map_fn(res, input_type = None, output_type = None, n_view = 2):\n",
    "    # input_type as \"supervised\", \"with_label\", \"with label\" / OR / \"unsupervised\", \"without label\", \"without_label\", \"image_only\", \"image\"\n",
    "    # output_ytpe as \"ssl\", \"ssl_with_label\" / if ssl, image output : image1, image2, ..., image_n_view\n",
    "    assert n_view >= 2, \"Augmented View number must be >= 2\"\n",
    "    assert input_type in [\"supervised\", \"with_label\", \"with label\", \"unsupervised\", \"without label\", \"without_label\", \"image_only\", \"image\"], 'Pick one of input_type : \"supervised\", \"with_label\", \"with label\",\\n \"unsupervised\", \"without label\", \"without_label\", \"image_only\", \"image\"'\n",
    "    assert output_type in [\"ssl\", \"ssl_with_label\"], 'Pick one of output_type : \"ssl\", \"ssl_with_label\"'\n",
    "    crop_resize_global, crop_resize_medium, crop_resize_local = get_other_augs_(res)\n",
    "    def map_fn(input_data):\n",
    "        if input_type in [\"supervised\", \"with_label\", \"with label\"]:\n",
    "            image, label = input_data\n",
    "        elif input_type in [\"unsupervised\", \"without label\", \"without_label\", \"image_only\", \"image\"]:\n",
    "            image = input_data\n",
    "        batch_size = ops.shape(image)[0]\n",
    "        \n",
    "        aug_ = basic_aug(image)\n",
    "        aug_ = randaug(aug_)\n",
    "        \n",
    "        global_image = crop_resize_global(aug_)\n",
    "        \n",
    "        if n_view == 2:\n",
    "            if output_type == \"ssl\":\n",
    "                return (image, global_image)\n",
    "            elif output_type == \"ssl_with_label\":\n",
    "                return (image, global_image, label)\n",
    "        elif n_view == 3:\n",
    "            medium_image = crop_resize_medium(aug_)\n",
    "            if output_type == \"ssl\":\n",
    "                return (image, global_image, medium_image)\n",
    "            elif output_type == \"ssl_with_label\":\n",
    "                return (image, global_image, medium_image, label)\n",
    "        elif n_view > 3:\n",
    "            medium_image = crop_resize_medium(aug_)\n",
    "            local_images = [crop_resize_local(image) for _ in range(n_view - 3)]\n",
    "            local_images = tuple(local_images)\n",
    "            outputs = (image, global_image, medium_image) + local_images\n",
    "            if output_type == \"ssl\":\n",
    "                return outputs\n",
    "            elif output_type == \"ssl_with_label\":\n",
    "                return outputs+(label)\n",
    "    return map_fn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452d6a2e",
   "metadata": {
    "papermill": {
     "duration": 0.016633,
     "end_time": "2024-05-17T03:33:20.091749",
     "exception": false,
     "start_time": "2024-05-17T03:33:20.075116",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28444863",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-05-17T03:33:20.126685Z",
     "iopub.status.busy": "2024-05-17T03:33:20.126296Z",
     "iopub.status.idle": "2024-05-17T03:33:20.137699Z",
     "shell.execute_reply": "2024-05-17T03:33:20.136689Z"
    },
    "papermill": {
     "duration": 0.031872,
     "end_time": "2024-05-17T03:33:20.140168",
     "exception": false,
     "start_time": "2024-05-17T03:33:20.108296",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_flops(model, model_inputs) -> float:\n",
    "        \"\"\"\n",
    "        Calculate FLOPS [GFLOPs] for a tf.keras.Model or tf.keras.Sequential model\n",
    "        in inference mode. It uses tf.compat.v1.profiler under the hood.\n",
    "        Code reference : https://github.com/tensorflow/tensorflow/issues/32809\n",
    "        \"\"\"\n",
    "        # if not hasattr(model, \"model\"):\n",
    "        #     raise wandb.Error(\"self.model must be set before using this method.\")\n",
    "        \n",
    "        if not isinstance(\n",
    "            model, (tf.keras.models.Sequential, tf.keras.models.Model, keras.models.Model, keras.Sequential, keras.Model)\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                \"Calculating FLOPS is only supported for \"\n",
    "                \"`tf.keras.Model` and `tf.keras.Sequential` instances.\"\n",
    "            )\n",
    "\n",
    "        from tensorflow.python.framework.convert_to_constants import (\n",
    "            convert_variables_to_constants_v2_as_graph,\n",
    "        )\n",
    "\n",
    "        # Compute FLOPs for one sample\n",
    "        batch_size = 1\n",
    "        inputs = [\n",
    "            tf.TensorSpec([batch_size] + inp.shape[1:], inp.dtype)\n",
    "            for inp in model_inputs\n",
    "        ]\n",
    "\n",
    "        # convert tf.keras model into frozen graph to count FLOPs about operations used at inference\n",
    "        real_model = tf.function(model).get_concrete_function(inputs)\n",
    "        frozen_func, _ = convert_variables_to_constants_v2_as_graph(real_model)\n",
    "\n",
    "        # Calculate FLOPs with tf.profiler\n",
    "        run_meta = tf.compat.v1.RunMetadata()\n",
    "        opts = (\n",
    "            tf.compat.v1.profiler.ProfileOptionBuilder(\n",
    "                tf.compat.v1.profiler.ProfileOptionBuilder().float_operation()\n",
    "            )\n",
    "            .with_empty_output()\n",
    "            .build()\n",
    "        )\n",
    "\n",
    "        flops = tf.compat.v1.profiler.profile(\n",
    "            graph=frozen_func.graph, run_meta=run_meta, cmd=\"scope\", options=opts\n",
    "        )\n",
    "\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "\n",
    "        # convert to GFLOPs\n",
    "        return (flops.total_float_ops / 1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0dc63134",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T03:33:20.177156Z",
     "iopub.status.busy": "2024-05-17T03:33:20.176744Z",
     "iopub.status.idle": "2024-05-17T03:33:20.198832Z",
     "shell.execute_reply": "2024-05-17T03:33:20.197798Z"
    },
    "papermill": {
     "duration": 0.043645,
     "end_time": "2024-05-17T03:33:20.201312",
     "exception": false,
     "start_time": "2024-05-17T03:33:20.157667",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AttentionPooling(keras.layers.Layer):\n",
    "    def __init__(self, attention_heads, attention_dims = None, bias = False, scale = None, \n",
    "                 dropout_rate = 0.05, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.n_heads = attention_heads\n",
    "        self.n_dims = attention_dims\n",
    "        self.bias = bias\n",
    "        self.scale = scale\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # query, key\n",
    "        query_dims = input_shape[0][-1]\n",
    "        key_length = input_shape[1][1]\n",
    "        \n",
    "        if self.n_dims == None:\n",
    "            embed_dims = query_dims\n",
    "        else:\n",
    "            embed_dims = self.n_dims\n",
    "        self.embed_dims = embed_dims\n",
    "        self.per_head_dims = embed_dims//self.n_heads\n",
    "        \n",
    "        self.scale = self.scale if self.scale != None else embed_dims**-0.5\n",
    "        \n",
    "        self.query_embed_fn = keras.layers.Dense(units = embed_dims, use_bias = self.bias,\n",
    "                                               name = \"Q_Embedding_Dense_layer\")\n",
    "        self.key_embed_fn = keras.layers.Dense(units = embed_dims, use_bias = self.bias,\n",
    "                                               name = \"K_Embedding_Dense_layer\")\n",
    "        self.value_embed_fn = keras.layers.Dense(units = embed_dims, use_bias = self.bias,\n",
    "                                               name = \"V_Embedding_Dense_layer\")\n",
    "        \n",
    "        self.softmax = keras.layers.Activation(\"softmax\", name = \"AttentionWeightSoftmax\")\n",
    "        self.proj = keras.layers.Dense(units = query_dims, use_bias = self.bias, \n",
    "                                      name = \"ProjectToOriginalDimension\")\n",
    "        self.att_dropout = keras.layers.Dropout(self.dropout_rate)\n",
    "        self.proj_dropout = keras.layers.Dropout(self.dropout_rate)\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, inputs, **kwargs):\n",
    "        if len(inputs) == 2:\n",
    "            q, k = inputs\n",
    "            value_ = False\n",
    "        elif len(inputs) == 3:\n",
    "            q, k, v = inputs\n",
    "            value_ = True\n",
    "            \n",
    "        if len(ops.shape(q)) == 2:\n",
    "            q = q[:, tf.newaxis, :]\n",
    "        if len(ops.shape(k)) == 4:\n",
    "            b_, w_, h_, dims_ = ops.shape(k)\n",
    "            k = ops.reshape(k, [b_, w_*h_, dims_])\n",
    "        if (value_) and (len(ops.shape(v))) == 4:\n",
    "            b_, w_, h_, dims_ = ops.shape(v)\n",
    "            v = ops.reshape(v, [b_, w_*h_, dims_])\n",
    "            \n",
    "        batch_size, query_length, q_dims = ops.shape(q)\n",
    "        _, key_length, k_dms = ops.shape(k)\n",
    "        \n",
    "        query = self.query_embed_fn(q) * self.scale #batch, 1(or, query length), q_dims\n",
    "        query = ops.reshape(query, [batch_size, query_length, self.n_heads, self.per_head_dims])\n",
    "        \n",
    "        key = self.key_embed_fn(k)\n",
    "        key = ops.reshape(key, [batch_size, key_length, self.n_heads, self.per_head_dims])\n",
    "        \n",
    "        if value_:\n",
    "            value = self.value_embed_fn(v)#각각 batch, token_length, heads, per_head_dims\n",
    "        else:\n",
    "            value = self.value_embed_fn(k)\n",
    "        value = ops.reshape(value, [batch_size, key_length, self.n_heads, self.per_head_dims])\n",
    "        attention_score = keras.ops.einsum(\"abhd, achd -> ahbc\",\n",
    "                                          query, key) #b = query length, c = key length\n",
    "        attention_weight = self.softmax(attention_score)\n",
    "        attention_weight = self.att_dropout(attention_weight)\n",
    "        self.attention_weight = attention_weight \n",
    "        attended_output = keras.ops.einsum(\"ahbc, achd -> abhd\",\n",
    "                                          attention_weight, value)\n",
    "        attended_output = keras.ops.reshape(attended_output, \n",
    "                                           [batch_size, query_length, self.n_heads*self.per_head_dims]\n",
    "                                           )\n",
    "        attended_output = self.proj(attended_output)\n",
    "        attended_output = self.proj_dropout(attended_output)\n",
    "        return attended_output, attention_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5269e423",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T03:33:20.236260Z",
     "iopub.status.busy": "2024-05-17T03:33:20.235471Z",
     "iopub.status.idle": "2024-05-17T03:33:20.244477Z",
     "shell.execute_reply": "2024-05-17T03:33:20.243436Z"
    },
    "papermill": {
     "duration": 0.028903,
     "end_time": "2024-05-17T03:33:20.246750",
     "exception": false,
     "start_time": "2024-05-17T03:33:20.217847",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PatchEncoder(keras.layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super().__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = keras.layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = keras.layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = ops.expand_dims(\n",
    "            ops.arange(start=0, stop=self.num_patches, step=1), axis=0\n",
    "        )\n",
    "        projected_patches = self.projection(patch)\n",
    "        encoded = projected_patches + self.position_embedding(positions)\n",
    "        return encoded\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"num_patches\": self.num_patches})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a17f4f0",
   "metadata": {
    "papermill": {
     "duration": 0.016154,
     "end_time": "2024-05-17T03:33:20.279362",
     "exception": false,
     "start_time": "2024-05-17T03:33:20.263208",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# General Context Vision Transformer implementation\n",
    "- Reference : [another kaggle notebook, identical implementation](https://www.kaggle.com/code/hskimjjys/general-context-vit-script/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c16b620",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T03:33:20.314508Z",
     "iopub.status.busy": "2024-05-17T03:33:20.314143Z",
     "iopub.status.idle": "2024-05-17T03:33:20.417225Z",
     "shell.execute_reply": "2024-05-17T03:33:20.416100Z"
    },
    "papermill": {
     "duration": 0.124436,
     "end_time": "2024-05-17T03:33:20.420126",
     "exception": false,
     "start_time": "2024-05-17T03:33:20.295690",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SE(keras.layers.Layer):\n",
    "    def __init__(self, output_dim = None, squeeze_rate = 0.25, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.output_dim = output_dim\n",
    "        self.rate = squeeze_rate\n",
    "    def build(self, input_shape) : #batch_size, h, w, dims\n",
    "        if self.output_dim == None:\n",
    "            self.output_dim = input_shape[-1]\n",
    "        else:\n",
    "            pass\n",
    "        self.avg_pool = keras.layers.GlobalAveragePooling2D(keepdims = True, name = \"AvgPooling\")\n",
    "        self.mlps = keras.Sequential([keras.layers.Dense(units = int(self.rate * self.output_dim),\n",
    "                                                        use_bias = False, name = \"Dense1\"),\n",
    "                                      keras.layers.Activation(\"gelu\", name = \"GeluAct\"),\n",
    "                                      keras.layers.Dense(units = self.output_dim, use_bias = False, name = \"Dense2\"),\n",
    "                                      keras.layers.Activation(\"sigmoid\", name = \"Excitation_Sigmoid\")\n",
    "                                     ])\n",
    "        #super().build(input_shape)\n",
    "    def call(self, inputs, **kwargs):\n",
    "        pooled = self.avg_pool(inputs)\n",
    "        weights = self.mlps(pooled)\n",
    "        return inputs * weights\n",
    "    \n",
    "class DownSampler(keras.layers.Layer):\n",
    "    def __init__(self, keepdims = False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.keepdims = keepdims\n",
    "    def build(self, input_shape):\n",
    "        embed_dims = input_shape[-1]\n",
    "        out_dim = embed_dims if self.keepdims else 2*embed_dims\n",
    "        self.fused_mbconv = keras.Sequential([keras.layers.DepthwiseConv2D(kernel_size = 3, padding = 'same', use_bias = False, name = \"DWConv\"),\n",
    "                                             keras.layers.Activation(\"gelu\", name = 'GeluAct'),\n",
    "                                             SE(name = \"SqueezeAndExcitation2D\"),\n",
    "                                             keras.layers.Conv2D(filters = embed_dims, kernel_size = 1, padding = 'same', use_bias = False, name = \"PointWiseConv\")],\n",
    "                                            name = \"Fused_MBConvLayer\")\n",
    "        self.down_conv = keras.layers.Conv2D(filters = out_dim, kernel_size = 3, strides = 2, padding = 'same', use_bias = False, name = \"DownConvolution\")\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon = 1e-5, name = 'LayerNorm1')\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon = 1e-5, name = 'LayerNorm2')\n",
    "    def call(self, inputs, **kwargs):\n",
    "        x = self.layernorm1(inputs)\n",
    "        x += self.fused_mbconv(inputs)\n",
    "        x = self.down_conv(x)\n",
    "        return self.layernorm2(x)\n",
    "    \n",
    "class MLP(keras.layers.Layer):\n",
    "    def __init__(self, middle_dim = None, output_dim = None,\n",
    "                activation = 'gelu', dropout = 0.2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.middle_dim = middle_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.activation = activation\n",
    "        self.dropout_rate = dropout\n",
    "    def build(self, input_shape):\n",
    "        self.input_dims = input_shape[-1]\n",
    "        self.middle_dim = int(1.5*self.input_dims) if self.middle_dim == None else self.middle_dim\n",
    "        self.output_dim = self.input_dims if self.output_dim == None else self.output_dim\n",
    "        self.mlp1 = keras.layers.Dense(units = self.middle_dim, name = \"FirstMLP\")\n",
    "        self.act = keras.layers.Activation(self.activation, name = \"MiddleActivation\")\n",
    "        self.mlp2 = keras.layers.Dense(units = self.output_dim, name = \"SecondMLP\")\n",
    "        self.drop1 = keras.layers.Dropout(self.dropout_rate, name = \"Dropout1\")\n",
    "        self.drop2 = keras.layers.Dropout(self.dropout_rate, name = \"Dropout2\")\n",
    "    def call(self, inputs, **kwargs):\n",
    "        x = self.mlp1(inputs)\n",
    "        x = self.act(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.mlp2(x)\n",
    "        x = self.drop2(x)\n",
    "        return x\n",
    "    \n",
    "class PatchEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, patching_type = \"conv\", #conv or tokenlearner\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.patching_type = patching_type\n",
    "    def build(self, input_shape):\n",
    "        if (self.patching_type == \"tokenlearner\") or (self.patching_type == \"token_learner\"):\n",
    "            self.proj = keras.layers.Conv2D(self.embed_dim, kernel_size = 3, strides = 2, padding = 'same', name = \"projection_conv\") #Overlapping patches\n",
    "            #token learner implementation\n",
    "            batch_size, w, h, filters = input_shape\n",
    "            n_tokens = (w//4) * (h//4) ; self.resized_w, self.resized_h = int(w//4), int(h/4)\n",
    "            self.input_seq_flatten = keras.layers.Reshape([1, -1, self.embed_dim], name = \"image_to_sequence_reshape\")\n",
    "            self.layer_norm = keras.layers.LayerNormalization(epsilon = 1e-5)\n",
    "            self.attention_ops = keras.Sequential([keras.layers.Conv2D(n_tokens, kernel_size = 3, activation = \"gelu\", use_bias = False, padding = 'same'),\n",
    "                                                  keras.layers.Conv2D(n_tokens, kernel_size = 3, activation = \"gelu\", use_bias = False, padding = 'same'),\n",
    "                                                  keras.layers.Conv2D(n_tokens, kernel_size = 3, activation = \"sigmoid\", use_bias = False, padding = 'same'),\n",
    "                                                  keras.layers.Reshape([-1, n_tokens]), #batch_size, HW, n_tokens\n",
    "                                                  keras.layers.Permute([2,1])], #batch_size, n_tokens, HW\n",
    "                                                 name = \"Conv_for_attention_weight\")\n",
    "        else:\n",
    "            self.proj = keras.layers.Conv2D(self.embed_dim, kernel_size = 3, strides = 2, padding = 'same') #Overlapping patches\n",
    "            self.down_sample = DownSampler(keepdims = True, name = \"DownSampler_after_projection\")\n",
    "    def call(self, inputs, **kwargs):\n",
    "        if (self.patching_type == \"tokenlearner\") or (self.patching_type == \"token_learner\"):\n",
    "            #token learner implementation\n",
    "            norm_input = self.layer_norm(inputs)\n",
    "            proj_inputs = self.proj(norm_input)\n",
    "            seq_inputs = self.input_seq_flatten(proj_inputs) #batch, 1, HW, embed_dims\n",
    "            att_weights = self.attention_ops(proj_inputs) #batch, n_tokens, HW\n",
    "            att_weights = ops.expand_dims(att_weights, axis = -1) #batch, n_tokens, HW, 1\n",
    "            attended = att_weights * seq_inputs #batch, n_tokens, HW, embed_dims\n",
    "            attended = ops.mean(attended, axis = 2) #batch, n_tokens, embed_dims\n",
    "            #reshape to 2D array\n",
    "            attended = ops.reshape(attended, [-1, self.resized_w, self.resized_h, self.embed_dim]\n",
    "                                  )\n",
    "            return attended\n",
    "        else:\n",
    "            x = self.proj(inputs)\n",
    "            x = self.down_sample(x)\n",
    "            return x\n",
    "        \n",
    "class FeatExtract(keras.layers.Layer):\n",
    "    def __init__(self, keepdims = False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.keepdims = keepdims\n",
    "    def build(self, input_shape):\n",
    "        batch_size, H, W, embed_dims = input_shape\n",
    "        self.fused_mbconv = keras.Sequential([keras.layers.DepthwiseConv2D(kernel_size = 3, padding = 'same', use_bias = False, name = \"DWConv\"),\n",
    "                                             keras.layers.Activation(\"gelu\", name = 'GeluAct'),\n",
    "                                             SE(name = \"SqueezeAndExcitation2D\"),\n",
    "                                             keras.layers.Conv2D(filters = embed_dims, kernel_size = 1, padding = 'same', use_bias = False, name = \"PointWiseConv\")],\n",
    "                                            name = \"Fused_MBConvLayer\")\n",
    "        if self.keepdims == False:\n",
    "            self.pool = keras.layers.MaxPooling2D(name = \"FeatExtractMaxPool2D\")\n",
    "    def call(self, inputs):\n",
    "        x = inputs + self.fused_mbconv(inputs)\n",
    "        if self.keepdims == False:\n",
    "            return self.pool(x)\n",
    "        return x\n",
    "    \n",
    "class GlobalQueryGenerator(keras.layers.Layer):\n",
    "    def __init__(self, keepdims = False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.keepdims = keepdims #Keepdims는 여기서 0과 1로 이루어진 list도 될 수 있다 -> FeatExtract layer를 keepdims의 원소 갯수만큼 repeat!\n",
    "    def build(self, input_shape):\n",
    "        self.q_generator = keras.Sequential([FeatExtract(keepdims = keepdim, name = f\"FeatureExtraction_{idx+1}\") for idx, keepdim in enumerate(self.keepdims)])\n",
    "    def call(self, inputs):\n",
    "        return self.q_generator(inputs)\n",
    "    \n",
    "class WindowAttention(keras.layers.Layer):\n",
    "    def __init__(self, window_size, n_heads, global_query, #제공된다면 global, 아니라면 local mHSA -> 0 or 1\n",
    "                qkv_bias = True, qk_scale = None,\n",
    "                dropout_rate = 0.05, return_attention_weights = False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.window_size = (window_size, window_size)\n",
    "        self.n_heads = n_heads\n",
    "        self.global_query = global_query\n",
    "        self.bias = qkv_bias\n",
    "        self.scale = qk_scale\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.return_attention_weights = return_attention_weights\n",
    "    def build(self, input_shape) :\n",
    "        #input = [query, key, value]\n",
    "        embed_dims = input_shape[0][-1]\n",
    "        head_dims = embed_dims//self.n_heads\n",
    "        self.scale = self.scale if self.scale != None else embed_dims**-0.5\n",
    "        self.qkv_size = 3-int(self.global_query)\n",
    "        self.qkv_embed_fn = keras.layers.Dense(units = embed_dims * self.qkv_size, use_bias = self.bias,\n",
    "                                 name = \"QKV_Embedding_Dense_layer\")\n",
    "        self.softmax = keras.layers.Activation(\"softmax\", name = \"AttentionWeightSoftmax\") #for attention weight computation\n",
    "        self.proj = keras.layers.Dense(units = embed_dims, use_bias = self.bias, name = \"Projection\")\n",
    "        self.attention_dropout = keras.layers.Dropout(self.dropout_rate)\n",
    "        self.projection_dropout = keras.layers.Dropout(self.dropout_rate)\n",
    "        self.relative_position_bias_table = self.add_weight(\n",
    "            name=\"relative_position_bias_table\",\n",
    "            shape=[\n",
    "                (2 * self.window_size[0] - 1) * (2 * self.window_size[1] - 1),\n",
    "                self.n_heads,\n",
    "            ],\n",
    "            initializer=keras.initializers.TruncatedNormal(stddev=0.02),\n",
    "            trainable=True,\n",
    "            dtype=self.dtype,\n",
    "        ) #<- learnable weight of relational position. 위 window size = 4의 예시에서, 임의의 두 지점 간 거리의 경우의 수는 총 49개 -> 이에 해당하는 weight tensor를 만듬.\n",
    "        super().build(input_shape)\n",
    "    def get_relative_position_index(self): #<- window 내 2 지점 간 거리의 index matrix.\n",
    "        coords_h = ops.arange(self.window_size[0])\n",
    "        coords_w = ops.arange(self.window_size[1])\n",
    "        coords = ops.stack(ops.meshgrid(coords_h, coords_w, indexing=\"ij\"), axis=0)\n",
    "        coords_flatten = ops.reshape(coords, [2, -1])\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
    "        relative_coords = ops.transpose(relative_coords, axes=[1, 2, 0])\n",
    "        relative_coords_xx = relative_coords[:, :, 0] + self.window_size[0] - 1\n",
    "        relative_coords_yy = relative_coords[:, :, 1] + self.window_size[1] - 1\n",
    "        relative_coords_xx = relative_coords_xx * (2 * self.window_size[1] - 1)\n",
    "        relative_position_index = relative_coords_xx + relative_coords_yy\n",
    "        return relative_position_index\n",
    "    def call(self, inputs, **kwargs):\n",
    "        #input : key(=value), global query OR key only\n",
    "        # input component shape : batch*n_windows, h_window*w_window, embed_dims -> level/block 설계 시 repeat 처리 후 attention에 feed\n",
    "        if self.global_query :\n",
    "            inputs, q_global = inputs\n",
    "            batch_size = ops.shape(q_global)[0]\n",
    "        else:\n",
    "            inputs = inputs[0]\n",
    "        batch_, token_length, embed_dims = ops.shape(inputs) #global query는 query generator에 의해 token_length 개 만큼의 token으로 전체 이미지/feature map을 압축한 상태\n",
    "        \n",
    "        qkv = self.qkv_embed_fn(inputs) #batch*n_windows, h_w * w_w, qkv_size * embed_dims\n",
    "        \n",
    "        qkv = ops.reshape(qkv, [-1, token_length, self.qkv_size, self.n_heads, embed_dims//self.n_heads])\n",
    "        qkv = ops.transpose(qkv, [2, 0, 3, 1, 4]) #qkv_size, batch_, n_heads, token_length, C\n",
    "        \n",
    "        #QKV 분리\n",
    "        if self.global_query:\n",
    "            k, v = ops.split(qkv, 2, axis = 0) #각각 batch_, n_heads, token_length, C\n",
    "            #repeat the global query tensor\n",
    "            # batch_size, n_query_tokens, dims -> batch_(=batch * n_windows), n_query_tokens, dims\n",
    "            q_global = ops.repeat(q_global, batch_//batch_size, axis = 0) #->batch_, n_query_tokens, dims\n",
    "            q = ops.reshape(q_global, [batch_, token_length, self.n_heads, embed_dims//self.n_heads])\n",
    "            q = ops.transpose(q, [0, 2, 1, 3]\n",
    "                             )\n",
    "        else:\n",
    "            q, k, v = ops.split(qkv, 3, axis = 0)\n",
    "            q = ops.squeeze(q, axis = 0)\n",
    "        k = ops.squeeze(k, axis = 0)\n",
    "        v = ops.squeeze(v, axis = 0)\n",
    "        \n",
    "        q *= self.scale #batch_, n_heads, token_length, dimension_per_heads(=C)\n",
    "        attention_score = q@ops.transpose(k, [0, 1, 3, 2]) #batch_, n_heads, token_length, token_length\n",
    "        \n",
    "        #positional encoding(bias) 계산 -> attention score에 더해 주기\n",
    "        # Code from original keras homepage\n",
    "        relative_position_bias = ops.take(\n",
    "            self.relative_position_bias_table,\n",
    "            ops.reshape(self.get_relative_position_index(), [-1]),\n",
    "        )\n",
    "        relative_position_bias = ops.reshape(\n",
    "            relative_position_bias,\n",
    "            [\n",
    "                self.window_size[0] * self.window_size[1],\n",
    "                self.window_size[0] * self.window_size[1],\n",
    "                -1,\n",
    "            ],\n",
    "        )\n",
    "        relative_position_bias = ops.transpose(relative_position_bias, axes=[2, 0, 1])\n",
    "        attention_score += relative_position_bias[None,]\n",
    "        attention_weight = self.softmax(attention_score)\n",
    "        attention_weight = self.attention_dropout(attention_weight) #batch_, n_heads, token_length, token_length\n",
    "        #value tensor shape : batch_, n_heads, token_length, dimension_per_heads(=C)\n",
    "        attended_output = attention_weight@v\n",
    "        attended_output = ops.transpose(attended_output, [0, 2, 1, 3])\n",
    "        attended_output = ops.reshape(attended_output, [batch_, token_length, embed_dims])\n",
    "        attended_output = self.projection_dropout(self.proj(attended_output))\n",
    "        self.attention_weight = attention_weight\n",
    "        if self.return_attention_weights:\n",
    "            return attended_output, attention_weight\n",
    "        else:\n",
    "            return attended_output\n",
    "        \n",
    "        \n",
    "class Block(keras.layers.Layer):\n",
    "    def __init__(self, #이하는 Window Attention configurations\n",
    "                 window_size, num_heads, global_query, \n",
    "                 qkv_bias = True, qk_scale = None, dropout_rate = 0.05, \n",
    "                 # 이하는 MLP module의 configuration\n",
    "                 mlp_ratio = 4.0, layer_scale = None, return_attention_weights = False,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.window_size = window_size\n",
    "        self.n_heads = num_heads\n",
    "        self.global_query = global_query\n",
    "        self.qkv_bias = qkv_bias\n",
    "        self.qk_scale = qk_scale\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        self.layer_scale = layer_scale\n",
    "        self.return_attention_weights = return_attention_weights\n",
    "    def build(self, input_shape):\n",
    "        #input tensor : list of key/query or key only\n",
    "        # each tensor is batch_size, w, h, channel dims shape tensor\n",
    "        batch_size, H, W, dims = input_shape[0]\n",
    "        self.norm1 = keras.layers.LayerNormalization(epsilon = 1e-5)\n",
    "        self.norm2 = keras.layers.LayerNormalization(epsilon = 1e-5)\n",
    "        self.window_attention = WindowAttention(window_size = self.window_size,\n",
    "                                               n_heads = self.n_heads,\n",
    "                                               global_query = self.global_query,\n",
    "                                               qkv_bias = self.qkv_bias,\n",
    "                                               qk_scale = self.qk_scale,\n",
    "                                               dropout_rate = self.dropout_rate,\n",
    "                                                return_attention_weights = self.return_attention_weights)\n",
    "        self.mlps = MLP(middle_dim = int(self.mlp_ratio * dims), dropout = self.dropout_rate)\n",
    "        if self.layer_scale != None:\n",
    "            self.gamma1 = self.add_weight(shape = [dims], name = \"Gamma1\", trainable = True,\n",
    "                                         initializer = keras.initializer.Constant(self.layer_scale), dtype = self.dtype)\n",
    "            self.gamma2 = self.add_weight(shape = [dims], name = \"Gamma2\", trainable = True,\n",
    "                                         initializer = keras.initializer.Constant(self.layer_scale), dtype = self.dtype)\n",
    "        else:\n",
    "            self.gamma1, self.gamma2 = 1.0, 1.0\n",
    "        self.n_windows = int(H//self.window_size) * int(W//self.window_size)\n",
    "        \n",
    "    #input feature map을 일정 크기의 window로 partition을 만들어주는 함수 및\n",
    "    # 그 partition을 받아 원래의 feature map으로 돌려주는 함수를 만들자\n",
    "    def window_partition(self, inputs): #feature map -> multiple windows\n",
    "        batch_size, H, W, dims = ops.shape(inputs)\n",
    "        h, w = H//self.window_size, W//self.window_size\n",
    "        inputs = ops.reshape(inputs, [batch_size, \n",
    "                                      h, self.window_size,\n",
    "                                     w, self.window_size, \n",
    "                                     dims])\n",
    "        inputs = ops.transpose(inputs, [0,#batch_size\n",
    "                                        1,3, #h, w\n",
    "                                        2,4, #winsize, winsize\n",
    "                                        5])\n",
    "        return ops.reshape(inputs, [-1, self.window_size, self.window_size, dims]) #batch_size*n_windows, window_size, window_size, dims\n",
    "        \n",
    "    def window_reverse(self, inputs, H, W, dims): #window partition -> original feature map\n",
    "        x = ops.reshape(inputs, [-1, H//self.window_size, W//self.window_size, self.window_size, self.window_size, dims])\n",
    "        x = ops.transpose(x, [0, 1, 3, 2, 4, 5])\n",
    "        return ops.reshape(x, [-1, H, W, dims])\n",
    "    \n",
    "    def call(self, inputs, **kwargs):\n",
    "        if self.global_query:\n",
    "            inputs, global_query = inputs\n",
    "        else:\n",
    "            inputs = inputs[0]\n",
    "        batch_size, H, W, dims = ops.shape(inputs)\n",
    "        x = self.norm1(inputs)\n",
    "        x = self.window_partition(x) \n",
    "        x = ops.reshape(x, [-1, self.window_size*self.window_size, dims])\n",
    "        if self.global_query:\n",
    "            outputs_ = self.window_attention([x, global_query]\n",
    "                                     )\n",
    "        else:\n",
    "            outputs_ = self.window_attention([x])\n",
    "        if self.return_attention_weights:\n",
    "            x, attention_weight = outputs_\n",
    "        else:\n",
    "            x = outputs_\n",
    "        x = self.window_reverse(x, H, W, dims)\n",
    "        x = inputs + self.gamma1*x\n",
    "        x += self.gamma2*(self.mlps(self.norm2(x)))\n",
    "        if self.return_attention_weights:\n",
    "            return x, attention_weight\n",
    "        else:\n",
    "            return x\n",
    "    \n",
    "class Level(keras.layers.Layer):\n",
    "    def __init__(self, \n",
    "                depth, #<- Block repetition depth\n",
    "                num_heads, window_size, keepdims, #downsampler 및 block의 hyperparameter\n",
    "                downsample = True, mlp_ratio = 4.0,\n",
    "                qkv_bias = True, qk_scale = None,\n",
    "                dropout = 0.05, layer_scale = None, return_attention_weights = True,\n",
    "                **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.depth = depth\n",
    "        self.n_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.keepdims = keepdims\n",
    "        self.downsample = downsample\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        self.qkv_bias = qkv_bias\n",
    "        self.qk_scale = qk_scale\n",
    "        self.dropout_rate = dropout\n",
    "        self.layer_scale = layer_scale\n",
    "        self.return_attention_weights = return_attention_weights\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        #input tensor : feature map / patches\n",
    "        batch_size, H, W, dims = input_shape\n",
    "        self.blocks = [Block(window_size = self.window_size, num_heads = self.n_heads, global_query = bool(idx%2), \n",
    "                             qkv_bias = self.qkv_bias, qk_scale = self.qk_scale, dropout_rate = self.dropout_rate, \n",
    "                             mlp_ratio = self.mlp_ratio, layer_scale = self.layer_scale, return_attention_weights = self.return_attention_weights,\n",
    "                             name = f\"GCViTBlock{idx+1}\") for idx in range(self.depth)]\n",
    "        self.downsampler = DownSampler(name = \"Downsampler\")\n",
    "        self.query_generator = GlobalQueryGenerator(keepdims = self.keepdims, name = \"GlobalQueryGenerator\")\n",
    "        \n",
    "    def call(self, inputs, **kwargs):\n",
    "        patches = inputs\n",
    "        global_query = self.query_generator(inputs)\n",
    "        for idx, block in enumerate(self.blocks):\n",
    "            if idx % 2 :\n",
    "                outputs_ = block([patches, global_query])\n",
    "            else:\n",
    "                outputs_ = block([patches])\n",
    "            if self.return_attention_weights:\n",
    "                patches, attention_weights = outputs_\n",
    "            else:\n",
    "                patches = outputs_\n",
    "        if self.downsample == False:\n",
    "            return patches\n",
    "        else:\n",
    "            return self.downsampler(patches)\n",
    "def get_gcvit_configs(res, initial_embedding_dims, name = None):\n",
    "    return {'res' : res,\n",
    "            'embed_dims' : initial_embedding_dims,\n",
    "            \"patch_embedding_type\" : \"conv\", #conv or tokenlearner\n",
    "            \"level_depth\" : [2,4,6,8],\n",
    "            \"level_heads\" : [2,4,8,16],\n",
    "            \"level_keepdims\" : [[0,0,0],\n",
    "                                   [0,0],\n",
    "                                   [1], \n",
    "                                    [1]\n",
    "                                   ], #3번째 level부터는 window attention == global attention\n",
    "            \"level_window_size\" : [res//32, res//32, res//16, res//32],\n",
    "            \"model_name\" : f\"GCViT_res{res}\" if name == None else name\n",
    "                }\n",
    "def get_gcvit(configs):\n",
    "    res = configs[\"res\"]\n",
    "    inputs = Input([res,res,3], name = \"ImageInput\")\n",
    "    patcher = PatchEmbedding(embed_dim = configs['embed_dims'], patching_type = configs[\"patch_embedding_type\"],\n",
    "                             name = \"PatchEmbedding\")\n",
    "    patches = patcher(inputs)\n",
    "    \n",
    "    for idx, (depth, heads, keepdims, window_size) in enumerate(zip(configs[\"level_depth\"], configs[\"level_heads\"], configs[\"level_keepdims\"], configs[\"level_window_size\"])):\n",
    "        if idx == len(configs['level_depth'])-1:\n",
    "            downsample = False\n",
    "        else:\n",
    "            downsample = True\n",
    "        level = Level(depth = depth, num_heads = heads, window_size = window_size, keepdims = keepdims, downsample = downsample,\n",
    "                      name = f\"GCViT_Lv{idx+1}_downsample_{downsample}\")\n",
    "        patches = level(patches)\n",
    "    model = keras.Model(inputs, patches,\n",
    "                       name = configs[\"model_name\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7605cac7",
   "metadata": {
    "papermill": {
     "duration": 0.016184,
     "end_time": "2024-05-17T03:33:20.452904",
     "exception": false,
     "start_time": "2024-05-17T03:33:20.436720",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bf9b1d",
   "metadata": {
    "papermill": {
     "duration": 0.016101,
     "end_time": "2024-05-17T03:33:20.485469",
     "exception": false,
     "start_time": "2024-05-17T03:33:20.469368",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# MLP-mixer\n",
    "- for lower resource demand!\n",
    "- Code Reference : [Keras.io](https://keras.io/examples/vision/mlp_image_classification/#the-mlpmixer-model)\n",
    "\n",
    "![](https://velog.velcdn.com/images/minkyu4506/post/0237ee55-74eb-4836-952c-6bd33694aecc/image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "752f941b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T03:33:20.521260Z",
     "iopub.status.busy": "2024-05-17T03:33:20.520145Z",
     "iopub.status.idle": "2024-05-17T03:33:20.531563Z",
     "shell.execute_reply": "2024-05-17T03:33:20.530574Z"
    },
    "papermill": {
     "duration": 0.032198,
     "end_time": "2024-05-17T03:33:20.534334",
     "exception": false,
     "start_time": "2024-05-17T03:33:20.502136",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NaivePatchesExtraction(keras.layers.Layer): #untrainable layer\n",
    "    def __init__(self, patch_size, output_type = \"seq\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.patch_size = patch_size\n",
    "        self.output_type = output_type\n",
    "        assert output_type in [\"seq\", \"sequence\", \"Sequence\", \"3D\", \"conv\", \"feature_map\", \"2D\"]\n",
    "    def call(self, x):\n",
    "        if len(ops.shape(x)) == 3:\n",
    "            x = ops.expand_dims(x, 0)\n",
    "        patches = keras.ops.image.extract_patches(x, self.patch_size)\n",
    "        batch_size = keras.ops.shape(patches)[0]\n",
    "        num_patches = keras.ops.shape(patches)[1] * keras.ops.shape(patches)[2]\n",
    "        patch_dim = keras.ops.shape(patches)[3]\n",
    "        if self.output_type in [\"seq\", \"sequence\", \"Sequence\", \"3D\"] : \n",
    "            out = keras.ops.reshape(patches, (batch_size, num_patches, patch_dim))\n",
    "        elif self.output_type in [\"conv\", \"feature_map\", \"2D\"]:\n",
    "            out = keras.ops.reshape(patches, (batch_size, keras.ops.shape(patches)[1], keras.ops.shape(patches)[2],\n",
    "                                             patch_dim))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbef1878",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T03:33:20.569750Z",
     "iopub.status.busy": "2024-05-17T03:33:20.569361Z",
     "iopub.status.idle": "2024-05-17T03:33:20.585222Z",
     "shell.execute_reply": "2024-05-17T03:33:20.584142Z"
    },
    "papermill": {
     "duration": 0.036519,
     "end_time": "2024-05-17T03:33:20.587707",
     "exception": false,
     "start_time": "2024-05-17T03:33:20.551188",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLPMixer(keras.layers.Layer): #untrainable layer\n",
    "    def __init__(self, units = None, dropout_rate = 0.2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.layernorm1 = keras.layers.LayerNormalization()\n",
    "        self.layernorm2 = keras.layers.LayerNormalization()\n",
    "        self.drop_rate = dropout_rate\n",
    "        self.units = units\n",
    "        \n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        #batch, seq_len, dims\n",
    "        batch_size = input_shape[0]\n",
    "        n_tokens = input_shape[1]\n",
    "        channels = input_shape[2]\n",
    "        embed_dim_middle = channels if self.units is None else self.units\n",
    "        \n",
    "        self.mlp1 = keras.Sequential([keras.layers.Dense(units = n_tokens, use_bias = False),\n",
    "                                     keras.layers.Activation(\"gelu\"),\n",
    "                                     keras.layers.Dropout(self.drop_rate),\n",
    "                                     keras.layers.Dense(units = n_tokens, use_bias = False),],\n",
    "                                    name = \"MLP1_TokenWiseMixing\") #output : batch, embedding_dims(channels), n_tokens\n",
    "        \n",
    "        self.mlp2 = keras.Sequential([keras.layers.Dense(units = embed_dim_middle, use_bias = False),\n",
    "                                     keras.layers.Activation(\"gelu\"),\n",
    "                                     keras.layers.Dropout(self.drop_rate),\n",
    "                                     keras.layers.Dense(units = channels, use_bias = False),],\n",
    "                                    name = \"MLP2_ChannelWiseMixing\")\n",
    "    def enable_lora(self, rank):\n",
    "        print(\"Use only in pretrained model!\\n\\n\")\n",
    "        if rank == 0:\n",
    "            print(\"lora disabled\\n\\n\")\n",
    "            pass\n",
    "        else:\n",
    "            self.layernorm1.trainable = False\n",
    "            self.layernorm2.trainable = False\n",
    "            for layer in self.mlp1.layers:\n",
    "                if isinstance(layer, keras.layers.Dense):\n",
    "                    layer.enable_lora(rank)\n",
    "                else:\n",
    "                    layer.trainable = False\n",
    "            for layer in self.mlp2.layers:\n",
    "                if isinstance(layer, keras.layers.Dense):\n",
    "                    layer.enable_lora(rank)\n",
    "                else:\n",
    "                    layer.trainable = False\n",
    "    def call(self, x):\n",
    "        # x : patches, [batch, n_patches, embed_dims]\n",
    "        normed_patch = self.layernorm1(x)\n",
    "        normed_patch = ops.transpose(normed_patch, [0,2,1]) #batch, embed_dims, n_patches\n",
    "        normed_patch = self.mlp1(normed_patch)\n",
    "        normed_patch = ops.transpose(normed_patch, [0,2,1]) #batch, n_patches, embed_dims\n",
    "        normed_patch += x\n",
    "        normed_patch2 = self.layernorm2(normed_patch)\n",
    "        normed_patch2 = self.mlp2(normed_patch2)\n",
    "        normed_patch2 += normed_patch\n",
    "        return normed_patch2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e3209f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T03:33:20.622634Z",
     "iopub.status.busy": "2024-05-17T03:33:20.621822Z",
     "iopub.status.idle": "2024-05-17T03:33:20.630472Z",
     "shell.execute_reply": "2024-05-17T03:33:20.629402Z"
    },
    "papermill": {
     "duration": 0.028517,
     "end_time": "2024-05-17T03:33:20.632737",
     "exception": false,
     "start_time": "2024-05-17T03:33:20.604220",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_mlp_mixer(res, name = \"mlpmixer_16_4_768\"):\n",
    "    _, patch_size, depth, embed_dims = name.split(\"_\")\n",
    "    patch_size = int(patch_size)\n",
    "    depth = int(depth)\n",
    "    embed_dims = int(embed_dims)\n",
    "    \n",
    "    model_name = f\"Res{res}_{name}\"\n",
    "    inputs = Input([res,res,3], name = \"ImageInput\")\n",
    "    scaled_input = inputs/255.0\n",
    "    patches = NaivePatchesExtraction(patch_size, name = \"PatchExtraction\")(scaled_input)\n",
    "    patches_embedding = keras_nlp.layers.RotaryEmbedding(name = \"RotaryPositionalEmbedding\")(patches)\n",
    "    patches_embedding = keras.layers.Dense(units = embed_dims, name = \"SimpleMLP_Channelwise\")(patches_embedding)\n",
    "    for idx in range(depth):\n",
    "        patches_embedding = MLPMixer(name = f\"MLPMixer_{idx+1}\")(patches_embedding)\n",
    "    model = Model(inputs, patches_embedding,\n",
    "                 name = model_name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d182e83d",
   "metadata": {
    "papermill": {
     "duration": 0.016016,
     "end_time": "2024-05-17T03:33:20.665356",
     "exception": false,
     "start_time": "2024-05-17T03:33:20.649340",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ConvMixer\n",
    "- Dense layer inside MLPmixer --> Conv2D\n",
    "- Code Reference : [Keras.io](https://keras.io/examples/vision/convmixer/)\n",
    "\n",
    "![](https://i.imgur.com/yF8actg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3b98a17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T03:33:20.708997Z",
     "iopub.status.busy": "2024-05-17T03:33:20.708499Z",
     "iopub.status.idle": "2024-05-17T03:33:20.724216Z",
     "shell.execute_reply": "2024-05-17T03:33:20.723111Z"
    },
    "papermill": {
     "duration": 0.040273,
     "end_time": "2024-05-17T03:33:20.727028",
     "exception": false,
     "start_time": "2024-05-17T03:33:20.686755",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConvMixer(keras.layers.Layer): #untrainable layer\n",
    "    def __init__(self, filters = None, kernel_size = 5, dropout_rate = 0.2, output_type = \"2D\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.layernorm1 = keras.layers.LayerNormalization()\n",
    "        self.layernorm2 = keras.layers.LayerNormalization()\n",
    "        self.drop_rate = dropout_rate ; self.drop = keras.layers.Dropout(self.drop_rate)\n",
    "        \n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.output_type = output_type\n",
    "        assert output_type in [\"seq\", \"sequence\", \"Sequence\", \"3D\", \"conv\", \"feature_map\", \"2D\"]\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        #batch, seq_len, dims\n",
    "        batch_size = input_shape[0]\n",
    "        h = input_shape[1]\n",
    "        w = input_shape[2]\n",
    "        channels = input_shape[3]\n",
    "        c = channels if self.filters is None else self.filters\n",
    "        \n",
    "        self.depthconv = keras.layers.DepthwiseConv2D(kernel_size = self.kernel_size, padding = \"SAME\", use_bias = False)\n",
    "        self.pointconv = keras.layers.Conv2D(filters = c, kernel_size = 1, padding = 'SAME', use_bias = False)\n",
    "        self.gelu = keras.layers.Activation('gelu')\n",
    "\n",
    "    def call(self, x):\n",
    "        # x : feature_map, [batch, h,w, embed_dims]\n",
    "        fmap = self.depthconv(x)\n",
    "        fmap = self.gelu(fmap)\n",
    "        fmap = self.layernorm1(fmap) + x\n",
    "        fmap = self.pointconv(fmap)\n",
    "        fmap = self.gelu(fmap)\n",
    "        fmap = self.layernorm2(fmap)\n",
    "        \n",
    "        batch_size = ops.shape(fmap)[0]\n",
    "        h = ops.shape(fmap)[1]\n",
    "        w = ops.shape(fmap)[2]\n",
    "        dims_ = ops.shape(fmap)[3]\n",
    "        \n",
    "        if self.output_type in [\"seq\", \"sequence\", \"Sequence\", \"3D\"] : \n",
    "            fmap = keras.ops.reshape(fmap, (batch_size, h*w, dims_))\n",
    "            return fmap\n",
    "        elif self.output_type in [\"conv\", \"feature_map\", \"2D\"]:\n",
    "            return fmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "123a23c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T03:33:20.770977Z",
     "iopub.status.busy": "2024-05-17T03:33:20.769816Z",
     "iopub.status.idle": "2024-05-17T03:33:20.783598Z",
     "shell.execute_reply": "2024-05-17T03:33:20.782394Z"
    },
    "papermill": {
     "duration": 0.035729,
     "end_time": "2024-05-17T03:33:20.786020",
     "exception": false,
     "start_time": "2024-05-17T03:33:20.750291",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_conv_mixer(res, name = \"convmixer_16_4_768\"):\n",
    "    _, patch_size, depth, embed_dims = name.split(\"_\")\n",
    "    patch_size = int(patch_size)\n",
    "    depth = int(depth)\n",
    "    embed_dims = int(embed_dims)\n",
    "    \n",
    "    model_name = f\"Res{res}_{name}\"\n",
    "    inputs = Input([res,res,3], name = \"ImageInput\")\n",
    "    scaled_input = inputs/255.0\n",
    "    patches = NaivePatchesExtraction(patch_size, name = \"PatchExtraction\")(scaled_input)\n",
    "    patches_embedding = keras_nlp.layers.RotaryEmbedding(name = \"RotaryPositionalEmbedding\")(patches)\n",
    "    batch, n_patches, dims = ops.shape(patches_embedding) ; res_ = int(ops.sqrt(ops.cast(n_patches, \"float32\")))\n",
    "    patches_embedding = ops.reshape(patches_embedding, (-1, res_, res_, dims))\n",
    "    \n",
    "    patches_embedding = keras.layers.Dense(units = embed_dims, name = \"SimpleMLP_Channelwise\")(patches_embedding)\n",
    "    for idx in range(depth):\n",
    "        if idx == depth-1:\n",
    "            types = \"seq\"\n",
    "        else:\n",
    "            types = \"2D\"\n",
    "        patches_embedding = ConvMixer(name = f\"ConvMixer_{idx+1}\", output_type = types)(patches_embedding)\n",
    "    model = Model(inputs, patches_embedding,\n",
    "                 name = model_name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e95006",
   "metadata": {
    "papermill": {
     "duration": 0.021082,
     "end_time": "2024-05-17T03:33:20.824332",
     "exception": false,
     "start_time": "2024-05-17T03:33:20.803250",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae34ba61",
   "metadata": {
    "papermill": {
     "duration": 0.016565,
     "end_time": "2024-05-17T03:33:20.858423",
     "exception": false,
     "start_time": "2024-05-17T03:33:20.841858",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Get final Feature extractor\n",
    "- according to [recent study](https://arxiv.org/abs/2309.16588), add cls token and register tokens \"after\" the patches!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67844efe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T03:33:20.894821Z",
     "iopub.status.busy": "2024-05-17T03:33:20.894381Z",
     "iopub.status.idle": "2024-05-17T03:33:20.937047Z",
     "shell.execute_reply": "2024-05-17T03:33:20.935695Z"
    },
    "papermill": {
     "duration": 0.065011,
     "end_time": "2024-05-17T03:33:20.940258",
     "exception": false,
     "start_time": "2024-05-17T03:33:20.875247",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_feature_extractor(conv_base, #if None, Vanilla ViT\n",
    "                         embed_dims, res, pe_type = \"rotary\",\n",
    "                          patch_size = 16,\n",
    "                        att_depth = 4, att_heads = 16) : \n",
    "    inputs = Input([res,res,3], name = \"Input_images\")\n",
    "    batch_size = ops.shape(inputs)[0]\n",
    "    if conv_base in [None, 'vit', \"ViT\"] : #Vanilla Vision Transformer\n",
    "        scaled_inputs = keras.layers.LayerNormalization(name = \"InitialLN\")(inputs)\n",
    "        patches = Conv2D(filters = embed_dims, activation = \"gelu\", kernel_size = patch_size, strides = patch_size, padding = 'SAME', name = \"PatchingStem\")(scaled_inputs)\n",
    "        _, w, h, dims = ops.shape(patches) ; n_patches = w*h \n",
    "        \n",
    "        patches = ops.reshape(patches, [-1, w*h, dims])\n",
    "        if pe_type in ['rotary', 'rotation', 'rotatory', 'roformer']:\n",
    "            patches = keras_nlp.layers.RotaryEmbedding(name = \"RotaryPositionalEmbedding\")(patches)\n",
    "        elif pe_type in [\"learnable\", 'absolute']:\n",
    "            patches = PatchEncoder(num_patches = w*h, projection_dim = embed_dims)(patches)\n",
    "        elif pe_type == None:\n",
    "            pass\n",
    "        \n",
    "        cls_token = ops.expand_dims(keras.layers.GlobalAveragePooling1D(name = 'GAPforClsToken')(patches),\n",
    "                                    axis = 1)\n",
    "        register_tokens = ops.tile(ops.ones_like(cls_token), \n",
    "                                   [1,2,1])\n",
    "        patches = ops.concatenate([patches, cls_token, register_tokens], axis = 1)\n",
    "    \n",
    "        for idx in range(att_depth):\n",
    "            x0 = LayerNormalization(name = f\"PreLN{idx+1}\")(patches)\n",
    "            x1, attention_score = AttentionPooling(att_heads, embed_dims, name = f\"MHA_after_Conv_{idx+1}\")([x0, x0])\n",
    "            x2 = keras.layers.Add(name = f\"PreAdd{idx+1}\")([patches, x1])\n",
    "            x3 = LayerNormalization(name = f\"PostLN{idx+1}\")(x2)\n",
    "            x4 = Dense(units = embed_dims, activation = 'gelu', name = f\"TokenMixMLP{idx+1}\")(x3)\n",
    "            patches = keras.layers.Add(name = \"Encoded_Patches\" if idx == att_depth-1 else f\"PostAdd{idx+1}\")([x4, x2])\n",
    "        learned_token = keras.layers.Identity(name = \"feature_vector\")(patches[:, -3, :])\n",
    "        patches = keras.layers.Identity(name = \"encoded_patches\")(patches[:, :n_patches, :])\n",
    "        attention_score = attention_score[:, :, n_patches, :-3]\n",
    "        attention_score = ops.expand_dims(attention_score, axis = -2)\n",
    "        attention_score = keras.layers.Identity(name = \"attention_weight\")(attention_score) \n",
    "        model_name = f\"ViT_depth{att_depth}_dims{embed_dims}_heads{att_heads}_patch{patch_size}\"\n",
    "    else:\n",
    "        feature_map = conv_base(inputs)\n",
    "        if len(ops.shape(feature_map)) == 4:\n",
    "            _, w, h, dims = ops.shape(feature_map)\n",
    "            dims = ops.shape(feature_map)[-1] ; batch_size = ops.shape(feature_map)[0]\n",
    "            feature_map = ops.reshape(feature_map, [-1, w*h, dims])\n",
    "        n_patches = ops.shape(feature_map)[1]\n",
    "        if pe_type in ['rotary', 'rotation', 'rotatory', 'roformer']:\n",
    "            feature_map = keras_nlp.layers.RotaryEmbedding(name = \"RotaryPositionalEmbedding\")(feature_map)\n",
    "        elif pe_type in [\"learnable\", 'absolute']:\n",
    "            feature_map = PatchEncoder(num_patches = n_patches, projection_dim = embed_dims)(feature_map)\n",
    "        elif pe_type == None:\n",
    "            pass\n",
    "\n",
    "        learned_token = keras.layers.GlobalAveragePooling1D(name = 'GAPforRepVec')(feature_map)\n",
    "        learned_token = learned_token[:, tf.newaxis, :]\n",
    "        register_tokens = ops.tile(ops.ones_like(learned_token), \n",
    "                                   [1,2,1])\n",
    "        feature_map = ops.concatenate([feature_map,\n",
    "                                      learned_token,\n",
    "                                      register_tokens], axis = 1)\n",
    "        \n",
    "        for idx in range(att_depth):\n",
    "            feature_map, attention_score = AttentionPooling(att_heads, embed_dims, name = f\"MHA_after_Conv_{idx+1}\")([feature_map, feature_map])\n",
    "        learned_token = keras.layers.Identity(name = \"feature_vector\")(feature_map[:, -3, :])\n",
    "        attention_score = attention_score[:, :, n_patches, :-3]\n",
    "        attention_score = ops.expand_dims(attention_score, axis = -2)\n",
    "        attention_score = keras.layers.Identity(name = \"attention_weight\")(attention_score) \n",
    "        patches = keras.layers.Identity(name = \"encoded_patches\")(feature_map[:, :n_patches, :])\n",
    "        \n",
    "        model_name = f\"{conv_base.name}_depth{att_depth}_dims{embed_dims}_heads{att_heads}\"\n",
    "    \n",
    "    model = Model(inputs, [learned_token, attention_score],\n",
    "                  name = model_name)\n",
    "    return model\n",
    "\n",
    "def get_full_model(conv_base_name, res, embed_dims = 1280, patch_size = 16, pe_type = 'rotary',\n",
    "                   att_depth = 4, att_heads = 8,\n",
    "                  extra_configs = None):\n",
    "    if conv_base_name in [\"effnet\", 'EfficientNet']:\n",
    "        conv_base = keras.applications.EfficientNetV2B1(input_shape = [res,res,3],\n",
    "                                                       include_top = False)\n",
    "    elif conv_base_name in [\"effnet_small\", \"EfficientNetSmall\"]:\n",
    "        conv_base = keras.applications.EfficientNetV2S(input_shape = [res,res,3],\n",
    "                                                       include_top = False)\n",
    "    elif conv_base_name in [\"effnet_base\", \"EfficientNetBase\"]:\n",
    "        conv_base = keras.applications.EfficientNetV2M(input_shape = [res,res,3],\n",
    "                                                       include_top = False)\n",
    "    elif conv_base_name in [\"convnext\", 'ConvNeXt']:\n",
    "        conv_base = keras.applications.ConvNeXtTiny(input_shape = [res,res,3],\n",
    "                                                       include_top = False)\n",
    "    elif conv_base_name in [\"convnext_small\", 'ConvNeXtSmall']:\n",
    "        conv_base = keras.applications.ConvNeXtSmall(input_shape = [res,res,3],\n",
    "                                                       include_top = False)\n",
    "    elif conv_base_name in [\"convnext_base\", 'ConvNeXtBase']:\n",
    "        conv_base = keras.applications.ConvNeXtBase(input_shape = [res,res,3],\n",
    "                                                       include_top = False)\n",
    "    elif isinstance(conv_base_name, dict):\n",
    "        conv_base = get_gcvit(conv_base_name)\n",
    "    elif conv_base_name.split(\"_\")[0] in [\"MLPMixer\", \"mlpmixer\", \"MLP_mixer\", \"mlp_mixer\"]:\n",
    "        conv_base = get_mlp_mixer(res = res, name = conv_base_name)\n",
    "        pe_type = None\n",
    "    elif conv_base_name.split(\"_\")[0] in [\"ConvMixer\", \"convmixer\", \"Conv_mixer\", \"conv_mixer\"]:\n",
    "        conv_base = get_conv_mixer(res = res, name = conv_base_name)\n",
    "        pe_type = None\n",
    "    else:\n",
    "        conv_base = None\n",
    "    return get_feature_extractor(conv_base, pe_type = pe_type, res = res, patch_size = patch_size, embed_dims = embed_dims, att_depth = att_depth, att_heads = att_heads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09b16b0",
   "metadata": {
    "papermill": {
     "duration": 0.022632,
     "end_time": "2024-05-17T03:33:20.986856",
     "exception": false,
     "start_time": "2024-05-17T03:33:20.964224",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---------\n",
    "# Information-maximization method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90b2c3a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T03:33:21.023043Z",
     "iopub.status.busy": "2024-05-17T03:33:21.022067Z",
     "iopub.status.idle": "2024-05-17T03:33:21.044309Z",
     "shell.execute_reply": "2024-05-17T03:33:21.043182Z"
    },
    "papermill": {
     "duration": 0.043321,
     "end_time": "2024-05-17T03:33:21.046810",
     "exception": false,
     "start_time": "2024-05-17T03:33:21.003489",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#helper functions\n",
    "# For Barlow Twins\n",
    "def get_cor_matrix(z1, z2, e = 1e-5):\n",
    "    #z1, z2 = [batch, embed_dims] shape tensor\n",
    "    # 각각 Normalize한 뒤 내적 -> dims by dims correlation matrix\n",
    "    z1_mean, z1_std = ops.mean(z1, axis = 0), ops.std(z1, axis = 0)\n",
    "    z2_mean, z2_std = ops.mean(z2, axis = 0), ops.std(z2, axis = 0)\n",
    "    z1 = (z1 - z1_mean) / (z1_std + e)\n",
    "    z2 = (z2 - z2_mean) / (z2_std + e)\n",
    "    bs = tf.cast(ops.shape(z1)[0], tf.float32)\n",
    "    matrix = (ops.transpose(z1)@z2) / bs\n",
    "    matrix = tf.cast(matrix, tf.float32)\n",
    "    return matrix\n",
    "\n",
    "# For VICreg\n",
    "def invariance_loss(za, zb): #invariance\n",
    "    l2_distances = keras.losses.MeanSquaredError(reduction = None)(za, zb)\n",
    "    return ops.cast(l2_distances, \"float32\")\n",
    "\n",
    "def variance(za, e = 1e-4, gamma = 5.0):\n",
    "    mu, var = tf.nn.moments(za, axes = 0)\n",
    "    mu = tf.cast(mu, tf.float32)\n",
    "    var = tf.cast(var, tf.float32)\n",
    "    var += tf.cast(e, tf.float32)\n",
    "    \n",
    "    s_xe_hinge = gamma - tf.math.sqrt(var)\n",
    "    s_val = tf.math.maximum(0.0, s_xe_hinge)\n",
    "    return ops.cast(s_val, \"float32\")\n",
    "\n",
    "def covariance(za, e = 1e-4, testing = False):\n",
    "    mu, var = tf.nn.moments(za, axes = 0) ; bs = tf.cast(ops.shape(za)[0], tf.float32) ; embed_dims = tf.cast(ops.shape(za)[-1], tf.float32)\n",
    "    mu = tf.cast(mu, tf.float32)\n",
    "    var = tf.cast(var, tf.float32)\n",
    "    div_ = ops.maximum(bs-1, 1)\n",
    "    \n",
    "    za -= mu #batch, dims\n",
    "    cov_mat = tf.transpose(za)@za/div_\n",
    "    lower_tri = cov_mat - tf.linalg.band_part(cov_mat, 0, -1)\n",
    "    upper_tri = cov_mat - tf.linalg.band_part(cov_mat, -1, 0)\n",
    "    off_diag = lower_tri + upper_tri\n",
    "    off_diag = ops.sum(tf.math.square(off_diag))/embed_dims\n",
    "    off_diag = ops.cast(off_diag, \"float32\")\n",
    "    if testing : \n",
    "        print(f\"Covariacne matrix shape : {ops.shape(cov_mat)}\")\n",
    "    return off_diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94a0d188",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T03:33:21.082554Z",
     "iopub.status.busy": "2024-05-17T03:33:21.082078Z",
     "iopub.status.idle": "2024-05-17T03:33:21.143014Z",
     "shell.execute_reply": "2024-05-17T03:33:21.141827Z"
    },
    "papermill": {
     "duration": 0.082152,
     "end_time": "2024-05-17T03:33:21.145691",
     "exception": false,
     "start_time": "2024-05-17T03:33:21.063539",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BarlowModel(keras.Model):\n",
    "    def __init__(self, feature_extractor, embed_dims, probe = False, multiview = False,\n",
    "                 probe_heads = None, probe_activation = \"sigmoid\",\n",
    "                 diag = 0.6, off_diag = 0.4, \n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dims = embed_dims\n",
    "        self.multiview = multiview\n",
    "        self.feature_extractor = feature_extractor #weight sharing for VICreg, Barlow Twins\n",
    "        self.projector_a = self.get_projector()\n",
    "        self.projector_b = self.get_projector()\n",
    "        self.diag = diag ; self.off_diag = off_diag\n",
    "        self.loss_tracker = tf.keras.metrics.Mean(\"Barlow_loss_tracker\")\n",
    "        self.linear_probing = probe\n",
    "        if probe:\n",
    "            self.probe_loss_tracker = tf.keras.metrics.Mean(\"Probe_loss_tracker\")\n",
    "            self.probe_metrics = [\n",
    "                                  keras.metrics.F1Score(average=\"micro\", threshold=None, name=\"probe_micro_f1_score\"),\n",
    "                                 keras.metrics.TruePositives(name = \"probe_TP\"),\n",
    "                                 keras.metrics.TrueNegatives(name = \"probe_TN\"),\n",
    "                                 keras.metrics.FalsePositives(name = \"probe_FP\"),\n",
    "                                 keras.metrics.FalseNegatives(name = \"probe_FN\")]\n",
    "            \n",
    "            self.probe_categories = probe_heads\n",
    "            self.probe_act = probe_activation\n",
    "            self.linear_probe = keras.layers.Dense(units = probe_heads, activation = self.probe_act, dtype = \"float32\")\n",
    "            if self.probe_act == \"sigmoid\":\n",
    "                self.probe_loss_fn = keras.losses.BinaryCrossentropy(reduction = None, label_smoothing = 0.1)\n",
    "                self.probe_metrics.append(keras.metrics.BinaryAccuracy(name=\"probe_accuracy\", threshold=0.5))\n",
    "            elif self.probe_act == \"softmax\":\n",
    "                self.probe_loss_fn = keras.losses.CategoricalCrossentropy(reduction = None, label_smoothing = 0.1)\n",
    "                self.probe_metrics.append(keras.metrics.CategoricalAccuracy(name=\"probe_accuracy\", threshold=0.5))\n",
    "    def compile(self, optimizer, probe_optimizer = None, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "        self.optimizer = optimizer\n",
    "        if self.linear_probing:\n",
    "            self.probe_optimizer = probe_optimizer\n",
    "        \n",
    "    def get_config(self):\n",
    "        return {\"feature_extractor_name\" : self.feature_extractor.name,\n",
    "               \"embed_dims\" : self.embed_dims,\n",
    "                \"Multiview(>2)\" : self.multiview,\n",
    "               'diag_part_coefficient' : self.diag,\n",
    "               'off_diag_coefficient' : self.off_diag,\n",
    "               \"SSL_method\" : \"Barlow_Twins\",\n",
    "               \"Linear Probe\" : self.linear_probing,\n",
    "               \"N_Categories\" : self.probe_categories if self.linear_probing else 0,\n",
    "               \"Probe Activation\" : self.probe_act if self.linear_probing else \"NA\"}\n",
    "    \n",
    "    def get_projector(self):\n",
    "        \n",
    "        model = keras.Sequential([Dense(units = self.embed_dims),\n",
    "                                 Dense(units = self.embed_dims),\n",
    "                                 Dense(units = self.embed_dims, dtype = \"float32\")]\n",
    "                                )\n",
    "        return model\n",
    "    \n",
    "    def compute_loss(self, correlation_matrix):\n",
    "        diag_component = tf.linalg.diag_part(correlation_matrix)\n",
    "        zero_diag = tf.zeros(correlation_matrix.shape[-1])\n",
    "        off_diag_matrix = tf.linalg.set_diag(correlation_matrix, zero_diag)\n",
    "        \n",
    "        diag_loss = tf.pow(diag_component-1, 2) * self.diag\n",
    "        off_diag_loss = tf.pow(off_diag_matrix, 2) * self.off_diag\n",
    "        loss = tf.reduce_mean(diag_loss) + tf.reduce_mean(off_diag_loss)\n",
    "        loss = tf.cast(loss, tf.float32)\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    def train_step(self, dataset):\n",
    "        \n",
    "        if self.linear_probing:\n",
    "            data, aug_data, labels = dataset\n",
    "        else:\n",
    "            if self.multiview:\n",
    "                data = dataset[0]\n",
    "                aug_data = dataset[1:] ; n_augs = len(aug_data)\n",
    "                aug_data = ops.concatenate(aug_data, axis = 0)\n",
    "            else:\n",
    "                data, aug_data = dataset\n",
    "        ## 1. SSL encoder loss ##\n",
    "        with tf.GradientTape() as tape:\n",
    "            if len(self.feature_extractor.outputs) == 2 :\n",
    "                feature_seq, weights = self.feature_extractor(data)\n",
    "                feature_seq_aug, weights_aug = self.feature_extractor(aug_data)\n",
    "            elif len(self.feature_extractor.outputs) == 1:\n",
    "                feature_seq = self.feature_extractor(data)\n",
    "                feature_seq_aug = self.feature_extractor(aug_data)\n",
    "                \n",
    "            if len(ops.shape(feature_seq)) == 3: \n",
    "                rep_vector = keras.layers.GlobalAveragePooling1D()(feature_seq)\n",
    "                rep_vector_aug = keras.layers.GlobalAveragePooling1D()(feature_seq_aug)\n",
    "            elif len(ops.shape(feature_seq)) == 4:\n",
    "                rep_vector = keras.layers.GlobalAveragePooling2D()(feature_seq)\n",
    "                rep_vector_aug = keras.layers.GlobalAveragePooling2D()(feature_seq_aug)\n",
    "            elif len(ops.shape(feature_seq)) == 2:\n",
    "                rep_vector = feature_seq\n",
    "                rep_vector_aug = feature_seq_aug\n",
    "            original_rep_vector = rep_vector #for Linear probing\n",
    "            \n",
    "            rep_vector = self.projector_a(rep_vector)\n",
    "            rep_vector_aug = self.projector_b(rep_vector_aug)\n",
    "            if self.multiview:\n",
    "                rep_vector_aug_set = ops.split(rep_vector_aug, n_augs, 0)\n",
    "                loss = []\n",
    "                for idx in range(n_augs):\n",
    "                    correlation_matrix = get_cor_matrix(rep_vector, rep_vector_aug_set[idx])\n",
    "                    loss.append(self.compute_loss(correlation_matrix)\n",
    "                               )\n",
    "                loss = ops.mean(loss)\n",
    "            else:\n",
    "                correlation_matrix = get_cor_matrix(rep_vector, rep_vector_aug)\n",
    "                loss = self.compute_loss(correlation_matrix)\n",
    "    \n",
    "\n",
    "        gradients = tape.gradient(loss, \n",
    "                                 (self.feature_extractor.trainable_variables + self.projector_a.trainable_variables + self.projector_b.trainable_variables))\n",
    "        self.optimizer.apply_gradients(zip(gradients, \n",
    "                                          (self.feature_extractor.trainable_variables + self.projector_a.trainable_variables + self.projector_b.trainable_variables)\n",
    "                                          ))\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        output_dict = {'ssl_loss' : self.loss_tracker.result()}\n",
    "        if self.linear_probing:\n",
    "            ##2. linear probing and calculate metrics##\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                if len(self.feature_extractor.outputs) == 2 :\n",
    "                    feature_seq, weights = self.feature_extractor(data, training = False)\n",
    "                elif len(self.feature_extractor.outputs) == 1:\n",
    "                    feature_seq = self.feature_extractor(data, training = False)\n",
    "                if len(ops.shape(feature_seq)) == 3: \n",
    "                    original_rep_vector = keras.layers.GlobalAveragePooling1D()(feature_seq)\n",
    "                elif len(ops.shape(feature_seq)) == 4:\n",
    "                    original_rep_vector = keras.layers.GlobalAveragePooling2D()(feature_seq)\n",
    "                elif len(ops.shape(feature_seq)) == 2:\n",
    "                    rep_vector = feature_seq\n",
    "        \n",
    "                class_logits = self.linear_probe(original_rep_vector, training = True)\n",
    "                class_probe_loss = self.probe_loss_fn(y_true = labels, y_pred = class_logits)\n",
    "                class_probe_loss = ops.mean(class_probe_loss)\n",
    "            gradients = tape.gradient(class_probe_loss, \n",
    "                                            self.linear_probe.trainable_weights)\n",
    "            self.probe_optimizer.apply_gradients(zip(gradients, self.linear_probe.trainable_weights))\n",
    "            self.probe_loss_tracker.update_state(class_probe_loss)\n",
    "            output_dict.update({\"probe_classification_loss\" : self.probe_loss_tracker.result()})\n",
    "            #metrics\n",
    "            for comp in self.probe_metrics:\n",
    "                comp.update_state(labels, class_logits)\n",
    "                \n",
    "            probe_metric_dict = {comp.name :  comp.result() for comp in self.probe_metrics}\n",
    "            output_dict.update(probe_metric_dict)\n",
    "        else:\n",
    "            pass\n",
    "        del tape\n",
    "        return output_dict\n",
    "    def test_step(self, dataset):\n",
    "        if self.linear_probing:\n",
    "            data, aug_data, labels = dataset\n",
    "        else:\n",
    "            if self.multiview:\n",
    "                data = dataset[0]\n",
    "                aug_data = dataset[1:] ; n_augs = len(aug_data)\n",
    "                aug_data = ops.concatenate(aug_data, axis = 0)\n",
    "            else:\n",
    "                data, aug_data = dataset\n",
    "            \n",
    "        if len(self.feature_extractor.outputs) == 2 :\n",
    "            feature_seq, weights = self.feature_extractor(data)\n",
    "            feature_seq_aug, weights_aug = self.feature_extractor(aug_data)\n",
    "        elif len(self.feature_extractor.outputs) == 1:\n",
    "            feature_seq = self.feature_extractor(data)\n",
    "            feature_seq_aug = self.feature_extractor(aug_data)\n",
    "        if len(ops.shape(feature_seq)) == 3: \n",
    "            rep_vector = keras.layers.GlobalAveragePooling1D()(feature_seq)\n",
    "            rep_vector_aug = keras.layers.GlobalAveragePooling1D()(feature_seq_aug)\n",
    "        elif len(ops.shape(feature_seq)) == 4:\n",
    "            rep_vector = keras.layers.GlobalAveragePooling2D()(feature_seq)\n",
    "            rep_vector_aug = keras.layers.GlobalAveragePooling2D()(feature_seq_aug)\n",
    "        elif len(ops.shape(feature_seq)) == 2:\n",
    "            rep_vector = feature_seq\n",
    "            rep_vector_aug = feature_seq_aug\n",
    "        \n",
    "        rep_vector = self.projector_a(rep_vector)\n",
    "        rep_vector_aug = self.projector_b(rep_vector_aug)\n",
    "        \n",
    "        if self.multiview:\n",
    "            rep_vector_aug_set = ops.split(rep_vector_aug, n_augs, 0)\n",
    "            loss = []\n",
    "            for idx in range(n_augs):\n",
    "                correlation_matrix = get_cor_matrix(rep_vector, rep_vector_aug_set[idx])\n",
    "                loss.append(self.compute_loss(correlation_matrix)\n",
    "                            )\n",
    "            loss = ops.mean(loss)\n",
    "        else:\n",
    "            correlation_matrix = get_cor_matrix(rep_vector, rep_vector_aug)\n",
    "            loss = self.compute_loss(correlation_matrix)\n",
    "        \n",
    "        self.loss_tracker.update_state(loss)\n",
    "        output_dict = {'ssl_loss' : self.loss_tracker.result()}\n",
    "        \n",
    "        if self.linear_probing:\n",
    "            ##2. linear probing and calculate metrics##\n",
    "            if len(self.feature_extractor.outputs) == 2 :\n",
    "                feature_seq, weights = self.feature_extractor(data, training = False)\n",
    "            elif len(self.feature_extractor.outputs) == 1:\n",
    "                feature_seq = self.feature_extractor(data, training = False)\n",
    "            if len(ops.shape(feature_seq)) == 3: \n",
    "                original_rep_vector = keras.layers.GlobalAveragePooling1D()(feature_seq)\n",
    "            elif len(ops.shape(feature_seq)) == 4:\n",
    "                original_rep_vector = keras.layers.GlobalAveragePooling2D()(feature_seq)\n",
    "            elif len(ops.shape(feature_seq)) == 2:\n",
    "                original_rep_vector = feature_seq\n",
    "\n",
    "            class_logits = self.linear_probe(original_rep_vector, training = False)\n",
    "            class_probe_loss = self.probe_loss_fn(y_true = labels, y_pred = class_logits)\n",
    "            class_probe_loss = ops.mean(class_probe_loss)\n",
    "            self.probe_loss_tracker.update_state(class_probe_loss)\n",
    "            output_dict.update({\"probe_classification_loss\" : self.probe_loss_tracker.result()})\n",
    "            #metrics\n",
    "            for comp in self.probe_metrics:\n",
    "                comp.update_state(labels, class_logits)\n",
    "                \n",
    "            probe_metric_dict = {comp.name :  comp.result() for comp in self.probe_metrics}\n",
    "            output_dict.update(probe_metric_dict)\n",
    "        else:\n",
    "            pass\n",
    "        return output_dict\n",
    "    def get_classifier_model(self):\n",
    "        self.feature_extractor.trainable = True\n",
    "        if self.linear_probing == False:\n",
    "            return self.feature_extractor\n",
    "        self.linear_probe.trainable = True\n",
    "        inputs = self.feature_extractor.input\n",
    "        outputs = self.feature_extractor.outputs\n",
    "        if len(self.feature_extractor.outputs) == 2 :\n",
    "            feature_map, att_weights = outputs\n",
    "        elif len(self.feature_extractor.outputs) == 1:\n",
    "            feature_map = outputs[0]\n",
    "        \n",
    "        if len(ops.shape(feature_map)) == 3: \n",
    "            rep_vector = keras.layers.GlobalAveragePooling1D()(feature_map)\n",
    "        elif len(ops.shape(feature_map)) == 4:\n",
    "            rep_vector = keras.layers.GlobalAveragePooling2D()(feature_map)\n",
    "        elif len(ops.shape(feature_map)) == 2:\n",
    "            rep_vector = feature_map\n",
    "            \n",
    "        class_logits = self.linear_probe(rep_vector)\n",
    "        classifier_model = keras.Model(inputs, class_logits,\n",
    "                                      name = f\"SSL_pretrained_{self.feature_extractor.name}\")\n",
    "        return classifier_model\n",
    "    def call(self, dataset):\n",
    "        return self.test_step(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1a05d42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T03:33:21.190107Z",
     "iopub.status.busy": "2024-05-17T03:33:21.189624Z",
     "iopub.status.idle": "2024-05-17T03:33:21.264331Z",
     "shell.execute_reply": "2024-05-17T03:33:21.263156Z"
    },
    "papermill": {
     "duration": 0.098817,
     "end_time": "2024-05-17T03:33:21.266997",
     "exception": false,
     "start_time": "2024-05-17T03:33:21.168180",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VICRegModel(keras.Model):\n",
    "    def __init__(self, feature_extractor, embed_dims, multiview = False,\n",
    "                 probe = False, \n",
    "                 probe_heads = None, probe_activation = \"sigmoid\",\n",
    "                 variance_coeff = 20, invariance_coeff = 20, covariance_coeff = 1, \n",
    "                 variance_gamma = 5.0,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dims = embed_dims\n",
    "        self.multiview = multiview\n",
    "        self.feature_extractor = feature_extractor #weight sharing for VICreg, Barlow Twins\n",
    "        self.projector_a = self.get_projector()\n",
    "        self.projector_b = self.get_projector()\n",
    "        self.var_coef = variance_coeff ; self.invar_coef = invariance_coeff ; self.cov_coef = covariance_coeff\n",
    "        self.gamma = variance_gamma\n",
    "        \n",
    "        self.loss_tracker = tf.keras.metrics.Mean(\"VIC_loss_tracker\")\n",
    "        self.invar_loss_tracker = tf.keras.metrics.Mean(\"Invariance_loss_tracker\")\n",
    "        self.var_loss_tracker = tf.keras.metrics.Mean(\"Variance_loss_tracker\")\n",
    "        self.covar_loss_tracker = tf.keras.metrics.Mean(\"Covariance_loss_tracker\")\n",
    "        self.linear_probing = probe\n",
    "        if probe:\n",
    "            self.probe_loss_tracker = tf.keras.metrics.Mean(\"Probe_loss_tracker\")\n",
    "            self.probe_metrics = [keras.metrics.Accuracy(name=\"probe_accuracy\"),\n",
    "                                  keras.metrics.F1Score(average=\"micro\", threshold=None, name=\"probe_micro_f1_score\"),\n",
    "                                 keras.metrics.TruePositives(name = \"probe_TP\"),\n",
    "                                 keras.metrics.TrueNegatives(name = \"probe_TN\"),\n",
    "                                 keras.metrics.FalsePositives(name = \"probe_FP\"),\n",
    "                                 keras.metrics.FalseNegatives(name = \"probe_FN\")]\n",
    "            \n",
    "            self.probe_categories = probe_heads\n",
    "            self.probe_act = probe_activation\n",
    "            self.linear_probe = keras.layers.Dense(units = probe_heads, activation = self.probe_act)\n",
    "            if self.probe_act == \"sigmoid\":\n",
    "                self.probe_loss_fn = keras.losses.BinaryCrossentropy(reduction = None, label_smoothing = 0.1)\n",
    "            elif self.probe_act == \"softmax\":\n",
    "                self.probe_loss_fn = keras.losses.CategoricalCrossentropy(reduction = None, label_smoothing = 0.1)\n",
    "    def compile(self, optimizer, probe_optimizer = None, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "        self.optimizer = optimizer\n",
    "        if self.linear_probing:\n",
    "            self.probe_optimizer = probe_optimizer     \n",
    "    def get_config(self):\n",
    "        return {\"feature_extractor_name\" : self.feature_extractor.name,\n",
    "               \"embed_dims\" : self.embed_dims,\n",
    "                 \"Multiview(>2)\" : self.multiview,\n",
    "               'Variance_coefficient' : self.var_coef,\n",
    "               'Invariance_coefficient' : self.invar_coef,\n",
    "               \"Covariance_coefficient\" : self.cov_coef,\n",
    "                \"Variance_gamma\" : self.gamma,\n",
    "               \"SSL_method\" : \"VICReg\",\n",
    "               \"Linear Probe\" : self.linear_probing,\n",
    "               \"N_Categories\" : self.probe_categories if self.linear_probing else 0,\n",
    "               \"Probe Activation\" : self.probe_act if self.linear_probing else \"NA\"}\n",
    "    def get_projector(self):\n",
    "        model = keras.Sequential([Dense(units = self.embed_dims),\n",
    "                                 Dense(units = self.embed_dims),\n",
    "                                 Dense(units = self.embed_dims,dtype = \"float32\")]\n",
    "                                )\n",
    "        return model\n",
    "    \n",
    "    def compute_loss(self, rep_a, rep_b):\n",
    "        invar_loss = invariance_loss(rep_a, rep_b) * self.invar_coef\n",
    "        invar_loss = ops.mean(invar_loss)\n",
    "        \n",
    "        variance_a = variance(rep_a, gamma = self.gamma)\n",
    "        variance_a = ops.mean(variance_a)\n",
    "        \n",
    "        covariance_a = covariance(rep_a)\n",
    "        \n",
    "        variance_b = variance(rep_b, gamma = self.gamma)\n",
    "        variance_b = ops.mean(variance_b)\n",
    "        \n",
    "        covariance_b = covariance(rep_b)\n",
    "        #Variance loss -> variance가 toward gamma\n",
    "        # covariance -> covariance가 toward zero\n",
    "        var_loss = (variance_a + variance_b) * self.var_coef\n",
    "        covar_loss = (covariance_a + covariance_b) * self.cov_coef\n",
    "        loss = invar_loss + var_loss + covar_loss\n",
    "        return loss, invar_loss, var_loss, covar_loss\n",
    "        \n",
    "    def train_step(self, dataset):\n",
    "        if self.linear_probing:\n",
    "            data, aug_data, labels = dataset\n",
    "        else:\n",
    "            if self.multiview:\n",
    "                data = dataset[0]\n",
    "                aug_data = dataset[1:] ; n_augs = len(aug_data)\n",
    "                aug_data = ops.concatenate(aug_data, axis = 0)\n",
    "            else:\n",
    "                data, aug_data = dataset\n",
    "            \n",
    "        with tf.GradientTape() as tape:\n",
    "            \n",
    "            if len(self.feature_extractor.outputs) == 2 :\n",
    "                feature_seq, weights = self.feature_extractor(data)\n",
    "                feature_seq_aug, weights_aug = self.feature_extractor(aug_data)\n",
    "            elif len(self.feature_extractor.outputs) == 1:\n",
    "                feature_seq = self.feature_extractor(data)\n",
    "                feature_seq_aug = self.feature_extractor(aug_data)\n",
    "            \n",
    "            if len(ops.shape(feature_seq)) == 3: \n",
    "                rep_vector = keras.layers.GlobalAveragePooling1D()(feature_seq)\n",
    "                rep_vector_aug = keras.layers.GlobalAveragePooling1D()(feature_seq_aug)\n",
    "            elif len(ops.shape(feature_seq)) == 4:\n",
    "                rep_vector = keras.layers.GlobalAveragePooling2D()(feature_seq)\n",
    "                rep_vector_aug = keras.layers.GlobalAveragePooling2D()(feature_seq_aug)\n",
    "            elif len(ops.shape(feature_seq)) == 2:\n",
    "                rep_vector = feature_seq\n",
    "                rep_vector_aug = feature_seq_aug\n",
    "            \n",
    "            \n",
    "            rep_vector = self.projector_a(rep_vector)\n",
    "            rep_vector_aug = self.projector_b(rep_vector_aug)\n",
    "            if self.multiview:\n",
    "                loss, invar_loss, var_loss, covar_loss = 0.0, 0.0, 0.0, 0.0\n",
    "                rep_vector_aug_set = ops.split(rep_vector_aug, n_augs, 0)\n",
    "                for idx in range(n_augs):\n",
    "                    loss_, invar_loss_, var_loss_, covar_loss_ = self.compute_loss(rep_vector, rep_vector_aug_set[idx])\n",
    "                    loss += loss_\n",
    "                    invar_loss += invar_loss_\n",
    "                    var_loss += var_loss_\n",
    "                    covar_loss += covar_loss_\n",
    "                loss /= n_augs\n",
    "                invar_loss /= n_augs\n",
    "                var_loss /= n_augs\n",
    "                covar_loss /= n_augs\n",
    "                \n",
    "            else:\n",
    "                loss, invar_loss, var_loss, covar_loss = self.compute_loss(rep_vector, rep_vector_aug)\n",
    "        \n",
    "        feature_extractor_var = self.feature_extractor.trainable_variables\n",
    "        proj_a_var = self.projector_a.trainable_variables\n",
    "        proj_b_var = self.projector_b.trainable_variables\n",
    "        \n",
    "        trainable_variables = (feature_extractor_var + proj_a_var + proj_b_var)\n",
    "        gradients = tape.gradient(loss, trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "        \n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.invar_loss_tracker.update_state(invar_loss)\n",
    "        self.var_loss_tracker.update_state(var_loss)\n",
    "        self.covar_loss_tracker.update_state(covar_loss)\n",
    "        output_dict = {'loss' : self.loss_tracker.result(),\n",
    "                        \"invariance_loss\" : self.invar_loss_tracker.result(),\n",
    "                        'variance_loss' : self.var_loss_tracker.result(),\n",
    "                        'covariance_loss' : self.covar_loss_tracker.result()\n",
    "                       }\n",
    "        \n",
    "        if self.linear_probing:\n",
    "            ##2. linear probing and calculate metrics##\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                if len(self.feature_extractor.outputs) == 2 :\n",
    "                    feature_seq, weights = self.feature_extractor(data, training = False)\n",
    "                elif len(self.feature_extractor.outputs) == 1:\n",
    "                    feature_seq = self.feature_extractor(data, training = False)\n",
    "                if len(ops.shape(feature_seq)) == 3: \n",
    "                    original_rep_vector = keras.layers.GlobalAveragePooling1D()(feature_seq)\n",
    "                elif len(ops.shape(feature_seq)) == 4:\n",
    "                    original_rep_vector = keras.layers.GlobalAveragePooling2D()(feature_seq)\n",
    "                elif len(ops.shape(feature_seq)) == 2:\n",
    "                    original_rep_vector = feature_seq\n",
    "                    \n",
    "                class_logits = self.linear_probe(original_rep_vector, training = True)\n",
    "                class_probe_loss = self.probe_loss_fn(y_true = labels, y_pred = class_logits)\n",
    "                class_probe_loss = ops.mean(class_probe_loss)\n",
    "            gradients = tape.gradient(class_probe_loss, \n",
    "                                            self.linear_probe.trainable_weights)\n",
    "            self.probe_optimizer.apply_gradients(zip(gradients, self.linear_probe.trainable_weights))\n",
    "            self.probe_loss_tracker.update_state(class_probe_loss)\n",
    "            output_dict.update({\"probe_classification_loss\" : self.probe_loss_tracker.result()})\n",
    "            #metrics\n",
    "            for comp in self.probe_metrics:\n",
    "                comp.update_state(labels, class_logits)\n",
    "                \n",
    "            probe_metric_dict = {comp.name :  comp.result() for comp in self.probe_metrics}\n",
    "            output_dict.update(probe_metric_dict)\n",
    "        else:\n",
    "            pass\n",
    "        del tape\n",
    "        return output_dict\n",
    "    def test_step(self, dataset):\n",
    "        if self.linear_probing:\n",
    "            data, aug_data, labels = dataset\n",
    "        else:\n",
    "            if self.multiview:\n",
    "                data = dataset[0]\n",
    "                aug_data = dataset[1:] ; n_augs = len(aug_data)\n",
    "                aug_data = ops.concatenate(aug_data, axis = 0)\n",
    "            else:\n",
    "                data, aug_data = dataset\n",
    "        \n",
    "        if len(self.feature_extractor.outputs) == 2 :\n",
    "            feature_seq, weights = self.feature_extractor(data)\n",
    "            feature_seq_aug, weights_aug = self.feature_extractor(aug_data)\n",
    "        elif len(self.feature_extractor.outputs) == 1:\n",
    "            feature_seq = self.feature_extractor(data)\n",
    "            feature_seq_aug = self.feature_extractor(aug_data)\n",
    "        \n",
    "        if len(ops.shape(feature_seq)) == 3: \n",
    "            rep_vector = keras.layers.GlobalAveragePooling1D()(feature_seq)\n",
    "            rep_vector_aug = keras.layers.GlobalAveragePooling1D()(feature_seq_aug)\n",
    "        elif len(ops.shape(feature_seq)) == 4:\n",
    "            rep_vector = keras.layers.GlobalAveragePooling2D()(feature_seq)\n",
    "            rep_vector_aug = keras.layers.GlobalAveragePooling2D()(feature_seq_aug)\n",
    "        elif len(ops.shape(feature_seq)) == 2:\n",
    "            rep_vector = feature_seq\n",
    "            rep_vector_aug = feature_seq_aug\n",
    "            \n",
    "        original_rep_vector = rep_vector\n",
    "        \n",
    "        rep_vector = self.projector_a(rep_vector, training = False)\n",
    "        rep_vector_aug = self.projector_b(rep_vector_aug, training = False)\n",
    "        if self.multiview:\n",
    "            loss, invar_loss, var_loss, covar_loss = [],[],[],[]\n",
    "            rep_vector_aug_set = ops.split(rep_vector_aug, n_augs, 0)\n",
    "            for idx in range(n_augs):\n",
    "                loss_, invar_loss_, var_loss_, covar_loss_ = self.compute_loss(rep_vector, rep_vector_aug_set[idx])\n",
    "                loss.append(loss_)\n",
    "                invar_loss.append(invar_loss_)\n",
    "                var_loss.append(var_loss_)\n",
    "                covar_loss.append(covar_loss_)\n",
    "            loss = ops.mean(loss)\n",
    "            invar_loss = ops.mean(invar_loss)\n",
    "            var_loss = ops.mean(var_loss)\n",
    "            covar_loss = ops.mean(covar_loss)    \n",
    "        else:\n",
    "            loss, invar_loss, var_loss, covar_loss = self.compute_loss(rep_vector, rep_vector_aug)\n",
    "        \n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.invar_loss_tracker.update_state(invar_loss)\n",
    "        self.var_loss_tracker.update_state(var_loss)\n",
    "        self.covar_loss_tracker.update_state(covar_loss)\n",
    "        \n",
    "        output_dict =  {'loss' : self.loss_tracker.result(),\n",
    "                \"invariance_loss\" : self.invar_loss_tracker.result(),\n",
    "                'variance_loss' : self.var_loss_tracker.result(),\n",
    "                'covariance_loss' : self.covar_loss_tracker.result()\n",
    "               }\n",
    "        if self.linear_probing:\n",
    "            ##2. linear probing and calculate metrics##\n",
    "            self.feature_extractor.trainable = False\n",
    "            class_logits = self.linear_probe(original_rep_vector, training = False)\n",
    "            class_probe_loss = self.probe_loss_fn(y_true = labels, y_pred = class_logits)\n",
    "            class_probe_loss = ops.mean(class_probe_loss)\n",
    "            self.probe_loss_tracker.update_state(class_probe_loss)\n",
    "            output_dict.update({\"probe_classification_loss\" : self.probe_loss_tracker.result()})\n",
    "            #metrics\n",
    "            for comp in self.probe_metrics:\n",
    "                comp.update_state(labels, class_logits)\n",
    "                \n",
    "            probe_metric_dict = {comp.name :  comp.result() for comp in self.probe_metrics}\n",
    "            output_dict.update(probe_metric_dict)\n",
    "            self.feature_extractor.trainable = True\n",
    "        return output_dict\n",
    "    def get_classifier_model(self):\n",
    "        self.feature_extractor.trainable = True\n",
    "        if self.linear_probing == False:\n",
    "            return self.feature_extractor\n",
    "        self.linear_probe.trainable = True\n",
    "        inputs = self.feature_extractor.input\n",
    "        outputs = self.feature_extractor.outputs\n",
    "        if len(self.feature_extractor.outputs) == 2 :\n",
    "            feature_map, att_weights = outputs\n",
    "        elif len(self.feature_extractor.outputs) == 1:\n",
    "            feature_map = outputs[0]\n",
    "        \n",
    "        if len(ops.shape(feature_map)) == 3: \n",
    "            rep_vector = keras.layers.GlobalAveragePooling1D()(feature_map)\n",
    "        elif len(ops.shape(feature_map)) == 4:\n",
    "            rep_vector = keras.layers.GlobalAveragePooling2D()(feature_map)\n",
    "        elif len(ops.shape(feature_seq)) == 2:\n",
    "            rep_vector = feature_map\n",
    "            \n",
    "        class_logits = self.linear_probe(rep_vector)\n",
    "        classifier_model = keras.Model(inputs, class_logits,\n",
    "                                      name = f\"SSL_pretrained_{self.feature_extractor.name}\")\n",
    "        return classifier_model\n",
    "    def call(self, dataset):\n",
    "        return self.test_step(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1d92bd",
   "metadata": {
    "papermill": {
     "duration": 0.016022,
     "end_time": "2024-05-17T03:33:21.299638",
     "exception": false,
     "start_time": "2024-05-17T03:33:21.283616",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "-------------\n",
    "# Contrastive-based method\n",
    "- SimSiam\n",
    "- SimCLR\n",
    "- SwAV\n",
    "- DINO\n",
    "- MoCo and moco-based learnings:\n",
    "    - [Dense Contrastive learning](https://arxiv.org/abs/2011.09157v2) -> DCL\n",
    "    - [Nearest-Neighbor Contrastive Learning of Visual Representations](https://arxiv.org/abs/2104.14548v2) -> NNCLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91bd7d0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T03:33:21.334897Z",
     "iopub.status.busy": "2024-05-17T03:33:21.334495Z",
     "iopub.status.idle": "2024-05-17T03:33:21.376528Z",
     "shell.execute_reply": "2024-05-17T03:33:21.375401Z"
    },
    "papermill": {
     "duration": 0.062842,
     "end_time": "2024-05-17T03:33:21.379129",
     "exception": false,
     "start_time": "2024-05-17T03:33:21.316287",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SimSiam(keras.Model):\n",
    "    def __init__(self, feature_extractor, embed_dims = 256, multiview = False,\n",
    "                 probe = False, probe_heads = None, probe_activation = \"sigmoid\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.feature_extractor = feature_extractor #attentive pooling을 거친 encoder로 가정 : rep_vector, attention_Weight를 output으로\n",
    "        self.embed_dims = embed_dims\n",
    "        self.multiview = multiview\n",
    "        \n",
    "        self.train_type = 'SimSiam'\n",
    "        self.loss_tracker = keras.metrics.Mean(name = f\"{self.train_type}_loss\")\n",
    "        self.probe = probe ; self.probe_heads = probe_heads\n",
    "        self.probe_activation = probe_activation\n",
    "        \n",
    "        print(\"Train with Gradient Accumulation is recommended!\")\n",
    "        \n",
    "        self.embed_dims = embed_dims\n",
    "        self.predictor = keras.Sequential([keras.layers.Dense(units = self.embed_dims,\n",
    "                                                            use_bias = False,\n",
    "                                                            kernel_regularizer = keras.regularizers.l2(5e-4),\n",
    "                                                            ),\n",
    "                                          keras.layers.Activation(\"gelu\"),\n",
    "                                          keras.layers.BatchNormalization(),\n",
    "                                          keras.layers.Dense(units = self.embed_dims),\n",
    "                                          ], name = f\"{self.train_type}_predictor\")\n",
    "        if self.probe:\n",
    "            self.linear_probe = Dense(units = self.probe_heads, \n",
    "                                     activation = self.probe_activation)\n",
    "            self.probe_loss_tracker = tf.keras.metrics.Mean(\"Probe_loss_tracker\")\n",
    "            self.probe_metrics = [keras.metrics.Accuracy(name=\"probe_accuracy\"),\n",
    "                                  keras.metrics.F1Score(average=\"micro\", threshold=None, name=\"probe_micro_f1_score\"),\n",
    "                                 keras.metrics.TruePositives(name = \"probe_TP\"),\n",
    "                                 keras.metrics.TrueNegatives(name = \"probe_TN\"),\n",
    "                                 keras.metrics.FalsePositives(name = \"probe_FP\"),\n",
    "                                 keras.metrics.FalseNegatives(name = \"probe_FN\")]\n",
    "            if self.probe_activation == \"sigmoid\":\n",
    "                self.probe_loss_fn = keras.losses.BinaryCrossentropy(reduction = None, label_smoothing = 0.1)\n",
    "            elif self.probe_activation == \"softmax\":\n",
    "                self.probe_loss_fn = keras.losses.CategoricalCrossentropy(reduction = None, label_smoothing = 0.1)\n",
    "    def get_config(self):\n",
    "        return {\"feature_extractor_name\" : self.feature_extractor.name,\n",
    "               \"embed_dims\" : self.embed_dims,\n",
    "                \"Multiview(>2)\" : self.multiview,\n",
    "               \"SSL_method\" : \"SimSiam\",\n",
    "               \"Linear Probe\" : self.probe,\n",
    "               \"N_Categories\" : self.probe_categories if self.probe else 0,\n",
    "               \"Probe Activation\" : self.probe_act if self.probe else \"NA\"}\n",
    "    def compile(self, optimizer, probe_optimizer = None, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "        self.optimizer = optimizer\n",
    "        if self.probe:\n",
    "            self.probe_optimizer = probe_optimizer \n",
    "    def compute_loss(self, p, z, epsilon = 1e-5):\n",
    "        # stop gradient is essential in SimSiam structure\n",
    "        # p, z is representation vectors : batch_size, embed_dims\n",
    "        \n",
    "        z = ops.stop_gradient(p)\n",
    "        z = keras.utils.normalize(z, axis = -1, order = 2)\n",
    "        p = keras.utils.normalize(p, axis = -1, order = 2)\n",
    "        cos_sim = ops.mean(ops.sum(z*p, axis = -1)\n",
    "                          ) #batchwise mean\n",
    "        return -cos_sim\n",
    "\n",
    "    def train_step(self, dataset):\n",
    "        if self.probe:\n",
    "            img1, img2, labels = dataset\n",
    "        else:\n",
    "            if self.multiview:\n",
    "                img1 = dataset[0]\n",
    "                img2 = dataset[1:] ; n_augs = len(img2)\n",
    "                img2 = ops.concatenate(img2, axis = 0)\n",
    "            else:\n",
    "                img1, img2 = dataset\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            if len(self.feature_extractor.outputs) == 2:\n",
    "                z1, weight1 = self.feature_extractor(img1, training = True)\n",
    "                z2, weight2 = self.feature_extractor(img2, training = True)\n",
    "            elif len(self.feature_extractor.outputs) == 1:\n",
    "                z1 = self.feature_extractor(img1, training = True)\n",
    "                z2 = self.feature_extractor(img2, training = True)\n",
    "            v1, v2 = self.predictor(z1, training = True), self.predictor(z2, training = True)\n",
    "            \n",
    "            if self.multiview:\n",
    "                v2 = ops.split(v2, n_augs, 0)\n",
    "                loss = []\n",
    "                for idx in range(n_augs):\n",
    "                    loss.append(self.compute_loss(v1, v2[idx])\n",
    "                               )\n",
    "                loss = ops.mean(loss)\n",
    "            else:\n",
    "                loss = 0.5*(self.compute_loss(v1, v2) + self.compute_loss(v2, v1))\n",
    "        trainable_params = self.feature_extractor.trainable_variables + self.predictor.trainable_variables\n",
    "        grads = tape.gradient(loss, trainable_params)\n",
    "        self.optimizer.apply_gradients(zip(grads, trainable_params))\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        output_dict = {self.loss_tracker.name : self.loss_tracker.result()}\n",
    "        \n",
    "        if self.probe:\n",
    "            with tf.GradientTape() as tape:\n",
    "                class_logits = self.linear_probe(z1)\n",
    "                probe_loss = self.probe_loss_fn(y_true = labels, y_pred = class_logits)\n",
    "                probe_loss = ops.mean(probe_loss)\n",
    "            grads = tape.gradient(probe_loss, self.linear_probe.trainable_weights)\n",
    "            self.probe_optimizer.apply_gradients(zip(grads, self.linear_probe.trainable_weights))\n",
    "            self.probe_loss_tracker.update_state(probe_loss)\n",
    "            output_dict.update({\"probe_classification_loss\" : self.probe_loss_tracker.result()})\n",
    "            #metrics\n",
    "            for comp in self.probe_metrics:\n",
    "                comp.update_state(labels, class_logits)\n",
    "                \n",
    "            probe_metric_dict = {comp.name :  comp.result() for comp in self.probe_metrics}\n",
    "            output_dict.update(probe_metric_dict)\n",
    "        del tape\n",
    "        return output_dict\n",
    "    \n",
    "    def test_step(self, dataset):\n",
    "        if self.probe:\n",
    "            img1, img2, labels = dataset\n",
    "        else:\n",
    "            if self.multiview:\n",
    "                img1 = dataset[0]\n",
    "                img2 = dataset[1:] ; n_augs = len(img2)\n",
    "                img2 = ops.concatenate(img2, axis = 0)\n",
    "            else:\n",
    "                img1, img2 = dataset\n",
    "        \n",
    "        if len(self.feature_extractor.outputs) == 2:\n",
    "            z1, weight1 = self.feature_extractor(img1, training = False)\n",
    "            z2, weight2 = self.feature_extractor(img2, training = False)\n",
    "        elif len(self.feature_extractor.outputs) == 1:\n",
    "            z1 = self.feature_extractor(img1, training = False)\n",
    "            z2 = self.feature_extractor(img2, training = False)\n",
    "        v1, v2 = self.predictor(z1, training = False), self.predictor(z2, training = False)\n",
    "        if self.multiview:\n",
    "            v2 = ops.split(v2, n_augs, 0)\n",
    "            loss = []\n",
    "            for idx in range(n_augs):\n",
    "                loss.append(self.compute_loss(v1, v2[idx])\n",
    "                            )\n",
    "            loss = ops.mean(loss)\n",
    "        else:\n",
    "            loss = 0.5*(self.compute_loss(v1, v2) + self.compute_loss(v2, v1))\n",
    "        \n",
    "        self.loss_tracker.update_state(loss)\n",
    "        output_dict = {self.loss_tracker.name : self.loss_tracker.result()}\n",
    "        \n",
    "        if self.probe:\n",
    "            class_logits = self.linear_probe(z1)\n",
    "            probe_loss = self.probe_loss_fn(y_true = labels, y_pred = class_logits)\n",
    "            probe_loss = ops.mean(probe_loss)\n",
    "            self.probe_loss_tracker.update_state(probe_loss)\n",
    "            output_dict.update({\"probe_classification_loss\" : self.probe_loss_tracker.result()})\n",
    "            for comp in self.probe_metrics:\n",
    "                comp.update_state(labels, class_logits)\n",
    "                \n",
    "            probe_metric_dict = {comp.name :  comp.result() for comp in self.probe_metrics}\n",
    "            output_dict.update(probe_metric_dict)\n",
    "        \n",
    "        return output_dict\n",
    "    \n",
    "    def call(self, dataset):\n",
    "        return self.test_step(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20ae9798",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T03:33:21.414648Z",
     "iopub.status.busy": "2024-05-17T03:33:21.414263Z",
     "iopub.status.idle": "2024-05-17T03:33:21.448488Z",
     "shell.execute_reply": "2024-05-17T03:33:21.447134Z"
    },
    "papermill": {
     "duration": 0.054789,
     "end_time": "2024-05-17T03:33:21.450830",
     "exception": false,
     "start_time": "2024-05-17T03:33:21.396041",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SimCLR(keras.Model):\n",
    "    def __init__(self, feature_extractor, embed_dims = 256, multiview = False, probe = False,\n",
    "                 temperature = 0.1,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.feature_extractor = feature_extractor #attentive pooling을 거친 encoder로 가정 : rep_vector, attention_Weight를 output으로\n",
    "        self.embed_dims = embed_dims\n",
    "        self.multiview = multiview\n",
    "        self.t = temperature\n",
    "        self.train_type = 'SimCLR'\n",
    "        print(\"Train with Gradient Accumulation is recommended!\")\n",
    "        \n",
    "        self.embed_dims = embed_dims\n",
    "        self.predictor = keras.Sequential([keras.layers.Dense(units = self.embed_dims,\n",
    "                                                            use_bias = False,\n",
    "                                                            kernel_regularizer = keras.regularizers.l2(5e-4),\n",
    "                                                            ),\n",
    "                                          keras.layers.Activation(\"gelu\"),\n",
    "                                          keras.layers.BatchNormalization(),\n",
    "                                          keras.layers.Dense(units = self.embed_dims),\n",
    "                                          ], name = f\"{self.train_type}_predictor\")\n",
    "        self.probe = False\n",
    "        \n",
    "        self.contrastive_accuracy = keras.metrics.SparseCategoricalAccuracy(\n",
    "            name=\"simclr_contrastive_accuracy\"\n",
    "        )\n",
    "        self.val_contrastive_accuracy = keras.metrics.SparseCategoricalAccuracy(\n",
    "            name=\"simclr_contrastive_accuracy\"\n",
    "        )\n",
    "        \n",
    "        self.loss_tracker = keras.metrics.Mean(name = f\"{self.train_type}_loss\")\n",
    "        \n",
    "    def get_config(self):\n",
    "        return {\"feature_extractor_name\" : self.feature_extractor.name,\n",
    "               \"embed_dims\" : self.embed_dims,\n",
    "                \"Multiview(>2)\" : self.multiview,\n",
    "               \"SSL_method\" : \"SimSiam\",\n",
    "               \"Linear Probe\" : self.probe,\n",
    "               \"N_Categories\" : self.probe_categories if self.probe else 0,\n",
    "               \"Probe Activation\" : self.probe_act if self.probe else \"NA\"}\n",
    "    def compile(self, optimizer, probe_optimizer = None, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "        self.optimizer = optimizer\n",
    "        if self.probe:\n",
    "            self.probe_optimizer = probe_optimizer \n",
    "    def compute_loss(self, p, z, epsilon = 1e-5, training = True):\n",
    "        v1, v2 = ops.normalize(p, axis = 1, order = 2), ops.normalize(z, axis = 1, order = 2) #batch, dims\n",
    "        sim_mat = ops.matmul(v1, ops.transpose(v2))/self.t #batch by batch\n",
    "        batch_size = ops.shape(v1)[0]\n",
    "        pseudo_labels = ops.arange(batch_size)\n",
    "        if training : \n",
    "            self.contrastive_accuracy.update_state(pseudo_labels, sim_mat)\n",
    "            self.contrastive_accuracy.update_state(pseudo_labels, ops.transpose(sim_mat))\n",
    "        else:\n",
    "            self.val_contrastive_accuracy.update_state(pseudo_labels, sim_mat)\n",
    "            self.val_contrastive_accuracy.update_state(pseudo_labels, ops.transpose(sim_mat))\n",
    "        loss_1 = keras.losses.sparse_categorical_crossentropy(\n",
    "            pseudo_labels, sim_mat, from_logits=True\n",
    "        )\n",
    "        loss_2 = keras.losses.sparse_categorical_crossentropy(pseudo_labels, ops.transpose(sim_mat),\n",
    "                                                             from_logits = True)\n",
    "        loss = 0.5*(loss_1 + loss_2)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def train_step(self, dataset):\n",
    "        if self.probe:\n",
    "            img1, img2, labels = dataset\n",
    "        else:\n",
    "            if self.multiview:\n",
    "                img1 = dataset[0]\n",
    "                img2 = dataset[1:] ; n_augs = len(img2)\n",
    "                img2 = ops.concatenate(img2, axis = 0)\n",
    "            else:\n",
    "                img1, img2 = dataset\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            if len(self.feature_extractor.outputs) == 2:\n",
    "                z1, weight1 = self.feature_extractor(img1, training = True)\n",
    "                z2, weight2 = self.feature_extractor(img2, training = True)\n",
    "            elif len(self.feature_extractor.outputs) == 1:\n",
    "                z1 = self.feature_extractor(img1, training = True)\n",
    "                z2 = self.feature_extractor(img2, training = True)\n",
    "            v1, v2 = self.predictor(z1, training = True), self.predictor(z2, training = True)\n",
    "            \n",
    "            if self.multiview:\n",
    "                v2 = ops.split(v2, n_augs, 0)\n",
    "                loss = []\n",
    "                for idx in range(n_augs):\n",
    "                    loss.append(self.compute_loss(v1, v2[idx]))\n",
    "                    \n",
    "                loss = ops.mean(loss)\n",
    "                \n",
    "            else:\n",
    "                loss = self.compute_loss(v1, v2)\n",
    "        trainable_params = self.feature_extractor.trainable_variables + self.predictor.trainable_variables\n",
    "        grads = tape.gradient(loss, trainable_params)\n",
    "        self.optimizer.apply_gradients(zip(grads, trainable_params))\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        output_dict = {self.loss_tracker.name : self.loss_tracker.result(),\n",
    "                      self.contrastive_accuracy.name : self.contrastive_accuracy.result()}\n",
    "        return output_dict\n",
    "    \n",
    "    def test_step(self, dataset):\n",
    "        if self.probe:\n",
    "            img1, img2, labels = dataset\n",
    "        else:\n",
    "            if self.multiview:\n",
    "                img1 = dataset[0]\n",
    "                img2 = dataset[1:] ; n_augs = len(img2)\n",
    "                img2 = ops.concatenate(img2, axis = 0)\n",
    "            else:\n",
    "                img1, img2 = dataset\n",
    "        \n",
    "        if len(self.feature_extractor.outputs) == 2:\n",
    "            z1, weight1 = self.feature_extractor(img1, training = True)\n",
    "            z2, weight2 = self.feature_extractor(img2, training = True)\n",
    "        elif len(self.feature_extractor.outputs) == 1:\n",
    "            z1 = self.feature_extractor(img1, training = True)\n",
    "            z2 = self.feature_extractor(img2, training = True)\n",
    "        v1, v2 = self.predictor(z1, training = False), self.predictor(z2, training = False)\n",
    "            \n",
    "        if self.multiview:\n",
    "            v2 = ops.split(v2, n_augs, 0)\n",
    "            loss = []\n",
    "            for idx in range(n_augs):\n",
    "                loss.append(self.compute_loss(v1, v2[idx],\n",
    "                                               training = False))        \n",
    "            loss = ops.mean(loss)\n",
    "            \n",
    "        else:\n",
    "            loss = self.compute_loss(v1, v2)\n",
    "        \n",
    "        self.loss_tracker.update_state(loss)\n",
    "        output_dict = {self.loss_tracker.name : self.loss_tracker.result(),\n",
    "                       self.val_contrastive_accuracy.name : self.val_contrastive_accuracy.result()\n",
    "                      }\n",
    "        return output_dict\n",
    "    \n",
    "    def call(self, dataset):\n",
    "        return self.test_step(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ea4bbdb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T03:33:21.486131Z",
     "iopub.status.busy": "2024-05-17T03:33:21.485733Z",
     "iopub.status.idle": "2024-05-17T03:33:21.515967Z",
     "shell.execute_reply": "2024-05-17T03:33:21.514748Z"
    },
    "papermill": {
     "duration": 0.050953,
     "end_time": "2024-05-17T03:33:21.518456",
     "exception": false,
     "start_time": "2024-05-17T03:33:21.467503",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Moco(keras.Model):\n",
    "    def __init__(self, feature_extractor, embed_dims, q_size = 2**13, pool_heads = 8, t = 0.07,\n",
    "                 momentum_coefficient = 0.999,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.momentum_encoder = feature_extractor\n",
    "        self.momentum_encoder.set_weights(self.feature_extractor.get_weights())\n",
    "        self.m = momentum_coefficient\n",
    "        self.pool_heads = pool_heads\n",
    "        self.t = t\n",
    "        self.q_size = q_size\n",
    "        self.embed_dims = embed_dims\n",
    "        self.feature_queue = keras.Variable(\n",
    "            keras.utils.normalize(\n",
    "                keras.random.normal(shape=(self.q_size, self.embed_dims)),\n",
    "                axis=1,\n",
    "                order=2,\n",
    "            ),\n",
    "            trainable=False, dtype = \"float32\"\n",
    "        )\n",
    "        self.projector = keras.Sequential([keras.layers.Dense(units = embed_dims),\n",
    "                                           keras.layers.Activation(\"relu\"),\n",
    "                                          keras.layers.Dense(units = embed_dims, dtype = \"float32\")])\n",
    "        self.momentum_projector = keras.models.clone_model(self.projector)\n",
    "        \n",
    "        self.ce_loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
    "        self.loss_tracker = keras.metrics.Mean(name = \"MoCo_loss\")\n",
    "    def get_config(self):\n",
    "        return {\"feature_extractor_name\" : self.feature_extractor.name,\n",
    "               \"embed_dims\" : self.embed_dims,\n",
    "               \"SSL_method\" : \"MoCo\",\n",
    "               \"Queue size\" : self.q_size,\n",
    "               \"temperature\" : self.t,\n",
    "               \"Momentum\" : self.m}\n",
    "    def compute_loss(self, z_a, z_b, training = True): #Info-NCE loss in the original paper\n",
    "        z_b = ops.stop_gradient(z_b)\n",
    "        z_a, z_b = keras.utils.normalize(z_a, axis = -1, order = 2), keras.utils.normalize(z_b, axis = -1, order = 2)\n",
    "        z_a, z_b = ops.cast(z_a, \"float32\"), ops.cast(z_b, \"float32\")\n",
    "        \n",
    "        pos_sim_ = ops.diagonal(z_a@ops.transpose(z_b)) ; pseudolabel = ops.zeros_like(pos_sim_)\n",
    "        pos_pair_similarity = ops.expand_dims(pos_sim_, axis = -1) ; del pos_sim_\n",
    "        \n",
    "        neg_pair_similarity = z_a@ops.cast(ops.transpose(self.feature_queue), \"float32\")\n",
    "        logits = ops.concatenate([pos_pair_similarity, neg_pair_similarity], axis = 1)\n",
    "        logits = ops.exp(logits/self.t)\n",
    "        logits = logits / (ops.sum(logits, axis = -1, keepdims = True) + 1e-8)\n",
    "        loss = self.ce_loss_fn(y_true = pseudolabel, y_pred = logits)\n",
    "        \n",
    "        if training:\n",
    "            self.feature_queue.assign(\n",
    "                ops.concatenate([z_a, ops.cast(self.feature_queue[:-ops.shape(z_a)[0] ],\n",
    "                                              \"float32\")\n",
    "                                ], axis=0)\n",
    "            )\n",
    "\n",
    "        return loss\n",
    "    def train_step(self, dataset):\n",
    "        img1, img2 = dataset\n",
    "        with tf.GradientTape() as tape:\n",
    "            if len(self.feature_extractor.outputs) == 2:\n",
    "                z1, weight1 = self.feature_extractor(img1)\n",
    "                z2, weight2 = self.momentum_encoder(img2)\n",
    "            elif len(self.feature_extractor.outputs) == 1:\n",
    "                z1 = self.feature_extractor(img1)\n",
    "                z2 = self.momentum_encoder(img2)\n",
    "            v1, v2 = self.projector(z1), self.momentum_projector(z2)\n",
    "            moco_loss = self.compute_loss(v1, v2)\n",
    "        grads = tape.gradient(moco_loss, \n",
    "                             self.feature_extractor.trainable_weights + self.projector.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, \n",
    "                                          self.feature_extractor.trainable_weights + self.projector.trainable_weights)\n",
    "                                      )\n",
    "        self.loss_tracker.update_state(moco_loss)\n",
    "        \n",
    "        #momentum update\n",
    "        for weight, m_weight in zip(self.feature_extractor.weights, self.momentum_encoder.weights):\n",
    "            m_weight.assign(self.m * m_weight + (1-self.m)*weight)\n",
    "        for weight, m_weight in zip(self.projector.weights, self.momentum_projector.weights):\n",
    "            m_weight.assign(self.m * m_weight + (1-self.m)*weight)\n",
    "        return {self.loss_tracker.name : self.loss_tracker.result()}\n",
    "    def test_step(self, dataset):\n",
    "        img1, img2 = dataset\n",
    "        if len(self.feature_extractor.outputs) == 2:\n",
    "            z1, weight1 = self.feature_extractor(img1)\n",
    "            z2, weight2 = self.momentum_encoder(img2)\n",
    "        elif len(self.feature_extractor.outputs) == 1:\n",
    "            z1 = self.feature_extractor(img1)\n",
    "            z2 = self.momentum_encoder(img2)\n",
    "        v1, v2 = self.projector(z1), self.momentum_projector(z2)\n",
    "        moco_loss = self.compute_loss(v1, v2, training = False)\n",
    "        self.loss_tracker.update_state(moco_loss)\n",
    "        return {self.loss_tracker.name : self.loss_tracker.result()}\n",
    "    def call(self, dataset):\n",
    "        img1, img2 = dataset\n",
    "        if len(self.feature_extractor.outputs) == 2:\n",
    "            z1, weight1 = self.feature_extractor(img1)\n",
    "            z2, weight2 = self.momentum_encoder(img2)\n",
    "        elif len(self.feature_extractor.outputs) == 1:\n",
    "            z1 = self.feature_extractor(img1)\n",
    "            z2 = self.momentum_encoder(img2)\n",
    "        v1, v2 = self.projector(z1), self.momentum_projector(z2)\n",
    "        moco_loss = self.compute_loss(v1, v2, training = False)\n",
    "        self.loss_tracker.update_state(moco_loss)\n",
    "        return {self.loss_tracker.name : self.loss_tracker.result()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7ff2a2",
   "metadata": {
    "papermill": {
     "duration": 0.016171,
     "end_time": "2024-05-17T03:33:21.551507",
     "exception": false,
     "start_time": "2024-05-17T03:33:21.535336",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b340f78",
   "metadata": {
    "papermill": {
     "duration": 0.016078,
     "end_time": "2024-05-17T03:33:21.584241",
     "exception": false,
     "start_time": "2024-05-17T03:33:21.568163",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Clustering & Distillation\n",
    "- SwAV, DINO\n",
    "\n",
    "> DINO architecture:\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:1400/1*huuMgEbBryxXUufW33uhvQ.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e5d81da9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T03:33:21.619747Z",
     "iopub.status.busy": "2024-05-17T03:33:21.619292Z",
     "iopub.status.idle": "2024-05-17T03:33:21.658535Z",
     "shell.execute_reply": "2024-05-17T03:33:21.657415Z"
    },
    "papermill": {
     "duration": 0.060224,
     "end_time": "2024-05-17T03:33:21.661128",
     "exception": false,
     "start_time": "2024-05-17T03:33:21.600904",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DINO(keras.Model):\n",
    "    def __init__(self, feature_extractor, embed_dims = 256, multiview = False, probe = False,\n",
    "                 temperature = 0.1,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.feature_extractor = feature_extractor #attentive pooling을 거친 encoder로 가정 : rep_vector, attention_Weight를 output으로\n",
    "        self.embed_dims = embed_dims\n",
    "        self.multiview = multiview\n",
    "        self.t = temperature\n",
    "        self.train_type = 'DINO'\n",
    "        self.embed_dims = embed_dims\n",
    "        self.predictor = keras.Sequential([keras.layers.Dense(units = self.embed_dims,\n",
    "                                                            use_bias = False,\n",
    "                                                            kernel_regularizer = keras.regularizers.l2(5e-4),\n",
    "                                                            ),\n",
    "                                          keras.layers.Activation(\"gelu\"),\n",
    "                                          keras.layers.BatchNormalization(),\n",
    "                                          keras.layers.Dense(units = self.embed_dims),\n",
    "                                          ], name = f\"{self.train_type}_predictor\")\n",
    "        self.probe = False\n",
    "        \n",
    "        self.contrastive_accuracy = keras.metrics.SparseCategoricalAccuracy(\n",
    "            name=\"simclr_contrastive_accuracy\"\n",
    "        )\n",
    "        self.val_contrastive_accuracy = keras.metrics.SparseCategoricalAccuracy(\n",
    "            name=\"simclr_contrastive_accuracy\"\n",
    "        )\n",
    "        \n",
    "        self.loss_tracker = keras.metrics.Mean(name = f\"{self.train_type}_loss\")\n",
    "        \n",
    "    def get_config(self):\n",
    "        return {\"feature_extractor_name\" : self.feature_extractor.name,\n",
    "               \"embed_dims\" : self.embed_dims,\n",
    "                \"Multiview(>2)\" : self.multiview,\n",
    "               \"SSL_method\" : \"SimSiam\",\n",
    "               \"Linear Probe\" : self.probe,\n",
    "               \"N_Categories\" : self.probe_categories if self.probe else 0,\n",
    "               \"Probe Activation\" : self.probe_act if self.probe else \"NA\"}\n",
    "    def compile(self, optimizer, probe_optimizer = None, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "        self.optimizer = optimizer\n",
    "        if self.probe:\n",
    "            self.probe_optimizer = probe_optimizer \n",
    "    def compute_loss(self, p, z, epsilon = 1e-5, training = True):\n",
    "        v1, v2 = ops.normalize(p, axis = 1, order = 2), ops.normalize(z, axis = 1, order = 2) #batch, dims\n",
    "        sim_mat = ops.matmul(v1, ops.transpose(v2))/self.t #batch by batch\n",
    "        batch_size = ops.shape(v1)[0]\n",
    "        pseudo_labels = ops.arange(batch_size)\n",
    "        if training : \n",
    "            self.contrastive_accuracy.update_state(pseudo_labels, sim_mat)\n",
    "            self.contrastive_accuracy.update_state(pseudo_labels, ops.transpose(sim_mat))\n",
    "        else:\n",
    "            self.val_contrastive_accuracy.update_state(pseudo_labels, sim_mat)\n",
    "            self.val_contrastive_accuracy.update_state(pseudo_labels, ops.transpose(sim_mat))\n",
    "        loss_1 = keras.losses.sparse_categorical_crossentropy(\n",
    "            pseudo_labels, sim_mat, from_logits=True\n",
    "        )\n",
    "        loss_2 = keras.losses.sparse_categorical_crossentropy(pseudo_labels, ops.transpose(sim_mat),\n",
    "                                                             from_logits = True)\n",
    "        loss = 0.5*(loss_1 + loss_2)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def train_step(self, dataset):\n",
    "        if self.probe:\n",
    "            img1, img2, labels = dataset\n",
    "        else:\n",
    "            if self.multiview:\n",
    "                img1 = dataset[0]\n",
    "                img2 = dataset[1:] ; n_augs = len(img2)\n",
    "                img2 = ops.concatenate(img2, axis = 0)\n",
    "            else:\n",
    "                img1, img2 = dataset\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            if len(self.feature_extractor.outputs) == 2:\n",
    "                z1, weight1 = self.feature_extractor(img1, training = True)\n",
    "                z2, weight2 = self.feature_extractor(img2, training = True)\n",
    "            elif len(self.feature_extractor.outputs) == 1:\n",
    "                z1 = self.feature_extractor(img1, training = True)\n",
    "                z2 = self.feature_extractor(img2, training = True)\n",
    "            v1, v2 = self.predictor(z1, training = True), self.predictor(z2, training = True)\n",
    "            \n",
    "            if self.multiview:\n",
    "                v2 = ops.split(v2, n_augs, 0)\n",
    "                loss = []\n",
    "                for idx in range(n_augs):\n",
    "                    loss.append(self.compute_loss(v1, v2[idx]))\n",
    "                    \n",
    "                loss = ops.mean(loss)\n",
    "                \n",
    "            else:\n",
    "                loss = self.compute_loss(v1, v2)\n",
    "        trainable_params = self.feature_extractor.trainable_variables + self.predictor.trainable_variables\n",
    "        grads = tape.gradient(loss, trainable_params)\n",
    "        self.optimizer.apply_gradients(zip(grads, trainable_params))\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        output_dict = {self.loss_tracker.name : self.loss_tracker.result(),\n",
    "                      self.contrastive_accuracy.name : self.contrastive_accuracy.result()}\n",
    "        return output_dict\n",
    "    \n",
    "    def test_step(self, dataset):\n",
    "        if self.probe:\n",
    "            img1, img2, labels = dataset\n",
    "        else:\n",
    "            if self.multiview:\n",
    "                img1 = dataset[0]\n",
    "                img2 = dataset[1:] ; n_augs = len(img2)\n",
    "                img2 = ops.concatenate(img2, axis = 0)\n",
    "            else:\n",
    "                img1, img2 = dataset\n",
    "        \n",
    "        if len(self.feature_extractor.outputs) == 2:\n",
    "            z1, weight1 = self.feature_extractor(img1, training = True)\n",
    "            z2, weight2 = self.feature_extractor(img2, training = True)\n",
    "        elif len(self.feature_extractor.outputs) == 1:\n",
    "            z1 = self.feature_extractor(img1, training = True)\n",
    "            z2 = self.feature_extractor(img2, training = True)\n",
    "        v1, v2 = self.predictor(z1, training = False), self.predictor(z2, training = False)\n",
    "            \n",
    "        if self.multiview:\n",
    "            v2 = ops.split(v2, n_augs, 0)\n",
    "            loss = []\n",
    "            for idx in range(n_augs):\n",
    "                loss.append(self.compute_loss(v1, v2[idx],\n",
    "                                               training = False))        \n",
    "            loss = ops.mean(loss)\n",
    "            \n",
    "        else:\n",
    "            loss = self.compute_loss(v1, v2)\n",
    "        \n",
    "        self.loss_tracker.update_state(loss)\n",
    "        output_dict = {self.loss_tracker.name : self.loss_tracker.result(),\n",
    "                       self.val_contrastive_accuracy.name : self.val_contrastive_accuracy.result()\n",
    "                      }\n",
    "        return output_dict\n",
    "    \n",
    "    def call(self, dataset):\n",
    "        return self.test_step(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8cc1ac",
   "metadata": {
    "papermill": {
     "duration": 0.016149,
     "end_time": "2024-05-17T03:33:21.694089",
     "exception": false,
     "start_time": "2024-05-17T03:33:21.677940",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "----------------\n",
    "# Multimodal Contrastive method\n",
    "- CLIP\n",
    "- SigLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a8bef546",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T03:33:21.729677Z",
     "iopub.status.busy": "2024-05-17T03:33:21.729139Z",
     "iopub.status.idle": "2024-05-17T03:33:21.767377Z",
     "shell.execute_reply": "2024-05-17T03:33:21.766109Z"
    },
    "papermill": {
     "duration": 0.059081,
     "end_time": "2024-05-17T03:33:21.769703",
     "exception": false,
     "start_time": "2024-05-17T03:33:21.710622",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CLIP(keras.Model): #original CLIP + CLIP surgery\n",
    "    def __init__(self, image_encoder, text_encoder,\n",
    "                embed_dims, pool_heads = 8, t = 2.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.image_encoder = image_encoder\n",
    "        self.text_encoder = text_encoder\n",
    "        self.embed_dims = embed_dims\n",
    "        self.t = t\n",
    "        self.mlp_image = keras.layers.Dense(units = embed_dims)\n",
    "        self.mlp_text = keras.layers.Dense(units = embed_dims)\n",
    "        self.pe_fn = keras_nlp.layers.RotaryEmbedding(name = \"RotatoryPE\")\n",
    "        self.image_pooler = AttentionPooling(pool_heads, embed_dims, name = \"ImageAttentionPooler\")\n",
    "        self.text_pooler = AttentionPooling(pool_heads, embed_dims, name = \"TextAttentionPooler\")\n",
    "        self.loss_tracker = keras.metrics.Mean(name = \"CLIP_loss\")\n",
    "    def get_config(self):\n",
    "        return {\"Image encoder name\": self.image_encoder.name,\n",
    "               \"Text encoder name\" : self.text_encoder.name,\n",
    "               \"embed_dims\" : self.embed_dims,\n",
    "               \"temperature\" : self.t,\n",
    "               \"SSL_method\" : \"CLIP_with_Attentional_Pooling\"}\n",
    "    def get_clip_loss(self, image_vector, text_vector):\n",
    "        batch_size = ops.shape(image_vector)[0]\n",
    "        image_vector = keras.utils.normalize(image_vector, axis = -1, order = 2)\n",
    "        text_vector = keras.utils.normalize(text_vector, axis = -1, order = 2)\n",
    "        \n",
    "        cor_mat = tf.einsum(\"ab, cb->ac\", image_vector, text_vector) / self.t\n",
    "        cor_mat = ops.reshape(cor_mat, [batch_size, batch_size])\n",
    "        pseudo_label = ops.zeros_like(cor_mat)\n",
    "        diags = tf.linalg.diag_part(pseudo_label) + 1.0\n",
    "        pseudo_label = tf.linalg.set_diag(pseudo_label, diags)\n",
    "        pseudo_label = pseudo_label - ops.ones_like(pseudo_label)\n",
    "        loss = 0.5*(ops.mean(keras.losses.CategoricalFocalCrossentropy(reduction = None, label_smoothing = 0.05)(pseudo_label, cor_mat)) + \n",
    "                    ops.mean(keras.losses.CategoricalFocalCrossentropy(reduction = None, label_smoothing = 0.05)(pseudo_label, ops.transpose(cor_mat)))\n",
    "                   )\n",
    "        return loss\n",
    "        \n",
    "    def train_step(self, dataset):\n",
    "        image, text = dataset\n",
    "        with tf.GradientTape() as tape: \n",
    "            image_feature, text_feature = self.image_encoder(image, training = True), self.text_encoder(text, training = True)\n",
    "            batch_size, w, h, dims = ops.shape(image_feature) ; batch_size = ops.shape(image_feature)[0]\n",
    "            \n",
    "            image_feature = ops.reshape(image_feature, [batch_size, w*h, dims])\n",
    "            image_feature = self.pe_fn(image_feature)\n",
    "            text_vector = keras.layers.GlobalAveragePooling1D()(text_feature)\n",
    "            if len(ops.shape(image_feature)) == 3:\n",
    "                image_vector = keras.layers.GlobalAveragePooling1D()(image_feature)\n",
    "            elif len(ops.shape(image_feature)) == 4:\n",
    "                image_vector = keras.layers.GlobalAveragePooling2D()(image_feature)\n",
    "\n",
    "            image_vector, text_vector = self.mlp_image(image_vector, training = True), self.mlp_text(text_vector, training = True)\n",
    "            image_vector, text_vector = self.image_pooler([image_vector, image_feature], training = True)[0], self.text_pooler([text_vector, text_feature], training = True)[0]\n",
    "            loss = self.get_clip_loss(image_vector, text_vector)\n",
    "        \n",
    "        encoder_weights = self.image_encoder.trainable_weights + self.text_encoder.trainable_weights\n",
    "        mlp_weights = self.mlp_image.trainable_weights + self.mlp_text.trainable_weights\n",
    "        pool_weights = self.image_pooler.trainable_weights + self.text_pooler.trainable_weights\n",
    "        trainable_weights = encoder_weights + mlp_weights + pool_weights + self.pe_fn.trainable_weights\n",
    "        \n",
    "        grads = tape.gradient(loss, trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, trainable_weights))\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {self.loss_tracker.name : self.loss_tracker.result()}\n",
    "    \n",
    "    def test_step(self, dataset):\n",
    "        image, text = dataset\n",
    "        image_feature, text_feature = self.image_encoder(image, training = False), self.text_encoder(text, training = False)\n",
    "        batch_size, w, h, dims = ops.shape(image_feature) ; batch_size = ops.shape(image_feature)[0]\n",
    "            \n",
    "        image_feature = ops.reshape(image_feature, [batch_size, w*h, dims])\n",
    "        image_feature = self.pe_fn(image_feature)\n",
    "        \n",
    "        text_vector = keras.layers.GlobalAveragePooling1D()(text_feature)\n",
    "        if len(ops.shape(image_feature)) == 3:\n",
    "            image_vector = keras.layers.GlobalAveragePooling1D()(image_feature)\n",
    "        elif len(ops.shape(image_feature)) == 4:\n",
    "            image_vector = keras.layers.GlobalAveragePooling2D()(image_feature)\n",
    "\n",
    "        image_vector, text_vector = self.mlp_image(image_vector, training = False), self.mlp_text(text_vector, training = False)\n",
    "        image_vector, text_vector = self.image_pooler([image_vector, image_feature], training = False)[0], self.text_pooler([text_vector, text_feature], training = False)[0]\n",
    "        loss = self.get_clip_loss(image_vector, text_vector)\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {self.loss_tracker.name : self.loss_tracker.result()}\n",
    "    def call(self, dataset):\n",
    "        return self.test_step(dataset)\n",
    "    def get_full_model(self):\n",
    "        inputs = self.image_encoder.inputs\n",
    "        feature = self.image_encoder.output\n",
    "        if len(ops.shape(feature)) == 4:\n",
    "            batch_size, w, h, dims = ops.shape(feature)\n",
    "            batch_size = ops.shape(feature)[0]\n",
    "            feature = ops.reshape(feature, [-1, w*h, dims])\n",
    "        feature = self.pe_fn(feature)\n",
    "        image_vector = keras.layers.GlobalAveragePooling1D()(feature)\n",
    "        z_image = self.mlp_image(image_vector)\n",
    "        outputs = self.image_pooler([z_image, feature])\n",
    "        return keras.Model(inputs, outputs,\n",
    "                          name = f\"FullModel_{self.image_encoder.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d48fc6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T03:33:21.805179Z",
     "iopub.status.busy": "2024-05-17T03:33:21.804773Z",
     "iopub.status.idle": "2024-05-17T03:33:21.838182Z",
     "shell.execute_reply": "2024-05-17T03:33:21.836978Z"
    },
    "papermill": {
     "duration": 0.053865,
     "end_time": "2024-05-17T03:33:21.840401",
     "exception": false,
     "start_time": "2024-05-17T03:33:21.786536",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SigLIP(keras.Model):\n",
    "    def __init__(self, image_encoder, text_encoder,\n",
    "                embed_dims, pool_heads = 8, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.image_encoder = image_encoder\n",
    "        self.text_encoder = text_encoder\n",
    "        self.pe_fn = keras_nlp.layers.RotaryEmbedding(name = \"RotatoryPE\")\n",
    "        self.embed_dims = embed_dims\n",
    "        self.mlp_image = keras.layers.Dense(units = embed_dims)\n",
    "        self.mlp_text = keras.layers.Dense(units = embed_dims)\n",
    "        self.image_pooler = AttentionPooling(pool_heads, embed_dims, name = \"ImageAttentionPooler\")\n",
    "        self.text_pooler = AttentionPooling(pool_heads, embed_dims, name = \"TextAttentionPooler\")\n",
    "        self.loss_tracker = keras.metrics.Mean(name = \"SigLIP_loss\")\n",
    "        \n",
    "        self.t = tf.Variable(1.0, trainable = True, dtype = \"float32\")\n",
    "        self.b = tf.Variable(0.0, trainable = True, dtype = \"float32\")\n",
    "    def get_config(self):\n",
    "        return {\"Image encoder name\": self.image_encoder.name,\n",
    "               \"Text encoder name\" : self.text_encoder.name,\n",
    "               \"embed_dims\" : self.embed_dims,\n",
    "               \"SSL_method\" : \"SigLIP\"}\n",
    "    \n",
    "    def compute_loss(self, image_vector, text_vector):\n",
    "        batch_size = ops.shape(image_vector)[0]\n",
    "        z_img = keras.utils.normalize(image_vector, axis = -1, order = 2)\n",
    "        z_text = keras.utils.normalize(text_vector, axis = -1, order = 2)\n",
    "        z_img = tf.cast(z_img, tf.float32)\n",
    "        z_text = tf.cast(z_text, tf.float32)\n",
    "        cor_mat = ops.dot(z_img, ops.transpose(z_text)) * ops.exp(self.t) + self.b\n",
    "        cor_mat = ops.reshape(cor_mat, [batch_size, batch_size])\n",
    "        \n",
    "        pseudo_label = ops.zeros_like(cor_mat)\n",
    "        diags = tf.linalg.diag_part(pseudo_label) + 2.0\n",
    "        pseudo_label = tf.linalg.set_diag(pseudo_label, diags)\n",
    "        pseudo_label = pseudo_label - ops.ones_like(pseudo_label)\n",
    "        \n",
    "        loss = -ops.mean(ops.log_sigmoid(ops.multiply(cor_mat, pseudo_label)))\n",
    "        return loss\n",
    "        \n",
    "    def train_step(self, dataset):\n",
    "        image, text = dataset\n",
    "        with tf.GradientTape() as tape: \n",
    "            image_feature, text_feature = self.image_encoder(image, training = True), self.text_encoder(text, training = True)\n",
    "            batch_size, w, h, dims = ops.shape(image_feature) ; batch_size = ops.shape(image_feature)[0]\n",
    "            \n",
    "            image_feature = ops.reshape(image_feature, [batch_size, w*h, dims])\n",
    "            image_feature = self.pe_fn(image_feature)\n",
    "            \n",
    "            text_vector = keras.layers.GlobalAveragePooling1D()(text_feature)\n",
    "            if len(ops.shape(image_feature)) == 3:\n",
    "                image_vector = keras.layers.GlobalAveragePooling1D()(image_feature)\n",
    "            elif len(ops.shape(image_feature)) == 4:\n",
    "                image_vector = keras.layers.GlobalAveragePooling2D()(image_feature)\n",
    "\n",
    "            image_vector, text_vector = self.mlp_image(image_vector, training = True), self.mlp_text(text_vector, training = True)\n",
    "            image_vector, text_vector = self.image_pooler([image_vector, image_feature], training = True)[0], self.text_pooler([text_vector, text_feature], training = True)[0]\n",
    "            loss = 0.5*(self.compute_loss(image_vector, text_vector) + self.compute_loss(text_vector, image_vector))\n",
    "        \n",
    "        encoder_weights = self.image_encoder.trainable_weights + self.text_encoder.trainable_weights\n",
    "        mlp_weights = self.mlp_image.trainable_weights + self.mlp_text.trainable_weights\n",
    "        pool_weights = self.image_pooler.trainable_weights + self.text_pooler.trainable_weights\n",
    "        trainable_weights = encoder_weights + mlp_weights + pool_weights + [self.t, self.b] + self.pe_fn.trainable_weights\n",
    "        \n",
    "        grads = tape.gradient(loss, trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, trainable_weights))\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {self.loss_tracker.name : self.loss_tracker.result()}\n",
    "    \n",
    "    def test_step(self, dataset):\n",
    "        image, text = dataset\n",
    "        image_feature, text_feature = self.image_encoder(image, training = False), self.text_encoder(text, training = False)\n",
    "        batch_size, w, h, dims = ops.shape(image_feature) ; batch_size = ops.shape(image_feature)[0]\n",
    "            \n",
    "        image_feature = ops.reshape(image_feature, [batch_size, w*h, dims])\n",
    "        image_feature = self.pe_fn(image_feature)\n",
    "        text_vector = keras.layers.GlobalAveragePooling1D()(text_feature)\n",
    "        if len(ops.shape(image_feature)) == 3:\n",
    "            image_vector = keras.layers.GlobalAveragePooling1D()(image_feature)\n",
    "        elif len(ops.shape(image_feature)) == 4:\n",
    "            image_vector = keras.layers.GlobalAveragePooling2D()(image_feature)\n",
    "\n",
    "        image_vector, text_vector = self.mlp_image(image_vector, training = False), self.mlp_text(text_vector, training = False)\n",
    "        image_vector, text_vector = self.image_pooler([image_vector, image_feature], training = False)[0], self.text_pooler([text_vector, text_feature], training = False)[0]\n",
    "        loss = 0.5*(self.compute_loss(image_vector, text_vector) + self.compute_loss(text_vector, image_vector))\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {self.loss_tracker.name : self.loss_tracker.result()}\n",
    "    def call(self, dataset):\n",
    "        return self.test_step(dataset)\n",
    "    def get_full_model(self):\n",
    "        inputs = self.image_encoder.inputs\n",
    "        feature = self.image_encoder.output\n",
    "        if len(ops.shape(feature)) == 4:\n",
    "            batch_size, w, h, dims = ops.shape(feature)\n",
    "            batch_size = ops.shape(feature)[0]\n",
    "            feature = ops.reshape(feature, [-1, w*h, dims])\n",
    "        feature = self.pe_fn(feature)\n",
    "        \n",
    "        image_vector = keras.layers.GlobalAveragePooling1D()(feature)\n",
    "        z_image = self.mlp_image(image_vector)\n",
    "        outputs = self.image_pooler([z_image, feature])\n",
    "        return keras.Model(inputs, outputs,\n",
    "                          name = f\"FullModel_{self.image_encoder.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6556bb64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T03:33:21.875761Z",
     "iopub.status.busy": "2024-05-17T03:33:21.875369Z",
     "iopub.status.idle": "2024-05-17T03:33:21.922177Z",
     "shell.execute_reply": "2024-05-17T03:33:21.921078Z"
    },
    "papermill": {
     "duration": 0.067604,
     "end_time": "2024-05-17T03:33:21.924806",
     "exception": false,
     "start_time": "2024-05-17T03:33:21.857202",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SPARC(keras.Model): \n",
    "    #Reference : https://arxiv.org/abs/2401.09865 \"Improving fine-grained understanding in image-text pre-training\"\n",
    "    def __init__(self, image_encoder, text_encoder,\n",
    "                embed_dims, pool_heads = 8, preprocessor = None,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.image_encoder = image_encoder\n",
    "        self.text_encoder = text_encoder\n",
    "        self.preprocessor = preprocessor\n",
    "        self.pe_fn = keras_nlp.layers.RotaryEmbedding(name = \"RotatoryPE\")\n",
    "        self.embed_dims = embed_dims\n",
    "        self.mlp_image = keras.layers.Dense(units = embed_dims)\n",
    "        self.mlp_text = keras.layers.Dense(units = embed_dims)\n",
    "        self.image_pooler = AttentionPooling(pool_heads, embed_dims, name = \"ImageAttentionPooler\")\n",
    "        self.text_pooler = AttentionPooling(pool_heads, embed_dims, name = \"TextAttentionPooler\")\n",
    "        self.ce_fn = keras.losses.CategoricalCrossentropy(from_logits=True, reduction = None)\n",
    "        self.loss_tracker = keras.metrics.Mean(name = \"SPARC_loss\")\n",
    "        self.global_loss_tracker = keras.metrics.Mean(name = \"SPARC_global_loss\")\n",
    "        self.local_loss_tracker = keras.metrics.Mean(name = \"SPARC_local_loss\")\n",
    "        \n",
    "        self.t = tf.Variable(1.0, trainable = True, dtype = \"float32\")\n",
    "        self.b = tf.Variable(0.0, trainable = True, dtype = \"float32\")\n",
    "        self.threshold = tf.Variable(0.5, trainable = True, dtype = \"float32\")\n",
    "        \n",
    "    def get_config(self):\n",
    "        return {\"Image encoder name\": self.image_encoder.name,\n",
    "               \"Text encoder name\" : self.text_encoder.name,\n",
    "               \"embed_dims\" : self.embed_dims,\n",
    "               \"SSL_method\" : \"SPARC\"}\n",
    "    \n",
    "    def compute_global_loss(self, image_vector, text_vector): #<- SigLIP\n",
    "        batch_size = ops.shape(image_vector)[0]\n",
    "        z_img = keras.utils.normalize(image_vector, axis = -1, order = 2)\n",
    "        z_text = keras.utils.normalize(text_vector, axis = -1, order = 2)\n",
    "        z_img = tf.cast(z_img, tf.float32)\n",
    "        z_text = tf.cast(z_text, tf.float32)\n",
    "        cor_mat = ops.dot(z_img, ops.transpose(z_text)) * ops.exp(self.t) + self.b\n",
    "        cor_mat = ops.reshape(cor_mat, [batch_size, batch_size])\n",
    "        \n",
    "        pseudo_label = ops.zeros_like(cor_mat)\n",
    "        diags = tf.linalg.diag_part(pseudo_label) + 2.0\n",
    "        pseudo_label = tf.linalg.set_diag(pseudo_label, diags)\n",
    "        pseudo_label = pseudo_label - ops.ones_like(pseudo_label)\n",
    "        \n",
    "        loss = -ops.mean(ops.log_sigmoid(ops.multiply(cor_mat, pseudo_label)))\n",
    "        loss = tf.cast(loss, tf.float32)\n",
    "        return loss\n",
    "    \n",
    "    def text_compare(self, text_label, text_logit, mask = None):\n",
    "        loss = self.ce_fn(text_label, text_logit)\n",
    "        loss = tf.cast(loss, tf.float32)\n",
    "        if mask == None:\n",
    "            return ops.mean(loss)\n",
    "        else:\n",
    "            loss *= tf.cast(mask, tf.float32)\n",
    "            loss = (ops.sum(loss))/(ops.sum(tf.cast(mask, tf.float32)) + 1e-4)\n",
    "            return loss\n",
    "        \n",
    "    def compute_loss(self, image_sequence, text_sequence, mask = None):\n",
    "        batch_size = ops.shape(image_sequence)[0]\n",
    "        _, token_len, dim_ = ops.shape(text_sequence)\n",
    "        if len(ops.shape(image_sequence)) == 3:\n",
    "            _, image_len, dim_ = ops.shape(image_sequence)\n",
    "        elif len(ops.shape(image_sequence)) == 4:\n",
    "            _, w, h, dim_ = ops.shape(image_sequence)\n",
    "            image_sequence = ops.reshape(image_sequence, [batch_size, w*h, dim_])\n",
    "            image_len = w*h\n",
    "            \n",
    "        image_sequence, text_sequence = self.mlp_image(image_sequence), self.mlp_text(text_sequence) \n",
    "        image_sequence = self.pe_fn(image_sequence)\n",
    "        # [batch, image_len, embed_dims] / [batch, token_len, embed_dims], respectively.\n",
    "        \n",
    "        z_image, att_weight = self.image_pooler([keras.layers.GlobalAveragePooling1D()(image_sequence),\n",
    "                                                     image_sequence])\n",
    "        z_text, att_weight_ = self.text_pooler([keras.layers.GlobalAveragePooling1D()(text_sequence),\n",
    "                                                     text_sequence])\n",
    "        #1. global alignment loss\n",
    "        global_loss = 0.5*(self.compute_global_loss(z_image, z_text) + self.compute_global_loss(z_text, z_image))\n",
    "        #2. Fine Grained local loss\n",
    "        \n",
    "        #a. get similarity matrix b/w text sequence and image sequence\n",
    "        sim_matrix = ops.einsum(\"atd, aid -> ati\", text_sequence, image_sequence) #batch, token_len, image_len\n",
    "        sim_matrix = (sim_matrix - ops.min(sim_matrix, axis = -1, keepdims = True)) / (1e-4 + ops.max(sim_matrix, axis = -1, keepdims = True) - ops.min(sim_matrix, axis = -1, keepdims = True))\n",
    "        sim_matrix = tf.cast(sim_matrix, tf.float32)\n",
    "        \n",
    "        sim_matrix = ops.clip(sim_matrix, self.threshold, 1e4)\n",
    "        attended_text_sequence = ops.einsum(\"ati, aid -> atd\", sim_matrix, image_sequence)\n",
    "        \n",
    "        text_logit = ops.einsum(\"atd, aqd -> atq\", \n",
    "                                keras.utils.normalize(text_sequence, axis = -1, order = 2), \n",
    "                                keras.utils.normalize(attended_text_sequence, axis = -1, order = 2))/self.t\n",
    "        \n",
    "        pseudo_text_label = ops.tile(ops.expand_dims(ops.eye(token_len), axis = 0),\n",
    "                                     [batch_size,1,1])\n",
    "        local_loss = self.text_compare(pseudo_text_label, text_logit, mask)\n",
    "        loss = 0.5*global_loss + 1.0*local_loss\n",
    "        return loss, global_loss, local_loss\n",
    "        \n",
    "    def train_step(self, dataset):\n",
    "        image, text = dataset\n",
    "        if self.preprocessor != None:\n",
    "            text_mask = self.preprocessor(text)[\"padding_mask\"]\n",
    "            text_mask = tf.cast(text_mask, dtype = tf.int32)\n",
    "        else:\n",
    "            text_mask = None\n",
    "        with tf.GradientTape() as tape: \n",
    "            image_feature, text_feature = self.image_encoder(image, training = True), self.text_encoder(text, training = True)\n",
    "            loss, global_loss, local_loss = self.compute_loss(image_feature, text_feature, text_mask)\n",
    "        \n",
    "        encoder_weights = self.image_encoder.trainable_weights + self.text_encoder.trainable_weights\n",
    "        mlp_weights = self.mlp_image.trainable_weights + self.mlp_text.trainable_weights\n",
    "        pool_weights = self.image_pooler.trainable_weights + self.text_pooler.trainable_weights\n",
    "        trainable_weights = encoder_weights + mlp_weights + pool_weights + [self.t, self.b] + self.pe_fn.trainable_weights\n",
    "        \n",
    "        grads = tape.gradient(loss, trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, trainable_weights))\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.global_loss_tracker.update_state(global_loss)\n",
    "        self.local_loss_tracker.update_state(local_loss)\n",
    "        return {self.loss_tracker.name : self.loss_tracker.result(),\n",
    "               self.global_loss_tracker.name : self.global_loss_tracker.result(),\n",
    "               self.local_loss_tracker.name : self.local_loss_tracker.result()\n",
    "               }\n",
    "    \n",
    "    def test_step(self, dataset):\n",
    "        image, text = dataset\n",
    "        if self.preprocessor != None:\n",
    "            text_mask = self.preprocessor(text)[\"padding_mask\"]\n",
    "            text_mask = tf.cast(text_mask, tf.int32)\n",
    "        else:\n",
    "            text_mask = None\n",
    "            \n",
    "        image_feature, text_feature = self.image_encoder(image, training = False), self.text_encoder(text, training = False)\n",
    "        loss, global_loss, local_loss = self.compute_loss(image_feature, text_feature, text_mask)\n",
    "        \n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.global_loss_tracker.update_state(global_loss)\n",
    "        self.local_loss_tracker.update_state(local_loss)\n",
    "        return {self.loss_tracker.name : self.loss_tracker.result(),\n",
    "               self.global_loss_tracker.name : self.global_loss_tracker.result(),\n",
    "               self.local_loss_tracker.name : self.local_loss_tracker.result()\n",
    "               }\n",
    "    \n",
    "    def call(self, dataset):\n",
    "        return self.test_step(dataset)\n",
    "    def get_full_model(self):\n",
    "        inputs = self.image_encoder.inputs\n",
    "        feature = self.image_encoder.output\n",
    "        feature = self.mlp_image(feature)\n",
    "        if len(ops.shape(feature)) == 4:\n",
    "            batch_size, w, h, dims = ops.shape(feature)\n",
    "            batch_size = ops.shape(feature)[0]\n",
    "            feature = ops.reshape(feature, [-1, w*h, dims])\n",
    "        feature = self.pe_fn(feature)\n",
    "        \n",
    "        image_vector = keras.layers.GlobalAveragePooling1D()(feature)\n",
    "        outputs = self.image_pooler([image_vector, feature])\n",
    "        return keras.Model(inputs, outputs,\n",
    "                          name = f\"FullModel_{self.image_encoder.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fa91e1",
   "metadata": {
    "papermill": {
     "duration": 0.016595,
     "end_time": "2024-05-17T03:33:22.014572",
     "exception": false,
     "start_time": "2024-05-17T03:33:21.997977",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5ad25b67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T03:33:22.049522Z",
     "iopub.status.busy": "2024-05-17T03:33:22.049085Z",
     "iopub.status.idle": "2024-05-17T03:33:22.093307Z",
     "shell.execute_reply": "2024-05-17T03:33:22.092229Z"
    },
    "papermill": {
     "duration": 0.064684,
     "end_time": "2024-05-17T03:33:22.095877",
     "exception": false,
     "start_time": "2024-05-17T03:33:22.031193",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def att_visualize(model, images, res, thresholding = False):\n",
    "    # Use jet colormap to colorize heatmap\n",
    "    try:\n",
    "        model = model.get_full_model()\n",
    "    except Exception as e:\n",
    "        print(\"Using Raw model with Att pooling\",\"\\n\",\"Possible error:\",\"\\n\",e)\n",
    "        pass\n",
    "    try:\n",
    "        att_weights = model(images)[-1] \n",
    "        att_weights_ = att_weights[:, :, 0, :] #batch, heads, cls_token, w*h\n",
    "        _, heads, token_length = ops.shape(att_weights_) ; batch_size = ops.shape(att_weights_)[0]\n",
    "        token_length = tf.cast(token_length, tf.float32)\n",
    "\n",
    "        w = tf.cast(tf.math.sqrt(token_length), tf.int32)\n",
    "        heatmap = tf.reshape(att_weights_, [batch_size, heads, w, w])\n",
    "        M = tf.reduce_max(heatmap, axis = [2, 3], keepdims = True)\n",
    "        m = tf.reduce_min(heatmap, axis = [2, 3], keepdims = True)\n",
    "        heatmap = (heatmap - m) / (M-m + 1e-4)\n",
    "        if thresholding:\n",
    "            threshold = ops.mean(heatmap, [2,3], keepdims = True)\n",
    "            heatmap = ops.where(heatmap < threshold, 0.0, heatmap)\n",
    "        else:\n",
    "            pass\n",
    "        heatmap = ops.reshape(heatmap, [-1, w, w])\n",
    "        heatmap = tf.map_fn(fn = lambda x : cv2.resize(np.array(x).astype(\"float32\"), [res, res]),\n",
    "                           elems = heatmap)\n",
    "        heatmap = ops.reshape(heatmap, [batch_size, heads, res, res, 1])\n",
    "        imposed = ops.multiply(images[:, tf.newaxis, ...], heatmap)\n",
    "        imposed = tf.cast(imposed, tf.uint8)    \n",
    "        return imposed\n",
    "    except Exception as e:\n",
    "        print(\"Error raised:\", e)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d91ddcc",
   "metadata": {
    "papermill": {
     "duration": 0.016671,
     "end_time": "2024-05-17T03:33:22.135842",
     "exception": false,
     "start_time": "2024-05-17T03:33:22.119171",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Test - drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fd26894d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T03:33:22.172620Z",
     "iopub.status.busy": "2024-05-17T03:33:22.172230Z",
     "iopub.status.idle": "2024-05-17T03:33:22.176985Z",
     "shell.execute_reply": "2024-05-17T03:33:22.175957Z"
    },
    "papermill": {
     "duration": 0.026034,
     "end_time": "2024-05-17T03:33:22.179363",
     "exception": false,
     "start_time": "2024-05-17T03:33:22.153329",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_drive = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0a4b0f24",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-05-17T03:33:22.215448Z",
     "iopub.status.busy": "2024-05-17T03:33:22.215059Z",
     "iopub.status.idle": "2024-05-17T03:33:22.225475Z",
     "shell.execute_reply": "2024-05-17T03:33:22.224537Z"
    },
    "papermill": {
     "duration": 0.030728,
     "end_time": "2024-05-17T03:33:22.227881",
     "exception": false,
     "start_time": "2024-05-17T03:33:22.197153",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if test_drive:\n",
    "    res = 256\n",
    "    batch_size = 8\n",
    "    gc_configs = get_gcvit_configs(res, 64, \"GC_ViT_xxtiny\")\n",
    "    gc_configs[\"level_depth\"] = [1,1,2,2]\n",
    "    print(gc_configs)\n",
    "    dataset = tfds.load(\"beans\", split = 'test')\n",
    "    def map_fn(dataset):\n",
    "        image = tf.image.resize_with_pad(dataset[\"image\"], res, res, antialias = True)\n",
    "        image = tf.cast(image, tf.uint8)\n",
    "        return image\n",
    "    train_ds = dataset.map(map_fn).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    ssl_fn = get_map_fn(res, \"image\", \"ssl\", 4)\n",
    "    train_ds = train_ds.unbatch().map(ssl_fn).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    for imgs in train_ds.take(1):\n",
    "        sets = imgs\n",
    "    for index in range(batch_size):\n",
    "        fig, axes = plt.subplots(2,2, figsize = (7,7))\n",
    "        axes = axes.flatten()\n",
    "        for i, ax in enumerate(axes):\n",
    "            ax.imshow(ops.cast(sets[i][index], \"uint8\"))\n",
    "            ax.set_title(f\"In-Batch index : {index}\")\n",
    "        print(\"=================================================\")\n",
    "        plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6a41dc77",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-05-17T03:33:22.263298Z",
     "iopub.status.busy": "2024-05-17T03:33:22.262781Z",
     "iopub.status.idle": "2024-05-17T03:33:22.271928Z",
     "shell.execute_reply": "2024-05-17T03:33:22.270726Z"
    },
    "papermill": {
     "duration": 0.029774,
     "end_time": "2024-05-17T03:33:22.274304",
     "exception": false,
     "start_time": "2024-05-17T03:33:22.244530",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if test_drive:\n",
    "    \n",
    "    vit_model = get_full_model(\"effnet\", res = res, pe_type = None, att_depth = 1)\n",
    "    vit_model.summary()\n",
    "    \n",
    "    barlow = SimCLR(vit_model, embed_dims = 768, \n",
    "                  multiview = True\n",
    "                 )\n",
    "    barlow.compile(optimizer = keras.optimizers.Adam())\n",
    "    print(barlow(sets))\n",
    "    print(f\"GFlops : {get_flops(vit_model, imgs[:1])}\")\n",
    "    result = barlow.train_step(sets)\n",
    "    print(result)\n",
    "    \n",
    "    imgs = att_visualize(barlow.feature_extractor, sets[0], res = res)\n",
    "    fig, axes = plt.subplots(8, 8, figsize = (25,25))\n",
    "    for row in range(8):\n",
    "        for col in range(8):\n",
    "            axes[row][col].imshow(imgs[row, col, ...])\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 31.094942,
   "end_time": "2024-05-17T03:33:24.746576",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-17T03:32:53.651634",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
