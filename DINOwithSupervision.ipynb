{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8556597,"sourceType":"datasetVersion","datasetId":5112865}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os, sys\nimport random\nimport pydicom\nimport sklearn\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import AgglomerativeClustering, KMeans\nfrom sklearn.preprocessing import StandardScaler\nimport re\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nseed = 42\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport tensorflow as tf\nimport keras\n\nkeras.mixed_precision.set_global_policy(\"mixed_float16\")\n\nfrom keras import layers, Model\nfrom keras import ops, layers, models, losses, optimizers, metrics\nimport keras_hub\nimport keras_cv\nimport keras_nlp\n\nimport cv2\nfrom skimage.io import imread\nkeras.utils.set_random_seed(seed)\nimport tensorflow_io as tfio\nfrom kaggle_datasets import KaggleDatasets\nimport tensorflow_datasets as tfds\nimport tensorflow_probability as tfp\n\nprint(f\"Tensorflow version : {tf.__version__}\")\ntry:\n    print(f\"Keras version : {keras.__version__}\")\nexcept:\n    pass\n\nfrom keras import Input, Model, ops\nfrom keras.models import load_model\n\nfrom keras.layers import Conv2D, DepthwiseConv2D, Dense, Activation, BatchNormalization, LayerNormalization, MultiHeadAttention, Embedding, Subtract, Add, Multiply, GlobalAveragePooling2D, GlobalAveragePooling1D, LayerNormalization\nfrom keras.utils import load_img, img_to_array\nfrom keras.applications import *\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nfrom sklearn.model_selection import train_test_split\nfrom keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\nfrom tqdm.notebook import tqdm\nimport wandb\ndef wandb_config():\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    try:\n        secret_value_0 = user_secrets.get_secret(\"__gcloud_sdk_auth__\")\n        secret_value_1 = user_secrets.get_secret(\"huggingface_key\")\n        secret_value_2 = user_secrets.get_secret(\"wandb_key\")\n        !wandb login $secret_value_2\n    except:\n        secret_value_0 = user_secrets.get_secret(\"huggingface_key\")\n        secret_value_1 = user_secrets.get_secret(\"wandb_key\")\n        !wandb login $secret_value_1\n    \ndef auto_select_accelerator():\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n        print(\"Running on TPU:\", tpu.master())\n    except ValueError:\n        tpu = False\n        strategy = tf.distribute.MirroredStrategy() # for GPU or multi-GPU machines\n    print(f\"Running on {strategy.num_replicas_in_sync} replicas\")\n    \n    return tpu, strategy\n\ntpu, strategy = auto_select_accelerator()\n\nimport PIL\nfrom PIL import Image as PILImage\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nimport pprint\nfrom pprint import pprint as pp\n\nres = 384\nbatch_size = 16","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def _parse_tfrecord(res = res):\n    def parse_tfrecord(tfrecord):\n        features = {'image': tf.io.FixedLenFeature([], tf.string),\n                    'label': tf.io.FixedLenFeature([], tf.int64),\n                    }\n        x = tf.io.parse_single_example(tfrecord, features)\n        image_train = tf.image.decode_jpeg(x['image'], channels=1)\n        image_train = _transform_images(res = res)(image_train)\n        label = tf.cast(x[\"label\"], tf.int32)\n        return (image_train, label)\n    \n    return parse_tfrecord\n\n\ndef _transform_images(res = res):\n    def transform_images(x_train):\n        x_train = tf.image.resize_with_pad(x_train, res, res, antialias = True)\n        x_train = tf.cast(x_train, tf.uint8)\n        return x_train\n    return transform_images\n\ndef load_tfrecord_dataset(tfrecord_name, res = res, batch_size = batch_size, shuffle=True, buffer_size=10240):\n    \"\"\"load dataset from tfrecord\"\"\"\n    raw_dataset = tf.data.TFRecordDataset(tfrecord_name, compression_type = \"GZIP\")\n    raw_dataset = raw_dataset.repeat()\n    if shuffle:\n        raw_dataset = raw_dataset.shuffle(buffer_size=buffer_size)\n    dataset = raw_dataset.map(\n        _parse_tfrecord(),\n        num_parallel_calls=tf.data.AUTOTUNE\n    )\n    dataset = dataset.batch(batch_size, drop_remainder = True)\n    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n    return dataset\n\ntrain_radimagenet_ds = load_tfrecord_dataset(\"/kaggle/input/radimagenet-and-nih-cxr-dataset-tfrecord/RagImageNet_Train_GZIP.tfrecord\")\nval_ds = load_tfrecord_dataset(\"/kaggle/input/radimagenet-and-nih-cxr-dataset-tfrecord/RagImageNet_Test_GZIP.tfrecord\")","metadata":{"trusted":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train = pd.read_csv(\"/kaggle/input/radimagenet-and-nih-cxr-dataset-tfrecord/RadImgNet_train.csv\")\nplt.hist(df_train[\"label\"], bins = range(165))\nplt.title(\"Training dataset label-wise distribution\")\n\npp(\"+=\"*50)\npp(f\"Total Training case : {len(df_train)}\")\npp(\"                                     LABELS\")\ndf_label = pd.read_csv(\"/kaggle/input/radimagenet-and-nih-cxr-dataset-tfrecord/RadImgNet_label_encoding.csv\")\npp(df_label)\npp(\"+=\"*50)","metadata":{"trusted":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for img, lab in train_radimagenet_ds.take(1):\n    imgs = img\n    labs = lab\nfig, axes = plt.subplots(4,4, figsize = (15,15))\naxes = axes.flatten()\nfor idx, ax in enumerate(axes):\n    ax.imshow(imgs[idx], cmap = \"bone\")\n    lab_ = int(labs[idx])\n    name = df_label.loc[df_label.index == lab_, 'name'].values[0]\n    ax.set_title(f\"{lab_} : {name}\")\nplt.show()\nprint(keras.ops.max(imgs))","metadata":{"trusted":true,"_kg_hide-input":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ClassToken(keras.layers.Layer):\n    def build(self, input_shape):\n        self.cls = self.add_weight(\n            name=\"cls\",\n            shape=(1, 1, input_shape[-1]),\n            initializer=\"zeros\",\n            trainable=True\n        )\n    \n    def call(self, inputs):\n        # static shape: inputs.shape[0] → None\n        # dynamic shape: tf.shape(inputs)[0] → 실제 배치 크기(tensor)\n        batch_size = tf.shape(inputs)[0]\n        \n        # cls 텐서를 [batch_size, 1, embed_dims]로 브로드캐스트\n        cls_broadcasted = tf.broadcast_to(\n            self.cls, \n            [batch_size, 1, tf.shape(inputs)[-1]]\n        )\n        # CLS 토큰과 패치를 concat\n        return tf.concat([cls_broadcasted, inputs], axis=1)\n\n\ndef get_vit(att_heads = 8, att_depth = 6, embed_dims = 512, mode = \"naive\"):\n    inputs = Input((res,res,1), name = \"RadImgNetInput\")\n    if mode == \"vitdet\":\n        backbone = keras_hub.models.ViTDetBackbone( \n                                                    hidden_size = 768,\n                                                    num_layers = 8,\n                                                    intermediate_dim = 768,\n                                                    num_heads = 12,\n                                                    global_attention_layer_indices = [2,5,7],\n                                                    image_shape=(res, res, 1),\n                                                    patch_size=16,\n                                                    num_output_channels=embed_dims,\n                                                    use_bias=True,\n                                                    use_abs_pos=True,\n                                                    use_rel_pos=True,\n                                                    window_size=14,\n                                                    layer_norm_epsilon=1e-06,\n                                                    include_rescaling = True\n                                                )\n        \n    elif mode == \"effnet\":\n        backbone = keras.applications.EfficientNetV2S(input_shape = [res,res,1], include_top = False, weights = None)\n    else:\n        backbone = Conv2D(filters = embed_dims, padding = 'SAME', kernel_size = 16, strides = 16, name = \"PatchingConv\", activation = \"gelu\")\n    \n    if (mode == \"effnet\") or (mode == \"vitdet\"):\n        patches = backbone(inputs)\n    else:\n        patches = backbone(inputs/255)\n        patches = keras_hub.layers.RotaryEmbedding(name = \"RoPE\")(patches)\n    _, w, h, d_ = keras.ops.shape(patches)\n    cls_layer = ClassToken()\n    \n    patches = ops.reshape(patches, [-1, w*h, embed_dims]) ; seq_len = w*h\n    patches = cls_layer(patches)\n    att_weights = {}\n    for idx in range(att_depth):\n        x0 = LayerNormalization(name = f'preLN{idx}')(patches)\n        x1, att_score = MultiHeadAttention(att_heads, embed_dims//att_heads, name = f\"MHA{idx}\")(query = x0, key = x0, value = x0,\n                                                                                     return_attention_scores = True)\n        att_weights[idx] = att_score\n        x2 = x1 + patches\n        x3 = LayerNormalization(name = f'postLN{idx}')(x2)\n        x4 = Dense(units = embed_dims, name = f\"MLP{idx}\", activation = \"gelu\")(x3)\n        patches = x2 + x4\n        patches = keras.layers.Identity(name = f\"EncodedPatches_{idx}\")(patches)\n    feature_vector = patches[:, 0, :]\n    model = Model(inputs, [feature_vector, att_weights],\n                 name = f\"{mode}ViT_depth{att_depth}_heads{att_heads}_dims{embed_dims}\")\n    return model","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"> ViTOL 등을 위한 pADL 및 vision transformer","metadata":{}},{"cell_type":"code","source":"class pADL(layers.Layer):\n    def __init__(self, embedding_drop_rate=0.5, drop_threshold=0.5, **kwargs):\n        super().__init__(**kwargs)\n        self.embedding_drop_rate = embedding_drop_rate\n        self.drop_threshold = drop_threshold\n\n    def call(self, attention_output, training=None):\n        if not training:\n            return attention_output\n\n        # 어텐션 출력에서 CLS 토큰에 해당하는 부분만 사용\n        cls_attention = attention_output[:, 0, :] # [batch_size, projection_dim]\n        \n        # 패치 중요도 맵 (Patch Importance Map)\n        importance_map = ops.sigmoid(cls_attention)\n        \n        # 패치 드롭 마스크 (Patch Drop Mask)\n        # 활성화가 가장 큰 패치를 드롭\n        max_indices = ops.argmax(cls_attention, axis=-1)\n        drop_mask = tf.one_hot(max_indices, depth=ops.shape(cls_attention)[-1])\n        drop_mask = 1.0 - drop_mask\n        \n        # 드롭아웃 적용 여부 랜덤 선택\n        random_value = tf.random.uniform(shape=())\n        if random_value > self.embedding_drop_rate:\n            # 드롭 마스크 적용\n            selected_map = drop_mask\n        else:\n            # 중요도 맵 적용\n            selected_map = importance_map\n            \n        # 원래 어텐션 출력에 마스크 적용\n        # 브로드캐스팅을 위해 차원 확장\n        selected_map = ops.expand_dims(selected_map, axis=1) # [batch_size, 1, projection_dim]\n        selected_map = keras.ops.cast(selected_map, \"float16\")\n        return attention_output * selected_map\n\nclass FreeViTEncoder(Model):\n    \"\"\"\n    상태 제어가 가능한 ViT 인코더 클래스.\n    pADL 레이어를 포함하며, use_padl 플래그로 활성화 여부를 제어합니다.\n    \"\"\"\n    def __init__(self, att_heads=12, att_depth=8, embed_dims=512, name=\"FreeViTEncoder\", **kwargs):\n        super().__init__(name=name, **kwargs)\n        self.att_heads = att_heads\n        self.att_depth = att_depth\n        self.embed_dims = embed_dims\n        \n        # 외부에서 제어할 플래그\n        self.use_padl = False \n\n        # 1. Patch Embedding Layers (기존 로직과 동일)\n        self.patch_conv = layers.Conv2D(filters=embed_dims // 4, padding='SAME', \n                                        kernel_size=16, strides=16, name=\"PatchingConv\", activation=\"gelu\")\n        self.mini_res_blocks = []\n        for r in range(4):\n            self.mini_res_blocks.append([\n                layers.LayerNormalization(name=f\"MiniResLN_pre_{r}\"),\n                layers.Conv2D(filters=embed_dims // 4, padding='SAME', kernel_size=3, name=f\"MiniResConv_{r}\", activation='gelu'),\n                layers.LayerNormalization(name=f\"MiniResLN_post_{r}\"),\n                layers.Dense(units=embed_dims // 4, activation=\"gelu\", name=f\"MiniResMLP_{r}\")\n            ])\n        self.final_embedding = layers.Dense(units=embed_dims, activation=\"gelu\", name=\"FinalEmbedding\")\n        self.reshape_layer = layers.Reshape((-1, embed_dims), name='Reshape')\n        self.cls_layer = ClassToken(name = \"AppendLearnableCLSToken\")\n\n        # 2. Transformer Blocks with pADL\n        self.transformer_blocks = []\n        for idx in range(att_depth):\n            self.transformer_blocks.append({\n                \"pre_ln\": layers.LayerNormalization(name=f'preLN{idx}'),\n                \"mha\": layers.MultiHeadAttention(att_heads, embed_dims // att_heads, name=f\"MHA{idx}\"),\n                \"padl\": pADL(name=f\"pADL_{idx}\"), # pADL 레이어를 미리 생성\n                \"post_ln\": layers.LayerNormalization(name=f'postLN{idx}'),\n                \"mlp\": layers.Dense(units=embed_dims, name=f\"MLP{idx}\", activation=\"gelu\")\n            })\n            \n        # 3. Output Identity Layers for clear naming\n        self.cls_token_out = layers.Identity(name=\"CLS_Token\")\n        self.patch_tokens_out = layers.Identity(name=\"Patch_Tokens\")\n\n    def call(self, inputs, training=True, apply_padl = True):\n        # Patch Embedding\n        x = self.patch_conv(inputs / 255)\n        for r_block in self.mini_res_blocks:\n            x_res = x\n            x_norm = r_block[0](x)\n            x = r_block[1](x_norm)\n            x = x + x_res\n            x_res = x\n            x_norm = r_block[2](x)\n            x = r_block[3](x_norm)\n            x = x + x_res\n        x = self.final_embedding(x)\n        x = self.reshape_layer(x)\n        x = self.cls_layer(x)\n        # Transformer Blocks\n        for block in self.transformer_blocks:\n            x_res = x\n            x_norm = block[\"pre_ln\"](x)\n            attn_out = block[\"mha\"](query=x_norm, key=x_norm, value=x_norm, training=training)\n            \n            # 여기서 pADL 활성화 여부를 제어\n            if self.use_padl and training and apply_padl:\n                attn_out = block[\"padl\"](attn_out, training=training)\n\n            x = attn_out + x_res\n            x_res = x\n            x_norm = block[\"post_ln\"](x)\n            x = block[\"mlp\"](x_norm)\n            x = x + x_res\n        \n        cls_token = self.cls_token_out(x[:, 0, :])\n        patch_tokens = self.patch_tokens_out(x[:, 1:, :])\n        return cls_token, patch_tokens","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 실험계획\n- 비교군 : original github의 result 및 ViT실험\n- Supervised contrastive learning\n- N회 SSL 후 1회 classifier까지 learning을 반복 --> called 1 \"set\"","metadata":{}},{"cell_type":"code","source":"# dino trainer\nresize_fn = keras.layers.Resizing(res,res)\ndef get_two_views(images):\n    global_view_1 = keras.layers.RandomCrop(256,256)(images)\n    global_veiw_2 = keras.layers.RandomCrop(180,180)(images)\n    global_view_1, global_view_2 = resize_fn(global_view_1), resize_fn(global_view_2)\n    return (global_view_1, global_view_2)\n\n\n\nclass MiniDINO(Model):\n    \"\"\"\n    Miniature DINO model using a pre-existing Vision Transformer backbone.\n    This implementation follows the principles from the DINO and DINOv2 papers.\n    \"\"\"\n    def __init__(\n        self,\n        student_backbone,\n        teacher_backbone,\n        projection_dim=1024,\n        latent_dim=512,\n        teacher_momentum=0.996,\n        center_momentum=0.9,\n        student_temp=0.1,\n        teacher_temp=0.04,\n        **kwargs\n    ):\n        \"\"\"\n        Args:\n            student_backbone (Model): The student ViT model.\n            teacher_backbone (Model): The teacher ViT model (should have the same architecture).\n            projection_dim (int): The dimension of the projection head's hidden layer.\n            latent_dim (int): The output dimension of the projection head (and input from backbone).\n            teacher_momentum (float): The momentum for the teacher network update.\n            center_momentum (float): The momentum for the center update.\n            student_temp (float): The temperature for the student's softmax.\n            teacher_temp (float): The temperature for the teacher's softmax (sharpening).\n        \"\"\"\n        super().__init__(**kwargs)\n        self.student_backbone = student_backbone\n        self.teacher_backbone = teacher_backbone\n        self.teacher_momentum = teacher_momentum\n        self.center_momentum = center_momentum\n        self.student_temp = student_temp\n        self.teacher_temp = teacher_temp\n        self.latent_dim = latent_dim\n        self.resize_fn = keras.layers.Resizing(res,res)\n        self.rc1 = keras.layers.RandomCrop(256,256)\n        self.rc2 = keras.layers.RandomCrop(180,180)\n        # DINO Projection Head\n        self.student_projector = self._build_projector(latent_dim, projection_dim)\n        self.teacher_projector = self._build_projector(latent_dim, projection_dim)\n\n        # Initialize teacher weights with student weights\n        self.teacher_backbone.set_weights(self.student_backbone.get_weights())\n        self.teacher_projector.set_weights(self.student_projector.get_weights())\n\n        # Center vector for teacher output centering [14]\n        self.center = self.add_weight(\n            shape=(1, latent_dim), initializer=\"zeros\", trainable=False, name=\"center\"\n        )\n        \n        # Trackers for metrics\n        self.loss_tracker = keras.metrics.Mean(name=\"dino_loss\")\n    def _get_two_views(self, images):\n        global_view_1 = self.rc1(images)\n        global_view_2 = self.rc2(images)\n        global_view_1, global_view_2 = self.resize_fn(global_view_1), self.resize_fn(global_view_2)\n        return (global_view_1, global_view_2)\n    def _build_projector(self, latent_dim, projection_dim):\n        \"\"\"Builds the MLP projection head as described in DINO.\"\"\"\n        # A simpler 2-layer MLP for this miniature version.\n        # Original DINO uses a 3-layer MLP.\n        return keras.Sequential(\n            [\n                layers.Input(shape=(latent_dim,)),\n                layers.Dense(projection_dim, activation=\"gelu\"),\n                layers.Dense(latent_dim),\n            ],\n            name=\"projector\",\n        )\n\n    def _update_teacher(self):\n        \"\"\"Update teacher network weights using EMA of student weights.\"\"\"\n        for student_w, teacher_w in zip(self.student_backbone.weights, self.teacher_backbone.weights):\n            teacher_w.assign(self.teacher_momentum * teacher_w + (1 - self.teacher_momentum) * student_w)\n        for student_w, teacher_w in zip(self.student_projector.weights, self.teacher_projector.weights):\n            teacher_w.assign(self.teacher_momentum * teacher_w + (1 - self.teacher_momentum) * student_w)\n\n    @tf.function\n    def _update_center(self, teacher_output):\n        \"\"\"Update the center vector using EMA of teacher outputs.\"\"\"\n        batch_center = tf.reduce_mean(teacher_output, axis=0, keepdims=True)\n        self.center.assign(self.center_momentum * self.center + (1 - self.center_momentum) * batch_center)\n\n    def compile(self, optimizer, **kwargs):\n        super().compile(**kwargs)\n        self.optimizer = optimizer\n\n    def train_step(self, data):\n        # The data should be a tuple of two lists of augmented views: (global_crops, local_crops)\n        # For simplicity, this example assumes two global crops.\n        images, label = data\n        view1, view2 = self._get_two_views(images)\n\n        with tf.GradientTape() as tape:\n            # === Teacher Forward Pass (no gradients) ===\n            teacher_repr1 = self.teacher_backbone(view1, training=False)\n            teacher_repr2 = self.teacher_backbone(view2, training=False)\n            \n            teacher_proj1 = self.teacher_projector(teacher_repr1, training=False)\n            teacher_proj2 = self.teacher_projector(teacher_repr2, training=False)\n\n            # Center and sharpen teacher outputs [14]\n            teacher_out1 = tf.nn.softmax((teacher_proj1 - self.center) / self.teacher_temp, axis=-1)\n            teacher_out2 = tf.nn.softmax((teacher_proj2 - self.center) / self.teacher_temp, axis=-1)\n\n            # === Student Forward Pass ===\n            student_repr1 = self.student_backbone(view1, training=True)\n            student_repr2 = self.student_backbone(view2, training=True)\n\n            student_proj1 = self.student_projector(student_repr1, training=True)\n            student_proj2 = self.student_projector(student_repr2, training=True)\n\n            student_out1 = tf.nn.log_softmax(student_proj1 / self.student_temp, axis=-1)\n            student_out2 = tf.nn.log_softmax(student_proj2 / self.student_temp, axis=-1)\n\n            # === Compute DINO Loss (Cross-Entropy) ===\n            # The student predicts the teacher's output for a different view.\n            loss1 = -tf.reduce_mean(tf.reduce_sum(teacher_out2 * student_out1, axis=-1))\n            loss2 = -tf.reduce_mean(tf.reduce_sum(teacher_out1 * student_out2, axis=-1))\n            total_loss = (loss1 + loss2) / 2\n\n        # === Gradient Descent ===\n        trainable_vars = self.student_backbone.trainable_variables + self.student_projector.trainable_variables\n        grads = tape.gradient(total_loss, trainable_vars)\n        self.optimizer.apply_gradients(zip(grads, trainable_vars))\n\n        # === EMA Updates ===\n        self._update_teacher()\n        self._update_center(tf.concat([teacher_proj1, teacher_proj2], axis=0))\n\n        self.loss_tracker.update_state(total_loss)\n        return {\"loss\": self.loss_tracker.result()}\n    \n    def call(self, inputs):\n        # For inference, only the student backbone is used.\n        return self.student_backbone(inputs)\n\nstudent_vit = get_vit()\nteacher_vit = get_vit()\n\ndino_model = MiniDINO(\n    student_backbone=student_vit,\n    teacher_backbone=teacher_vit,\n    #latent_dim=EMBED_DIM\n)\n\ndino_model.compile(optimizer=keras.optimizers.SGD(learning_rate=1e-4))\n#dino_model.fit(train_radimagenet_ds, epochs = 1, steps_per_epoch = 500)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# DINO with supervision","metadata":{}},{"cell_type":"code","source":"class MiniSupDINO(Model):\n    def __init__(\n        self,\n        student_backbone,\n        teacher_backbone,\n        mode = 'supervised', #supervised or dino\n        projection_dim=1024,\n        latent_dim=512,\n        teacher_momentum=0.996,\n        center_momentum=0.9,\n        student_temp=0.1,\n        teacher_temp=0.04,\n        **kwargs\n    ):\n        super().__init__(**kwargs)\n        self.mode = mode\n        self.student_backbone = student_backbone\n        self.teacher_backbone = teacher_backbone\n        self.teacher_momentum = teacher_momentum\n        self.center_momentum = center_momentum\n        self.student_temp = student_temp\n        self.teacher_temp = teacher_temp\n        self.latent_dim = latent_dim\n        self.rc = keras.layers.RandomCrop(int(0.75*res),int(0.75*res))\n        # DINO Projection Head\n        self.student_projector = self._build_projector(latent_dim, projection_dim)\n        self.teacher_projector = self._build_projector(latent_dim, projection_dim)\n        # Supervision (Classifier) Head\n        self.student_classifier = keras.layers.Dense(units = 165, name = \"Student_RadImgNetClassifier\", dtype=\"float32\")\n        self.teacher_classifier = keras.layers.Dense(units = 165, name = \"Teacher_RadImgNetClassifier\", dtype=\"float32\")\n        # Initialize teacher weights with student weights\n        self.teacher_backbone.set_weights(self.student_backbone.get_weights())\n        self.teacher_projector.set_weights(self.student_projector.get_weights())\n        self.teacher_classifier.set_weights(self.student_classifier.get_weights())\n\n        # Center vector for teacher output centering [14]\n        self.center = self.add_weight(\n            shape=(1, latent_dim), initializer=\"zeros\", trainable=False, name=\"center\"\n        )\n        self.center_cls = self.add_weight(\n            shape=(1, 165), initializer=\"zeros\", trainable=False, name=\"center_class_proba\"\n        )\n        \n        # Trackers for metrics\n        self.loss_tracker = keras.metrics.Mean(name=\"Total_loss\")\n        self.dino_loss_tracker = keras.metrics.Mean(name=\"DINO_loss\")\n        self.student_cls_loss_tracker = keras.metrics.Mean(name=\"Student_class_loss\")\n        self.cls_distil_loss_tracker = keras.metrics.Mean(name=\"Class_distil_loss\")\n\n        self.student_acc_tracker = keras.metrics.Mean(name = \"Student_class_Accuracy\")\n        self.teacher_acc_tracker = keras.metrics.Mean(name = \"Teacher_class_Accuracy\")\n        self.compute_acc = keras.metrics.SparseCategoricalAccuracy()\n\n    def _get_two_views(self, images):\n        global_view_1 = images\n        global_view_2 = self.rc(images)\n        #global_view_2 = keras.layers.Resizing(res,res)(global_view_2)\n        return (global_view_1, global_view_2)\n    def _build_projector(self, latent_dim, projection_dim):\n        \"\"\"Builds the MLP projection head as described in DINO.\"\"\"\n        # A simpler 2-layer MLP for this miniature version.\n        # Original DINO uses a 3-layer MLP.\n        return keras.Sequential(\n            [\n                layers.Input(shape=(latent_dim,)),\n                layers.Dense(projection_dim, activation=\"gelu\"),\n                layers.Dense(latent_dim, dtype=\"float32\"),\n            ],\n            name=\"projector\",\n        )\n\n    def _update_teacher(self):\n        \"\"\"Update teacher network weights using EMA of student weights.\"\"\"\n        for student_w, teacher_w in zip(self.student_backbone.weights, self.teacher_backbone.weights):\n            teacher_w.assign(self.teacher_momentum * teacher_w + (1 - self.teacher_momentum) * student_w)\n        for student_w, teacher_w in zip(self.student_projector.weights, self.teacher_projector.weights):\n            teacher_w.assign(self.teacher_momentum * teacher_w + (1 - self.teacher_momentum) * student_w)\n        if (self.mode == \"sup\") or (self.mode == \"supervised\"):\n            for student_w, teacher_w in zip(self.student_classifier.weights, self.teacher_classifier.weights):\n                teacher_w.assign(self.teacher_momentum * teacher_w + (1 - self.teacher_momentum) * student_w)\n    @tf.function\n    def _update_center(self, teacher_output, teacher_proba_output = None):\n        \"\"\"Update the center vector using EMA of teacher outputs.\"\"\"\n        batch_center = tf.reduce_mean(teacher_output, axis=0, keepdims=True)\n        self.center.assign(self.center_momentum * self.center + (1 - self.center_momentum) * batch_center)\n        if (self.mode == \"sup\") or (self.mode == \"supervised\"):\n            batch_center_proba = tf.reduce_mean(teacher_proba_output, axis = 0, keepdims = True)\n            self.center_cls.assign(self.center_momentum * self.center_cls + (1 - self.center_momentum)*batch_center_proba)\n\n    def compile(self, optimizer, **kwargs):\n        super().compile(**kwargs)\n        self.optimizer = optimizer\n\n    def train_step(self, data):\n        # The data should be a tuple of two lists of augmented views: (global_crops, local_crops)\n        # For simplicity, this example assumes two global crops.\n        images, label = data\n        view1, view2 = self._get_two_views(images)\n\n        with tf.GradientTape() as tape:\n            # === Teacher Forward Pass (no gradients) ===\n            teacher_repr1 = self.teacher_backbone(view1, training=False)[0]\n            teacher_repr2 = self.teacher_backbone(view2, training=False)[0]\n            \n            teacher_proj1 = self.teacher_projector(teacher_repr1, training=False)\n            teacher_proj2 = self.teacher_projector(teacher_repr2, training=False)\n\n            # Center and sharpen teacher outputs [14]\n            teacher_out1 = tf.nn.softmax((teacher_proj1 - self.center) / self.teacher_temp, axis=-1)\n            teacher_out2 = tf.nn.softmax((teacher_proj2 - self.center) / self.teacher_temp, axis=-1)\n\n            # === Student Forward Pass ===\n            student_repr1 = self.student_backbone(view1, training=True)[0]\n            student_repr2 = self.student_backbone(view2, training=True)[0]\n\n            student_proj1 = self.student_projector(student_repr1, training=True)\n            student_proj2 = self.student_projector(student_repr2, training=True)\n\n            student_out1 = tf.nn.log_softmax(student_proj1 / self.student_temp, axis=-1)\n            student_out2 = tf.nn.log_softmax(student_proj2 / self.student_temp, axis=-1)\n\n            # === Compute DINO Loss (Cross-Entropy) ===\n            # The student predicts the teacher's output for a different view.\n            loss1 = -tf.reduce_mean(tf.reduce_sum(teacher_out2 * student_out1, axis=-1))\n            loss2 = -tf.reduce_mean(tf.reduce_sum(teacher_out1 * student_out2, axis=-1))\n            dino_loss = (loss1 + loss2) / 2\n            if (self.mode == \"sup\") or (self.mode == \"supervised\"):\n                # calculate student class proba\n                cls_s_1 = self.student_classifier(student_repr1, training = True)\n                cls_s_2 = self.student_classifier(student_repr2, training = True)\n                cls_t_1 = self.teacher_classifier(teacher_repr1, training = False)\n                cls_t_2 = self.teacher_classifier(teacher_repr2, training = False)\n                # supervised loss\n                student_cls_loss = 0.5*(keras.losses.SparseCategoricalCrossentropy(from_logits = True, reduction = None)(y_true = label, y_pred = cls_s_1) + keras.losses.SparseCategoricalCrossentropy(from_logits = True, reduction = None)(y_true = label, y_pred = cls_s_2))\n                student_accuracy = 0.5*(self.compute_acc(y_true = label, y_pred = cls_s_1) + self.compute_acc(y_true = label, y_pred = cls_s_2))\n                student_cls_loss = keras.ops.mean(student_cls_loss)\n                \n                teacher_accuracy = 0.5*(self.compute_acc(y_true = label, y_pred = cls_t_1) + self.compute_acc(y_true = label, y_pred = cls_t_2))\n                \n                # classifier dino loss\n                cls_t_1 = tf.nn.softmax((cls_t_1 - self.center_cls) / self.teacher_temp, axis=-1)\n                cls_t_2 = tf.nn.softmax((cls_t_2 - self.center_cls) / self.teacher_temp, axis=-1)\n\n                cls_s_1 = tf.nn.log_softmax(cls_s_1 / self.student_temp, axis=-1)\n                cls_s_2 = tf.nn.log_softmax(cls_s_2 / self.student_temp, axis=-1)\n                cls_distil_loss_1 = -tf.reduce_mean(tf.reduce_sum(cls_t_2 * cls_s_1, axis=-1))\n                cls_distil_loss_2 = -tf.reduce_mean(tf.reduce_sum(cls_t_1 * cls_s_2, axis=-1))\n                cls_distil_loss = (cls_distil_loss_1 + cls_distil_loss_2) / 2\n\n                total_loss = dino_loss + student_cls_loss + cls_distil_loss\n            else:\n                total_loss = dino_loss\n                student_cls_loss = 0.0\n                cls_distil_loss = 0.0\n                student_accuracy = 0.0\n                teacher_accuracy = 0.0\n                cls_t_1, cls_t_2 = [0.0], [0.0]\n            \n        # === Gradient Descent ===\n        trainable_vars = self.student_backbone.trainable_variables + self.student_projector.trainable_variables\n        if (self.mode == \"sup\") or (self.mode == \"supervised\"):\n            trainable_vars += self.student_classifier.trainable_variables\n        grads = tape.gradient(total_loss, trainable_vars)\n        self.optimizer.apply_gradients(zip(grads, trainable_vars))\n\n        # === EMA Updates ===\n        self._update_teacher()\n        self._update_center(tf.concat([teacher_proj1, teacher_proj2], axis=0),\n                           tf.concat([cls_t_1, cls_t_2], axis = 0)\n                           )\n\n        self.loss_tracker.update_state(total_loss)\n        self.dino_loss_tracker.update_state(dino_loss)\n        self.student_cls_loss_tracker.update_state(student_cls_loss)\n        self.cls_distil_loss_tracker.update_state(cls_distil_loss)\n\n        self.student_acc_tracker.update_state(student_accuracy)\n        self.teacher_acc_tracker.update_state(teacher_accuracy)\n        \n        return {\"total_loss\": self.loss_tracker.result(),\n               \"dino_loss\" : self.dino_loss_tracker.result(),\n               \"student_classification_loss\" : self.student_cls_loss_tracker.result(),\n               \"CLS_distil_loss\" : self.cls_distil_loss_tracker.result(),\n               \n               \"Student_Classification_Accuracy\" : self.student_acc_tracker.result(),\n               \"Teacher_Classification_Accuracy\" : self.teacher_acc_tracker.result()\n               }\n    \n    def call(self, inputs):\n        # For inference, only the student backbone is used.\n        return self.student_backbone(inputs)\n    def get_teacher_model(self):\n        teacher_input = self.teacher_backbone.input\n        teacher_output, attention_weights = self.teacher_backbone.output\n        teacher_proba = self.teacher_classifier(teacher_output)\n        whole_teacher_model = Model(teacher_input, [teacher_proba, attention_weights],\n                                   name = f'{self.teacher_backbone.name}_DINO_Teacher')\n        result = {'feature_extractor' : self.teacher_backbone,\n                 \"classifier\" : self.teacher_classifier,\n                 \"projector\" : self.teacher_projector,\n                 \"whole_model\" : whole_teacher_model}\n        return result","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if False:\n    wandb_config()\n    embed_dims = 512\n    run = wandb.init(project=\"RadImageNet\", \n                             entity=\"gongbungkim\",\n                            name = f'MiniSupDino_embed_dim{embed_dims}',\n                    notes = f\"gray input, res{res}\")\n    wb_callback = wandb.keras.WandbMetricsLogger(log_freq = 10)\n    student_vit = get_vit(att_heads = 8, att_depth = 2, embed_dims = embed_dims, mode = \"vitdet\")\n    teacher_vit = get_vit(att_heads = 8, att_depth = 2, embed_dims = embed_dims, mode = \"vitdet\")\n    teacher_vit.summary()\n    \n    dino_model = MiniSupDINO(\n        student_backbone=student_vit,\n        teacher_backbone=teacher_vit,\n        latent_dim=embed_dims,\n        mode = \"sup\"\n    )\n    optimizer = keras.optimizers.SGD(learning_rate=5e-5)\n    scaled_optimizer = keras.mixed_precision.LossScaleOptimizer(optimizer)\n    dino_model.compile(optimizer=scaled_optimizer\n                      )\n    dino_model.fit(train_radimagenet_ds, epochs = 1, \n                   steps_per_epoch = len(df_train)//batch_size,\n                   callbacks = [wb_callback]\n                  )","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Patchwise DINO","metadata":{}},{"cell_type":"code","source":"# 추후 구현 예정","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# WSOL : ViTOL\n- pADL을 apply한 ViT로 DINO와 같이, 혹은 ViTOL 단독으로 학습 수행","metadata":{}},{"cell_type":"code","source":"# VITOL와 MiniSupDINO 결합\nif True:\n    wandb_config()\n    embed_dims = 512\n    run = wandb.init(project=\"RadImageNet\", \n                             entity=\"gongbungkim\",\n                            name = f'FreeViT_ViTOL+MiniSupDino_embed_dim{embed_dims}',\n                    notes = f\"gray input, res{res}\")\n    wb_callback = wandb.keras.WandbMetricsLogger(log_freq = 10)\n    student_vit = FreeViTEncoder(name = 'FreeViTWithResNet_Student')\n    teacher_vit = FreeViTEncoder(name = 'FreeViTWithResNet_Student')\n    \n    teacher_vit.use_padl = True\n    student_vit.use_padl = True\n    student_vit(keras.random.normal([2, res, res, 1]))\n    teacher_vit(keras.random.normal([2, res, res, 1]))\n    \n    teacher_vit.summary()\n    \n    dino_model = MiniSupDINO(\n        student_backbone=student_vit,\n        teacher_backbone=teacher_vit,\n        latent_dim=embed_dims,\n        mode = \"sup\"\n    )\n    optimizer = keras.optimizers.SGD(learning_rate=5e-5)\n    scaled_optimizer = keras.mixed_precision.LossScaleOptimizer(optimizer)\n    dino_model.compile(optimizer=scaled_optimizer\n                      )\n    dino_model.fit(train_radimagenet_ds, epochs = 1, \n                   steps_per_epoch = len(df_train)//batch_size,\n                   callbacks = [wb_callback]\n                  )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# WSOL + SSL","metadata":{}}]}