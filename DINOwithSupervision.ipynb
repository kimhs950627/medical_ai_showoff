{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8556597,"sourceType":"datasetVersion","datasetId":5112865}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os, sys\nimport random\nimport pydicom\nimport sklearn\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import AgglomerativeClustering, KMeans\nfrom sklearn.preprocessing import StandardScaler\nimport re\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nseed = 42\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport tensorflow as tf\nimport keras\n\nkeras.mixed_precision.set_global_policy(\"mixed_float16\")\n\nfrom keras import layers, Model\nfrom keras import ops, layers, models, losses, optimizers, metrics\nimport keras_hub\nimport keras_cv\nimport keras_nlp\n\nimport cv2\nfrom skimage.io import imread\nkeras.utils.set_random_seed(seed)\nimport tensorflow_io as tfio\nfrom kaggle_datasets import KaggleDatasets\nimport tensorflow_datasets as tfds\nimport tensorflow_probability as tfp\n\nprint(f\"Tensorflow version : {tf.__version__}\")\ntry:\n    print(f\"Keras version : {keras.__version__}\")\nexcept:\n    pass\n\nfrom keras import Input, Model, ops\nfrom keras.models import load_model\n\nfrom keras.layers import Conv2D, DepthwiseConv2D, Dense, Activation, BatchNormalization, LayerNormalization, MultiHeadAttention, Embedding, Subtract, Add, Multiply, GlobalAveragePooling2D, GlobalAveragePooling1D, LayerNormalization\nfrom keras.utils import load_img, img_to_array\nfrom keras.applications import *\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nfrom sklearn.model_selection import train_test_split\nfrom keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\nfrom tqdm.notebook import tqdm\nimport wandb\ndef wandb_config():\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    try:\n        secret_value_0 = user_secrets.get_secret(\"__gcloud_sdk_auth__\")\n        secret_value_1 = user_secrets.get_secret(\"huggingface_key\")\n        secret_value_2 = user_secrets.get_secret(\"wandb_key\")\n        !wandb login $secret_value_2\n    except:\n        secret_value_0 = user_secrets.get_secret(\"huggingface_key\")\n        secret_value_1 = user_secrets.get_secret(\"wandb_key\")\n        !wandb login $secret_value_1\n    \ndef auto_select_accelerator():\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n        print(\"Running on TPU:\", tpu.master())\n    except ValueError:\n        tpu = False\n        strategy = tf.distribute.MirroredStrategy() # for GPU or multi-GPU machines\n    print(f\"Running on {strategy.num_replicas_in_sync} replicas\")\n    \n    return tpu, strategy\n\ntpu, strategy = auto_select_accelerator()\n\nimport PIL\nfrom PIL import Image as PILImage\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nimport pprint\nfrom pprint import pprint as pp\n\nres = 384\nbatch_size = 16\nviz = False","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-07-22T11:10:12.708243Z","iopub.execute_input":"2025-07-22T11:10:12.708620Z","iopub.status.idle":"2025-07-22T11:10:18.986529Z","shell.execute_reply.started":"2025-07-22T11:10:12.708581Z","shell.execute_reply":"2025-07-22T11:10:18.985787Z"}},"outputs":[{"name":"stdout","text":"Tensorflow version : 2.18.0\nKeras version : 3.8.0\nRunning on 1 replicas\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"def _parse_tfrecord(res = res):\n    def parse_tfrecord(tfrecord):\n        features = {'image': tf.io.FixedLenFeature([], tf.string),\n                    'label': tf.io.FixedLenFeature([], tf.int64),\n                    }\n        x = tf.io.parse_single_example(tfrecord, features)\n        image_train = tf.image.decode_jpeg(x['image'], channels=1)\n        image_train = _transform_images(res = res)(image_train)\n        label = tf.cast(x[\"label\"], tf.int32)\n        return (image_train, label)\n    \n    return parse_tfrecord\n\n\ndef _transform_images(res = res):\n    def transform_images(x_train):\n        x_train = tf.image.resize_with_pad(x_train, res, res, antialias = True)\n        x_train = tf.cast(x_train, tf.uint8)\n        return x_train\n    return transform_images\n\ndef load_tfrecord_dataset(tfrecord_name, res = res, batch_size = batch_size, shuffle=True, buffer_size=10240):\n    \"\"\"load dataset from tfrecord\"\"\"\n    raw_dataset = tf.data.TFRecordDataset(tfrecord_name, compression_type = \"GZIP\")\n    raw_dataset = raw_dataset.repeat()\n    if shuffle:\n        raw_dataset = raw_dataset.shuffle(buffer_size=buffer_size)\n    dataset = raw_dataset.map(\n        _parse_tfrecord(),\n        num_parallel_calls=tf.data.AUTOTUNE\n    )\n    dataset = dataset.batch(batch_size, drop_remainder = True)\n    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n    return dataset\n\ntrain_radimagenet_ds = load_tfrecord_dataset(\"/kaggle/input/radimagenet-and-nih-cxr-dataset-tfrecord/RagImageNet_Train_GZIP.tfrecord\")\nval_ds = load_tfrecord_dataset(\"/kaggle/input/radimagenet-and-nih-cxr-dataset-tfrecord/RagImageNet_Test_GZIP.tfrecord\")","metadata":{"trusted":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2025-07-22T11:10:18.987764Z","iopub.execute_input":"2025-07-22T11:10:18.988143Z","iopub.status.idle":"2025-07-22T11:10:25.387265Z","shell.execute_reply.started":"2025-07-22T11:10:18.988112Z","shell.execute_reply":"2025-07-22T11:10:25.386641Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"df_train = pd.read_csv(\"/kaggle/input/radimagenet-and-nih-cxr-dataset-tfrecord/RadImgNet_train.csv\")\nplt.hist(df_train[\"label\"], bins = range(165))\nplt.title(\"Training dataset label-wise distribution\")\n\npp(\"+=\"*50)\npp(f\"Total Training case : {len(df_train)}\")\npp(\"                                     LABELS\")\ndf_label = pd.read_csv(\"/kaggle/input/radimagenet-and-nih-cxr-dataset-tfrecord/RadImgNet_label_encoding.csv\")\npp(df_label)\npp(\"+=\"*50)","metadata":{"trusted":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2025-07-22T11:10:25.388023Z","iopub.execute_input":"2025-07-22T11:10:25.388260Z","iopub.status.idle":"2025-07-22T11:10:33.494071Z","shell.execute_reply.started":"2025-07-22T11:10:25.388235Z","shell.execute_reply":"2025-07-22T11:10:33.493251Z"}},"outputs":[{"name":"stdout","text":"'+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+='\n'Total Training case : 1191117'\n'                                     LABELS'\n     index                      name\n0        0                  US-aorta\n1        1                US-bladder\n2        2                    US-cbd\n3        3                US-fibroid\n4        4                     US-gb\n..     ...                       ...\n160    160  spine-foraminal pathlogy\n161    161              spine-normal\n162    162         spine-osseous abn\n163    163           spine-scoliosis\n164    164            thyroid-nodule\n\n[165 rows x 2 columns]\n'+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+='\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"if viz:\n    for img, lab in train_radimagenet_ds.take(1):\n        imgs = img\n        labs = lab\n    fig, axes = plt.subplots(4,4, figsize = (15,15))\n    axes = axes.flatten()\n    for idx, ax in enumerate(axes):\n        ax.imshow(imgs[idx], cmap = \"bone\")\n        lab_ = int(labs[idx])\n        name = df_label.loc[df_label.index == lab_, 'name'].values[0]\n        ax.set_title(f\"{lab_} : {name}\")\n    plt.show()\n    print(keras.ops.max(imgs))","metadata":{"trusted":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ClassToken(keras.layers.Layer):\n    def build(self, input_shape):\n        self.cls = self.add_weight(\n            name=\"cls\",\n            shape=(1, 1, input_shape[-1]),\n            initializer=\"zeros\",\n            trainable=True\n        )\n    \n    def call(self, inputs):\n        # static shape: inputs.shape[0] → None\n        # dynamic shape: tf.shape(inputs)[0] → 실제 배치 크기(tensor)\n        batch_size = tf.shape(inputs)[0]\n        \n        # cls 텐서를 [batch_size, 1, embed_dims]로 브로드캐스트\n        cls_broadcasted = tf.broadcast_to(\n            self.cls, \n            [batch_size, 1, tf.shape(inputs)[-1]]\n        )\n        # CLS 토큰과 패치를 concat\n        return tf.concat([cls_broadcasted, inputs], axis=1)\n\n\ndef get_vit(att_heads = 8, att_depth = 6, embed_dims = 512, mode = \"naive\"):\n    inputs = Input((res,res,1), name = \"RadImgNetInput\")\n    if mode == \"vitdet\":\n        backbone = keras_hub.models.ViTDetBackbone( \n                                                    hidden_size = 768,\n                                                    num_layers = 8,\n                                                    intermediate_dim = 768,\n                                                    num_heads = 12,\n                                                    global_attention_layer_indices = [2,5,7],\n                                                    image_shape=(res, res, 1),\n                                                    patch_size=16,\n                                                    num_output_channels=embed_dims,\n                                                    use_bias=True,\n                                                    use_abs_pos=True,\n                                                    use_rel_pos=True,\n                                                    window_size=14,\n                                                    layer_norm_epsilon=1e-06,\n                                                    include_rescaling = True\n                                                )\n        \n    elif mode == \"effnet\":\n        backbone = keras.applications.EfficientNetV2S(input_shape = [res,res,1], include_top = False, weights = None)\n    else:\n        backbone = Conv2D(filters = embed_dims, padding = 'SAME', kernel_size = 16, strides = 16, name = \"PatchingConv\", activation = \"gelu\")\n    \n    if (mode == \"effnet\") or (mode == \"vitdet\"):\n        patches = backbone(inputs)\n    else:\n        patches = backbone(inputs/255)\n        patches = keras_hub.layers.RotaryEmbedding(name = \"RoPE\")(patches)\n    _, w, h, d_ = keras.ops.shape(patches)\n    cls_layer = ClassToken()\n    \n    patches = ops.reshape(patches, [-1, w*h, embed_dims]) ; seq_len = w*h\n    patches = cls_layer(patches)\n    att_weights = {}\n    for idx in range(att_depth):\n        x0 = LayerNormalization(name = f'preLN{idx}')(patches)\n        x1, att_score = MultiHeadAttention(att_heads, embed_dims//att_heads, name = f\"MHA{idx}\")(query = x0, key = x0, value = x0,\n                                                                                     return_attention_scores = True)\n        att_weights[idx] = att_score\n        x2 = x1 + patches\n        x3 = LayerNormalization(name = f'postLN{idx}')(x2)\n        x4 = Dense(units = embed_dims, name = f\"MLP{idx}\", activation = \"gelu\")(x3)\n        patches = x2 + x4\n        patches = keras.layers.Identity(name = f\"EncodedPatches_{idx}\")(patches)\n    feature_vector = patches[:, 0, :]\n    model = Model(inputs, [feature_vector, att_weights],\n                 name = f\"{mode}ViT_depth{att_depth}_heads{att_heads}_dims{embed_dims}\")\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T11:10:33.495892Z","iopub.execute_input":"2025-07-22T11:10:33.496133Z","iopub.status.idle":"2025-07-22T11:10:39.523929Z","shell.execute_reply.started":"2025-07-22T11:10:33.496115Z","shell.execute_reply":"2025-07-22T11:10:39.523079Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"> ViTOL 등을 위한 pADL 및 vision transformer","metadata":{}},{"cell_type":"code","source":"class pADL(layers.Layer):\n    def __init__(self, embedding_drop_rate=0.5, drop_threshold=0.5, **kwargs):\n        super().__init__(**kwargs)\n        self.embedding_drop_rate = embedding_drop_rate\n        self.drop_threshold = drop_threshold\n\n    \n    @tf.function\n    def call(self, attention_output, training=None):\n        if not training:\n            return attention_output\n\n        # 어텐션 출력에서 CLS 토큰에 해당하는 부분만 사용\n        cls_attention = attention_output[:, 0, :] # [batch_size, projection_dim]\n        \n        # 패치 중요도 맵 (Patch Importance Map)\n        importance_map = ops.sigmoid(cls_attention)\n        #importance\n        # 패치 드롭 마스크 (Patch Drop Mask)\n        # 활성화가 가장 큰 패치를 드롭\n        max_indices = ops.argmax(cls_attention, axis=-1)\n        drop_mask = tf.one_hot(max_indices, depth=ops.shape(cls_attention)[-1])\n        drop_mask = 1.0 - drop_mask\n        \n        # 드롭아웃 적용 여부 랜덤 선택\n        random_value = tf.random.uniform(shape=())\n        if random_value > self.embedding_drop_rate:\n            # 드롭 마스크 적용\n            selected_map = ops.cast(drop_mask, \"float16\")\n        else:\n            # 중요도 맵 적용\n            selected_map = importance_map\n            \n        # 원래 어텐션 출력에 마스크 적용\n        # 브로드캐스팅을 위해 차원 확장\n        selected_map = ops.expand_dims(selected_map, axis=1) # [batch_size, 1, projection_dim]\n        selected_map = keras.ops.cast(selected_map, \"float16\")\n        return attention_output * selected_map\n\nclass FreeViTEncoder(Model):\n    \"\"\"\n    상태 제어가 가능한 ViT 인코더 클래스.\n    pADL 레이어를 포함하며, use_padl 플래그로 활성화 여부를 제어합니다.\n    \"\"\"\n    def __init__(self, att_heads=12, att_depth=8, embed_dims=512, name=\"FreeViTEncoder\", **kwargs):\n        super().__init__(name=name, **kwargs)\n        self.att_heads = att_heads\n        self.att_depth = att_depth\n        self.embed_dims = embed_dims\n        \n        # 외부에서 제어할 플래그\n        self.use_padl = False \n\n        # 1. Patch Embedding Layers (기존 로직과 동일)\n        self.patch_conv = layers.Conv2D(filters=embed_dims // 4, padding='SAME', \n                                        kernel_size=16, strides=16, name=\"PatchingConv\", activation=\"gelu\")\n        self.mini_res_blocks = []\n        for r in range(4):\n            self.mini_res_blocks.append([\n                layers.LayerNormalization(name=f\"MiniResLN_pre_{r}\"),\n                layers.Conv2D(filters=embed_dims // 4, padding='SAME', kernel_size=3, name=f\"MiniResConv_{r}\", activation='gelu'),\n                layers.LayerNormalization(name=f\"MiniResLN_post_{r}\"),\n                layers.Dense(units=embed_dims // 4, activation=\"gelu\", name=f\"MiniResMLP_{r}\")\n            ])\n        self.final_embedding = layers.Dense(units=embed_dims, activation=\"gelu\", name=\"FinalEmbedding\")\n        self.reshape_layer = layers.Reshape((-1, embed_dims), name='Reshape')\n        self.cls_layer = ClassToken(name = \"AppendLearnableCLSToken\")\n\n        # 2. Transformer Blocks with pADL\n        self.transformer_blocks = []\n        for idx in range(att_depth):\n            self.transformer_blocks.append({\n                \"pre_ln\": layers.LayerNormalization(name=f'preLN{idx}'),\n                \"mha\": layers.MultiHeadAttention(att_heads, embed_dims // att_heads, name=f\"MHA{idx}\"),\n                \"padl\": pADL(name=f\"pADL_{idx}\"), # pADL 레이어를 미리 생성\n                \"post_ln\": layers.LayerNormalization(name=f'postLN{idx}'),\n                \"mlp\": layers.Dense(units=embed_dims, name=f\"MLP{idx}\", activation=\"gelu\")\n            })\n            \n        # 3. Output Identity Layers for clear naming\n        self.cls_token_out = layers.Identity(name=\"CLS_Token\")\n        self.patch_tokens_out = layers.Identity(name=\"Patch_Tokens\")\n\n    def call(self, inputs, training=True, apply_padl = True):\n        # Patch Embedding\n        x = self.patch_conv(inputs / 255)\n        for r_block in self.mini_res_blocks:\n            x_res = x\n            x_norm = r_block[0](x)\n            x = r_block[1](x_norm)\n            x = x + x_res\n            x_res = x\n            x_norm = r_block[2](x)\n            x = r_block[3](x_norm)\n            x = x + x_res\n        x = self.final_embedding(x)\n        x = keras.layers.Reshape((-1, self.embed_dims))(x)\n        x = self.cls_layer(x)\n        # Transformer Blocks\n        for block in self.transformer_blocks:\n            x_res = x\n            x_norm = block[\"pre_ln\"](x)\n            attn_out = block[\"mha\"](query=x_norm, key=x_norm, value=x_norm, training=training)\n            \n            # 여기서 pADL 활성화 여부를 제어\n            if self.use_padl :\n                attn_out = block[\"padl\"](attn_out, training=training)\n\n            x = attn_out + x_res\n            x_res = x\n            x_norm = block[\"post_ln\"](x)\n            x = block[\"mlp\"](x_norm)\n            x = x + x_res\n        \n        cls_token = self.cls_token_out(x[:, 0, :])\n        patch_tokens = self.patch_tokens_out(x[:, 1:, :])\n        return cls_token, patch_tokens","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T11:10:39.524768Z","iopub.execute_input":"2025-07-22T11:10:39.524985Z","iopub.status.idle":"2025-07-22T11:10:45.468565Z","shell.execute_reply.started":"2025-07-22T11:10:39.524969Z","shell.execute_reply":"2025-07-22T11:10:45.467888Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"# 실험계획\n- 비교군 : original github의 result 및 ViT실험\n- Supervised contrastive learning\n- N회 SSL 후 1회 classifier까지 learning을 반복 --> called 1 \"set\"","metadata":{}},{"cell_type":"code","source":"# dino trainer\nresize_fn = keras.layers.Resizing(res,res)\ndef get_two_views(images):\n    global_view_1 = keras.layers.RandomCrop(256,256)(images)\n    global_veiw_2 = keras.layers.RandomCrop(180,180)(images)\n    global_view_1, global_view_2 = resize_fn(global_view_1), resize_fn(global_view_2)\n    return (global_view_1, global_view_2)\n\n\n\nclass MiniDINO(Model):\n    \"\"\"\n    Miniature DINO model using a pre-existing Vision Transformer backbone.\n    This implementation follows the principles from the DINO and DINOv2 papers.\n    \"\"\"\n    def __init__(\n        self,\n        student_backbone,\n        teacher_backbone,\n        projection_dim=1024,\n        latent_dim=512,\n        teacher_momentum=0.996,\n        center_momentum=0.9,\n        student_temp=0.1,\n        teacher_temp=0.04,\n        **kwargs\n    ):\n        \"\"\"\n        Args:\n            student_backbone (Model): The student ViT model.\n            teacher_backbone (Model): The teacher ViT model (should have the same architecture).\n            projection_dim (int): The dimension of the projection head's hidden layer.\n            latent_dim (int): The output dimension of the projection head (and input from backbone).\n            teacher_momentum (float): The momentum for the teacher network update.\n            center_momentum (float): The momentum for the center update.\n            student_temp (float): The temperature for the student's softmax.\n            teacher_temp (float): The temperature for the teacher's softmax (sharpening).\n        \"\"\"\n        super().__init__(**kwargs)\n        self.student_backbone = student_backbone\n        self.teacher_backbone = teacher_backbone\n        self.teacher_momentum = teacher_momentum\n        self.center_momentum = center_momentum\n        self.student_temp = student_temp\n        self.teacher_temp = teacher_temp\n        self.latent_dim = latent_dim\n        self.resize_fn = keras.layers.Resizing(res,res)\n        self.rc1 = keras.layers.RandomCrop(256,256)\n        self.rc2 = keras.layers.RandomCrop(180,180)\n        # DINO Projection Head\n        self.student_projector = self._build_projector(latent_dim, projection_dim)\n        self.teacher_projector = self._build_projector(latent_dim, projection_dim)\n\n        # Initialize teacher weights with student weights\n        self.teacher_backbone.set_weights(self.student_backbone.get_weights())\n        self.teacher_projector.set_weights(self.student_projector.get_weights())\n\n        # Center vector for teacher output centering [14]\n        self.center = self.add_weight(\n            shape=(1, latent_dim), initializer=\"zeros\", trainable=False, name=\"center\"\n        )\n        \n        # Trackers for metrics\n        self.loss_tracker = keras.metrics.Mean(name=\"dino_loss\")\n    def _get_two_views(self, images):\n        global_view_1 = self.rc1(images)\n        global_view_2 = self.rc2(images)\n        global_view_1, global_view_2 = self.resize_fn(global_view_1), self.resize_fn(global_view_2)\n        return (global_view_1, global_view_2)\n    def _build_projector(self, latent_dim, projection_dim):\n        \"\"\"Builds the MLP projection head as described in DINO.\"\"\"\n        # A simpler 2-layer MLP for this miniature version.\n        # Original DINO uses a 3-layer MLP.\n        return keras.Sequential(\n            [\n                layers.Input(shape=(latent_dim,)),\n                layers.Dense(projection_dim, activation=\"gelu\"),\n                layers.Dense(latent_dim),\n            ],\n            name=\"projector\",\n        )\n\n    def _update_teacher(self):\n        \"\"\"Update teacher network weights using EMA of student weights.\"\"\"\n        for student_w, teacher_w in zip(self.student_backbone.weights, self.teacher_backbone.weights):\n            teacher_w.assign(self.teacher_momentum * teacher_w + (1 - self.teacher_momentum) * student_w)\n        for student_w, teacher_w in zip(self.student_projector.weights, self.teacher_projector.weights):\n            teacher_w.assign(self.teacher_momentum * teacher_w + (1 - self.teacher_momentum) * student_w)\n\n    @tf.function\n    def _update_center(self, teacher_output):\n        \"\"\"Update the center vector using EMA of teacher outputs.\"\"\"\n        batch_center = tf.reduce_mean(teacher_output, axis=0, keepdims=True)\n        self.center.assign(self.center_momentum * self.center + (1 - self.center_momentum) * batch_center)\n\n    def compile(self, optimizer, **kwargs):\n        super().compile(**kwargs)\n        self.optimizer = optimizer\n\n    def train_step(self, data):\n        # The data should be a tuple of two lists of augmented views: (global_crops, local_crops)\n        # For simplicity, this example assumes two global crops.\n        images, label = data\n        view1, view2 = self._get_two_views(images)\n\n        with tf.GradientTape() as tape:\n            # === Teacher Forward Pass (no gradients) ===\n            teacher_repr1 = self.teacher_backbone(view1, training=False)\n            teacher_repr2 = self.teacher_backbone(view2, training=False)\n            \n            teacher_proj1 = self.teacher_projector(teacher_repr1, training=False)\n            teacher_proj2 = self.teacher_projector(teacher_repr2, training=False)\n\n            # Center and sharpen teacher outputs [14]\n            teacher_out1 = tf.nn.softmax((teacher_proj1 - self.center) / self.teacher_temp, axis=-1)\n            teacher_out2 = tf.nn.softmax((teacher_proj2 - self.center) / self.teacher_temp, axis=-1)\n\n            # === Student Forward Pass ===\n            student_repr1 = self.student_backbone(view1, training=True)\n            student_repr2 = self.student_backbone(view2, training=True)\n\n            student_proj1 = self.student_projector(student_repr1, training=True)\n            student_proj2 = self.student_projector(student_repr2, training=True)\n\n            student_out1 = tf.nn.log_softmax(student_proj1 / self.student_temp, axis=-1)\n            student_out2 = tf.nn.log_softmax(student_proj2 / self.student_temp, axis=-1)\n\n            # === Compute DINO Loss (Cross-Entropy) ===\n            # The student predicts the teacher's output for a different view.\n            loss1 = -tf.reduce_mean(tf.reduce_sum(teacher_out2 * student_out1, axis=-1))\n            loss2 = -tf.reduce_mean(tf.reduce_sum(teacher_out1 * student_out2, axis=-1))\n            total_loss = (loss1 + loss2) / 2\n\n        # === Gradient Descent ===\n        trainable_vars = self.student_backbone.trainable_variables + self.student_projector.trainable_variables\n        grads = tape.gradient(total_loss, trainable_vars)\n        self.optimizer.apply_gradients(zip(grads, trainable_vars))\n\n        # === EMA Updates ===\n        self._update_teacher()\n        self._update_center(tf.concat([teacher_proj1, teacher_proj2], axis=0))\n\n        self.loss_tracker.update_state(total_loss)\n        return {\"loss\": self.loss_tracker.result()}\n    \n    def call(self, inputs):\n        # For inference, only the student backbone is used.\n        return self.student_backbone(inputs)\n\nstudent_vit = get_vit()\nteacher_vit = get_vit()\n\ndino_model = MiniDINO(\n    student_backbone=student_vit,\n    teacher_backbone=teacher_vit,\n    #latent_dim=EMBED_DIM\n)\n\ndino_model.compile(optimizer=keras.optimizers.SGD(learning_rate=1e-4))\n#dino_model.fit(train_radimagenet_ds, epochs = 1, steps_per_epoch = 500)","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-07-22T11:10:45.469409Z","iopub.execute_input":"2025-07-22T11:10:45.469661Z","iopub.status.idle":"2025-07-22T11:10:52.108425Z","shell.execute_reply.started":"2025-07-22T11:10:45.469644Z","shell.execute_reply":"2025-07-22T11:10:52.107853Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"# DINO with supervision","metadata":{}},{"cell_type":"code","source":"class MiniSupDINO(Model):\n    def __init__(\n        self,\n        student_backbone,\n        teacher_backbone,\n        mode = 'supervised', #supervised or dino\n        projection_dim=1024,\n        latent_dim=512,\n        teacher_momentum=0.996,\n        center_momentum=0.9,\n        student_temp=0.1,\n        teacher_temp=0.04,\n        **kwargs\n    ):\n        super().__init__(**kwargs)\n        self.mode = mode\n        self.student_backbone = student_backbone\n        self.teacher_backbone = teacher_backbone\n        self.teacher_momentum = teacher_momentum\n        self.center_momentum = center_momentum\n        self.student_temp = student_temp\n        self.teacher_temp = teacher_temp\n        self.latent_dim = latent_dim\n        self.rc = keras.layers.RandomCrop(int(0.75*res),int(0.75*res))\n        # DINO Projection Head\n        self.student_projector = self._build_projector(latent_dim, projection_dim)\n        self.teacher_projector = self._build_projector(latent_dim, projection_dim)\n        # Supervision (Classifier) Head\n        self.student_classifier = keras.layers.Dense(units = 165, name = \"Student_RadImgNetClassifier\", dtype=\"float32\")\n        self.teacher_classifier = keras.layers.Dense(units = 165, name = \"Teacher_RadImgNetClassifier\", dtype=\"float32\")\n        # Initialize teacher weights with student weights\n        self.teacher_backbone.set_weights(self.student_backbone.get_weights())\n        self.teacher_projector.set_weights(self.student_projector.get_weights())\n        self.teacher_classifier.set_weights(self.student_classifier.get_weights())\n\n        # Center vector for teacher output centering [14]\n        self.center = self.add_weight(\n            shape=(1, latent_dim), initializer=\"zeros\", trainable=False, name=\"center\"\n        )\n        self.center_cls = self.add_weight(\n            shape=(1, 165), initializer=\"zeros\", trainable=False, name=\"center_class_proba\"\n        )\n        \n        # Trackers for metrics\n        self.loss_tracker = keras.metrics.Mean(name=\"Total_loss\")\n        self.dino_loss_tracker = keras.metrics.Mean(name=\"DINO_loss\")\n        self.student_cls_loss_tracker = keras.metrics.Mean(name=\"Student_class_loss\")\n        self.cls_distil_loss_tracker = keras.metrics.Mean(name=\"Class_distil_loss\")\n\n        self.student_acc_tracker = keras.metrics.Mean(name = \"Student_class_Accuracy\")\n        self.teacher_acc_tracker = keras.metrics.Mean(name = \"Teacher_class_Accuracy\")\n        self.compute_acc = keras.metrics.SparseCategoricalAccuracy()\n\n    def _get_two_views(self, images):\n        global_view_1 = images\n        global_view_2 = self.rc(images)\n        #global_view_2 = keras.layers.Resizing(res,res)(global_view_2)\n        return (global_view_1, global_view_2)\n    def _build_projector(self, latent_dim, projection_dim):\n        \"\"\"Builds the MLP projection head as described in DINO.\"\"\"\n        # A simpler 2-layer MLP for this miniature version.\n        # Original DINO uses a 3-layer MLP.\n        return keras.Sequential(\n            [\n                layers.Input(shape=(latent_dim,)),\n                layers.Dense(projection_dim, activation=\"gelu\"),\n                layers.Dense(latent_dim, dtype=\"float32\"),\n            ],\n            name=\"projector\",\n        )\n\n    def _update_teacher(self):\n        \"\"\"Update teacher network weights using EMA of student weights.\"\"\"\n        for student_w, teacher_w in zip(self.student_backbone.weights, self.teacher_backbone.weights):\n            teacher_w.assign(self.teacher_momentum * teacher_w + (1 - self.teacher_momentum) * student_w)\n        for student_w, teacher_w in zip(self.student_projector.weights, self.teacher_projector.weights):\n            teacher_w.assign(self.teacher_momentum * teacher_w + (1 - self.teacher_momentum) * student_w)\n        if (self.mode == \"sup\") or (self.mode == \"supervised\"):\n            for student_w, teacher_w in zip(self.student_classifier.weights, self.teacher_classifier.weights):\n                teacher_w.assign(self.teacher_momentum * teacher_w + (1 - self.teacher_momentum) * student_w)\n    @tf.function\n    def _update_center(self, teacher_output, teacher_proba_output = None):\n        \"\"\"Update the center vector using EMA of teacher outputs.\"\"\"\n        batch_center = tf.reduce_mean(teacher_output, axis=0, keepdims=True)\n        self.center.assign(self.center_momentum * self.center + (1 - self.center_momentum) * batch_center)\n        if (self.mode == \"sup\") or (self.mode == \"supervised\"):\n            batch_center_proba = tf.reduce_mean(teacher_proba_output, axis = 0, keepdims = True)\n            self.center_cls.assign(self.center_momentum * self.center_cls + (1 - self.center_momentum)*batch_center_proba)\n\n    def compile(self, optimizer, **kwargs):\n        super().compile(**kwargs)\n        self.optimizer = optimizer\n\n    def train_step(self, data):\n        # The data should be a tuple of two lists of augmented views: (global_crops, local_crops)\n        # For simplicity, this example assumes two global crops.\n        images, label = data\n        view1, view2 = self._get_two_views(images)\n\n        with tf.GradientTape() as tape:\n            # === Teacher Forward Pass (no gradients) ===\n            teacher_repr1 = self.teacher_backbone(view1, training=False)[0]\n            teacher_repr2 = self.teacher_backbone(view2, training=False)[0]\n            \n            teacher_proj1 = self.teacher_projector(teacher_repr1, training=False)\n            teacher_proj2 = self.teacher_projector(teacher_repr2, training=False)\n\n            # Center and sharpen teacher outputs [14]\n            teacher_out1 = tf.nn.softmax((teacher_proj1 - self.center) / self.teacher_temp, axis=-1)\n            teacher_out2 = tf.nn.softmax((teacher_proj2 - self.center) / self.teacher_temp, axis=-1)\n\n            # === Student Forward Pass ===\n            student_repr1 = self.student_backbone(view1, training=True)[0]\n            student_repr2 = self.student_backbone(view2, training=True)[0]\n\n            student_proj1 = self.student_projector(student_repr1, training=True)\n            student_proj2 = self.student_projector(student_repr2, training=True)\n\n            student_out1 = tf.nn.log_softmax(student_proj1 / self.student_temp, axis=-1)\n            student_out2 = tf.nn.log_softmax(student_proj2 / self.student_temp, axis=-1)\n\n            # === Compute DINO Loss (Cross-Entropy) ===\n            # The student predicts the teacher's output for a different view.\n            loss1 = -tf.reduce_mean(tf.reduce_sum(teacher_out2 * student_out1, axis=-1))\n            loss2 = -tf.reduce_mean(tf.reduce_sum(teacher_out1 * student_out2, axis=-1))\n            loss1, loss2 = keras.ops.clip(loss1, -100.0,100.0), keras.ops.clip(loss2, -100.0,100.0)\n            dino_loss = (loss1 + loss2) / 2\n            if (self.mode == \"sup\") or (self.mode == \"supervised\"):\n                # calculate student class proba\n                cls_s_1 = self.student_classifier(student_repr1, training = True)\n                cls_s_2 = self.student_classifier(student_repr2, training = True)\n                cls_t_1 = self.teacher_classifier(teacher_repr1, training = False)\n                cls_t_2 = self.teacher_classifier(teacher_repr2, training = False)\n                # supervised loss\n                student_cls_loss = 0.5*(keras.losses.SparseCategoricalCrossentropy(from_logits = True, reduction = None)(y_true = label, y_pred = cls_s_1) + keras.losses.SparseCategoricalCrossentropy(from_logits = True, reduction = None)(y_true = label, y_pred = cls_s_2))\n                student_accuracy = 0.5*(self.compute_acc(y_true = label, y_pred = cls_s_1) + self.compute_acc(y_true = label, y_pred = cls_s_2))\n                student_cls_loss = keras.ops.mean(student_cls_loss)\n                \n                teacher_accuracy = 0.5*(self.compute_acc(y_true = label, y_pred = cls_t_1) + self.compute_acc(y_true = label, y_pred = cls_t_2))\n                \n                # classifier dino loss\n                cls_t_1 = tf.nn.softmax((cls_t_1 - self.center_cls) / self.teacher_temp, axis=-1)\n                cls_t_2 = tf.nn.softmax((cls_t_2 - self.center_cls) / self.teacher_temp, axis=-1)\n\n                cls_s_1 = tf.nn.log_softmax(cls_s_1 / self.student_temp, axis=-1)\n                cls_s_2 = tf.nn.log_softmax(cls_s_2 / self.student_temp, axis=-1)\n                cls_distil_loss_1 = -tf.reduce_mean(tf.reduce_sum(cls_t_2 * cls_s_1, axis=-1))\n                cls_distil_loss_2 = -tf.reduce_mean(tf.reduce_sum(cls_t_1 * cls_s_2, axis=-1))\n                cls_distil_loss = (cls_distil_loss_1 + cls_distil_loss_2) / 2\n\n                total_loss = dino_loss + student_cls_loss + cls_distil_loss\n            else:\n                total_loss = dino_loss\n                student_cls_loss = 0.0\n                cls_distil_loss = 0.0\n                student_accuracy = 0.0\n                teacher_accuracy = 0.0\n                cls_t_1, cls_t_2 = [0.0], [0.0]\n            \n        # === Gradient Descent ===\n        trainable_vars = self.student_backbone.trainable_variables + self.student_projector.trainable_variables\n        if (self.mode == \"sup\") or (self.mode == \"supervised\"):\n            trainable_vars += self.student_classifier.trainable_variables\n        grads = tape.gradient(total_loss, trainable_vars)\n        self.optimizer.apply_gradients(zip(grads, trainable_vars))\n\n        # === EMA Updates ===\n        self._update_teacher()\n        self._update_center(tf.concat([teacher_proj1, teacher_proj2], axis=0),\n                           tf.concat([cls_t_1, cls_t_2], axis = 0)\n                           )\n\n        self.loss_tracker.update_state(total_loss)\n        self.dino_loss_tracker.update_state(dino_loss)\n        self.student_cls_loss_tracker.update_state(student_cls_loss)\n        self.cls_distil_loss_tracker.update_state(cls_distil_loss)\n\n        self.student_acc_tracker.update_state(student_accuracy)\n        self.teacher_acc_tracker.update_state(teacher_accuracy)\n        \n        return {\"total_loss\": self.loss_tracker.result(),\n               \"dino_loss\" : self.dino_loss_tracker.result(),\n               \"student_classification_loss\" : self.student_cls_loss_tracker.result(),\n               \"CLS_distil_loss\" : self.cls_distil_loss_tracker.result(),\n               \n               \"Student_Classification_Accuracy\" : self.student_acc_tracker.result(),\n               \"Teacher_Classification_Accuracy\" : self.teacher_acc_tracker.result()\n               }\n    \n    def call(self, inputs):\n        # For inference, only the student backbone is used.\n        return self.student_backbone(inputs)\n    def get_teacher_model(self):\n        teacher_input = self.teacher_backbone.input\n        teacher_output, attention_weights = self.teacher_backbone.output\n        teacher_proba = self.teacher_classifier(teacher_output)\n        whole_teacher_model = Model(teacher_input, [teacher_proba, attention_weights],\n                                   name = f'{self.teacher_backbone.name}_DINO_Teacher')\n        result = {'feature_extractor' : self.teacher_backbone,\n                 \"classifier\" : self.teacher_classifier,\n                 \"projector\" : self.teacher_projector,\n                 \"whole_model\" : whole_teacher_model}\n        return result","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-07-22T11:12:59.144658Z","iopub.execute_input":"2025-07-22T11:12:59.145363Z","iopub.status.idle":"2025-07-22T11:13:05.660475Z","shell.execute_reply.started":"2025-07-22T11:12:59.145335Z","shell.execute_reply":"2025-07-22T11:13:05.659629Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"if False:\n    wandb_config()\n    embed_dims = 512\n    run = wandb.init(project=\"RadImageNet\", \n                             entity=\"gongbungkim\",\n                            name = f'MiniSupDino_embed_dim{embed_dims}',\n                    notes = f\"gray input, res{res}\")\n    wb_callback = wandb.keras.WandbMetricsLogger(log_freq = 10)\n    student_vit = get_vit(att_heads = 8, att_depth = 2, embed_dims = embed_dims, mode = \"vitdet\")\n    teacher_vit = get_vit(att_heads = 8, att_depth = 2, embed_dims = embed_dims, mode = \"vitdet\")\n    teacher_vit.summary()\n    \n    dino_model = MiniSupDINO(\n        student_backbone=student_vit,\n        teacher_backbone=teacher_vit,\n        latent_dim=embed_dims,\n        mode = \"sup\"\n    )\n    optimizer = keras.optimizers.SGD(learning_rate=5e-5)\n    scaled_optimizer = keras.mixed_precision.LossScaleOptimizer(optimizer)\n    dino_model.compile(optimizer=scaled_optimizer\n                      )\n    dino_model.fit(train_radimagenet_ds, epochs = 1, \n                   steps_per_epoch = len(df_train)//batch_size,\n                   callbacks = [wb_callback]\n                  )","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-07-22T11:10:58.319493Z","iopub.execute_input":"2025-07-22T11:10:58.319696Z","iopub.status.idle":"2025-07-22T11:11:04.317708Z","shell.execute_reply.started":"2025-07-22T11:10:58.319680Z","shell.execute_reply":"2025-07-22T11:11:04.317050Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"# Patchwise DINO","metadata":{}},{"cell_type":"code","source":"# 추후 구현 예정","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T11:11:04.318589Z","iopub.execute_input":"2025-07-22T11:11:04.318832Z","iopub.status.idle":"2025-07-22T11:11:10.479990Z","shell.execute_reply.started":"2025-07-22T11:11:04.318816Z","shell.execute_reply":"2025-07-22T11:11:10.479152Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"# WSOL : ViTOL\n- pADL을 apply한 ViT로 DINO와 같이, 혹은 ViTOL 단독으로 학습 수행","metadata":{}},{"cell_type":"code","source":"# VITOL 단독\nif False:\n    wandb_config()\n    embed_dims = 512\n    run = wandb.init(project=\"RadImageNet\", \n                             entity=\"gongbungkim\",\n                            name = f'FreeViT_ViTOL+MiniSupDino_embed_dim{embed_dims}',\n                    notes = f\"gray input, res{res}\")\n    wb_callback = wandb.keras.WandbMetricsLogger(log_freq = 10)\n    student_vit = FreeViTEncoder(name = 'FreeViTWithResNet_Student')\n    teacher_vit = FreeViTEncoder(name = 'FreeViTWithResNet_Teacher')\n    \n    teacher_vit.use_padl = True\n    student_vit.use_padl = True\n    \n    teacher_vit.summary()\n    \n    optimizer = keras.optimizers.SGD(learning_rate=5e-5)\n    scaled_optimizer = keras.mixed_precision.LossScaleOptimizer(optimizer)\n    teacher_vit.compile(optimizer=scaled_optimizer, loss = 'sparse_categorical_crossentropy', metrics = 'accuracy'\n                      )\n    teacher_vit.fit(train_radimagenet_ds, epochs = 1, \n                   steps_per_epoch = len(df_train)//batch_size,\n                   callbacks = [wb_callback]\n                  )","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# VITOL와 MiniSupDINO 결합\nif True:\n    wandb_config()\n    embed_dims = 512\n    run = wandb.init(project=\"RadImageNet\", \n                             entity=\"gongbungkim\",\n                            name = f'FreeViT_ViTOL+MiniSupDino_embed_dim{embed_dims}',\n                    notes = f\"gray input, res{res}\")\n    wb_callback = wandb.keras.WandbMetricsLogger(log_freq = 10)\n    student_vit = FreeViTEncoder(name = 'FreeViTWithResNet_Student')\n    teacher_vit = FreeViTEncoder(name = 'FreeViTWithResNet_Teacher')\n    \n    teacher_vit.use_padl = True\n    student_vit.use_padl = True\n    \n    teacher_vit.summary()\n    \n    dino_model = MiniSupDINO(\n        student_backbone=student_vit,\n        teacher_backbone=teacher_vit,\n        latent_dim=embed_dims,\n        mode = \"sup\"\n    )\n    optimizer = keras.optimizers.SGD(learning_rate=5e-5)\n    scaled_optimizer = keras.mixed_precision.LossScaleOptimizer(optimizer)\n    dino_model.compile(optimizer=scaled_optimizer\n                      )\n    dino_model.fit(train_radimagenet_ds, epochs = 1, \n                   steps_per_epoch = len(df_train)//batch_size,\n                   callbacks = [wb_callback]\n                  )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T11:13:27.456212Z","iopub.execute_input":"2025-07-22T11:13:27.456499Z"},"jupyter":{"source_hidden":true}},"outputs":[{"name":"stdout","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Finishing previous runs because reinit is set to 'default'."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">FreeViT_ViTOL+MiniSupDino_embed_dim512</strong> at: <a href='https://wandb.ai/gongbungkim/RadImageNet/runs/gvmj27ph' target=\"_blank\">https://wandb.ai/gongbungkim/RadImageNet/runs/gvmj27ph</a><br> View project at: <a href='https://wandb.ai/gongbungkim/RadImageNet' target=\"_blank\">https://wandb.ai/gongbungkim/RadImageNet</a><br>Synced 5 W&B file(s), 0 media file(s), 7 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250722_111202-gvmj27ph/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.20.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250722_111336-9x7tim9m</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/gongbungkim/RadImageNet/runs/9x7tim9m' target=\"_blank\">FreeViT_ViTOL+MiniSupDino_embed_dim512</a></strong> to <a href='https://wandb.ai/gongbungkim/RadImageNet' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/gongbungkim/RadImageNet' target=\"_blank\">https://wandb.ai/gongbungkim/RadImageNet</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/gongbungkim/RadImageNet/runs/9x7tim9m' target=\"_blank\">https://wandb.ai/gongbungkim/RadImageNet/runs/9x7tim9m</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"FreeViTWithResNet_Teacher\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"FreeViTWithResNet_Teacher\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ PatchingConv (\u001b[38;5;33mConv2D\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MiniResLN_pre_0                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MiniResConv_0 (\u001b[38;5;33mConv2D\u001b[0m)          │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MiniResLN_post_0                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MiniResMLP_0 (\u001b[38;5;33mDense\u001b[0m)            │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MiniResLN_pre_1                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MiniResConv_1 (\u001b[38;5;33mConv2D\u001b[0m)          │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MiniResLN_post_1                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MiniResMLP_1 (\u001b[38;5;33mDense\u001b[0m)            │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MiniResLN_pre_2                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MiniResConv_2 (\u001b[38;5;33mConv2D\u001b[0m)          │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MiniResLN_post_2                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MiniResMLP_2 (\u001b[38;5;33mDense\u001b[0m)            │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MiniResLN_pre_3                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MiniResConv_3 (\u001b[38;5;33mConv2D\u001b[0m)          │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MiniResLN_post_3                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MiniResMLP_3 (\u001b[38;5;33mDense\u001b[0m)            │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ FinalEmbedding (\u001b[38;5;33mDense\u001b[0m)          │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ Reshape (\u001b[38;5;33mReshape\u001b[0m)               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ AppendLearnableCLSToken         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n│ (\u001b[38;5;33mClassToken\u001b[0m)                    │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ preLN0 (\u001b[38;5;33mLayerNormalization\u001b[0m)     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MHA0 (\u001b[38;5;33mMultiHeadAttention\u001b[0m)       │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ pADL_0 (\u001b[38;5;33mpADL\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ postLN0 (\u001b[38;5;33mLayerNormalization\u001b[0m)    │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MLP0 (\u001b[38;5;33mDense\u001b[0m)                    │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ preLN1 (\u001b[38;5;33mLayerNormalization\u001b[0m)     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MHA1 (\u001b[38;5;33mMultiHeadAttention\u001b[0m)       │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ pADL_1 (\u001b[38;5;33mpADL\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ postLN1 (\u001b[38;5;33mLayerNormalization\u001b[0m)    │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MLP1 (\u001b[38;5;33mDense\u001b[0m)                    │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ preLN2 (\u001b[38;5;33mLayerNormalization\u001b[0m)     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MHA2 (\u001b[38;5;33mMultiHeadAttention\u001b[0m)       │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ pADL_2 (\u001b[38;5;33mpADL\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ postLN2 (\u001b[38;5;33mLayerNormalization\u001b[0m)    │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MLP2 (\u001b[38;5;33mDense\u001b[0m)                    │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ preLN3 (\u001b[38;5;33mLayerNormalization\u001b[0m)     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MHA3 (\u001b[38;5;33mMultiHeadAttention\u001b[0m)       │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ pADL_3 (\u001b[38;5;33mpADL\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ postLN3 (\u001b[38;5;33mLayerNormalization\u001b[0m)    │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MLP3 (\u001b[38;5;33mDense\u001b[0m)                    │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ preLN4 (\u001b[38;5;33mLayerNormalization\u001b[0m)     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MHA4 (\u001b[38;5;33mMultiHeadAttention\u001b[0m)       │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ pADL_4 (\u001b[38;5;33mpADL\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ postLN4 (\u001b[38;5;33mLayerNormalization\u001b[0m)    │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MLP4 (\u001b[38;5;33mDense\u001b[0m)                    │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ preLN5 (\u001b[38;5;33mLayerNormalization\u001b[0m)     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MHA5 (\u001b[38;5;33mMultiHeadAttention\u001b[0m)       │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ pADL_5 (\u001b[38;5;33mpADL\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ postLN5 (\u001b[38;5;33mLayerNormalization\u001b[0m)    │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MLP5 (\u001b[38;5;33mDense\u001b[0m)                    │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ preLN6 (\u001b[38;5;33mLayerNormalization\u001b[0m)     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MHA6 (\u001b[38;5;33mMultiHeadAttention\u001b[0m)       │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ pADL_6 (\u001b[38;5;33mpADL\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ postLN6 (\u001b[38;5;33mLayerNormalization\u001b[0m)    │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MLP6 (\u001b[38;5;33mDense\u001b[0m)                    │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ preLN7 (\u001b[38;5;33mLayerNormalization\u001b[0m)     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MHA7 (\u001b[38;5;33mMultiHeadAttention\u001b[0m)       │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ pADL_7 (\u001b[38;5;33mpADL\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ postLN7 (\u001b[38;5;33mLayerNormalization\u001b[0m)    │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MLP7 (\u001b[38;5;33mDense\u001b[0m)                    │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ CLS_Token (\u001b[38;5;33mIdentity\u001b[0m)            │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ Patch_Tokens (\u001b[38;5;33mIdentity\u001b[0m)         │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ PatchingConv (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MiniResLN_pre_0                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MiniResConv_0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)          │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MiniResLN_post_0                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MiniResMLP_0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MiniResLN_pre_1                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MiniResConv_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)          │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MiniResLN_post_1                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MiniResMLP_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MiniResLN_pre_2                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MiniResConv_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)          │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MiniResLN_post_2                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MiniResMLP_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MiniResLN_pre_3                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MiniResConv_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)          │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MiniResLN_post_3                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MiniResMLP_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ FinalEmbedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ Reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ AppendLearnableCLSToken         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ClassToken</span>)                    │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ preLN0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MHA0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)       │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ pADL_0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">pADL</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ postLN0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)    │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MLP0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ preLN1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MHA1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)       │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ pADL_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">pADL</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ postLN1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)    │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MLP1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ preLN2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MHA2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)       │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ pADL_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">pADL</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ postLN2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)    │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MLP2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ preLN3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MHA3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)       │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ pADL_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">pADL</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ postLN3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)    │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MLP3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ preLN4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MHA4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)       │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ pADL_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">pADL</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ postLN4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)    │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MLP4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ preLN5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MHA5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)       │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ pADL_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">pADL</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ postLN5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)    │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MLP5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ preLN6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MHA6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)       │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ pADL_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">pADL</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ postLN6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)    │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MLP6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ preLN7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MHA7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)       │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ pADL_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">pADL</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ postLN7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)    │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MLP7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ CLS_Token (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Identity</span>)            │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ Patch_Tokens (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Identity</span>)         │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"\u001b[1m  633/74444\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15:12:43\u001b[0m 742ms/step - CLS_distil_loss: nan - Student_Classification_Accuracy: 0.0046 - Teacher_Classification_Accuracy: 0.0046 - dino_loss: nan - student_classification_loss: nan - total_loss: nan","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"# WSOL + SSL","metadata":{}}]}