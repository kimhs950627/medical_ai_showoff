{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8556597,"sourceType":"datasetVersion","datasetId":5112865}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os, sys\nimport random\nimport pydicom\nimport sklearn\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import AgglomerativeClustering, KMeans\nfrom sklearn.preprocessing import StandardScaler\nimport re\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nseed = 42\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport tensorflow as tf\nimport keras# ; keras.config.set_dtype_policy(\"mixed_float16\")\nfrom keras import layers, Model\nfrom keras import ops, layers, models, losses, optimizers, metrics\nimport keras_hub\nimport keras_cv\nimport keras_nlp\n\nimport cv2\nfrom skimage.io import imread\nkeras.utils.set_random_seed(seed)\nimport tensorflow_io as tfio\nfrom kaggle_datasets import KaggleDatasets\nimport tensorflow_datasets as tfds\nimport tensorflow_probability as tfp\n\nprint(f\"Tensorflow version : {tf.__version__}\")\ntry:\n    print(f\"Keras version : {keras.__version__}\")\nexcept:\n    pass\n\nfrom keras import Input, Model, ops\nfrom keras.models import load_model\n\nfrom keras.layers import Conv2D, DepthwiseConv2D, Dense, Activation, BatchNormalization, LayerNormalization, MultiHeadAttention, Embedding, Subtract, Add, Multiply, GlobalAveragePooling2D, GlobalAveragePooling1D, LayerNormalization\nfrom keras.utils import load_img, img_to_array\nfrom keras.applications import *\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nfrom sklearn.model_selection import train_test_split\nfrom keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\nfrom tqdm.notebook import tqdm\nimport wandb\ndef wandb_config():\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    try:\n        secret_value_0 = user_secrets.get_secret(\"__gcloud_sdk_auth__\")\n        secret_value_1 = user_secrets.get_secret(\"huggingface_key\")\n        secret_value_2 = user_secrets.get_secret(\"wandb_key\")\n        !wandb login $secret_value_2\n    except:\n        secret_value_0 = user_secrets.get_secret(\"huggingface_key\")\n        secret_value_1 = user_secrets.get_secret(\"wandb_key\")\n        !wandb login $secret_value_1\n    \ndef auto_select_accelerator():\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n        print(\"Running on TPU:\", tpu.master())\n    except ValueError:\n        tpu = False\n        strategy = tf.distribute.MirroredStrategy() # for GPU or multi-GPU machines\n    print(f\"Running on {strategy.num_replicas_in_sync} replicas\")\n    \n    return tpu, strategy\n\ntpu, strategy = auto_select_accelerator()\n\nimport PIL\nfrom PIL import Image as PILImage\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nimport pprint\nfrom pprint import pprint as pp\n\nres = 384\nbatch_size = 16","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def _parse_tfrecord(res = res):\n    def parse_tfrecord(tfrecord):\n        features = {'image': tf.io.FixedLenFeature([], tf.string),\n                    'label': tf.io.FixedLenFeature([], tf.int64),\n                    }\n        x = tf.io.parse_single_example(tfrecord, features)\n        image_train = tf.image.decode_jpeg(x['image'], channels=1)\n        image_train = _transform_images(res = res)(image_train)\n        label = tf.cast(x[\"label\"], tf.int32)\n        return (image_train, label)\n    \n    return parse_tfrecord\n\n\ndef _transform_images(res = res):\n    def transform_images(x_train):\n        x_train = tf.image.resize_with_pad(x_train, res, res, antialias = True)\n        x_train = tf.cast(x_train, tf.uint8)\n        return x_train\n    return transform_images\n\ndef load_tfrecord_dataset(tfrecord_name, res = res, batch_size = batch_size, shuffle=True, buffer_size=10240):\n    \"\"\"load dataset from tfrecord\"\"\"\n    raw_dataset = tf.data.TFRecordDataset(tfrecord_name, compression_type = \"GZIP\")\n    raw_dataset = raw_dataset.repeat()\n    if shuffle:\n        raw_dataset = raw_dataset.shuffle(buffer_size=buffer_size)\n    dataset = raw_dataset.map(\n        _parse_tfrecord(),\n        num_parallel_calls=tf.data.AUTOTUNE\n    )\n    dataset = dataset.batch(batch_size, drop_remainder = True)\n    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n    return dataset\n\ntrain_radimagenet_ds = load_tfrecord_dataset(\"/kaggle/input/radimagenet-and-nih-cxr-dataset-tfrecord/RagImageNet_Train_GZIP.tfrecord\")\nval_ds = load_tfrecord_dataset(\"/kaggle/input/radimagenet-and-nih-cxr-dataset-tfrecord/RagImageNet_Test_GZIP.tfrecord\")","metadata":{"trusted":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train = pd.read_csv(\"/kaggle/input/radimagenet-and-nih-cxr-dataset-tfrecord/RadImgNet_train.csv\")\nplt.hist(df_train[\"label\"], bins = range(165))\nplt.title(\"Training dataset label-wise distribution\")\n\npp(\"+=\"*50)\npp(f\"Total Training case : {len(df_train)}\")\npp(\"                                     LABELS\")\ndf_label = pd.read_csv(\"/kaggle/input/radimagenet-and-nih-cxr-dataset-tfrecord/RadImgNet_label_encoding.csv\")\npp(df_label)\npp(\"+=\"*50)","metadata":{"trusted":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for img, lab in train_radimagenet_ds.take(1):\n    imgs = img\n    labs = lab\nfig, axes = plt.subplots(4,4, figsize = (15,15))\naxes = axes.flatten()\nfor idx, ax in enumerate(axes):\n    ax.imshow(imgs[idx], cmap = \"bone\")\n    lab_ = int(labs[idx])\n    name = df_label.loc[df_label.index == lab_, 'name'].values[0]\n    ax.set_title(f\"{lab_} : {name}\")\nplt.show()","metadata":{"trusted":true,"_kg_hide-input":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_vit(att_heads = 8, att_depth = 6, embed_dims = 512):\n    inputs = Input((res,res,1), name = \"RadImgNetInput\")\n    patches = Conv2D(filters = embed_dims, padding = 'SAME', kernel_size = 16, strides = 16, name = \"PatchingConv\", activation = \"gelu\")(inputs)\n    _, w, h, d_ = keras.ops.shape(patches)\n    patches = ops.reshape(patches, [-1, w*h, embed_dims]) ; seq_len = w*h\n    # positional encoding with RoPE\n    patches = keras_hub.layers.RotaryEmbedding(name = \"RoPE\")(patches)\n    cls_token = keras.layers.GlobalAveragePooling1D()(patches)\n    patches = tf.concat([cls_token, patches],\n                       axis = 1)\n    att_weights = {}\n    for idx in range(att_depth):\n        x0 = LayerNormalization(name = f'preLN{idx}')(patches)\n        x1, att_score = MultiHeadAttention(att_heads, embed_dims//att_heads, name = f\"MHA{idx}\")(query = x0, key = x0, value = x0,\n                                                                                     return_attention_scores = True)\n        att_weights[idx] = att_score\n        x2 = x1 + patches\n        x3 = LayerNormalization(name = f'postLN{idx}')(x2)\n        x4 = Dense(units = embed_dims, name = f\"MLP{idx}\", activation = \"gelu\")(x3)\n        patches = x2 + x4\n        patches = keras.layers.Identity(name = f\"EncodedPatches_{idx}\")(patches)\n    feature_vector = patches[:, 0, :]\n    #classifier = Dense(units = 165, activation = 'softmax', name = \"label_classifier\")(feature_vector)\n    model = Model(inputs, [feature_vector, att_weights],\n                 name = f\"SimpleViT_depth{att_depth}_heads{att_heads}_dims{embed_dims}\")\n    return model\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 실험계획\n- 비교군 : original github의 result 및 ViT실험\n- Supervised contrastive learning\n- N회 SSL 후 1회 classifier까지 learning을 반복 --> called 1 \"set\"","metadata":{}},{"cell_type":"code","source":"# dino trainer\nresize_fn = keras.layers.Resizing(res,res)\ndef get_two_views(images):\n    global_view_1 = keras.layers.RandomCrop(256,256)(images)\n    global_veiw_2 = keras.layers.RandomCrop(180,180)(images)\n    global_view_1, global_view_2 = resize_fn(global_view_1), resize_fn(global_view_2)\n    return (global_view_1, global_view_2)\n\n\n\nclass MiniDINO(Model):\n    \"\"\"\n    Miniature DINO model using a pre-existing Vision Transformer backbone.\n    This implementation follows the principles from the DINO and DINOv2 papers.\n    \"\"\"\n    def __init__(\n        self,\n        student_backbone,\n        teacher_backbone,\n        projection_dim=1024,\n        latent_dim=512,\n        teacher_momentum=0.996,\n        center_momentum=0.9,\n        student_temp=0.1,\n        teacher_temp=0.04,\n        **kwargs\n    ):\n        \"\"\"\n        Args:\n            student_backbone (Model): The student ViT model.\n            teacher_backbone (Model): The teacher ViT model (should have the same architecture).\n            projection_dim (int): The dimension of the projection head's hidden layer.\n            latent_dim (int): The output dimension of the projection head (and input from backbone).\n            teacher_momentum (float): The momentum for the teacher network update.\n            center_momentum (float): The momentum for the center update.\n            student_temp (float): The temperature for the student's softmax.\n            teacher_temp (float): The temperature for the teacher's softmax (sharpening).\n        \"\"\"\n        super().__init__(**kwargs)\n        self.student_backbone = student_backbone\n        self.teacher_backbone = teacher_backbone\n        self.teacher_momentum = teacher_momentum\n        self.center_momentum = center_momentum\n        self.student_temp = student_temp\n        self.teacher_temp = teacher_temp\n        self.latent_dim = latent_dim\n        self.resize_fn = keras.layers.Resizing(res,res)\n        self.rc1 = keras.layers.RandomCrop(256,256)\n        self.rc2 = keras.layers.RandomCrop(180,180)\n        # DINO Projection Head\n        self.student_projector = self._build_projector(latent_dim, projection_dim)\n        self.teacher_projector = self._build_projector(latent_dim, projection_dim)\n\n        # Initialize teacher weights with student weights\n        self.teacher_backbone.set_weights(self.student_backbone.get_weights())\n        self.teacher_projector.set_weights(self.student_projector.get_weights())\n\n        # Center vector for teacher output centering [14]\n        self.center = self.add_weight(\n            shape=(1, latent_dim), initializer=\"zeros\", trainable=False, name=\"center\"\n        )\n        \n        # Trackers for metrics\n        self.loss_tracker = keras.metrics.Mean(name=\"dino_loss\")\n    def _get_two_views(self, images):\n        global_view_1 = self.rc1(images)\n        global_view_2 = self.rc2(images)\n        global_view_1, global_view_2 = self.resize_fn(global_view_1), self.resize_fn(global_view_2)\n        return (global_view_1, global_view_2)\n    def _build_projector(self, latent_dim, projection_dim):\n        \"\"\"Builds the MLP projection head as described in DINO.\"\"\"\n        # A simpler 2-layer MLP for this miniature version.\n        # Original DINO uses a 3-layer MLP.\n        return keras.Sequential(\n            [\n                layers.Input(shape=(latent_dim,)),\n                layers.Dense(projection_dim, activation=\"gelu\"),\n                layers.Dense(latent_dim),\n            ],\n            name=\"projector\",\n        )\n\n    def _update_teacher(self):\n        \"\"\"Update teacher network weights using EMA of student weights.\"\"\"\n        for student_w, teacher_w in zip(self.student_backbone.weights, self.teacher_backbone.weights):\n            teacher_w.assign(self.teacher_momentum * teacher_w + (1 - self.teacher_momentum) * student_w)\n        for student_w, teacher_w in zip(self.student_projector.weights, self.teacher_projector.weights):\n            teacher_w.assign(self.teacher_momentum * teacher_w + (1 - self.teacher_momentum) * student_w)\n\n    @tf.function\n    def _update_center(self, teacher_output):\n        \"\"\"Update the center vector using EMA of teacher outputs.\"\"\"\n        batch_center = tf.reduce_mean(teacher_output, axis=0, keepdims=True)\n        self.center.assign(self.center_momentum * self.center + (1 - self.center_momentum) * batch_center)\n\n    def compile(self, optimizer, **kwargs):\n        super().compile(**kwargs)\n        self.optimizer = optimizer\n\n    def train_step(self, data):\n        # The data should be a tuple of two lists of augmented views: (global_crops, local_crops)\n        # For simplicity, this example assumes two global crops.\n        images, label = data\n        view1, view2 = self._get_two_views(images)\n\n        with tf.GradientTape() as tape:\n            # === Teacher Forward Pass (no gradients) ===\n            teacher_repr1 = self.teacher_backbone(view1, training=False)\n            teacher_repr2 = self.teacher_backbone(view2, training=False)\n            \n            teacher_proj1 = self.teacher_projector(teacher_repr1, training=False)\n            teacher_proj2 = self.teacher_projector(teacher_repr2, training=False)\n\n            # Center and sharpen teacher outputs [14]\n            teacher_out1 = tf.nn.softmax((teacher_proj1 - self.center) / self.teacher_temp, axis=-1)\n            teacher_out2 = tf.nn.softmax((teacher_proj2 - self.center) / self.teacher_temp, axis=-1)\n\n            # === Student Forward Pass ===\n            student_repr1 = self.student_backbone(view1, training=True)\n            student_repr2 = self.student_backbone(view2, training=True)\n\n            student_proj1 = self.student_projector(student_repr1, training=True)\n            student_proj2 = self.student_projector(student_repr2, training=True)\n\n            student_out1 = tf.nn.log_softmax(student_proj1 / self.student_temp, axis=-1)\n            student_out2 = tf.nn.log_softmax(student_proj2 / self.student_temp, axis=-1)\n\n            # === Compute DINO Loss (Cross-Entropy) ===\n            # The student predicts the teacher's output for a different view.\n            loss1 = -tf.reduce_mean(tf.reduce_sum(teacher_out2 * student_out1, axis=-1))\n            loss2 = -tf.reduce_mean(tf.reduce_sum(teacher_out1 * student_out2, axis=-1))\n            total_loss = (loss1 + loss2) / 2\n\n        # === Gradient Descent ===\n        trainable_vars = self.student_backbone.trainable_variables + self.student_projector.trainable_variables\n        grads = tape.gradient(total_loss, trainable_vars)\n        self.optimizer.apply_gradients(zip(grads, trainable_vars))\n\n        # === EMA Updates ===\n        self._update_teacher()\n        self._update_center(tf.concat([teacher_proj1, teacher_proj2], axis=0))\n\n        self.loss_tracker.update_state(total_loss)\n        return {\"loss\": self.loss_tracker.result()}\n    \n    def call(self, inputs):\n        # For inference, only the student backbone is used.\n        return self.student_backbone(inputs)\n\nstudent_vit = get_vit()\nteacher_vit = get_vit()\n\ndino_model = MiniDINO(\n    student_backbone=student_vit,\n    teacher_backbone=teacher_vit,\n    #latent_dim=EMBED_DIM\n)\n\ndino_model.compile(optimizer=keras.optimizers.SGD(learning_rate=1e-4))\n#dino_model.fit(train_radimagenet_ds, epochs = 1, steps_per_epoch = 500)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# DINO with supervision","metadata":{}},{"cell_type":"code","source":"class MiniSupDINO(Model):\n    def __init__(\n        self,\n        student_backbone,\n        teacher_backbone,\n        mode = 'supervised', #supervised or dino\n        projection_dim=1024,\n        latent_dim=512,\n        teacher_momentum=0.996,\n        center_momentum=0.9,\n        student_temp=0.1,\n        teacher_temp=0.04,\n        **kwargs\n    ):\n        super().__init__(**kwargs)\n        self.mode = mode\n        self.student_backbone = student_backbone\n        self.teacher_backbone = teacher_backbone\n        self.teacher_momentum = teacher_momentum\n        self.center_momentum = center_momentum\n        self.student_temp = student_temp\n        self.teacher_temp = teacher_temp\n        self.latent_dim = latent_dim\n        self.rc = keras.layers.RandomCrop(int(0.75*res),int(0.75*res))\n        # DINO Projection Head\n        self.student_projector = self._build_projector(latent_dim, projection_dim)\n        self.teacher_projector = self._build_projector(latent_dim, projection_dim)\n        # Supervision (Classifier) Head\n        self.student_classifier = keras.layers.Dense(units = 165, name = \"Student_RadImgNetClassifier\")\n        self.teacher_classifier = keras.layers.Dense(units = 165, name = \"Teacher_RadImgNetClassifier\")\n        # Initialize teacher weights with student weights\n        self.teacher_backbone.set_weights(self.student_backbone.get_weights())\n        self.teacher_projector.set_weights(self.student_projector.get_weights())\n        self.teacher_classifier.set_weights(self.student_classifier.get_weights())\n\n        # Center vector for teacher output centering [14]\n        self.center = self.add_weight(\n            shape=(1, latent_dim), initializer=\"zeros\", trainable=False, name=\"center\"\n        )\n        self.center_cls = self.add_weight(\n            shape=(1, 165), initializer=\"zeros\", trainable=False, name=\"center_class_proba\"\n        )\n        \n        # Trackers for metrics\n        self.loss_tracker = keras.metrics.Mean(name=\"Total_loss\")\n        self.dino_loss_tracker = keras.metrics.Mean(name=\"DINO_loss\")\n        self.student_cls_loss_tracker = keras.metrics.Mean(name=\"Student_class_loss\")\n        self.cls_distil_loss_tracker = keras.metrics.Mean(name=\"Class_distil_loss\")\n\n        self.student_acc_tracker = keras.metrics.Mean(name = \"Student_class_Accuracy\")\n        self.teacher_acc_tracker = keras.metrics.Mean(name = \"Teacher_class_Accuracy\")\n        self.compute_acc = keras.metrics.SparseCategoricalAccuracy()\n\n    def _get_two_views(self, images):\n        global_view_1 = images\n        global_view_2 = self.rc(images)\n        global_view_2 = keras.layers.Resizing(res,res)(global_view_2)\n        return (global_view_1, global_view_2)\n    def _build_projector(self, latent_dim, projection_dim):\n        \"\"\"Builds the MLP projection head as described in DINO.\"\"\"\n        # A simpler 2-layer MLP for this miniature version.\n        # Original DINO uses a 3-layer MLP.\n        return keras.Sequential(\n            [\n                layers.Input(shape=(latent_dim,)),\n                layers.Dense(projection_dim, activation=\"gelu\"),\n                layers.Dense(latent_dim),\n            ],\n            name=\"projector\",\n        )\n\n    def _update_teacher(self):\n        \"\"\"Update teacher network weights using EMA of student weights.\"\"\"\n        for student_w, teacher_w in zip(self.student_backbone.weights, self.teacher_backbone.weights):\n            teacher_w.assign(self.teacher_momentum * teacher_w + (1 - self.teacher_momentum) * student_w)\n        for student_w, teacher_w in zip(self.student_projector.weights, self.teacher_projector.weights):\n            teacher_w.assign(self.teacher_momentum * teacher_w + (1 - self.teacher_momentum) * student_w)\n        if (self.mode == \"sup\") or (self.mode == \"supervised\"):\n            for student_w, teacher_w in zip(self.student_classifier.weights, self.teacher_classifier.weights):\n                teacher_w.assign(self.teacher_momentum * teacher_w + (1 - self.teacher_momentum) * student_w)\n    @tf.function\n    def _update_center(self, teacher_output, teacher_proba_output = None):\n        \"\"\"Update the center vector using EMA of teacher outputs.\"\"\"\n        batch_center = tf.reduce_mean(teacher_output, axis=0, keepdims=True)\n        self.center.assign(self.center_momentum * self.center + (1 - self.center_momentum) * batch_center)\n        if (self.mode == \"sup\") or (self.mode == \"supervised\"):\n            batch_center_proba = tf.reduce_mean(teacher_proba_output, axis = 0, keepdims = True)\n            self.center_cls.assign(self.center_momentum * self.center_cls + (1 - self.center_momentum)*batch_center_proba)\n\n    def compile(self, optimizer, **kwargs):\n        super().compile(**kwargs)\n        self.optimizer = optimizer\n\n    def train_step(self, data):\n        # The data should be a tuple of two lists of augmented views: (global_crops, local_crops)\n        # For simplicity, this example assumes two global crops.\n        images, label = data\n        view1, view2 = self._get_two_views(images)\n\n        with tf.GradientTape() as tape:\n            # === Teacher Forward Pass (no gradients) ===\n            teacher_repr1,_ = self.teacher_backbone(view1, training=False)\n            teacher_repr2,_ = self.teacher_backbone(view2, training=False)\n            \n            teacher_proj1 = self.teacher_projector(teacher_repr1, training=False)\n            teacher_proj2 = self.teacher_projector(teacher_repr2, training=False)\n\n            # Center and sharpen teacher outputs [14]\n            teacher_out1 = tf.nn.softmax((teacher_proj1 - self.center) / self.teacher_temp, axis=-1)\n            teacher_out2 = tf.nn.softmax((teacher_proj2 - self.center) / self.teacher_temp, axis=-1)\n\n            # === Student Forward Pass ===\n            student_repr1,_ = self.student_backbone(view1, training=True)\n            student_repr2,_ = self.student_backbone(view2, training=True)\n\n            student_proj1 = self.student_projector(student_repr1, training=True)\n            student_proj2 = self.student_projector(student_repr2, training=True)\n\n            student_out1 = tf.nn.log_softmax(student_proj1 / self.student_temp, axis=-1)\n            student_out2 = tf.nn.log_softmax(student_proj2 / self.student_temp, axis=-1)\n\n            # === Compute DINO Loss (Cross-Entropy) ===\n            # The student predicts the teacher's output for a different view.\n            loss1 = -tf.reduce_mean(tf.reduce_sum(teacher_out2 * student_out1, axis=-1))\n            loss2 = -tf.reduce_mean(tf.reduce_sum(teacher_out1 * student_out2, axis=-1))\n            dino_loss = (loss1 + loss2) / 2\n            if (self.mode == \"sup\") or (self.mode == \"supervised\"):\n                # calculate student class proba\n                cls_s_1 = self.student_classifier(student_repr1, training = True)\n                cls_s_2 = self.student_classifier(student_repr2, training = True)\n                cls_t_1 = self.teacher_classifier(teacher_repr1, training = False)\n                cls_t_2 = self.teacher_classifier(teacher_repr2, training = False)\n                # supervised loss\n                student_cls_loss = 0.5*(keras.losses.SparseCategoricalCrossentropy(from_logits = True, reduction = None)(y_true = label, y_pred = cls_s_1) + keras.losses.SparseCategoricalCrossentropy(from_logits = True, reduction = None)(y_true = label, y_pred = cls_s_2))\n                student_accuracy = 0.5*(self.compute_acc(y_true = label, y_pred = cls_s_1) + self.compute_acc(y_true = label, y_pred = cls_s_2))\n                student_cls_loss = keras.ops.mean(student_cls_loss)\n                \n                teacher_accuracy = 0.5*(self.compute_acc(y_true = label, y_pred = cls_t_1) + self.compute_acc(y_true = label, y_pred = cls_t_2))\n                \n                # classifier dino loss\n                cls_t_1 = tf.nn.softmax((cls_t_1 - self.center_cls) / self.teacher_temp, axis=-1)\n                cls_t_2 = tf.nn.softmax((cls_t_2 - self.center_cls) / self.teacher_temp, axis=-1)\n\n                cls_s_1 = tf.nn.log_softmax(cls_s_1 / self.student_temp, axis=-1)\n                cls_s_2 = tf.nn.log_softmax(cls_s_2 / self.student_temp, axis=-1)\n                cls_distil_loss_1 = -tf.reduce_mean(tf.reduce_sum(cls_t_2 * cls_s_1, axis=-1))\n                cls_distil_loss_2 = -tf.reduce_mean(tf.reduce_sum(cls_t_1 * cls_s_2, axis=-1))\n                cls_distil_loss = (cls_distil_loss_1 + cls_distil_loss_2) / 2\n\n                total_loss = dino_loss + student_cls_loss + cls_distil_loss\n            else:\n                total_loss = dino_loss\n                student_cls_loss = 0.0\n                cls_distil_loss = 0.0\n                student_accuracy = 0.0\n                teacher_accuracy = 0.0\n                cls_t_1, cls_t_2 = [0.0], [0.0]\n            \n        # === Gradient Descent ===\n        trainable_vars = self.student_backbone.trainable_variables + self.student_projector.trainable_variables\n        if (self.mode == \"sup\") or (self.mode == \"supervised\"):\n            trainable_vars += self.student_classifier.trainable_variables\n        grads = tape.gradient(total_loss, trainable_vars)\n        self.optimizer.apply_gradients(zip(grads, trainable_vars))\n\n        # === EMA Updates ===\n        self._update_teacher()\n        self._update_center(tf.concat([teacher_proj1, teacher_proj2], axis=0),\n                           tf.concat([cls_t_1, cls_t_2], axis = 0)\n                           )\n\n        self.loss_tracker.update_state(total_loss)\n        self.dino_loss_tracker.update_state(dino_loss)\n        self.student_cls_loss_tracker.update_state(student_cls_loss)\n        self.cls_distil_loss_tracker.update_state(cls_distil_loss)\n\n        self.student_acc_tracker.update_state(student_accuracy)\n        self.teacher_acc_tracker.update_state(teacher_accuracy)\n        \n        return {\"total_loss\": self.loss_tracker.result(),\n               \"dino_loss\" : self.dino_loss_tracker.result(),\n               \"student_classification_loss\" : self.student_cls_loss_tracker.result(),\n               \"CLS_distil_loss\" : self.cls_distil_loss_tracker.result(),\n               \n               \"Student_Classification_Accuracy\" : self.student_acc_tracker.result(),\n               \"Teacher_Classification_Accuracy\" : self.teacher_acc_tracker.result()\n               }\n    \n    def call(self, inputs):\n        # For inference, only the student backbone is used.\n        return self.student_backbone(inputs)\n    def get_teacher_model(self):\n        teacher_input = self.teacher_backbone.input\n        teacher_output, attention_weights = self.teacher_backbone.output\n        teacher_proba = self.teacher_classifier(teacher_output)\n        whole_teacher_model = Model(teacher_input, [teacher_proba, attention_weights],\n                                   name = f'{self.teacher_backbone.name}_DINO_Teacher')\n        result = {'feature_extractor' : self.teacher_backbone,\n                 \"classifier\" : self.teacher_classifier,\n                 \"projector\" : self.teacher_projector,\n                 \"whole_model\" : whole_teacher_model}\n        return result","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with strategy.scope():\n    student_vit = get_vit(embed_dims = 512)\n    teacher_vit = get_vit(embed_dims = 512)\n    \n    dino_model = MiniSupDINO(\n        student_backbone=student_vit,\n        teacher_backbone=teacher_vit,\n        latent_dim=512,\n        mode = \"sup\"\n    )\n    dino_model.compile(optimizer=keras.optimizers.AdamW(learning_rate=1e-4))\n    dino_model.fit(train_radimagenet_ds, epochs = 1, steps_per_epoch = 500)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}