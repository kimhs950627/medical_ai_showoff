{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13532159,"sourceType":"datasetVersion","datasetId":5112865}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport math\n\nfrom typing import List, Dict, Optional, Union, Tuple\nfrom abc import ABC, abstractmethod\n\nimport os, sys\nimport random\nimport pydicom\nimport sklearn\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import AgglomerativeClustering, KMeans\nfrom sklearn.preprocessing import StandardScaler\nimport re\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nseed = 42\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport tensorflow as tf\nimport keras\n\n#keras.mixed_precision.set_global_policy(\"mixed_float16\")\n\nfrom keras import layers, Model\nfrom keras import ops, layers, models, losses, optimizers, metrics\nimport keras_hub\nimport keras_cv\nimport keras_nlp\n\nimport cv2\nfrom skimage.io import imread\nkeras.utils.set_random_seed(seed)\nimport tensorflow_io as tfio\nfrom kaggle_datasets import KaggleDatasets\nimport tensorflow_datasets as tfds\nimport tensorflow_probability as tfp\n\nprint(f\"Tensorflow version : {tf.__version__}\")\ntry:\n    print(f\"Keras version : {keras.__version__}\")\nexcept:\n    pass\n\nfrom keras import Input, Model, ops\nfrom keras.models import load_model\n\nfrom keras.layers import Conv2D, DepthwiseConv2D, Dense, Activation, BatchNormalization, LayerNormalization, MultiHeadAttention, Embedding, Subtract, Add, Multiply, GlobalAveragePooling2D, GlobalAveragePooling1D, LayerNormalization\nfrom keras.utils import load_img, img_to_array\nfrom keras.applications import *\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nfrom sklearn.model_selection import train_test_split\nfrom keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\nfrom tqdm.notebook import tqdm\nimport wandb\ndef wandb_config():\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    try:\n        secret_value_0 = user_secrets.get_secret(\"__gcloud_sdk_auth__\")\n        secret_value_1 = user_secrets.get_secret(\"huggingface_key\")\n        secret_value_2 = user_secrets.get_secret(\"wandb_key\")\n        !wandb login $secret_value_2\n    except:\n        secret_value_0 = user_secrets.get_secret(\"wandb_key\")\n        !wandb login $secret_value_0\n    \ndef auto_select_accelerator():\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n        print(\"Running on TPU:\", tpu.master())\n    except ValueError:\n        tpu = False\n        strategy = tf.distribute.MirroredStrategy() # for GPU or multi-GPU machines\n    print(f\"Running on {strategy.num_replicas_in_sync} replicas\")\n    \n    return tpu, strategy\n\ntpu, strategy = auto_select_accelerator()\n\nimport PIL\nfrom PIL import Image as PILImage\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nimport pprint\nfrom pprint import pprint as pp\nimport wandb\nwandb_config()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2025-11-14T02:11:27.729213Z","iopub.execute_input":"2025-11-14T02:11:27.729513Z","iopub.status.idle":"2025-11-14T02:12:18.249137Z","shell.execute_reply.started":"2025-11-14T02:11:27.729490Z","shell.execute_reply":"2025-11-14T02:12:18.247777Z"},"jupyter":{"source_hidden":true}},"outputs":[{"name":"stderr","text":"2025-11-14 02:11:35.444924: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763086295.745478      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763086295.833659      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stdout","text":"Tensorflow version : 2.18.0\nKeras version : 3.8.0\nINFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\nRunning on 1 replicas\n","output_type":"stream"},{"name":"stderr","text":"2025-11-14 02:12:16.054377: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n","output_type":"stream"},{"name":"stdout","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Hyperparameter setting","metadata":{}},{"cell_type":"code","source":"image_size = res = 512\nbatch_size = 8\nviz = False\npatch_size = 16\nembed_dims = 384\nuse_hybrid = True\nsegment_metric = 'dot'\nif use_hybrid:\n    interleaved = False\n    n_fourier = 0\n    n_attention = 2\nelse:\n    interleaved = False\n    n_fourier = 0\n    n_attention = 8\n\nwandb_configs = config = {\n    \"res\": res,\n    \"batch_size\": batch_size,\n    \"patch_size\" : patch_size,\n    \"embed_dims\" : embed_dims,\n    'F_A_interleaved' : interleaved,\n    'n_fourier' : n_fourier,\n    'n_attention' : n_attention,\n    'use_hybrid' : use_hybrid,\n    'segment_metric' : segment_metric\n}\n\npprint.pprint(wandb_configs)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-14T02:12:18.252037Z","iopub.execute_input":"2025-11-14T02:12:18.253179Z","iopub.status.idle":"2025-11-14T02:12:18.262631Z","shell.execute_reply.started":"2025-11-14T02:12:18.253142Z","shell.execute_reply":"2025-11-14T02:12:18.260882Z"}},"outputs":[{"name":"stdout","text":"{'F_A_interleaved': False,\n 'batch_size': 8,\n 'embed_dims': 384,\n 'n_attention': 2,\n 'n_fourier': 0,\n 'patch_size': 16,\n 'res': 512,\n 'segment_metric': 'dot',\n 'use_hybrid': True}\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"def _parse_tfrecord(res = res):\n    def parse_tfrecord(tfrecord):\n        features = {'image': tf.io.FixedLenFeature([], tf.string),\n                    'label': tf.io.FixedLenFeature([], tf.int64),\n                    }\n        x = tf.io.parse_single_example(tfrecord, features)\n        image_train = tf.image.decode_jpeg(x['image'], channels=1)\n        image_train = _transform_images(res = res)(image_train)\n        label = tf.cast(x[\"label\"], tf.int32)\n        return (image_train, label)\n    \n    return parse_tfrecord\n\n\ndef _transform_images(res = res):\n    def transform_images(x_train):\n        x_train = tf.image.resize_with_pad(x_train, res, res, antialias = True)\n        x_train = tf.cast(x_train, tf.uint8)\n        return x_train\n    return transform_images\n\ndef load_tfrecord_dataset(tfrecord_name, res = res, batch_size = batch_size, shuffle=True, buffer_size=10240):\n    \"\"\"load dataset from tfrecord\"\"\"\n    raw_dataset = tf.data.TFRecordDataset(tfrecord_name, compression_type = \"GZIP\")\n    raw_dataset = raw_dataset.repeat()\n    if shuffle:\n        raw_dataset = raw_dataset.shuffle(buffer_size=buffer_size)\n    dataset = raw_dataset.map(\n        _parse_tfrecord(),\n        num_parallel_calls=tf.data.AUTOTUNE\n    )\n    dataset = dataset.batch(batch_size, drop_remainder = True)\n    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n    return dataset\n\ntrain_radimagenet_ds = load_tfrecord_dataset(\"/kaggle/input/radimagenet-and-nih-cxr-dataset-tfrecord/RagImageNet_Train_GZIP.tfrecord\")\nval_ds = load_tfrecord_dataset(\"/kaggle/input/radimagenet-and-nih-cxr-dataset-tfrecord/RagImageNet_Test_GZIP.tfrecord\")","metadata":{"trusted":true,"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-11-14T02:12:18.264074Z","iopub.execute_input":"2025-11-14T02:12:18.264484Z","iopub.status.idle":"2025-11-14T02:12:18.740777Z","shell.execute_reply.started":"2025-11-14T02:12:18.264448Z","shell.execute_reply":"2025-11-14T02:12:18.739805Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"df_train = pd.read_csv(\"/kaggle/input/radimagenet-and-nih-cxr-dataset-tfrecord/RadImgNet_train.csv\") ; total_steps = int(len(df_train)//batch_size)\nplt.hist(df_train[\"label\"], bins = range(165))\nplt.title(\"Training dataset label-wise distribution\")\n\npp(\"+=\"*50)\npp(f\"Total Training case : {len(df_train)}\")\npp(\"                                     LABELS\")\ndf_label = pd.read_csv(\"/kaggle/input/radimagenet-and-nih-cxr-dataset-tfrecord/RadImgNet_label_encoding.csv\")\npp(df_label)\npp(\"+=\"*50)\n\nfor img, lab in train_radimagenet_ds.take(1):\n    imgs = img\n    labs = lab","metadata":{"trusted":true,"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-11-14T02:12:18.741969Z","iopub.execute_input":"2025-11-14T02:12:18.742365Z","iopub.status.idle":"2025-11-14T02:12:31.730645Z","shell.execute_reply.started":"2025-11-14T02:12:18.742301Z","shell.execute_reply":"2025-11-14T02:12:31.729653Z"}},"outputs":[{"name":"stdout","text":"'+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+='\n'Total Training case : 1191117'\n'                                     LABELS'\n     index                      name\n0        0                  US-aorta\n1        1                US-bladder\n2        2                    US-cbd\n3        3                US-fibroid\n4        4                     US-gb\n..     ...                       ...\n160    160  spine-foraminal pathlogy\n161    161              spine-normal\n162    162         spine-osseous abn\n163    163           spine-scoliosis\n164    164            thyroid-nodule\n\n[165 rows x 2 columns]\n'+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+='\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"if viz:\n    for img, lab in train_radimagenet_ds.take(1):\n        imgs = img\n        labs = lab\n    print(keras.ops.max(imgs))\n    fig, axes = plt.subplots(4,4, figsize = (15,15))\n    axes = axes.flatten()\n    for idx, ax in enumerate(axes):\n        ax.imshow(imgs[idx], cmap = \"bone\")\n        lab_ = int(labs[idx])\n        name = df_label.loc[df_label.index == lab_, 'name'].values[0]\n        ax.set_title(f\"{lab_} : {name}\")\n    plt.show()\n    ","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-11-14T02:12:31.732009Z","iopub.execute_input":"2025-11-14T02:12:31.732279Z","iopub.status.idle":"2025-11-14T02:12:31.739267Z","shell.execute_reply.started":"2025-11-14T02:12:31.732257Z","shell.execute_reply":"2025-11-14T02:12:31.737872Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# Stepwise learning\n- Multi stage : SSL(instance level) and classification (Multi task learning) -> WSOL (patch level) -> weakly supervised, or unsupervised segmentation(pixel level)\n  \n- 추가로 구현해야 하는 Additional layer들은 최소한으로 사용하기\n- 모듈은 크게 4가지 작성: ViT module, 그 ViT를 인자로 삼아 training하는 SSL and Classification Trainer, WSOL trainer, Segmentation trainer 모듈\n    - 각자의 모듈에 optimizer 및 learning rate 미리 세팅\n    - ViT module : input as [batch_size, res, res, 1] shape tensor, output as [representation_vector, encoded_patches, attention_weights]\n        - representation vector : [batch_size, embed_dims] shape matrix\n        - encoded patches : [batch_sise, n_patches, embed_dims] shape tensor\n        - attention_weights = list of attnetion weights, attention weight of n-th MHSA layer : [batch_size, n_heads, n_patches, n_patches] shape stochastic tensor.\n    - SSL and classification trainer module : SSL as NNCLR, 두 loss를 결합하여 한 개의 single loss를 반환 (즉, total_loss, ssl_loss, classification_loss, classification_accuracy 반환)\n    - WSOL : label과 encoded_patches 등을 이용해서 진행. \n    - segmentation : pixel level learning, pixel level annotation은 없음. U-Net이나 그 유사 구조를 활용할 경우, learnable weight를 최소화할 것.","metadata":{}},{"cell_type":"markdown","source":"# Stage 0: preparing with helper functions","metadata":{}},{"cell_type":"code","source":"# 2D RoPE, Fourier block, self attention blocks - helper function for (fourier) ViT\n\nclass RoPE2D_KerasNLP(keras.layers.Layer):\n    \"\"\"\n    2D Rotary Position Embedding using keras_nlp\n    \n    keras_nlp.layers.RotaryEmbedding을 2D vision에 확장\n    \"\"\"\n    def __init__(self, \n                 embed_dims: int,\n                 max_wavelength: int = 10000,\n                 scaling_factor: float = 1.0,\n                 **kwargs):\n        super().__init__(**kwargs)\n        \n        assert embed_dims % 4 == 0, \"embed_dims must be divisible by 4\"\n        \n        self.embed_dims = embed_dims\n        self.dim_per_axis = embed_dims // 2\n        \n        # Height용 RoPE (첫 번째 절반 dimensions)\n        self.rope_height = keras_nlp.layers.RotaryEmbedding(\n            max_wavelength=max_wavelength,\n            scaling_factor=scaling_factor,\n            sequence_axis=1,  # Height axis\n            feature_axis=-1\n        )\n        \n        # Width용 RoPE (두 번째 절반 dimensions)\n        self.rope_width = keras_nlp.layers.RotaryEmbedding(\n            max_wavelength=max_wavelength,\n            scaling_factor=scaling_factor,\n            sequence_axis=2,  # Width axis\n            feature_axis=-1\n        )\n    \n    def call(self, x, spatial_shape):\n        \"\"\"\n        Args:\n            x: [batch, num_patches, embed_dims]\n            spatial_shape: (height, width) in patches\n        \n        Returns:\n            x_rope: [batch, num_patches, embed_dims] with RoPE applied\n        \"\"\"\n        batch_size = ops.shape(x)[0]\n        height, width = spatial_shape\n        \n        # Reshape to 2D spatial layout\n        x_2d = ops.reshape(x, [batch_size, height, width, self.embed_dims])\n        \n        # Split features: first half for height, second half for width\n        x_h = x_2d[..., :self.dim_per_axis]\n        x_w = x_2d[..., self.dim_per_axis:]\n        \n        # Apply RoPE separately\n        x_h_rope = self.rope_height(x_h)\n        x_w_rope = self.rope_width(x_w)\n        \n        # Concatenate\n        x_rope_2d = ops.concatenate([x_h_rope, x_w_rope], axis=-1)\n        \n        # Reshape back\n        x_rope = ops.reshape(x_rope_2d, [batch_size, height * width, self.embed_dims])\n        \n        return x_rope\nclass RoPEMultiHeadAttention(layers.MultiHeadAttention):\n    \"\"\"\n    Keras MHA를 상속받아 RoPE를 Q, K에 주입하는 레이어.\n    FlashAttention 등 Keras의 최적화를 그대로 활용.\n    \"\"\"\n    def __init__(self, rope_2d: RoPE2D_KerasNLP, **kwargs):\n        super().__init__(**kwargs)\n        self.rope_2d = rope_2d\n\n    def call(self,\n             query,\n             value,\n             key=None,\n             spatial_shape=None,  # RoPE를 위한 추가 인자\n             attention_mask=None,\n             return_attention_scores=False,\n             training=False):\n        \n        if spatial_shape is None:\n            raise ValueError(\"RoPEMultiHeadAttention을 호출할 때는 'spatial_shape' 인자가 반드시 필요합니다.\")\n\n        # Self-Attention을 위한 기본 로직\n        if key is None:\n            key = value\n\n        # --- RoPE 적용을 위한 핵심 수정 ---\n        \n        # 1. 부모 클래스(MHA)의 Dense 레이어로 Q, K, V를 프로젝션\n        q_proj = self._query_dense(query)\n        k_proj = self._key_dense(key) \n        v_proj = self._value_dense(value)\n\n        # 2. CLS 토큰과 Patch 토큰 분리\n        # (CLS 토큰은 RoPE를 적용하지 않음)\n        q_cls, q_patch = q_proj[:, :1, :], q_proj[:, 1:, :]\n        k_cls, k_patch = k_proj[:, :1, :], k_proj[:, 1:, :]\n        batch_, n_patch_, n_heads_, dims_ = ops.shape(q_patch)\n        \n        # 3. 프로젝션된 Q, K의 Patch 부분에 2D RoPE 적용\n        q_patch_rope = self.rope_2d(ops.reshape(q_patch, [batch_, n_patch_, n_heads_*dims_]), \n                                    spatial_shape=spatial_shape)\n        k_patch_rope = self.rope_2d(ops.reshape(k_patch, [batch_, n_patch_, n_heads_*dims_]), \n                                    spatial_shape=spatial_shape)\n        q_patch_rope = ops.reshape(q_patch_rope, [batch_, n_patch_, n_heads_, dims_])\n        k_patch_rope = ops.reshape(k_patch_rope, [batch_, n_patch_, n_heads_, dims_])\n        # 4. CLS 토큰과 RoPE 적용된 Patch 재결합\n        q_final = ops.concatenate([q_cls, q_patch_rope], axis=1)\n        k_final = ops.concatenate([k_cls, k_patch_rope], axis=1)\n        # V는 RoPE가 적용되지 않은 'v_proj'를 그대로 사용\n        \n        # --- Keras MHA의 나머지 로직 재사용 ---\n        \n        # 5. 최적화된 어텐션 연산 (부모 클래스의 핵심 메서드)\n        attn_output, attn_scores = self._compute_attention(\n            q_final,\n            k_final,\n            v_proj,\n            attention_mask=attention_mask,\n            training=training,\n        )\n        \n        # 6. Head 결합 및 출력 프로젝션 (부모 클래스의 내부 메서드)\n        attn_output = self._output_dense(attn_output)\n        # 8. 결과 반환\n        if return_attention_scores:\n            return attn_output, attn_scores\n        return attn_output\n\n# --- 3. Fourier Block (API 시그니처 수정) ---\nclass FourierTransformBlock(keras.layers.Layer):\n    \"\"\"\n    Fourier Transform Block for Token Mixing\n    \"\"\"\n    def __init__(self, \n                 embed_dims: int,\n                 mlp_ratio: int = 4,\n                 dropout_rate: float = 0.1,\n                 **kwargs):\n        super().__init__(**kwargs)\n        \n        self.embed_dims = embed_dims\n        self.norm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.norm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.mlp = keras.Sequential([\n            layers.Dense(embed_dims * mlp_ratio, activation='gelu'),\n            layers.Dropout(dropout_rate),\n            layers.Dense(embed_dims),\n        ])\n        self.spatial_mlp = keras.Sequential([\n            layers.Dense(embed_dims * mlp_ratio, activation='gelu'),\n            layers.Dropout(dropout_rate),\n            layers.Dense(embed_dims),\n        ])\n        \n    def call(self, inputs, spatial_shape=None, training=False):\n        \"\"\"\n        spatial_shape는 API 호환성을 위해 받지만 사용하지 않습니다.\n        \"\"\"\n        real_part = inputs\n        real_part_fft, im_part_fft = keras.ops.fft((real_part, keras.ops.zeros_like(real_part)))\n        \n        # Real Part\n        real_part_fft = self.norm1(real_part_fft)\n        real_part_fft_ffn = self.mlp(real_part_fft, training=training)\n        real_part_fft = real_part_fft + real_part_fft_ffn\n        real_part_fft = self.norm2(real_part_fft)\n        \n        # Imaginary part\n        im_part_fft = self.norm1(im_part_fft)\n        im_part_fft_ffn = self.mlp(im_part_fft, training=training)\n        im_part_fft = im_part_fft + im_part_fft_ffn\n        im_part_fft = self.norm2(im_part_fft)\n        \n        x = keras.ops.irfft((real_part_fft, im_part_fft))\n        return self.spatial_mlp(x, training=training)\n\n# --- 4. Self Attention Block (RoPEMultiHeadAttention 사용하도록 수정) ---\nclass SelfAttentionBlock(keras.layers.Layer):\n    \"\"\"\n    RoPEMultiHeadAttention을 사용하는 ViT Self-Attention Block\n    \"\"\"\n    def __init__(self,\n                 num_heads: int,\n                 embed_dims: int,\n                 rope_2d: RoPE2D_KerasNLP,  # RoPE 레이어 주입\n                 mlp_ratio: int = 4,\n                 dropout_rate: float = 0.1,\n                 **kwargs):\n        super().__init__(**kwargs)\n        \n        self.num_heads = num_heads\n        self.embed_dims = embed_dims\n        \n        # Layer Normalization\n        self.norm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.norm2 = layers.LayerNormalization(epsilon=1e-6)\n        \n        # (MODIFIED) Keras MHA 대신 RoPEMultiHeadAttention 사용\n        self.mhsa = RoPEMultiHeadAttention(\n            rope_2d=rope_2d,\n            num_heads=num_heads,\n            key_dim=embed_dims // num_heads,\n            dropout=dropout_rate\n        )\n        \n        # MLP\n        self.mlp = keras.Sequential([\n            layers.Dense(embed_dims * mlp_ratio, activation='gelu'),\n            layers.Dropout(dropout_rate),\n            layers.Dense(embed_dims),\n            layers.Dropout(dropout_rate)\n        ])\n    \n    def call(self, x, spatial_shape, training=False):\n        \"\"\"\n        Args:\n            x: [batch, num_patches+1, embed_dims] (with CLS token)\n            spatial_shape: (height, width) RoPE 적용에 필요\n        \"\"\"\n        # Multi-Head Self-Attention\n        x_norm = self.norm1(x)\n        \n        # (MODIFIED) spatial_shape를 RoPEMultiHeadAttention에 전달\n        attn_output, attn_weights = self.mhsa(\n            query=x_norm,\n            value=x_norm,\n            key=x_norm, # Self-attention\n            spatial_shape=spatial_shape,\n            return_attention_scores=True,\n            training=training\n        )\n        x = x + attn_output\n        \n        # MLP\n        x_norm = self.norm2(x)\n        mlp_output = self.mlp(x_norm, training=training)\n        x = x + mlp_output\n        \n        return x, attn_weights\n\n# --- 5. Interleaved Block (API 시그니처 수정 및 CLS 토큰 처리 수정) ---\nclass InterleavedAttentionBlock(keras.layers.Layer):\n    def __init__(self, \n                 embed_dims: int,\n                 num_heads: int = 8, \n                 mlp_ratio: int = 4,\n                 dropout_rate: float = 0.1,\n                 fourier_first = True,\n                 rope_2d = None, # RoPE 레이어 주입\n                 **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dims = embed_dims\n        self.num_heads = num_heads\n        self.mlp_ratio = mlp_ratio\n        self.dropout_rate = dropout_rate\n        self.fourier_first = fourier_first\n        \n        self.mhsa_block = SelfAttentionBlock(num_heads, embed_dims, rope_2d, mlp_ratio, dropout_rate)\n        self.fourier_block = FourierTransformBlock(embed_dims, mlp_ratio, dropout_rate)\n        \n    def call(self, x, spatial_shape, training=False):\n        \"\"\"\n        Fourier Block가 CLS 토큰을 처리하지 않도록 수정\n        \"\"\"\n        cls_token, patches = x[:, :1, :], x[:, 1:, :] # CLS와 Patch 분리\n        \n        if self.fourier_first:\n            patches = self.fourier_block(patches, spatial_shape = spatial_shape, training=training)\n            x = ops.concatenate([cls_token, patches], axis=1) # 재결합\n            x, attn_weights = self.mhsa_block(x, spatial_shape = spatial_shape, training=training)\n        else:\n            x, attn_weights = self.mhsa_block(x, spatial_shape = spatial_shape, training=training)\n            \n            cls_token, patches = x[:, :1, :], x[:, 1:, :] # MHSA 후 다시 분리\n            patches = self.fourier_block(patches, spatial_shape = spatial_shape, training=training)\n            x = ops.concatenate([cls_token, patches], axis=1) # 재결합\n            \n        return x, attn_weights","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-11-14T02:12:31.740592Z","iopub.execute_input":"2025-11-14T02:12:31.741456Z","iopub.status.idle":"2025-11-14T02:12:31.792964Z","shell.execute_reply.started":"2025-11-14T02:12:31.741423Z","shell.execute_reply":"2025-11-14T02:12:31.791751Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# visualize_token_attention_map ; helper for attention weight viz\ndef visualize_token_attention_map(vit, image):\n    res = ops.shape(image)[-2]\n    cls_token, encoded_patches, att_wt_list = vit(image)\n    last_attention_weight = att_wt_list[-1]\n    batch_size, n_heads, L, _ = tf.shape(last_attention_weight) ; n_patches = L-1\n    patch_size = ops.cast(ops.sqrt(ops.cast(n_patches, \"float32\")),\n                         \"int32\")\n    head_colors = ['Blues', 'Oranges', 'Greens', 'Reds', 'Purples', 'Greys', 'YlOrBr', 'YlGnBu']\n    cls_att = last_attention_weight[:,:, 0, 1:] #batch, n_heads, n_patches\n    for b in range(batch_size):\n        fig, axes = plt.subplots(1, 3, figsize = (20,6))\n        \n        axes = axes.flatten()\n        axes[0].set_title(f\"original image {b+1}\")\n        axes[0].imshow(image[b], cmap = \"bone\")\n        axes[1].set_title(\"All head W, quantized\")\n        axes[2].set_title(f\"overlay image {b+1}\")\n        axes[2].imshow(image[b], cmap = \"bone\", alpha = 0.6)\n        #head merge\n        for h in range(n_heads):\n            att_map = cls_att[b, h]\n            att_map = ops.reshape(att_map, [patch_size, patch_size])\n            att_map = (att_map-ops.min(att_map)) / (ops.max(att_map) - ops.min(att_map) + 1e-5)\n            att_map_bin = ops.cast(att_map > ops.mean(att_map),\n                              \"int32\")\n            att_map *= 255\n            att_map_bin *= 255\n            \n            att_map = ops.cast(att_map, \"uint8\")\n            upsampled_map = tf.image.resize(att_map[None, ..., None], [res,res])[0,:,:,0]\n            upsampled_map = ops.cast(upsampled_map, \"uint8\")\n            axes[1].imshow(att_map_bin, cmap=head_colors[h % len(head_colors)], alpha=0.6)\n            axes[2].imshow(upsampled_map, cmap=head_colors[h % len(head_colors)], alpha=0.5)\n    plt.tight_layout()\n    plt.show()\n    flatten_w = ops.reshape(cls_att, [-1,])\n    plt.hist(flatten_w, density = True, histtype = \"stepfilled\", bins = 100)\n    plt.title(\"CLS Attention Weight (last layer) Distribution\")\n    plt.show()","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-11-14T02:12:31.796241Z","iopub.execute_input":"2025-11-14T02:12:31.796613Z","iopub.status.idle":"2025-11-14T02:12:31.828719Z","shell.execute_reply.started":"2025-11-14T02:12:31.796587Z","shell.execute_reply":"2025-11-14T02:12:31.827549Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# --- 6. MedicalViT (RoPE 적용 로직 수정) ---\nclass MedicalViT(keras.Model):\n    \"\"\"\n    Fourier Vision Transformer with Flexible Resolution\n    RoPE가 어텐션 블록 내부의 Q, K에만 적용되도록 수정됨.\n    \"\"\"\n    def __init__(self,\n                 patch_size,\n                 conv_base = None,\n                 interleaved = False, num_interleaved_layers = 6, fourier_first = True,\n                 num_fourier_layers: int = 8,  # N개의 Fourier blocks\n                 num_attention_layers: int = 4,\n                 num_heads: int = 6,\n                 embed_dims: int = 384,\n                 mlp_ratio: int = 2,\n                 dropout_rate: float = 0.1,\n                 **kwargs):\n        super().__init__(**kwargs)\n        \n        self.patch_size = patch_size\n        self.conv_base = conv_base\n        self.num_fourier_layers = num_fourier_layers\n        self.num_attention_layers = num_attention_layers\n        self.embed_dims = embed_dims\n        self.interleaved = interleaved\n        self.num_interleaved_layers = num_interleaved_layers\n        self.fourier_first = fourier_first\n        \n        # Patch Embedding\n        self.patch_embed = layers.Conv2D(embed_dims, \n                                         kernel_size=patch_size, \n                                         strides=patch_size, \n                                         padding='valid', activation = \"gelu\"\n                                        )\n        if self.conv_base:\n            self.conv_base.trainable = True\n            self.patch_embed = layers.Dense(units = embed_dims, activation = 'gelu', name = \"EmbedAfterConv\")\n        # CLS Token\n        self.cls_token = self.add_weight(\n            shape=(1, 1, embed_dims),\n            initializer=keras.initializers.TruncatedNormal(stddev=0.02),\n            trainable=True,\n            name='cls_token'\n        )\n        \n        # 2D RoPE (인스턴스 생성)\n        self.rope_2d = RoPE2D_KerasNLP(\n            embed_dims=embed_dims,\n            max_wavelength=10000,\n            scaling_factor=1.0\n        )\n        \n        # Fourier Transform Blocks (N layers)\n        self.fourier_blocks = []\n        for i in range(self.num_fourier_layers):\n            self.fourier_blocks.append(\n                FourierTransformBlock(\n                    embed_dims=embed_dims,\n                    mlp_ratio=mlp_ratio,\n                    dropout_rate=dropout_rate,\n                    name=f'fourier_block_{i}'\n                )\n            )\n        \n        # Self-Attention Block (RoPE 주입)\n        self.mhsa_blocks = []\n        for i in range(self.num_attention_layers):\n            self.mhsa_blocks.append(SelfAttentionBlock(\n                num_heads=num_heads,\n                embed_dims=embed_dims,\n                rope_2d=self.rope_2d,  # RoPE 레이어 전달\n                mlp_ratio=mlp_ratio,\n                dropout_rate=dropout_rate,\n                name=f'attention_block_{i}'\n            ))\n            \n        if self.interleaved : \n            self.interleaved_blocks = []\n            for i in range(self.num_interleaved_layers):\n                self.interleaved_blocks.append(InterleavedAttentionBlock(\n                    embed_dims = embed_dims,\n                    num_heads = num_heads,\n                    mlp_ratio = mlp_ratio,\n                    dropout_rate = dropout_rate,\n                    fourier_first = self.fourier_first,\n                    rope_2d=self.rope_2d  # RoPE 레이어 전달\n                  )\n                )\n        \n        # Final Normalization\n        self.norm = layers.LayerNormalization(epsilon=1e-6, name='final_norm')\n\n    def call(self, inputs, training=False):\n        \"\"\"\n        Forward pass - RoPE가 `x`에서 제거되고 블록 내부로 이동\n        \"\"\"\n        batch_size = ops.shape(inputs)[0]\n        input_height = ops.shape(inputs)[1]\n        input_width = ops.shape(inputs)[2]\n        \n        # Compute spatial dimensions\n        patch_height = input_height // self.patch_size\n        patch_width = input_width // self.patch_size\n        num_patches = patch_height * patch_width\n        spatial_shape = (patch_height, patch_width)\n        \n        # 1. Patch Embedding (순수 시맨틱)\n        if self.conv_base:\n            x = self.conv_base(inputs/255)\n            x = self.patch_embed(x)\n            batch_size, patch_height, patch_width, d_ = ops.shape(x)\n            num_patches = patch_height * patch_width\n            spatial_shape = (patch_height, patch_width)\n        else:\n            x = self.patch_embed(inputs/255)  # [batch, h//p, w//p, embed_dims]\n        x = ops.reshape(x, [batch_size, num_patches, self.embed_dims])\n        \n        # --- XX RoPE 적용 제거 (가장 큰 수정) XX ---\n        # x = self.rope_2d(x, spatial_shape=spatial_shape)\n        \n        cls_tokens = ops.repeat(self.cls_token, batch_size, axis=0)\n        attention_weights = []\n        \n        if self.interleaved:\n            x = ops.concatenate([cls_tokens, x], axis=1)\n            for interleaved_attention in self.interleaved_blocks:\n                # spatial_shape와 training 전달\n                x, att_wt = interleaved_attention(x, spatial_shape=spatial_shape, training=training)\n                attention_weights.append(att_wt)\n        else:\n            # 3. Fourier Transform Blocks (CLS 토큰 없이)\n            # spatial_shape와 training 전달\n            for fourier_block in self.fourier_blocks:\n                # (Fourier는 CLS 토큰을 처리하지 않음)\n                x = fourier_block(x, spatial_shape=spatial_shape, training=training)\n            \n            # 4. Add CLS token\n            x = ops.concatenate([cls_tokens, x], axis=1)\n            \n            # 5. Self-Attention Block\n            # spatial_shape와 training 전달\n            for mhsa_block in self.mhsa_blocks:\n                x, att_wt = mhsa_block(x, spatial_shape=spatial_shape, training=training)\n                attention_weights.append(att_wt)\n        \n        # 6. Final Normalization\n        x = self.norm(x)\n        \n        # 7. Extract outputs\n        feature_vector = x[:, 0, :]  # CLS token [batch, embed_dims]\n        encoded_patches = x[:, 1:, :]  # Patches [batch, num_patches, embed_dims]\n        \n        return feature_vector, encoded_patches, attention_weights","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T02:12:31.829570Z","iopub.execute_input":"2025-11-14T02:12:31.829960Z","iopub.status.idle":"2025-11-14T02:12:31.856457Z","shell.execute_reply.started":"2025-11-14T02:12:31.829935Z","shell.execute_reply":"2025-11-14T02:12:31.855118Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"> Unsupervised image segmentation?\n\n- pretrained ViT를 vit, input medical images를 img라 하자. vit(img)는 [cls_token, encoded_patches, ]야. 목적은 encoded_patches에서 unsupervised semantic segmentation을 수행하는 것임. \n\n- encoded_patches shape을 [batch_size, n_patches, embed_dims]라 하자. encoded_patch에 cluster function C를 가해 agglomerative clustering, k-means clustering 혹은 learnable parameter를 사용하여 각 patch가 1, 2, ..., K 개의 category 중 한 개에 속하도록 만들 거야.\n즉 cluster_patches = C(encoded_patches) 는 [batch_size, n_patches] shape tensor고 각 원소는 1, 2, ..., K 중 한 개의 원소야. clustering 시 cosine similarity를 distance로 활용함.\ncluster_patches의 예시는 [[1,2,10,9,4,2,4,1,...], [2,3,7,9,10,1,12,...], ...]야. 이 discrete한 tensor를  확률분포로 변환할 건데 (변환 함수를 f_d라 하자)\ncluster_proba = f_d(cluster_patches)는 category 1, 2, 3, ..., K 각각의 확률질량값을 나타내. 즉  [P(category=1), P(category=2), ...]야. 이 다음, 가장 P 값이 높은 것부터 sort하여 결과값을 반환해 : [0.7, 0.08, 0.05, 0.01, ....]\ncluster_proba shape은 [batch_size, n_patches]야. keras.ops.sum(cluster_integer, axis = 1)은 [batch_size,] shape인 1로 이루어진 벡터야.\n\n- unsupervised segmentation loss는 cluster integer matrix에서 작동하는데, 이미지의 패치들이 1)한 카테고리에만 배정(collapse)되어서도 안 되고, 2)K개의 카테고리에 아무렇게나(즉 너무 uniform하게) 배치되어도 안 돼 (trivial solution).\n1을 위해서 KL_divergence(uniform 분포, cluster_proba)를 최소화하고, 2를 위해서 KL_divergence(target_dist, cluster_proba)를 최소화한다.\n이 때 target_dist는 a) 감마 분포, b) 표준정규분포(단, x>0로 제한) 등을 생각할 수 있다 (배경이 가장 많고, 그 다음은 category A, 그 다음은 Category B, ...)\ntarget_dist를 특정 분포에서 Sampling하여 KL divergence를 구할 수 있어야 한다","metadata":{}},{"cell_type":"code","source":"def create_target_distribution(k, method='gamma'): # 기본값을 gamma로 변경\n    \"\"\"\n    정렬된 목표 분포(target_dist)를 생성합니다.\n    \"\"\"\n    if method == 'gamma':\n        # Gamma(concentration < 1)는 0 근처에서 PDF가 가장 높음\n        lin = ops.linspace(0.1, 10.0, k) # 0을 피하기 위해 0.1에서 시작\n        dist = tfp.distributions.Gamma(concentration=0.75, rate=0.05) # concentration < 1\n        target_unnormalized = dist.prob(lin)\n        \n    elif method == \"trunc_normal\": # trunc_normal\n        # 0에서 감소하는 분포를 위해 loc=low\n        lin = ops.linspace(0.0, 3.0, k)\n        dist = tfp.distributions.TruncatedNormal(loc=0.0, scale=1.0, low=0.0, high=3.0) # high를 3.0으로 명시\n        target_unnormalized = dist.prob(lin)\n    elif method in ['zip', 'power', 'zipfian']:\n        print(method)\n        indices = ops.arange(k, dtype=\"float32\") + 1.0\n        target_unnormalized = 1.0 / ops.power(indices, 0.75)\n        return target_unnormalized / ops.sum(target_unnormalized)\n    # 요청대로 가장 P값이 높은 것부터 정렬\n    target_sorted = ops.sort(target_unnormalized)[::-1]\n    \n    # 확률분포로 정규화\n    target_dist = target_sorted / ops.sum(target_sorted)\n    return target_dist\ndist = create_target_distribution(k = 64, method = 'power')\nplt.hist(dist, bins = 64)\nplt.title(\"Target distribution(power) visualize\")\nplt.show()\n\ndist = create_target_distribution(k = 64, method = 'gamma')\nplt.hist(dist, bins = 64)\nplt.title(\"Target distribution(gamma) visualize\")\nplt.show()\n\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-11-14T02:12:31.857724Z","iopub.execute_input":"2025-11-14T02:12:31.858053Z","iopub.status.idle":"2025-11-14T02:12:32.505562Z","shell.execute_reply.started":"2025-11-14T02:12:31.858025Z","shell.execute_reply":"2025-11-14T02:12:32.504523Z"}},"outputs":[{"name":"stdout","text":"power\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"# How to make Encoded patches to K-categorical probability?\n\n- K-means 혹은 agglomerative clustering 진행; 이 clustering 연산을 f_d라고 하고, 코드에서는 ClusteringLayer로 구현함\n- cluster 결과 각 batch내 patch들이 K개 카테고리에 속할 확률을 알 수 있음 -> (Batch_size, n_patches, K) shape\n- 위 과정을 미분가능하게 만들어주는 것이 STEProbabilityLayer;\n> STE layer?\n \n- STEProbabilityLayer는 **순전파(Forward Pass)**와 역전파(Backward Pass) 때 서로 다른 계산을 하도록 \"속임수(trick)\"를 씁니다.\n- 이 레이어는 patch_proba_soft ([B, N, K])를 입력으로 받습니다.\n    - A. 순전파 (Forward Pass): \"이산적인 결정\":\n    - 순전파 시에는 우리가 원하는 대로 이산적인 argmax 결과를 사용합니다. cluster_patches_indices = ops.argmax(patch_proba_soft, axis=-1) 을 통해 \"Soft\" 확률([0.9, 0.1])을 \"Hard\" 결정(0)으로 바꿉니다;\n        - 출력 1: cluster_patches ([B, N]): 이산적인 인덱스 맵 (시각화 등에 사용 가능).\n          - one_hot_hard = tf.one_hot(cluster_patches_indices, ...) : 0을 [1, 0, 0, ...] 과 같은 one-hot 벡터로 만듭니다. (shape [B, N, K])\n\n    - B. 마법의 STE 공식 (순전파 + 역전파)\n\n> one_hot_ste = tf.stop_gradient(one_hot_hard - patch_proba_soft) + patch_proba_soft\n\n    순전파: tf.stop_gradient는 순전파 시 아무것도 안 합니다.\n\n    - (one_hot_hard - patch_proba_soft) + patch_proba_soft\n    - patch_proba_soft가 소거되어 결국 one_hot_hard만 남습니다.\n\n> 결론: 순전파 시에는 one_hot_hard ([B, N, K])가 사용됩니다.\n\n    - 역전파: tf.stop_gradient는 괄호 안의 모든 연산에 대한 그래디언트(기울기)를 강제로 0으로 만듭니다.\n    - 그래디언트 관점에서 tf.stop_gradient(...) 부분은 \"상수(constant)\" 취급됩니다. (기울기 0)\n    - 오직 + patch_proba_soft 부분만 그래디언트가 살아남아 뒤로 전달됩니다.\n    - 결론: 역전파 시에는 patch_proba_soft의 그래디언트가 사용됩니다.","metadata":{}},{"cell_type":"code","source":"class STEProbabilityLayer(keras.layers.Layer):\n    \"\"\"Soft-assignment([B, N, K]) -> cluster_proba([B, K]), cluster_patches([B, N])\"\"\"\n    def __init__(self, k, **kwargs):\n        super().__init__(**kwargs)\n        self.k = k\n\n    def call(self, patch_proba_soft):\n        # patch_proba_soft shape: [B, N, K]\n        \n        # 순전파용 Hard assignment\n        cluster_patches_indices = ops.argmax(patch_proba_soft, axis=-1) # [B, N]\n        one_hot_hard = tf.one_hot(cluster_patches_indices, depth=self.k, dtype=tf.float32)\n\n        # STE: 순전파(hard)와 역전파(soft) 분리\n        one_hot_ste = tf.stop_gradient(one_hot_hard - patch_proba_soft) + patch_proba_soft\n\n        # f_d: cluster_proba 계산\n        cluster_counts_ste = ops.sum(one_hot_ste, axis=1) # [B, K]\n        n_patches = ops.shape(patch_proba_soft)[1]\n        cluster_proba = cluster_counts_ste / ops.cast(n_patches, dtype=tf.float32)\n\n        return cluster_proba, cluster_patches_indices, patch_proba_soft","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-11-14T02:12:32.506674Z","iopub.execute_input":"2025-11-14T02:12:32.506954Z","iopub.status.idle":"2025-11-14T02:12:32.514371Z","shell.execute_reply.started":"2025-11-14T02:12:32.506933Z","shell.execute_reply":"2025-11-14T02:12:32.513036Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"class ClusteringLayer(keras.layers.Layer):\n    def __init__(self, k, n_iterations=3, temperature=0.07, method='soft_kmeans',\n                 spatial_scale: float = 0.1, # 좌표의 중요도 조절\n                 distance_metric: str = 'l2', # (NEW) 거리 척도 선택 ('cosine' or 'l2')\n                 **kwargs):\n        super().__init__(**kwargs)\n        if method not in ['soft_kmeans', 'soft_agglomerative', 'kmeans', 'agglomerative']:\n            raise ValueError(\"지원되지 않는 method입니다.\")\n        if distance_metric not in ['cosine', 'cos_sim', 'cos',\n                                   'l2', 'L2', 'euclidian']: # (NEW) 거리 척도 검증\n             raise ValueError(\"distance_metric은 'cosine', 'cos_sim', 'cos', 'l2', 'L2', 또는 'euclidian'여야 합니다.\")\n            \n        self.k = k\n        self.n_iterations = n_iterations\n        self.temperature = temperature\n        self.method = method\n        self.spatial_scale = spatial_scale\n        self.distance_metric = distance_metric # (NEW)\n        self.ste_layer = STEProbabilityLayer(self.k)\n        \n    def build(self, input_shape):\n        super().build(input_shape)\n\n    # --- 좌표 관련 헬퍼 (변경 없음) ---\n    def _create_coordinate_grid(self, batch_size, spatial_shape):\n        height, width = spatial_shape\n        y_coords = ops.linspace(-1.0, 1.0, height)\n        x_coords = ops.linspace(-1.0, 1.0, width)\n        y_grid, x_grid = tf.meshgrid(y_coords, x_coords, indexing='ij')\n        coords_2d = ops.stack([y_grid, x_grid], axis=-1)\n        coords_1d = ops.reshape(coords_2d, (height * width, 2))\n        coords_1d_expanded = ops.expand_dims(coords_1d, axis=0)\n        return ops.repeat(coords_1d_expanded, batch_size, axis=0)\n\n    def _prepare_spatial_features(self, encoded_patches, spatial_shape):\n        batch_size = ops.shape(encoded_patches)[0]\n        # 시맨틱 특징 정규화 (코사인 유사도 사용 시 중요)\n        # L2 사용 시에는 정규화하지 않는 것이 벡터 크기 정보를 보존하는 데 유리할 수 있음\n        # 여기서는 우선 정규화 유지 (사용자가 필요시 수정 가능)\n        if self.distance_metric in ['cosine', 'cos_sim', 'cos']:\n            patches_norm = ops.normalize(encoded_patches, axis=-1)\n        else:\n            patches_norm = encoded_patches\n        coords = self._create_coordinate_grid(batch_size, spatial_shape = spatial_shape)\n        coords_scaled = coords * self.spatial_scale\n        features_with_coords = ops.concatenate([patches_norm, coords_scaled], axis=-1)\n        return features_with_coords\n\n    # --- 거리/유사도 계산 헬퍼 ---\n    def _cosine_similarity(self, x, y, normalize=True):\n        \"\"\" 코사인 유사도 계산 (ops.normalize 내장) \"\"\"\n        # x: [B, N, D+2], y: [K, D+2] or [B, N, D+2] or [B, K, D+2]\n        if normalize:\n            x_norm = ops.normalize(x, axis=-1)\n            y_norm = ops.normalize(y, axis=-1)\n        else: # 이미 정규화된 경우\n             x_norm = x\n             y_norm = y\n\n        # y의 형태에 따라 einsum 경로 분기\n        if len(ops.shape(y)) == 2: # [K, D+2] (K-means 센터)\n            sim = ops.einsum('bnd,kd->bnk', x_norm, y_norm)\n        elif len(ops.shape(y)) == 3 and ops.shape(x)[1] == ops.shape(y)[1]: # [B, N, D+2] (Agglo 패치-패치)\n             y_transposed = ops.transpose(y_norm, (0, 2, 1))\n             sim = ops.einsum('bnd,bdn->bnn', x_norm, y_transposed) # [B, N, N]\n        elif len(ops.shape(y)) == 3 and ops.shape(y)[1] == self.k: # [B, K, D+2] (Agglo 초기 센터)\n             y_transposed = ops.transpose(y_norm, (0, 2, 1))\n             sim = ops.einsum('bnd,bdk->bnk', x_norm, y_transposed) # [B, N, K]\n        else:\n             raise ValueError(f\"지원되지 않는 y shape: {ops.shape(y)}\")\n        return sim\n\n    def _euclidean_distance_squared(self, x, y):\n        \"\"\" 제곱 유클리드 거리 계산 \"\"\"\n        # x: [B, N, D+2], y: [K, D+2] or [B, N, D+2] or [B, K, D+2]\n        x_sq = ops.sum(ops.square(x), axis=-1, keepdims=True) # [B, N, 1]\n\n        # y의 형태에 따라 계산 분기\n        if len(ops.shape(y)) == 2: # [K, D+2] (K-means 센터)\n            y_sq = ops.sum(ops.square(y), axis=-1, keepdims=True) # [K, 1]\n            y_sq_transposed = ops.transpose(y_sq, (1, 0)) # [1, K]\n            xy = ops.einsum('bnd,kd->bnk', x, y)\n            dist_sq = x_sq - 2 * xy + y_sq_transposed # [B, N, K]\n        elif len(ops.shape(y)) == 3 and ops.shape(x)[1] == ops.shape(y)[1]: # [B, N, D+2] (Agglo 패치-패치)\n             y_sq = ops.sum(ops.square(y), axis=-1, keepdims=True) # [B, N, 1]\n             y_sq_transposed = ops.transpose(y_sq, (0, 2, 1)) # [B, 1, N]\n             xy = ops.einsum('bnd,bmd->bnm', x, y) # [B, N, N]\n             dist_sq = x_sq - 2 * xy + y_sq_transposed # [B, N, N]\n        elif len(ops.shape(y)) == 3 and ops.shape(y)[1] == self.k: # [B, K, D+2] (Agglo 초기 센터)\n             y_sq = ops.sum(ops.square(y), axis=-1, keepdims=True) # [B, K, 1]\n             y_sq_transposed = ops.transpose(y_sq, (0, 2, 1)) # [B, 1, K]\n             xy = ops.einsum('bnd,bkd->bnk', x, y) # [B, N, K]\n             dist_sq = x_sq - 2 * xy + y_sq_transposed # [B, N, K]\n        else:\n             raise ValueError(f\"지원되지 않는 y shape: {ops.shape(y)}\")\n\n        return ops.maximum(dist_sq, 0.0) # 수치적 안정성\n\n    # --- 클러스터링 로직 ---\n    def _soft_kmeans(self, spatial_features):\n        b, n, d_plus_2 = ops.shape(spatial_features)\n        patches_flat = ops.reshape(spatial_features, (b * n, d_plus_2))\n        indices = tf.random.shuffle(tf.range(b * n))[:self.k]\n        centers = tf.gather(patches_flat, indices) # [K, D+2]\n\n        patch_proba_soft = None\n        for _ in range(self.n_iterations):\n            # [E-Step] 유사도 또는 거리 계산\n            if self.distance_metric in ['cosine', 'cos', 'cos_sim']:\n                # 코사인 유사도는 클수록 좋음\n                sim = self._cosine_similarity(spatial_features, centers) # [B, N, K]\n                patch_proba_soft = ops.softmax(sim / self.temperature, axis=-1)\n            else: # 'l2'\n                # L2 거리는 작을수록 좋음 (음수 취함)\n                dist_sq = self._euclidean_distance_squared(spatial_features, centers) # [B, N, K]\n                patch_proba_soft = ops.softmax(-dist_sq / self.temperature, axis=-1)\n\n            # [M-Step] 센터 업데이트 (동일)\n            R_sum = ops.transpose(ops.sum(patch_proba_soft, axis=1, keepdims=True), (0, 2, 1))\n            R_T = ops.transpose(patch_proba_soft, (0, 2, 1))\n            weighted_sum = ops.einsum('bkn,bnd->bkd', R_T, spatial_features)\n            centers_batch = weighted_sum / (R_sum + 1e-6)\n            centers = ops.mean(centers_batch, axis=0) # [K, D+2]\n        return patch_proba_soft\n\n    def _soft_agglomerative(self, spatial_features):\n        b, n, d_plus_2 = ops.shape(spatial_features)\n        \n        # 1. 유사도/거리 그래프(W) 생성\n        if self.distance_metric  in ['cosine', 'cos', 'cos_sim']:\n            W_sim = self._cosine_similarity(spatial_features, spatial_features) # [B, N, N]\n            W = ops.softmax(W_sim / self.temperature, axis=-1)\n        else: # 'l2'\n            W_dist_sq = self._euclidean_distance_squared(spatial_features, spatial_features) # [B, N, N]\n            W = ops.softmax(-W_dist_sq / self.temperature, axis=-1)\n\n        # 2. 초기 확률(P_t) 생성\n        indices = tf.random.shuffle(tf.range(n))[:self.k]\n        centers_0 = tf.gather(spatial_features, indices, axis=1) # [B, K, D+2]\n        \n        if self.distance_metric in ['cosine', 'cos', 'cos_sim']:\n            sim_0 = self._cosine_similarity(spatial_features, centers_0) # [B, N, K]\n            P_t = ops.softmax(sim_0 / self.temperature, axis=-1)\n        else: # 'l2'\n            dist_0_sq = self._euclidean_distance_squared(spatial_features, centers_0) # [B, N, K]\n            P_t = ops.softmax(-dist_0_sq / self.temperature, axis=-1)\n\n        # 3. 확산 반복 (동일)\n        for _ in range(self.n_iterations):\n            P_logits = ops.einsum('bnj,bjk->bnk', W, P_t)\n            P_t = ops.softmax(P_logits / (self.temperature * 0.5), axis=-1)\n        return P_t\n\n    # --- call 메서드 (변경 없음) ---\n    def call(self, encoded_patches, spatial_shape):\n        spatial_features = self._prepare_spatial_features(encoded_patches, spatial_shape)\n        \n        if self.method == 'soft_kmeans' or self.method == 'kmeans':\n            patch_proba_soft = self._soft_kmeans(spatial_features)\n        elif self.method == 'soft_agglomerative' or self.method == 'agglomerative':\n            patch_proba_soft = self._soft_agglomerative(spatial_features)\n        else:\n            raise ValueError(f\"'{self.method}'는 지원되지 않는 method입니다.\")\n            \n        cluster_proba, cluster_patches, patch_proba_soft = self.ste_layer(patch_proba_soft)\n        \n        return cluster_proba, cluster_patches, patch_proba_soft","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-11-14T02:12:32.515608Z","iopub.execute_input":"2025-11-14T02:12:32.516244Z","iopub.status.idle":"2025-11-14T02:12:32.555065Z","shell.execute_reply.started":"2025-11-14T02:12:32.516217Z","shell.execute_reply":"2025-11-14T02:12:32.553647Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"> Learnable clustering layer\n\n- Cluster Prototype [K, embed_dims] shape tensor, learnable, EMA update\n    - K개 클러스터의 embed space에서의 대표 벡터를 지정\n    - encoded patches와 distance 측정하여 clustering\n    - 1 step 후 EMA로 update\n- cluster layer에 loss function 넣고,\n    - clustering loss, 각 entropy 반환하여 모니터링","metadata":{}},{"cell_type":"code","source":"class LearnableClusteringLayer(keras.layers.Layer):\n    def __init__(self, k, embed_dims = embed_dims,\n                 temperature=0.07,\n                 distance_metric: str = 'l2', # (NEW) 거리 척도 선택 ('cosine' or 'l2' or 'dot')\n                 **kwargs):\n        super().__init__(**kwargs)\n        if distance_metric not in ['cosine', 'cos_sim', 'cos', #<- Cosine similarity\n                                   'l2', 'L2', 'euclidean',#<- Euclidean distance\n                                  \"dot\"]: \n             raise ValueError(\"distance_metric은 'cosine', 'cos_sim', 'cos', 'l2', 'L2', 'euclidean' 또는 'dot' 여야 합니다.\")\n            \n        self.k = k\n        self.embed_dims = embed_dims\n        self.temperature = temperature\n        self.distance_metric = distance_metric # L2 or cosine\n        self.cluster_center = self.add_weight(\n                                    shape=(k, self.embed_dims),\n                                    initializer=keras.initializers.Orthogonal(),\n                                    trainable=True,\n                                    name='cluster_center'\n                                )\n    def build(self, input_shape):\n        super().build(input_shape)\n\n    # --- 거리/유사도 계산 헬퍼 ---\n    def _dot_similarity(self, patches):\n        dim = ops.shape(self.cluster_center)[-1]\n        sim = ops.einsum(\"bnd, kd -> bnk\", patches, self.cluster_center)\n        sim /= ops.sqrt(dim) #scaled dot-product\n        return sim\n    def _cosine_similarity(self, patches, normalize=True):\n        \"\"\" 코사인 유사도 계산 (ops.normalize 내장) \"\"\"\n        # x: [B, N, D] shape tensor y: [K,D] shape tensor \n        x = patches\n        y = self.cluster_center\n        if normalize:\n            x_norm = ops.normalize(x, axis=-1)\n            y_norm = ops.normalize(y, axis=-1)\n        else: # 이미 정규화된 경우\n             x_norm = x\n             y_norm = y\n\n        sim = ops.einsum(\"bnd, kd -> bnk\", x_norm, y_norm)\n        return sim\n\n    def _euclidean_distance_squared(self, patches):\n        \"\"\" 제곱 유클리드 거리 계산 \"\"\"\n        # x: [B, N, D] shape feature tensor y: [K,D] shape cluster center tensor \n        # after broadcasting : x [B, N, 1, D] // y: [1, 1, K, D]\n        x = patches\n        y = self.cluster_center\n        batch_size, n, _ = ops.shape(x)\n        k, _ = ops.shape(y)\n        \n        x = ops.expand_dims(x, axis = 2)\n        y = ops.expand_dims(y, axis = [0,1])\n        distance = ops.sum(ops.square(x-y),\n                           axis = -1) #[B, N, K]\n        assert (batch_size, n, k) == ops.shape(distance), \"유클리드 거리 계산 내 에러\"\n        return distance\n\n    # call 시 mi loss, segment_per_patches, patch proba soft 리턴\n    # segment, soft proba는 similarity나 distance의 softmax.\n    def compute_mi_loss(self, soft_proba):\n        \n        soft_proba = ops.clip(soft_proba, 1e-6, 1.0)\n        \n        batch_size, n_patches, n_clusters = ops.shape(soft_proba)\n        p_c = ops.mean(soft_proba, axis = [0,1]) #K length vector\n        p_c = p_c/ops.sum(p_c)\n        h_c = -ops.sum(p_c*ops.log(p_c))\n\n        entropy_per_patch = -ops.sum(soft_proba * ops.log(soft_proba),\n                                     axis = -1) #batch_size, n_patches shape matrix\n        h_c_given_x = ops.mean(entropy_per_patch)\n        mi = h_c - h_c_given_x\n        return -mi, h_c, h_c_given_x\n        \n    def call(self, encoded_patches):\n        if self.distance_metric in [\"L2\", 'l2', 'euclidian']:\n            sim_matrix = self._euclidean_distance_squared(encoded_patches)\n        elif self.distance_metric == \"dot\":\n            sim_matrix = self._dot_similarity(encoded_patches) \n        else:\n            sim_matrix = self._cosine_similarity(encoded_patches)\n        sim_matrix /= self.temperature\n        #sim_matrix : [batch_size, N_patches, K]\n        patch_proba_soft = ops.softmax(sim_matrix, axis = -1) #[batch_size, N_patches, K] shape stochastic tensor\n        cluster_patches = ops.argmax(patch_proba_soft, axis = -1) #[batch_size, N_patches] shape integer matrix\n        mi_loss, h_c, h_c_given_x = self.compute_mi_loss(patch_proba_soft)\n        return mi_loss, h_c, h_c_given_x, cluster_patches, patch_proba_soft #cluster_patches :[batch_Size, N_patches]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T02:12:32.556883Z","iopub.execute_input":"2025-11-14T02:12:32.557238Z","iopub.status.idle":"2025-11-14T02:12:32.588649Z","shell.execute_reply.started":"2025-11-14T02:12:32.557205Z","shell.execute_reply":"2025-11-14T02:12:32.587561Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def compute_unsupervised_loss(cluster_proba, target_dist, uniform_dist):\n    \"\"\" Anti-collapse 및 Anti-trivial Loss 계산 \"\"\"\n    kl_loss_fn = keras.losses.KLDivergence(reduction='none')\n    cluster_proba_safe = ops.clip(cluster_proba, 1e-6, 1.0)\n    \n    # Loss 1 (Anti-collapse): KL(uniform || cluster_proba)\n    loss_1 = kl_loss_fn(ops.expand_dims(uniform_dist, 0), cluster_proba_safe)\n    \n    # Loss 2 (Anti-trivial): KL(target_dist || sorted(cluster_proba))\n    cluster_proba_sorted = tf.sort(cluster_proba_safe, axis=-1, direction='DESCENDING')\n    loss_2 = kl_loss_fn(ops.expand_dims(target_dist, 0), cluster_proba_sorted)\n    \n    return ops.mean(loss_1), ops.mean(loss_2)","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-11-14T02:12:32.589691Z","iopub.execute_input":"2025-11-14T02:12:32.589980Z","iopub.status.idle":"2025-11-14T02:12:32.620835Z","shell.execute_reply.started":"2025-11-14T02:12:32.589951Z","shell.execute_reply":"2025-11-14T02:12:32.619662Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"# ViT setting","metadata":{}},{"cell_type":"code","source":"import keras\nfrom keras import layers, ops\n\ndef mlp_block(x, hidden_dim):\n    \"\"\"Feedforward MLP: expand -> GELU -> project\"\"\"\n    filters = ops.shape(x)[-1]\n    x = layers.Conv2D(hidden_dim, 1, use_bias=True)(x)\n    x = layers.Activation(\"gelu\")(x)\n    x = layers.Conv2D(filters, 1, use_bias=True)(x)\n    return x\n\ndef transformer_style_conv_block(x, filters, stride=1, mlp_ratio=4):\n    \"\"\"\n    ViT-style block: \n      - Norm -> Conv -> Add (residual)\n      - Norm -> MLP  -> Add (residual)\n    If stride>1 or filters mismatch, project shortcut.\n    \"\"\"\n    shortcut = x\n    if stride != 1 or ops.shape(x)[-1] != filters:\n        shortcut = layers.Conv2D(filters, 1, strides=stride, padding=\"same\", use_bias=True)(shortcut)\n    \n    # Branch 1: Norm -> Conv -> Residual\n    out = layers.LayerNormalization(axis=-1, epsilon=1e-6)(x)\n    out = layers.Conv2D(filters, 3, strides=stride, padding=\"same\", use_bias=True)(out)\n    out = layers.Activation(\"gelu\")(out)\n    out = layers.Add()([out, shortcut])\n    \n    # Branch 2: Norm -> MLP -> Residual\n    residual = out\n    out = layers.LayerNormalization(axis=-1, epsilon=1e-6)(out)\n    out = mlp_block(out, hidden_dim=int(filters * mlp_ratio))\n    out = layers.Add()([out, residual])\n    \n    return out\n\ndef make_stage(x, filters, num_blocks, downsample, mlp_ratio=4):\n    x = transformer_style_conv_block(x, filters, stride=(2 if downsample else 1), mlp_ratio=mlp_ratio)\n    for _ in range(num_blocks - 1):\n        x = transformer_style_conv_block(x, filters, stride=1, mlp_ratio=mlp_ratio)\n    return x\n\ndef build_vit_style_conv_encoder(embed_dims: int = 256, blocks_per_stage=(1,1,1,1,1), mlp_ratio=4):\n    \"\"\"\n    - Input: uint8 [B, H, W, 1], dynamic H/W.\n    - Total downsample: 16× (4 stride=2 stages).\n    - Output: [B, n_patch, embed_dims], n_patch=(H/16)*(W/16).\n    - Block: ViT-style Norm -> Conv -> Add -> Norm -> MLP -> Add.\n    \"\"\"\n    inp = keras.Input(shape=(None, None, 1), dtype=\"uint8\", name = 'UniversalMedImageInput')\n    x = layers.Lambda(lambda t: ops.cast(t, \"float32\") / 255.0)(inp)\n    \n    # Stem: simple conv + gelu\n    x = layers.Conv2D(64, 3, strides=1, padding=\"same\", use_bias=True)(x)\n    x = layers.Activation(\"gelu\")(x)\n    \n    # 4× downsample (16x total), last stage no downsample and outputs embed_dims channels\n    filters_plan = [64, 96, 128, 160, embed_dims]\n    downsamples  = [True, True, True, True, False]\n    \n    for f, n, ds in zip(filters_plan, blocks_per_stage, downsamples):\n        x = make_stage(x, f, num_blocks=n, downsample=ds, mlp_ratio=mlp_ratio)\n    \n    # [B, H/16, W/16, embed_dims] -> [B, n_patch, embed_dims]\n    patches = keras.layers.Reshape((-1, embed_dims))(x)\n    #patches = layers.Lambda(lambda t: ops.reshape(t, (ops.shape(t)[0], ops.shape(t)[1]*ops.shape(t)[2], ops.shape(t)[3])),\n    #                        name=\"patch_embeddings\")(x)\n    return keras.Model(inp, patches, name=\"ViTStyleConvEncoder\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T02:12:32.622276Z","iopub.execute_input":"2025-11-14T02:12:32.622659Z","iopub.status.idle":"2025-11-14T02:12:32.649819Z","shell.execute_reply.started":"2025-11-14T02:12:32.622634Z","shell.execute_reply":"2025-11-14T02:12:32.648694Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"conv_stem = build_vit_style_conv_encoder(embed_dims=embed_dims, \n                                         blocks_per_stage=(1,1,1,1,1), \n                                         mlp_ratio=4)\n#conv_stem(ops.ones((2, 512, 512, 1), dtype=\"uint8\"))  # 출력: [2, 1024, 256]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T02:14:27.934423Z","iopub.execute_input":"2025-11-14T02:14:27.934807Z","iopub.status.idle":"2025-11-14T02:14:28.234808Z","shell.execute_reply.started":"2025-11-14T02:14:27.934780Z","shell.execute_reply":"2025-11-14T02:14:28.233211Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"hybrid = MedicalViT(conv_base = conv_stem,\n                    patch_size = patch_size, embed_dims = embed_dims,\n                 num_fourier_layers = n_fourier, num_attention_layers = n_attention,\n                 interleaved = interleaved, num_interleaved_layers= 6, \n                     )\nvit = MedicalViT(conv_base = None,\n                 patch_size = patch_size, embed_dims = embed_dims,\n                 num_fourier_layers = n_fourier, num_attention_layers = n_attention,\n                 interleaved = interleaved, num_interleaved_layers= 6, \n                     )\nhybrid.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T02:14:37.043983Z","iopub.execute_input":"2025-11-14T02:14:37.044329Z","iopub.status.idle":"2025-11-14T02:14:37.133792Z","shell.execute_reply.started":"2025-11-14T02:14:37.044285Z","shell.execute_reply":"2025-11-14T02:14:37.132898Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"medical_vi_t\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"medical_vi_t\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ ViTStyleConvEncoder             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m)      │     \u001b[38;5;34m2,675,648\u001b[0m │\n│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_84 (\u001b[38;5;33mConv2D\u001b[0m)              │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ EmbedAfterConv (\u001b[38;5;33mDense\u001b[0m)          │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ ro_pe2d__keras_nlp              │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n│ (\u001b[38;5;33mRoPE2D_KerasNLP\u001b[0m)               │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ attention_block_0               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n│ (\u001b[38;5;33mSelfAttentionBlock\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ attention_block_1               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n│ (\u001b[38;5;33mSelfAttentionBlock\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ final_norm (\u001b[38;5;33mLayerNormalization\u001b[0m) │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ ViTStyleConvEncoder             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,675,648</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_84 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ EmbedAfterConv (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ ro_pe2d__keras_nlp              │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RoPE2D_KerasNLP</span>)               │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ attention_block_0               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SelfAttentionBlock</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ attention_block_1               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SelfAttentionBlock</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ final_norm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>) │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,676,032\u001b[0m (10.21 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,676,032</span> (10.21 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,676,032\u001b[0m (10.21 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,676,032</span> (10.21 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}],"execution_count":21},{"cell_type":"markdown","source":"# Output Visualization helpers","metadata":{}},{"cell_type":"code","source":"#GradCAM compute\nfrom io import BytesIO\nfrom PIL import Image\n\nclass ViTGradCAM:\n    \"\"\"\n    GradCAM for Vision Transformer\n    - Handles patch-based outputs: [B, N, D]\n    - Vectorized for minimal loops\n    \"\"\"\n    def __init__(self, model):\n        self.model = model\n        self.vit = model.student_vit  # Use student for GradCAM\n        self.cls_head = model.cls_head\n    \n    def get_heatmap(self, images, class_indices=None):\n        \"\"\"\n        Computes GradCAM heatmaps for a batch of images.\n        \n        Args:\n            images: Tensor of shape [B, H, W, C].\n            class_indices: [B] tensor of target class indices. \n                           If None, uses the predicted class for each image.\n                           \n        Returns:\n            heatmaps: [B, H, W] GradCAM heatmaps.\n            preds: [B] predicted class indices.\n        \"\"\"\n        images = tf.cast(images, tf.float32)\n        \n        with tf.GradientTape() as tape:\n            tape.watch(images)\n            # Get patch embeddings (last layer before global average pooling)\n            cls_token, patch_embeddings, _ = self.vit(images, training=False)\n            \n            # Use CLS token for classification\n            logits = self.cls_head(cls_token)\n            \n            preds = tf.argmax(logits, axis=-1, output_type=tf.int32)\n            \n            if class_indices is None:\n                # If no class specified, use predicted class\n                target_indices = preds\n            else:\n                target_indices = tf.cast(class_indices, tf.int32)\n            \n            # Get the score for the target class\n            batch_size = tf.shape(images)[0]\n            gather_indices = tf.stack([tf.range(batch_size), target_indices], axis=1)\n            target_scores = tf.gather_nd(logits, gather_indices)\n        \n        # Gradients of the target score w.r.t. patch embeddings\n        grads = tape.gradient(target_scores, patch_embeddings)  # [B, N, D]\n        \n        # Global average pooling of gradients (weights for each patch)\n        weights = tf.reduce_mean(grads, axis=-1)  # [B, N]\n        \n        # Weighted sum of patch embeddings (CAM)\n        # Remove CLS token from patches and weights before multiplying\n        cam = tf.einsum('bn,bnd->bn', weights, patch_embeddings)  # [B, N]\n        cam = tf.nn.relu(cam)\n        \n        # Reshape to grid\n        num_patches_side = int(np.sqrt(cam.shape[1]))\n        cam_grid = tf.reshape(cam, [-1, num_patches_side, num_patches_side])\n        \n        # Upsample to original image size\n        H, W = tf.shape(images)[1], tf.shape(images)[2]\n        heatmap = tf.image.resize(cam_grid[..., tf.newaxis], [H, W], method='bicubic')\n        heatmap = tf.squeeze(heatmap, axis=-1)\n        \n        # Normalize per image\n        min_val = tf.reduce_min(heatmap, axis=[1, 2], keepdims=True)\n        max_val = tf.reduce_max(heatmap, axis=[1, 2], keepdims=True)\n        heatmap = (heatmap - min_val) / (max_val - min_val + 1e-8)\n        \n        return heatmap, preds\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T02:12:34.569647Z","iopub.status.idle":"2025-11-14T02:12:34.570111Z","shell.execute_reply.started":"2025-11-14T02:12:34.569906Z","shell.execute_reply":"2025-11-14T02:12:34.569927Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class WandbVisualizationCallback(keras.callbacks.Callback):\n    \"\"\"\n    Logs metrics and visualizations to WandB.\n    - Logs scalars every `log_freq` steps.\n    - Logs a table of visualizations every `viz_freq` steps.\n    \"\"\"\n    def __init__(self, \n                 validation_images, \n                 validation_labels=None, \n                 log_freq=100, \n                 viz_freq=1000, \n                 num_images_to_log=4):\n        super().__init__()\n        self.val_images = validation_images\n        self.val_labels = validation_labels\n        self.log_freq = log_freq\n        self.viz_freq = viz_freq\n        self.num_images = min(num_images_to_log, validation_images.shape[0])\n        \n        # Prepare data slice for visualization\n        self.viz_images = self.val_images[:self.num_images]\n        if self.val_labels is not None:\n            self.viz_labels = self.val_labels[:self.num_images]\n        else:\n            self.viz_labels = None\n            \n        self.gradcam_computer = None\n\n    def on_train_batch_end(self, batch, logs=None):\n        # Log metrics every log_freq steps\n        if batch % self.log_freq == 0:\n            wandb.log(logs, step=batch)\n            \n        # Log visualization table every viz_freq steps\n        if (batch % self.viz_freq) == 0:# and (batch > 1):\n            self.log_visualizations(step=batch)\n\n    def log_visualizations(self, step):\n        \"\"\"Creates and logs the visualization table to WandB.\"\"\"\n        # --- 1. Get model outputs for the visualization batch ---\n        seg_maps, logits = self.model(self.viz_images, training=False)\n        preds = ops.argmax(logits, axis = -1)\n        # --- 2. Get GradCAM heatmaps ---\n        #heatmaps, preds = self.gradcam_computer.get_heatmap(\n        #    self.viz_images,\n        #    class_indices=self.viz_labels  # If None, will use predictions\n        #)\n        \n        # --- 3. Create WandB Table ---\n        table = wandb.Table(columns=[\n            \"Original Label\", \"Original Image\", \"Predicted Label\",\n            \"Segmentation Map\", \"Overlayed Image\"\n        ])\n        \n        # --- 4. Process images and add to table (vectorized where possible) ---\n        # Convert tensors to numpy for plotting\n        images_np = self.viz_images.numpy().astype(np.uint8)\n        seg_maps_np = seg_maps.numpy()\n        preds_np = preds.numpy()\n        labels_np = self.viz_labels.numpy() if self.viz_labels is not None else [None] * self.num_images\n        \n        # Upsample segmentation maps in one go\n        H, W = images_np.shape[1], images_np.shape[2]\n        seg_maps_upsampled = tf.image.resize(seg_maps_np, [H, W]).numpy()\n\n        # Plot and add to table\n        for i in range(self.num_images):\n            gt_label = int(labels_np[i]) if labels_np[i] is not None else \"N/A\"\n            pred_label = int(preds_np[i])\n            \n            # Convert images to wandb.Image\n            img_orig = wandb.Image(images_np[i])\n            img_seg = self.plot_to_wandb_image(seg_maps_np[i], cmap='tab10', vmin=0, vmax=self.model.num_parts-1)\n            img_overlay = self.plot_overlay_to_wandb_image(images_np[i], seg_maps_upsampled[i])\n            \n            table.add_data(\n                f\"GT: {gt_label}\",\n                img_orig,\n                f\"Pred: {pred_label}\",\n                img_seg,\n                img_overlay,\n                \n            )\n            \n        # Log the table\n        wandb.log({f\"Validation_Results_Step_{step}\": table}, step=step)\n        print(f\"✅ Logged visualization table to WandB at step {step}\")\n\n    def plot_to_wandb_image(self, data, cmap='viridis', vmin=None, vmax=None):\n        \"\"\"Helper to plot a numpy array and return as a wandb.Image.\"\"\"\n        fig, ax = plt.subplots()\n        ax.imshow(data, cmap=cmap, vmin=vmin, vmax=vmax)\n        ax.axis('off')\n        \n        buf = BytesIO()\n        plt.savefig(buf, format='png', bbox_inches='tight', pad_inches=0)\n        buf.seek(0)\n        plt.close(fig)\n        return wandb.Image(Image.open(buf))\n\n    def plot_overlay_to_wandb_image(self, base_image, overlay, cmap='tab10', alpha=0.5):\n        \"\"\"Helper to plot an overlay and return as a wandb.Image.\"\"\"\n        fig, ax = plt.subplots()\n        ax.imshow(base_image)\n        ax.imshow(overlay, cmap=cmap, alpha=alpha)\n        ax.axis('off')\n\n        buf = BytesIO()\n        plt.savefig(buf, format='png', bbox_inches='tight', pad_inches=0)\n        buf.seek(0)\n        plt.close(fig)\n        return wandb.Image(Image.open(buf))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T02:12:34.572349Z","iopub.status.idle":"2025-11-14T02:12:34.572669Z","shell.execute_reply.started":"2025-11-14T02:12:34.572523Z","shell.execute_reply":"2025-11-14T02:12:34.572535Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Stage 1 instance level learning\n- NNCLR + classification","metadata":{}},{"cell_type":"code","source":"class Stage1_NNCLR_Classification(keras.Model):\n    def __init__(self,\n        vit_backbone,\n        temperature = 0.07, \n        queue_size = 10240,\n        n_classes = 165,\n    ):\n        super().__init__()\n        \n        self.encoder = vit_backbone\n        self.embed_dims = self.encoder.embed_dims\n        self.projection_head = keras.Sequential(\n            [   layers.Dense(self.embed_dims, activation=\"relu\"),\n                layers.Dense(self.embed_dims),\n            ],\n            name=\"projection_head\",\n        )\n        self.t = self.temperature = temperature\n\n        self.feature_queue = keras.Variable(\n            keras.utils.normalize(\n                keras.random.normal(shape=(queue_size, self.embed_dims)),\n                axis=1,\n                order=2,\n            ),\n            trainable=False,\n        )\n        self.contrastive_augmenter = keras.Sequential([\n            layers.RandomFlip(\"horizontal\"), layers.RandomFlip(\"vertical\"), \n            #layers.RandomGaussianBlur(factor = (0.2, 0.5)),\n            layers.RandomCrop(384,384),\n            layers.RandomBrightness(factor=(0.2,0.5)),\n        ])\n        self.classification_head = keras.layers.Dense(units = n_classes)\n        self.classification_acc = keras.metrics.SparseCategoricalAccuracy()\n        self.contrastive_optimizer = keras.optimizers.AdamW(learning_rate = 5e-4, \n                                                            gradient_accumulation_steps = 8)\n        \n    def nearest_neighbour(self, projections):\n        support_similarities = ops.matmul(projections, ops.transpose(self.feature_queue))\n        nn_projections = ops.take(\n            self.feature_queue, ops.argmax(support_similarities, axis=1), axis=0\n        )\n        return projections + ops.stop_gradient(nn_projections - projections)\n\n    def contrastive_loss(self, projections_1, projections_2):\n        projections_1 = keras.utils.normalize(projections_1, axis=1, order=2)\n        projections_2 = keras.utils.normalize(projections_2, axis=1, order=2)\n\n        similarities_1_2_1 = (\n            ops.matmul(\n                self.nearest_neighbour(projections_1), ops.transpose(projections_2)\n            )\n            / self.temperature\n        )\n        similarities_1_2_2 = (\n             ops.matmul(\n                projections_2, ops.transpose(self.nearest_neighbour(projections_1))\n            )\n            / self.temperature\n        )\n\n        similarities_2_1_1 = (\n            ops.matmul(\n                self.nearest_neighbour(projections_2), ops.transpose(projections_1)\n            )\n            / self.temperature\n        )\n        similarities_2_1_2 = (\n            ops.matmul(\n                projections_1, ops.transpose(self.nearest_neighbour(projections_2))\n            )\n            / self.temperature\n        )\n\n        batch_size = ops.shape(projections_1)[0]\n        contrastive_labels = ops.arange(batch_size)\n        loss = keras.losses.sparse_categorical_crossentropy(\n            ops.concatenate(\n                [\n                    contrastive_labels,\n                    contrastive_labels,\n                    contrastive_labels,\n                    contrastive_labels,\n                ],\n                axis=0,\n            ),\n            ops.concatenate(\n                [\n                    similarities_1_2_1,\n                    similarities_1_2_2,\n                    similarities_2_1_1,\n                    similarities_2_1_2,\n                ],\n                axis=0,\n            ),\n            from_logits=True,\n        )\n        loss = ops.mean(loss)\n        self.feature_queue.assign(\n            ops.concatenate([projections_1, self.feature_queue[:-batch_size]], axis=0)\n        )\n        return loss\n\n    def train_step(self, data):\n        (images, labels) = data\n        augmented_images_1 = images\n        augmented_images_2 = self.contrastive_augmenter(images)\n\n        with tf.GradientTape() as tape:\n            features_1, encoded_patches_1, att_weights_1 = self.encoder(augmented_images_1) ; logits = self.classification_head(features_1)\n            features_2, encoded_patches_1, att_weights_1 = self.encoder(augmented_images_2)\n            projections_1 = self.projection_head(features_1)\n            projections_2 = self.projection_head(features_2)\n            contrastive_loss = self.contrastive_loss(projections_1, projections_2)\n            class_loss = keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits = True)\n            loss = 0.8*contrastive_loss + 0.2*class_loss\n            self.classification_acc.update_state(labels, logits)\n        class_acc = self.classification_acc.result()\n        gradients = tape.gradient(\n            loss,\n            self.trainable_weights\n        )\n        self.contrastive_optimizer.apply_gradients(\n            zip(\n                gradients,\n                self.trainable_weights,\n            )\n        )\n\n        return {\n            \"nnclr_loss\": contrastive_loss, 'classification_loss' : class_loss, 'total_loss' : loss,\n            'class_accuracy' : class_acc\n        }\n    def visualize_output(self, imgs):\n        if len(ops.shape(imgs)) == 3:\n            imgs = imgs[1, ...]\n            batch_size = 1\n        else:\n            batch_size = ops.shape(imgs)[0]\n        f, encoded_patches, att_wts = self.encoder(imgs)\n        last_attention_weight = att_wts[-1] #batch, heads, n_patches +1, n_patches+1\n        w = int(ops.sqrt(float(ops.shape(encoded_patches)[1])))\n        encoded_patches = ops.reshape(encoded_patches, [-1,self.embed_dims])\n        \n        pca = PCA(n_components=3)\n        encoded_rgb = pca.fit_transform(encoded_patches)\n        encoded_rgb = ops.reshape(encoded_rgb, [-1,w,w,3])\n\n        attention_weight = ops.mean(last_attention_weight, axis = 1)\n        attention_weight = attention_weight[:,0,1:]\n        attention_weight = ops.reshape(attention_weight, [batch_size, w, w])\n        \n        for i in range(batch_size):\n            fig,axes = plt.subplots(1,3, figsize = (15,5))\n            axes = axes.flatten()\n            axes[0].imshow(imgs[i], cmap = 'bone')\n            axes[1].imshow(encoded_rgb[i])\n            axes[2].imshow(attention_weight[i])\n            plt.tight_layout()\n            plt.show()\n        print(\"+-----------------------------------------------------------+\")\n        \nif False:\n    nnclr = Stage1_NNCLR_Classification(vit)\n    sample_imgs = imgs\n    nnclr.compile()\n    nnclr.fit(train_radimagenet_ds, epochs = 1, steps_per_epoch = 50000)\n    nnclr.visualize_output(sample_imgs)\n    vit = nnclr.encoder\n    visualize_token_attention_map(vit = vit, image = imgs)","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-11-14T02:12:34.574225Z","iopub.status.idle":"2025-11-14T02:12:34.574723Z","shell.execute_reply.started":"2025-11-14T02:12:34.574587Z","shell.execute_reply":"2025-11-14T02:12:34.574602Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Stage 2 patch level learning\n- Unsupervised Semantic segmentation + $iBOT$","metadata":{}},{"cell_type":"code","source":"class Stage2_iBOT_PartSegmentation(keras.Model):\n    \"\"\"\n    Unified iBOT + Part Segmentation for Multi-Modal Medical Imaging\n    \n    Args:\n        vit_backbone: Pretrained ViT from Stage 1\n        num_classes: 165 (instance-level labels)\n        num_parts: 8 (parts per image)\n        global_crop_size: 224\n        local_crop_sizes: [96, 128]\n        num_local_crops: 6\n        mask_ratio: 0.4\n        masking_strategy: 'random' or 'block'\n        seg_weight: 0.5 (balance iBOT vs Seg)\n    \"\"\"\n    \n    def __init__(self,\n                 vit_backbone, teacher_model,\n                 num_classes: int = 165,\n                 num_parts: int = 32,\n                 global_crop_size: int = 224,\n                 local_crop_sizes: list = None,\n                 num_local_crops: int = 6,\n                 mask_ratio: float = 0.4,\n                 masking_strategy: str = 'random',\n                 ema_momentum = 0.99,\n                 teacher_temp: float = 0.07,\n                 student_temp: float = 0.1,\n                 seg_weight: float = 0.5,\n                 seg_target_dist_mode = \"power\",\n                 seg_method = 'agglomerative',\n                 seg_distance = \"l2\",\n                 **kwargs):\n        super().__init__(**kwargs)\n        \n        # Config\n        self.num_classes = num_classes\n        self.num_parts = num_parts\n        self.global_crop_size = global_crop_size\n        self.local_crop_sizes = local_crop_sizes or [96, 128]\n        self.num_local_crops = num_local_crops\n        self.mask_ratio = mask_ratio\n        self.masking_strategy = masking_strategy\n        self.teacher_temp = teacher_temp\n        self.student_temp = student_temp\n        self.ema_momentum = ema_momentum\n        self.seg_weight = seg_weight\n        \n        embed_dims = vit_backbone.embed_dims\n        self.embed_dims = embed_dims\n        \n        # Networks\n        self.student_vit = vit_backbone\n        self.teacher_vit = teacher_model\n        self.teacher_vit.trainable = False\n        self.patch_size = vit_backbone.patch_size\n        # Heads\n        self.patch_head = self._build_head(embed_dims, embed_dims, 'patch_head')\n        self.cls_head = self._build_head(embed_dims, num_classes, 'cls_head')\n        self.dense_head = self._build_head(embed_dims, embed_dims // 2, 'dense_head')\n        \n        self.center = self.add_weight(\n                                    shape=(1, 1, self.embed_dims),\n                                    initializer='zeros',\n                                    trainable=False,\n                                    name='center'\n                                )\n        \n        # Metrics\n        self.ibot_tracker = keras.metrics.Mean(name='ibot')\n        self.cls_tracker = keras.metrics.Mean(name='cls')\n        self.acc_tracker = keras.metrics.SparseCategoricalAccuracy(name='acc')\n        \n        #init for Unsup Seg\n        \n        self.seg_tracker = keras.metrics.Mean(name='seg')\n        self.uniform_dist = tf.fill((num_parts,), 1.0/num_parts)\n        self.target_dist = create_target_distribution(num_parts, method=seg_target_dist_mode)\n        self.cluster_function = ClusteringLayer(k = num_parts,\n                                               n_iterations = 3,\n                                               temperature = student_temp,\n                                               method = seg_method,\n                                               distance_metric = seg_distance)\n        self.kl_loss_fn_batch = keras.losses.KLDivergence(reduction='sum_over_batch_size')\n        self.kl_loss_fn = self.kl_loss_fn_per_image = keras.losses.KLDivergence(reduction='none')\n        # Optimizer\n        self.optimizer = keras.optimizers.AdamW(\n            learning_rate=1e-4,\n        )\n    \n    def build(self, input_shape):\n        super().build(input_shape)\n        self.teacher_vit.set_weights(self.student_vit.get_weights())\n    \n    # ========================================\n    # Network Components\n    # ========================================\n    \n    def _build_head(self, in_dims, out_dims, name):\n        \"\"\"Build head with LayerNorm + Dense\"\"\"\n        return keras.Sequential([\n            layers.LayerNormalization(),\n            layers.Dense(in_dims, activation='gelu'),\n            layers.LayerNormalization(),\n            layers.Dense(out_dims)\n        ], name=name)\n    \n    @staticmethod\n    def safe_normalize(x, axis=-1, eps=1e-6):\n        \"\"\"NaN-safe L2 normalization\"\"\"\n        norm = ops.sqrt(ops.sum(ops.square(x), axis=axis, keepdims=True) + eps)\n        return x / ops.maximum(norm, eps)\n    \n    def update_teacher(self):\n        \"\"\"EMA teacher update\"\"\"\n        m = self.ema_momentum\n        for s_var, t_var in zip(self.student_vit.trainable_variables,\n                                 self.teacher_vit.trainable_variables):\n            t_var.assign(m * t_var + (1 - m) * s_var)\n    \n    # ========================================\n    # Data Augmentation\n    # ========================================\n    \n    def create_crops(self, images):\n        \"\"\"Multi-resolution crops (local: no resize!)\"\"\"\n        images = images/255\n        g1 = tf.image.resize(images, [self.global_crop_size, self.global_crop_size])\n        g2 = self._augment(images, self.global_crop_size)\n        \n        local_crops = [\n            self._augment(images, np.random.choice(self.local_crop_sizes))\n            for _ in range(self.num_local_crops)\n        ]\n        \n        return [g1, g2], local_crops\n    \n    def _augment(self, images, size):\n        \"\"\"Random crop + augmentation\"\"\"\n        shape = ops.shape(images)\n        ratio = tf.random.uniform([], 0.5, 0.7)\n        crop_h = tf.cast(tf.cast(shape[1], tf.float32) * ratio, tf.int32)\n        \n        images = tf.image.random_crop(images, [shape[0], crop_h, crop_h, shape[3]])\n        images = tf.image.resize(images, [size, size])\n        images = tf.image.random_flip_left_right(images)\n        images = tf.image.random_brightness(images, 0.1)\n        images = tf.image.random_contrast(images, 0.95, 1.05)\n        \n        return tf.clip_by_value(images, 0.0, 1.0)\n    \n    # ========================================\n    # Masking\n    # ========================================\n    \n    def generate_mask(self, batch_size, num_patches, ratio):\n        \"\"\"Generate mask (random or block)\"\"\"\n        if ratio <= 0:\n            return tf.zeros([batch_size, num_patches], dtype=tf.bool)\n        \n        if self.masking_strategy == 'random':\n            return tf.random.uniform([batch_size, num_patches]) < ratio\n        \n        elif self.masking_strategy == 'block':\n            grid = tf.cast(ops.sqrt(num_patches), \"int32\")\n            block = tf.cast(tf.cast(grid, \"float32\") * ops.sqrt(ratio), \"int32\")\n            \n            masks = []\n            for b in range(batch_size):\n                # Random block position\n                max_pos = grid - block\n                y_start = tf.random.uniform([], 0, max_pos + 1, dtype=tf.int32)\n                x_start = tf.random.uniform([], 0, max_pos + 1, dtype=tf.int32)\n                \n                # Create coordinate grids\n                y_coords = tf.range(grid, dtype=tf.int32)\n                x_coords = tf.range(grid, dtype=tf.int32)\n                \n                y_grid, x_grid = tf.meshgrid(y_coords, x_coords, indexing='ij')\n                \n                # Check if coordinates are in block\n                y_in_block = tf.logical_and(\n                    y_grid >= y_start,\n                    y_grid < y_start + block\n                )\n                x_in_block = tf.logical_and(\n                    x_grid >= x_start,\n                    x_grid < x_start + block\n                )\n                \n                # Both conditions must be true\n                mask_2d = tf.logical_and(y_in_block, x_in_block)\n                \n                # Flatten\n                mask_1d = tf.reshape(mask_2d, [num_patches])\n                masks.append(mask_1d)\n            \n            return tf.stack(masks)\n        \n        return tf.random.uniform([batch_size, num_patches]) < ratio\n    \n    def apply_mask(self, tokens, mask):\n        \"\"\"Apply mask to tokens\"\"\"\n        return tf.where(mask[:, :, None], tf.zeros_like(tokens), tokens)\n    \n    def visualize_mask(self, images, save_path=None):\n        \"\"\"Visualize masking strategy\"\"\"\n        batch_size = min(tf.shape(images)[0].numpy(), 4)\n        _, patches, _ = self.student_vit(images, training=False)\n        num_patches = tf.shape(patches)[1]\n        \n        mask = self.generate_mask(batch_size, num_patches, self.mask_ratio)\n        grid_size = int(np.sqrt(num_patches))\n        \n        fig, axes = plt.subplots(batch_size, 2, figsize=(8, 4 * batch_size))\n        if batch_size == 1:\n            axes = axes[None, :]\n        \n        for i in range(batch_size):\n            axes[i, 0].imshow(images[i])\n            axes[i, 0].set_title('Original')\n            axes[i, 0].axis('off')\n            \n            mask_2d = tf.reshape(mask[i], [grid_size, grid_size])\n            mask_2d = tf.cast(mask_2d, \"int32\")\n            mask_up = tf.image.resize(\n                mask_2d[:, :, None],\n                [self.global_crop_size, self.global_crop_size],\n                method='nearest'\n            )\n            \n            masked_img = images[i] * (1 - tf.cast(mask_up, tf.float32)) + \\\n                         0.5 * tf.cast(mask_up, tf.float32)\n            \n            axes[i, 1].imshow(masked_img)\n            axes[i, 1].set_title(f'Masked ({self.masking_strategy})')\n            axes[i, 1].axis('off')\n        \n        plt.tight_layout()\n        if save_path:\n            plt.savefig(save_path, dpi=150, bbox_inches='tight')\n        \n        return fig\n    \n    # ========================================\n    # Forward Pass\n    # ========================================\n    \n    def forward_student(self, images, ratio):\n        \"\"\"Student forward with masking\"\"\"\n        cls, patches, _ = self.student_vit(images*255, training=True)\n        mask = self.generate_mask(ops.shape(patches)[0], ops.shape(patches)[1], ratio)\n        return cls, self.apply_mask(patches, mask), mask, patches\n    \n    def forward_teacher(self, images):\n        \"\"\"Teacher forward (no masking)\"\"\"\n        cls, patches, _ = self.teacher_vit(images*255, training=False)\n        return ops.stop_gradient(cls), ops.stop_gradient(patches)\n    \n    def forward_dense(self, patches):\n        \"\"\"Dense prediction for segmentation\"\"\"\n        features = self.dense_head(patches)\n        return features\n    \n    \n    # ========================================\n    # Loss Functions\n    # ========================================\n    \n    def compute_ibot_loss(self, s_patches, t_patches, mask):\n        \"\"\"iBOT MIM loss\"\"\"\n        s = self.patch_head(s_patches)\n        t = ops.stop_gradient(self.patch_head(t_patches))\n        \n        t_probs = ops.softmax((t - self.center)/self.teacher_temp)\n        s_log_probs = ops.log_softmax((s - self.center)/self.student_temp)\n        \n        # Masked CE\n        mask_flat = tf.reshape(mask, [-1])\n        s_flat = tf.reshape(s_log_probs, [-1, self.embed_dims])\n        t_flat = tf.reshape(t_probs, [-1, self.embed_dims])\n        \n        s_masked = tf.boolean_mask(s_flat, mask_flat)\n        t_masked = tf.boolean_mask(t_flat, mask_flat)\n        \n        loss = -ops.sum(t_masked * s_masked, axis=-1)\n        \n        return ops.clip(ops.mean(loss), 0.0, 10.0)\n    \n    def compute_seg_loss(self, encoded_patches, spatial_shape):\n        encoded_patches = self.forward_dense(encoded_patches)\n        cluster_proba, cluster_patches, patch_proba_soft = self.cluster_function(encoded_patches, spatial_shape = spatial_shape)\n        \n        cluster_proba_safe = ops.clip(cluster_proba, 1e-6, 1.0)\n        batchwise_cluster_proba = ops.mean(cluster_proba_safe, axis = 0)\n        anti_collapse_loss = self.kl_loss_fn_batch(ops.expand_dims(self.uniform_dist, 0),\n                                batchwise_cluster_proba)\n        cluster_proba_sorted = tf.sort(cluster_proba_safe, axis=-1, direction='DESCENDING')\n        anti_trivial_loss = self.kl_loss_fn_per_image(ops.expand_dims(self.target_dist, 0), \n                                            cluster_proba_sorted)\n        anti_collapse_loss, anti_trivial_loss = ops.mean(anti_collapse_loss), ops.mean(anti_trivial_loss)\n        loss = anti_collapse_loss + anti_trivial_loss\n        \n        return loss, anti_collapse_loss, anti_trivial_loss, cluster_patches, cluster_proba\n        \n    def compute_consistency_loss(self, proba_list):\n        \"\"\" 뷰 간의 클러스터 분포 일관성 손실 계산 \"\"\"\n        num_views = len(proba_list)\n        if num_views < 2:\n            return tf.constant(0.0, dtype=tf.float32)\n\n        total_consistency_loss = 0.0\n        num_pairs = 0\n\n        # 모든 쌍(pair)에 대해 대칭적 KL 계산\n        for i in range(num_views):\n            for j in range(i + 1, num_views):\n                p = ops.clip(proba_list[i], 1e-6, 1.0)\n                q = ops.clip(proba_list[j], 1e-6, 1.0)\n\n                # KL(p || stop_grad(q)) + KL(q || stop_grad(p))\n                loss_pq = self.kl_loss_fn_per_image(p, ops.stop_gradient(q))\n                loss_qp = self.kl_loss_fn_per_image(q, ops.stop_gradient(p))\n\n                # 배치 평균\n                consistency_pair = ops.mean(loss_pq + loss_qp) * 0.5\n                total_consistency_loss += consistency_pair\n                num_pairs += 1\n\n        return total_consistency_loss / float(num_pairs) if num_pairs > 0 else tf.constant(0.0, dtype=tf.float32)\n    \n    # ========================================\n    # Training\n    # ========================================\n    \n    def train_step(self, data):\n        \"\"\"Training step\"\"\"\n        images, labels = data #images는 0~255 uint8\n        batch_size, H, W, _ = ops.shape(images)\n        spatial_shape = (H//self.patch_size,W//self.patch_size)\n        global_crops, local_crops = self.create_crops(images) #0~1 사이 텐서로 변환 \n        student_spatial_shape = (self.global_crop_size//self.patch_size, self.global_crop_size//self.patch_size)\n        current_step = self.optimizer.iterations\n        \n        with tf.GradientTape() as tape:\n            \n            # Global: iBOT + Seg\n            _, h_global, w_global, _ = ops.shape(global_crops[0])\n            \n             \n            _, t_g1 = self.forward_teacher(global_crops[0])\n            _, t_g2 = self.forward_teacher(global_crops[1])\n            \n            s_cls_g1, s_g1, m_g1, student_patch_1 = self.forward_student(global_crops[0], self.mask_ratio)\n            s_cls_g2, s_g2, m_g2, student_patch_2 = self.forward_student(global_crops[1], self.mask_ratio)\n             \n            ibot_loss = self.compute_ibot_loss(s_g1, t_g2, m_g1) + \\\n                        self.compute_ibot_loss(s_g2, t_g1, m_g2)\n            center_new = ops.mean(ops.concatenate([t_g1, t_g2], axis = 0),\n                                  axis = (0,1))\n            # Local: iBOT only\n            for local in local_crops:\n                _, s_local, m_local, _ = self.forward_student(local, self.mask_ratio)\n                \n                t_target = t_g1\n                n_local = ops.shape(s_local)[1]\n                n_global = ops.shape(t_target)[1]\n                idx = tf.random.shuffle(tf.range(n_global))[:n_local]\n                t_target = tf.gather(t_target, idx, axis=1)\n                \n                ibot_loss += self.compute_ibot_loss(s_local, t_target, m_local)\n            \n            ibot_loss /= (2 + len(local_crops))\n            \n            # Segmentation\n            anti_collapse_coefficient = keras.ops.clip(1.0 - self.optimizer.iterations/total_steps, \n                                                       0.0, 1.0)\n            anti_trivial_coefficient = 1-anti_collapse_coefficient\n            \n            cls_whole, encoded_patches, _ = self.student_vit(images)\n            _, anti_collapse_loss, anti_trivial_loss, _, cluster_proba = self.compute_seg_loss(encoded_patches, spatial_shape = spatial_shape)\n            _, anti_collapse_loss_g1, anti_trivial_loss_g1, _, cluster_proba_g1 =self.compute_seg_loss(student_patch_1, spatial_shape = student_spatial_shape)\n            _, anti_collapse_loss_g2, anti_trivial_loss_g2, _, cluster_proba_g2 =self.compute_seg_loss(student_patch_2, spatial_shape = student_spatial_shape)\n            \n            #Semantic consistency loss\n            consistency_loss = self.compute_consistency_loss([cluster_proba,\n                                                     cluster_proba_g1,\n                                                     cluster_proba_g2])\n            seg_loss = consistency_loss/3\n            # Multi-View loss\n            seg_loss += anti_collapse_coefficient*(anti_collapse_loss + anti_collapse_loss_g1 + anti_collapse_loss_g2)/3 + anti_trivial_coefficient*(anti_trivial_loss +anti_trivial_loss_g1 + anti_trivial_loss_g2)/3\n            \n            # Classification\n            cls_logits = self.cls_head(cls_whole)\n            cls_loss = ops.mean(keras.losses.sparse_categorical_crossentropy(\n                labels, cls_logits, from_logits=True\n            ))\n            \n            # Total\n            total = (1 - self.seg_weight) * ibot_loss + \\\n                    self.seg_weight * seg_loss + 0.1*cls_loss\n        \n        self.center.assign(self.ema_momentum * self.center + (1 - self.ema_momentum) * center_new)\n        # Gradients\n        trainable = self.student_vit.trainable_variables + \\\n                    self.patch_head.trainable_variables + \\\n                    self.dense_head.trainable_variables + \\\n                    self.cls_head.trainable_variables \n        \n        grads = tape.gradient(total, trainable)\n        \n        # NaN check\n        has_nan = tf.reduce_any([\n            tf.reduce_any(tf.math.is_nan(g)) for g in grads if g is not None\n        ])\n        \n        self.optimizer.apply_gradients(zip(grads, trainable))\n        self.update_teacher()\n        \n        # Metrics\n        self.ibot_tracker.update_state(ibot_loss)\n        self.seg_tracker.update_state(seg_loss)\n        self.cls_tracker.update_state(cls_loss)\n        self.acc_tracker.update_state(labels, cls_logits)\n        \n        return {\n            'loss': total,\n            'ibot': self.ibot_tracker.result(),\n            'seg': self.seg_tracker.result(),\n            'cls': self.cls_tracker.result(),\n            'acc': self.acc_tracker.result(),\n            \"is_nan_error\" : keras.ops.mean(has_nan),\n            'seg_anti_collapse_loss' : (anti_collapse_loss + anti_collapse_loss_g1 + anti_collapse_loss_g2)/3,\n            \"seg_anti_trivial_loss\" : (anti_trivial_loss +anti_trivial_loss_g1 + anti_trivial_loss_g2)/3,\n            'seg_consistency_loss' : consistency_loss,\n        }\n    \n    # ========================================\n    # Inference\n    # ========================================\n    \n    def call(self, inputs, labels=None, training=False):\n        \"\"\"Inference\"\"\"\n        batch_size, H, W, _ = ops.shape(inputs)\n        spatial_shape = (H//patch_size, W//patch_size)\n        cls, patches, _ = self.teacher_vit(inputs, training=False)\n        \n        # Classification\n        logits = self.cls_head(cls)\n        if labels is None:\n            labels = ops.argmax(logits, axis=-1)\n        \n        # Segmentation\n        seg_loss, anti_collapse_loss, anti_trivial_loss, part_seg, _ = self.compute_seg_loss(patches, spatial_shape = spatial_shape)\n        # part_seg = [batch_size, n_patches]\n        batch_size, n_patches = ops.shape(part_seg)\n        grid_size = ops.cast(ops.sqrt(ops.cast(n_patches, \"float32\")\n                                     ),\n                            \"int32\")\n        part_seg = ops.reshape(part_seg, [batch_size, grid_size, grid_size, 1])\n        return part_seg, logits\n    \n    # ========================================\n    # Visualization\n    # ========================================\n    \n    def visualize_results(self, images, labels=None, save_path=None):\n        \"\"\"Visualize segmentation results\"\"\"\n        part_seg, logits = self(images, labels, training=False)\n        preds = ops.argmax(logits, axis=-1)\n        \n        batch_size = int(tf.shape(images)[0])\n        res = int(tf.shape(images)[1])\n        \n        fig, axes = plt.subplots(batch_size, 3, figsize=(12, 4 * batch_size))\n        if batch_size == 1:\n            axes = axes[None, :]\n        \n        cmap = plt.cm.get_cmap('tab10', self.num_parts)\n        \n        for i in range(batch_size):\n            # Original\n            axes[i, 0].imshow(images[i])\n            axes[i, 0].set_title(f'Class: {labels[i].numpy() if labels != None else \"DUMMY\"}')\n            axes[i, 0].axis('off')\n            \n            # Parts\n            axes[i, 1].imshow(part_seg[i], cmap=cmap, vmin=0, vmax=self.num_parts-1)\n            axes[i, 1].set_title(f'Pred label : {int(preds[i])}')\n            axes[i, 1].axis('off')\n            \n            # Overlay\n            axes[i, 2].imshow(images[i])\n            axes[i, 2].imshow(tf.image.resize(part_seg[i], [res,res]), \n                              cmap=cmap, alpha=0.5, vmin=0, vmax=self.num_parts-1)\n            axes[i, 2].set_title('Seg result Upsample and Overlay')\n            axes[i, 2].axis('off')\n        \n        plt.tight_layout()\n        if save_path:\n            plt.savefig(save_path, dpi=150, bbox_inches='tight')\n        \n        return fig\n    \n    @property\n    def metrics(self):\n        return [self.ibot_tracker, self.seg_tracker, self.cls_tracker, self.acc_tracker]\n\n\n# ========================================\n# Usage Example\n# ========================================\n\n# Create model\n#model = Stage2_iBOT_PartSegmentation(\n#        vit_backbone=vit, teacher_model = vit,\n#        num_classes=165,\n#        num_parts=32,\n#        global_crop_size=384,\n#        local_crop_sizes=[96, 128, 192, 256],\n#        num_local_crops=6,\n#        mask_ratio=0.7,\n#        masking_strategy='random',\n#        seg_weight=0.4,\n#    seg_target_dist_mode = \"power\",\n#    seg_method = 'agglomerative',\n#    seg_distance = \"l2\"\n#    )\n\n#model.compile()\n#model.fit(train_radimagenet_ds, epochs = 1, steps_per_epoch = 40000)\n#model.visualize_results(images = imgs, labels = labs)\n#visualize_token_attention_map(vit = model.teacher_vit, image = imgs)","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-11-14T02:12:34.578066Z","iopub.status.idle":"2025-11-14T02:12:34.578518Z","shell.execute_reply.started":"2025-11-14T02:12:34.578358Z","shell.execute_reply":"2025-11-14T02:12:34.578376Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Joint learner\n- Stage 1, 2를 동시에 실행하되 Loss_1은 점점 줄어들고 (cos annealing), Loss_2 coefficient는 1-coeff_loss_1로.","metadata":{}},{"cell_type":"code","source":"class JointLearner(keras.Model):\n    \"\"\"\n    Unified [classification] + [iBOT - class loss] + [iBOT-patches loss + Part Segmentation] for Multi-Modal Medical Imaging\n    Loss_1 = classification + iBOT_CLSToken_loss -> coefficient 1 to 0\n    Loss_2 = iBOT-patches loss + Part Segmentation -> coefficient 0 to 1\n    \n    Args:\n        vit_backbone: ViT\n        num_classes: 165 (instance-level labels)\n        num_parts: 8 (parts per image)\n        global_crop_size: 224\n        local_crop_sizes: [96, 128]\n        num_local_crops: 6\n        mask_ratio: 0.4\n        masking_strategy: 'random' or 'block'\n        seg_weight: 0.5 (balance iBOT vs Seg)\n    \"\"\"\n    \n    def __init__(self,\n                 vit_backbone, teacher_model,\n                 num_classes: int = 165,\n                 num_parts: int = 32,\n                 global_crop_size: int = 224,\n                 local_crop_sizes: list = None,\n                 num_local_crops: int = 6,\n                 mask_ratio: float = 0.4,\n                 masking_strategy: str = 'random',\n                 ema_momentum = 0.99,\n                 teacher_temp: float = 0.07,\n                 student_temp: float = 0.1,\n                 seg_weight: float = 0.5,\n                 seg_target_dist_mode = \"power\",\n                 seg_method = 'agglomerative',\n                 seg_distance = \"l2\",\n                 total_steps = total_steps,\n                 **kwargs):\n        super().__init__(**kwargs)\n        \n        # Config\n        self.total_steps = total_steps\n        self.num_classes = num_classes\n        self.num_parts = num_parts\n        self.global_crop_size = global_crop_size\n        self.local_crop_sizes = local_crop_sizes or [96, 128]\n        self.num_local_crops = num_local_crops\n        self.mask_ratio = mask_ratio\n        self.masking_strategy = masking_strategy\n        self.teacher_temp = teacher_temp\n        self.student_temp = student_temp\n        self.ema_momentum = ema_momentum\n        self.seg_weight = seg_weight\n        \n        embed_dims = vit_backbone.embed_dims\n        self.embed_dims = embed_dims\n        \n        # Networks\n        self.student_vit = vit_backbone\n        self.teacher_vit = teacher_model\n        self.teacher_vit.trainable = False\n        self.patch_size = vit_backbone.patch_size\n        # Heads\n        self.dino_head = self._build_head(embed_dims, embed_dims, 'dino_head')\n        self.teacher_dino_head = self._build_head(embed_dims, embed_dims, 'teacher_dino_head') ; self.teacher_dino_head.trainable = False\n        self.patch_head = self._build_head(embed_dims, embed_dims, 'patch_head')\n        self.cls_head = self._build_head(embed_dims, num_classes, 'cls_head')\n        self.dense_head = self._build_head(embed_dims, embed_dims // 2, 'dense_head')\n        \n        self.center = self.add_weight(\n                                    shape=(1, 1, self.embed_dims),\n                                    initializer='zeros',\n                                    trainable=False,\n                                    name='center'\n                                )\n        self.center_cls = self.add_weight(\n                                    shape=(1, self.embed_dims),\n                                    initializer='zeros',\n                                    trainable=False,\n                                    name='center_cls'\n                                )\n        \n        # Metrics\n        self.ibot_tracker = keras.metrics.Mean(name='ibot')\n        self.cls_tracker = keras.metrics.Mean(name='cls')\n        self.acc_tracker = keras.metrics.SparseCategoricalAccuracy(name='acc')\n        self.dino_tracker = keras.metrics.Mean(name='dino')\n        \n        #init for Unsup Seg\n        \n        self.seg_tracker = keras.metrics.Mean(name='seg')\n        self.uniform_dist = tf.fill((num_parts,), 1.0/num_parts)\n        self.target_dist = create_target_distribution(num_parts, method=seg_target_dist_mode)\n        self.cluster_function = ClusteringLayer(k = num_parts,\n                                               n_iterations = 3,\n                                               temperature = student_temp,\n                                               method = seg_method,\n                                               distance_metric = seg_distance)\n        self.kl_loss_fn_batch = keras.losses.KLDivergence(reduction='sum_over_batch_size')\n        self.kl_loss_fn = self.kl_loss_fn_per_image = keras.losses.KLDivergence(reduction='none')\n        # Optimizer\n        self.optimizer = keras.optimizers.AdamW(\n            learning_rate=1e-4,\n        )\n    \n    def build(self, input_shape):\n        super().build(input_shape)\n        self.teacher_vit.set_weights(self.student_vit.get_weights())\n    \n    # ========================================\n    # Network Components\n    # ========================================\n    \n    def _build_head(self, in_dims, out_dims, name):\n        \"\"\"Build head with LayerNorm + Dense\"\"\"\n        return keras.Sequential([\n            layers.LayerNormalization(),\n            layers.Dense(in_dims, activation='gelu'),\n            layers.LayerNormalization(),\n            layers.Dense(out_dims)\n        ], name=name)\n    \n    @staticmethod\n    def safe_normalize(x, axis=-1, eps=1e-6):\n        \"\"\"NaN-safe L2 normalization\"\"\"\n        norm = ops.sqrt(ops.sum(ops.square(x), axis=axis, keepdims=True) + eps)\n        return x / ops.maximum(norm, eps)\n    \n    def update_teacher(self):\n        \"\"\"EMA teacher update\"\"\"\n        m = self.ema_momentum\n        for s_var, t_var in zip(self.student_vit.trainable_variables,\n                                 self.teacher_vit.trainable_variables):\n            t_var.assign(m * t_var + (1 - m) * s_var)\n        for s_var, t_var in zip(self.dino_head.trainable_variables, self.teacher_dino_head.trainable_variables):\n            t_var.assign(m * t_var + (1-m)*s_var)\n    \n    # ========================================\n    # Data Augmentation\n    # ========================================\n    \n    def create_crops(self, images):\n        \"\"\"Multi-resolution crops (local: no resize!)\"\"\"\n        images = images/255\n        g1 = tf.image.resize(images, [self.global_crop_size, self.global_crop_size])\n        g2 = self._augment(images, self.global_crop_size)\n        \n        local_crops = [\n            self._augment(images, np.random.choice(self.local_crop_sizes))\n            for _ in range(self.num_local_crops)\n        ]\n        \n        return [g1, g2], local_crops #all unmasked.\n    \n    def _augment(self, images, size):\n        \"\"\"Random crop + augmentation\"\"\"\n        shape = ops.shape(images)\n        ratio = tf.random.uniform([], 0.5, 0.7)\n        crop_h = tf.cast(tf.cast(shape[1], tf.float32) * ratio, tf.int32)\n        \n        images = tf.image.random_crop(images, [shape[0], crop_h, crop_h, shape[3]])\n        images = tf.image.resize(images, [size, size])\n        images = tf.image.random_flip_left_right(images)\n        images = tf.image.random_brightness(images, 0.1)\n        images = tf.image.random_contrast(images, 0.95, 1.05)\n        \n        return tf.clip_by_value(images, 0.0, 1.0)\n    \n    # ========================================\n    # Masking\n    # ========================================\n    \n    def generate_mask(self, batch_size, num_patches, ratio):\n        \"\"\"Generate mask (random or block)\"\"\"\n        if ratio <= 0:\n            return tf.zeros([batch_size, num_patches], dtype=tf.bool)\n        \n        if self.masking_strategy == 'random':\n            return tf.random.uniform([batch_size, num_patches]) < ratio\n        \n        elif self.masking_strategy == 'block':\n            grid = tf.cast(ops.sqrt(num_patches), \"int32\")\n            block = tf.cast(tf.cast(grid, \"float32\") * ops.sqrt(ratio), \"int32\")\n            \n            masks = []\n            for b in range(batch_size):\n                # Random block position\n                max_pos = grid - block\n                y_start = tf.random.uniform([], 0, max_pos + 1, dtype=tf.int32)\n                x_start = tf.random.uniform([], 0, max_pos + 1, dtype=tf.int32)\n                \n                # Create coordinate grids\n                y_coords = tf.range(grid, dtype=tf.int32)\n                x_coords = tf.range(grid, dtype=tf.int32)\n                \n                y_grid, x_grid = tf.meshgrid(y_coords, x_coords, indexing='ij')\n                \n                # Check if coordinates are in block\n                y_in_block = tf.logical_and(\n                    y_grid >= y_start,\n                    y_grid < y_start + block\n                )\n                x_in_block = tf.logical_and(\n                    x_grid >= x_start,\n                    x_grid < x_start + block\n                )\n                \n                # Both conditions must be true\n                mask_2d = tf.logical_and(y_in_block, x_in_block)\n                \n                # Flatten\n                mask_1d = tf.reshape(mask_2d, [num_patches])\n                masks.append(mask_1d)\n            \n            return tf.stack(masks)\n        \n        return tf.random.uniform([batch_size, num_patches]) < ratio\n    \n    def apply_mask(self, tokens, mask):\n        \"\"\"Apply mask to tokens\"\"\"\n        return tf.where(mask[:, :, None], tf.zeros_like(tokens), tokens)\n    \n    \n    # ========================================\n    # Forward Pass\n    # ========================================\n    \n    def forward_student(self, images, ratio):\n        \"\"\"Student forward with masking : returns cls token, encoded masked view patches, mask, original patches\"\"\"\n        cls, patches, _ = self.student_vit(images*255, training=True)\n        mask = self.generate_mask(ops.shape(patches)[0], ops.shape(patches)[1], ratio)\n        return cls, self.apply_mask(patches, mask), mask, patches #cls token, masked view patches, mask, original patches\n    \n    def forward_teacher(self, images):\n        \"\"\"Teacher forward (no masking)\"\"\"\n        cls, patches, _ = self.teacher_vit(images*255, training=False)\n        return ops.stop_gradient(cls), ops.stop_gradient(patches)\n    \n    def forward_dense(self, patches):\n        \"\"\"Dense prediction for segmentation\"\"\"\n        features = self.dense_head(patches)\n        return features\n    \n    \n    # ========================================\n    # Loss Functions\n    # ========================================\n    def cos_schedule_decrease(self, initial_weight = 1.0, end_weight = 0.0):\n        #1 to 0, cosine을 따르는\n        current_step = ops.cast(self.optimizer.iterations, \"float32\")\n        total_steps = ops.maximum(ops.cast(self.total_steps, \"float32\"), 1.0)\n        progress_rate = ops.clip(current_step / total_steps, 0.0, 1.0)\n        \n        pi = ops.convert_to_tensor(math.pi, dtype=\"float32\")\n        cos_decay = 0.5*(1+ops.cos(pi*progress_rate))\n        decayed_wt = end_weight + cos_decay*(initial_weight - end_weight)\n        return decayed_wt\n        \n    def linear_schedule_decrease(self, initial_weight = 1.0, end_weight = 0.0):\n        current_step = ops.cast(self.optimizer.iterations, \"float32\")\n        total_steps = ops.maximum(ops.cast(self.total_steps, \"float32\"), 1.0)\n        progress_rate = ops.clip(current_step / total_steps, 0.0, 1.0)\n        lin_decay_wt = end_weight + (1.0-progress_rate)*(initial_weight - end_weight)\n        return lin_decay_wt\n        \n    def compute_ibot_loss(self, s_patches, t_patches, mask):\n        \"\"\"iBOT MIM loss\"\"\"\n        s = self.patch_head(s_patches)\n        t = ops.stop_gradient(self.patch_head(t_patches))\n        \n        t_probs = ops.softmax((t - self.center)/self.teacher_temp)\n        s_log_probs = ops.log_softmax((s - self.center)/self.student_temp)\n        \n        # Masked CE\n        mask_flat = tf.reshape(mask, [-1])\n        s_flat = tf.reshape(s_log_probs, [-1, self.embed_dims])\n        t_flat = tf.reshape(t_probs, [-1, self.embed_dims])\n        \n        s_masked = tf.boolean_mask(s_flat, mask_flat)\n        t_masked = tf.boolean_mask(t_flat, mask_flat)\n        \n        loss = -ops.sum(t_masked * s_masked, axis=-1)\n        \n        return ops.clip(ops.mean(loss), 0.0, 10.0)\n        \n    def compute_dino_loss(self, s_cls, t_cls): \n        t_cls = ops.stop_gradient(t_cls)\n        s_cls, t_cls = self.dino_head(s_cls), self.teacher_dino_head(t_cls) #both [batch, embed_dims]\n        t_cls = ops.softmax((t_cls - self.center_cls) / self.teacher_temp, axis = -1)\n        s_cls = ops.softmax(s_cls/self.student_temp, \n                            axis = -1)\n        t_cls = ops.clip(t_cls, 1e-5, 0.999)\n        s = ops.log_softmax(s_cls, axis = -1)\n        return ops.mean(ops.sum(-t_cls * s, axis = -1))\n        \n    def compute_seg_loss(self, encoded_patches, spatial_shape):\n        encoded_patches = self.forward_dense(encoded_patches)\n        cluster_proba, cluster_patches, patch_proba_soft = self.cluster_function(encoded_patches, spatial_shape = spatial_shape)\n        \n        cluster_proba_safe = ops.clip(cluster_proba, 1e-6, 1.0)\n        batchwise_cluster_proba = ops.mean(cluster_proba_safe, axis = 0)\n        anti_collapse_loss = self.kl_loss_fn_batch(ops.expand_dims(self.uniform_dist, 0),\n                                batchwise_cluster_proba)\n        cluster_proba_sorted = tf.sort(cluster_proba_safe, axis=-1, direction='DESCENDING')\n        anti_trivial_loss = self.kl_loss_fn_per_image(ops.expand_dims(self.target_dist, 0), \n                                            cluster_proba_sorted)\n        anti_collapse_loss, anti_trivial_loss = ops.mean(anti_collapse_loss), ops.mean(anti_trivial_loss)\n        loss = anti_collapse_loss + anti_trivial_loss\n        \n        return loss, anti_collapse_loss, anti_trivial_loss, cluster_patches, cluster_proba\n        \n    def compute_consistency_loss(self, proba_list):\n        \"\"\" 뷰 간의 클러스터 분포 일관성 손실 계산 \"\"\"\n        num_views = len(proba_list)\n        if num_views < 2:\n            return tf.constant(0.0, dtype=tf.float32)\n\n        total_consistency_loss = 0.0\n        num_pairs = 0\n\n        # 모든 쌍(pair)에 대해 대칭적 KL 계산\n        for i in range(num_views):\n            for j in range(i + 1, num_views):\n                p = ops.clip(proba_list[i], 1e-6, 1.0)\n                q = ops.clip(proba_list[j], 1e-6, 1.0)\n\n                # KL(p || stop_grad(q)) + KL(q || stop_grad(p))\n                loss_pq = self.kl_loss_fn_per_image(p, ops.stop_gradient(q))\n                loss_qp = self.kl_loss_fn_per_image(q, ops.stop_gradient(p))\n\n                # 배치 평균\n                consistency_pair = ops.mean(loss_pq + loss_qp) * 0.5\n                total_consistency_loss += consistency_pair\n                num_pairs += 1\n\n        return total_consistency_loss / float(num_pairs) if num_pairs > 0 else tf.constant(0.0, dtype=tf.float32)\n    \n    # ========================================\n    # Training\n    # ========================================\n    \n    def train_step(self, data):\n        \"\"\"Training step\"\"\"\n        images, labels = data #images는 0~255 uint8\n        batch_size, H, W, _ = ops.shape(images)\n        spatial_shape = (H//self.patch_size,W//self.patch_size)\n        global_crops, local_crops = self.create_crops(images) #0~1 사이 텐서로 변환 \n        student_spatial_shape = (self.global_crop_size//self.patch_size, self.global_crop_size//self.patch_size)\n        \n        loss_1_coeff = self.cos_schedule_decrease()\n        loss_2_coeff = 1-loss_1_coeff\n\n        anti_collapse_coefficient = self.linear_schedule_decrease()\n        anti_trivial_coefficient = 1-anti_collapse_coefficient\n        \n        with tf.GradientTape() as tape:\n            \n            # Global: iBOT + Seg\n            _, h_global, w_global, _ = ops.shape(global_crops[0])\n            \n             \n            t_cls_g1, t_g1 = self.forward_teacher(global_crops[0])\n            t_cls_g2, t_g2 = self.forward_teacher(global_crops[1])\n            \n            s_cls_g1, s_g1, m_g1, student_patch_1 = self.forward_student(global_crops[0], self.mask_ratio)\n            s_cls_g2, s_g2, m_g2, student_patch_2 = self.forward_student(global_crops[1], self.mask_ratio)\n             \n            ibot_loss = self.compute_ibot_loss(s_g1, t_g2, m_g1) + \\\n                        self.compute_ibot_loss(s_g2, t_g1, m_g2)\n            center_new = ops.mean(ops.concatenate([t_g1, t_g2], axis = 0),\n                                  axis = (0,1))\n            cls_dino_loss = self.compute_dino_loss(s_cls = s_cls_g2, t_cls = t_cls_g1) + \\\n                            self.compute_dino_loss(s_cls = s_cls_g1, t_cls = t_cls_g2)\n            center_new_cls = ops.mean(ops.concatenate([t_cls_g1, t_cls_g2], axis = 0),\n                                  axis = 0)\n            # Local: iBOT only\n            for local in local_crops:\n                s_cls_local, s_local, m_local, _ = self.forward_student(local, self.mask_ratio)\n                \n                t_target = t_g1\n                n_local = ops.shape(s_local)[1]\n                n_global = ops.shape(t_target)[1]\n                idx = tf.random.shuffle(tf.range(n_global))[:n_local]\n                t_target = tf.gather(t_target, idx, axis=1)\n                \n                ibot_loss += self.compute_ibot_loss(s_local, t_target, m_local)\n                cls_dino_loss += self.compute_dino_loss(t_cls = t_cls_g1, s_cls = s_cls_local) + \\\n                                self.compute_dino_loss(t_cls = t_cls_g2, s_cls = s_cls_local)\n            ibot_loss /= (2 + len(local_crops))\n            cls_dino_loss /= (2 + len(local_crops))\n            \n            # Segmentation\n            anti_collapse_coefficient = keras.ops.clip(1.0 - self.optimizer.iterations/total_steps, \n                                                       0.0, 1.0)\n            anti_trivial_coefficient = 1-anti_collapse_coefficient\n            \n            cls_whole, encoded_patches, _ = self.student_vit(images)\n            _, anti_collapse_loss, anti_trivial_loss, _, cluster_proba = self.compute_seg_loss(encoded_patches, spatial_shape = spatial_shape)\n            _, anti_collapse_loss_g1, anti_trivial_loss_g1, _, cluster_proba_g1 =self.compute_seg_loss(student_patch_1, spatial_shape = student_spatial_shape)\n            _, anti_collapse_loss_g2, anti_trivial_loss_g2, _, cluster_proba_g2 =self.compute_seg_loss(student_patch_2, spatial_shape = student_spatial_shape)\n            \n            #Semantic consistency loss\n            consistency_loss = self.compute_consistency_loss([cluster_proba,\n                                                     cluster_proba_g1,\n                                                     cluster_proba_g2])\n            seg_loss = consistency_loss/3\n            # Multi-View loss\n            seg_loss += anti_collapse_coefficient*(anti_collapse_loss + anti_collapse_loss_g1 + anti_collapse_loss_g2)/3 + anti_trivial_coefficient*(anti_trivial_loss +anti_trivial_loss_g1 + anti_trivial_loss_g2)/3\n            \n            # Classification\n            cls_logits = self.cls_head(cls_whole)\n            cls_loss = ops.mean(keras.losses.sparse_categorical_crossentropy(\n                labels, cls_logits, from_logits=True\n            ))\n            \n            # Total\n            #Loss 1 : CLS level loss = cls_loss + cls_dino_loss\n            # Loss 2 : patch level loss = (1-s_w)iBOT loss + s_w*seg_loss\n            \n            loss_1 = cls_loss + cls_dino_loss\n            loss_2 = (1-self.seg_weight) * ibot_loss + self.seg_weight*seg_loss\n            loss = loss_1_coeff*loss_1 + loss_2_coeff*loss_2\n            \n        self.center.assign(self.ema_momentum * self.center + (1 - self.ema_momentum) * center_new)\n        self.center_cls.assign(self.ema_momentum * self.center_cls + (1 - self.ema_momentum) * center_new_cls)\n        # Gradients\n        trainable = self.student_vit.trainable_variables + \\\n                    self.patch_head.trainable_variables + \\\n                    self.dense_head.trainable_variables + \\\n                    self.cls_head.trainable_variables + self.dino_head.trainable_variables\n        \n        grads = tape.gradient(loss, trainable)\n        \n        # NaN check\n        has_nan = tf.reduce_any([\n            tf.reduce_any(tf.math.is_nan(g)) for g in grads if g is not None\n        ])\n        \n        self.optimizer.apply_gradients(zip(grads, trainable))\n        self.update_teacher()\n        \n        # Metrics\n        self.ibot_tracker.update_state(ibot_loss)\n        self.dino_tracker.update_state(cls_dino_loss)\n        self.seg_tracker.update_state(seg_loss)\n        self.cls_tracker.update_state(cls_loss)\n        self.acc_tracker.update_state(labels, cls_logits)\n        \n        return {\n            'Total_loss': loss,\n            #1. Tokenwise(loss 1 component)\n            'cls': self.cls_tracker.result(),\n            'dino_cls': self.dino_tracker.result(),\n            'acc': self.acc_tracker.result(),\n            \n            #2. patchwise(loss 2 component)\n            'ibot': self.ibot_tracker.result(), \n            'seg': self.seg_tracker.result(),\n            'seg_anti_collapse_loss' : (anti_collapse_loss + anti_collapse_loss_g1 + anti_collapse_loss_g2)/3,\n            \"seg_anti_trivial_loss\" : (anti_trivial_loss +anti_trivial_loss_g1 + anti_trivial_loss_g2)/3,\n            'seg_consistency_loss' : consistency_loss,\n            \n            # 3. ETC\n            \"is_nan_error\" : keras.ops.mean(has_nan),\n        }\n    \n    # ========================================\n    # Inference\n    # ========================================\n    \n    def call(self, inputs, labels=None, training=False):\n        \"\"\"Inference\"\"\"\n        batch_size, H, W, _ = ops.shape(inputs)\n        spatial_shape = (H//patch_size, W//patch_size)\n        cls, patches, _ = self.teacher_vit(inputs, training=False)\n        \n        # Classification\n        logits = self.cls_head(cls)\n        if labels is None:\n            labels = ops.argmax(logits, axis=-1)\n        \n        # Segmentation\n        seg_loss, anti_collapse_loss, anti_trivial_loss, part_seg, _ = self.compute_seg_loss(patches, spatial_shape = spatial_shape)\n        # part_seg = [batch_size, n_patches]\n        batch_size, n_patches = ops.shape(part_seg)\n        grid_size = ops.cast(ops.sqrt(ops.cast(n_patches, \"float32\")\n                                     ),\n                            \"int32\")\n        part_seg = ops.reshape(part_seg, [batch_size, grid_size, grid_size, 1])\n        return part_seg, logits\n    \n    # ========================================\n    # Visualization\n    # ========================================\n    \n    def visualize_results(self, images, labels=None, save_path=None):\n        \"\"\"Visualize segmentation results\"\"\"\n        part_seg, logits = self(images, labels, training=False)\n        preds = ops.argmax(logits, axis=-1)\n        \n        batch_size = int(tf.shape(images)[0])\n        res = int(tf.shape(images)[1])\n        \n        fig, axes = plt.subplots(batch_size, 3, figsize=(12, 4 * batch_size))\n        if batch_size == 1:\n            axes = axes[None, :]\n        \n        cmap = plt.cm.get_cmap('tab10', self.num_parts)\n        \n        for i in range(batch_size):\n            # Original\n            axes[i, 0].imshow(images[i])\n            axes[i, 0].set_title(f'Class: {labels[i].numpy() if labels != None else \"DUMMY\"}')\n            axes[i, 0].axis('off')\n            \n            # Parts\n            axes[i, 1].imshow(part_seg[i], cmap=cmap, vmin=0, vmax=self.num_parts-1)\n            axes[i, 1].set_title(f'Pred label : {int(preds[i])}')\n            axes[i, 1].axis('off')\n            \n            # Overlay\n            axes[i, 2].imshow(images[i])\n            axes[i, 2].imshow(tf.image.resize(part_seg[i], [res,res]), \n                              cmap=cmap, alpha=0.5, vmin=0, vmax=self.num_parts-1)\n            axes[i, 2].set_title('Seg result Upsample and Overlay')\n            axes[i, 2].axis('off')\n        \n        plt.tight_layout()\n        if save_path:\n            plt.savefig(save_path, dpi=150, bbox_inches='tight')\n        \n        return fig\n    \n    @property\n    def metrics(self):\n        return [self.ibot_tracker, self.seg_tracker, self.cls_tracker, self.acc_tracker]\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-11-14T02:12:34.580574Z","iopub.status.idle":"2025-11-14T02:12:34.581044Z","shell.execute_reply.started":"2025-11-14T02:12:34.580833Z","shell.execute_reply":"2025-11-14T02:12:34.580854Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"> Unsupervised Segmentation with MI\n\n- $C$ : cluster vector [batch, K] shape tensor\n- $X$ : patch tensor [batch, N, K] shape, patch allocation info.\n- $MI(C,X)$를 최대화하는 것이 목적\n    - $MI = H(C) - H(C|X)$\n    - $H(C)$는 Category (전체 패치가 1, 2, ..., K로 배정되는 확률)의 불확실성 -> 이것이 커져야 배치, 패치가 최대한 다양한 category로 분류됨\n    - $H(C|X)$는 패치 정보가 주어졌을 때 (X=x) 카테고리의 불확실성 -> 모델이 trivial하게 모든 카테고리에 비슷한 갯수의 패치를 배정하는 것을 막음 (만일 그럴 경우 패치 정보가 주어져도 카테고리 불확실성이 큼! -> H(C|X)값이 커져 MI값이 낮아짐)\n    - H(C)가 너무 작지 않게 하되, 즉 증가하되, H(C|X)는 감소해야 함 (패치 정보를 알고 나서는 패치 allocation의 불확실성이 떨어져야 함)\n    - 즉 $MI = H(C) - H(C|X)$ 로 두면, MI 최대화 = H(C) 최대화와 동시에 H(C|X)의 최소화임.","metadata":{}},{"cell_type":"code","source":"class JointLearnerWithMI(keras.Model):\n    \"\"\"\n    Unified [classification] + [iBOT - class loss] + [iBOT-patches loss + Seg with MI] for Multi-Modal Medical Imaging\n    Loss_1 = classification + iBOT_CLSToken_loss -> coefficient 1 to 0\n    Loss_2 = iBOT-patches loss + Segmentation MI loss -> coefficient 0 to 1\n    \n    Args:\n        vit_backbone: ViT\n        num_classes: 165 (instance-level labels)\n        num_parts: 8 (parts per image)\n        global_crop_size: 224\n        local_crop_sizes: [96, 128]\n        num_local_crops: 6\n        mask_ratio: 0.4\n        masking_strategy: 'random' or 'block'\n        seg_weight: 0.5 (balance iBOT vs Seg)\n    \"\"\"\n    \n    def __init__(self,\n                 vit_backbone, teacher_model,\n                 num_classes: int = 165,\n                 num_parts: int = 32,\n                 global_crop_size: int = 224,\n                 local_crop_sizes: list = None,\n                 num_local_crops: int = 6,\n                 mask_ratio: float = 0.4,\n                 masking_strategy: str = 'random',\n                 ema_momentum = 0.99,\n                 teacher_temp: float = 0.06,\n                 student_temp: float = 0.1,\n                 seg_weight: float = 0.5,\n                 seg_distance = \"l2\",\n                 total_steps = 60000,\n                 **kwargs):\n        super().__init__(**kwargs)\n        \n        # Config\n        self.total_steps = total_steps\n        self.num_classes = num_classes\n        self.num_parts = num_parts\n        self.global_crop_size = global_crop_size\n        self.local_crop_sizes = local_crop_sizes or [96, 128]\n        self.num_local_crops = num_local_crops\n        self.mask_ratio = mask_ratio\n        self.masking_strategy = masking_strategy\n        self.teacher_temp = teacher_temp\n        self.student_temp = student_temp\n        self.initial_mi_temp = 0.5\n        self.ema_momentum = ema_momentum\n        self.seg_weight = seg_weight\n        \n        embed_dims = vit_backbone.embed_dims\n        self.embed_dims = embed_dims\n        \n        # Networks\n        self.student_vit = vit_backbone\n        self.teacher_vit = teacher_model\n        self.teacher_vit.trainable = False\n        for layer in self.student_vit.layers:\n            layer.trainable = True\n        self.patch_size = vit_backbone.patch_size\n        # Heads\n        self.dino_head = self._build_head(embed_dims, embed_dims, 'dino_head')\n        self.teacher_dino_head = self._build_head(embed_dims, embed_dims, 'teacher_dino_head') ; self.teacher_dino_head.trainable = False\n        self.patch_head = self._build_head(embed_dims, embed_dims, 'patch_head')\n        self.cls_head = self._build_head(embed_dims, num_classes, 'cls_head')\n        self.dense_head = self._build_head(embed_dims, embed_dims, 'dense_head')\n        \n        self.center = self.add_weight(\n                                    shape=(1, 1, self.embed_dims),\n                                    initializer='zeros',\n                                    trainable=False,\n                                    name='center'\n                                )\n        self.center_cls = self.add_weight(\n                                    shape=(1, self.embed_dims),\n                                    initializer='zeros',\n                                    trainable=False,\n                                    name='center_cls'\n                                )\n        \n        # Metrics\n        self.ibot_tracker = keras.metrics.Mean(name='ibot')\n        self.cls_tracker = keras.metrics.Mean(name='cls')\n        self.acc_tracker = keras.metrics.SparseCategoricalAccuracy(name='acc')\n        self.dino_tracker = keras.metrics.Mean(name='dino')\n        \n        #init for Unsup Seg\n        self.seg_tracker = keras.metrics.Mean(name='seg')\n        self.uniform_dist = tf.fill((num_parts,), 1.0/num_parts)\n        self.cluster_function = LearnableClusteringLayer(k = num_parts, distance_metric = seg_distance)\n        # Optimizer\n        self.optimizer = keras.optimizers.AdamW(\n            learning_rate=1e-4,\n        )\n    \n    def build(self, input_shape):\n        super().build(input_shape)\n        self.teacher_vit.set_weights(self.student_vit.get_weights())\n        self.student_vit.trainable = True\n        self.teacher_vit.trainable = False\n    \n    # ========================================\n    # Network Components\n    # ========================================\n    \n    def _build_head(self, in_dims, out_dims, name):\n        \"\"\"Build head with LayerNorm + Dense\"\"\"\n        return keras.Sequential([\n            layers.LayerNormalization(),\n            layers.Dense(in_dims, activation='gelu'),\n            layers.LayerNormalization(),\n            layers.Dense(out_dims)\n        ], name=name)\n    \n    @staticmethod\n    def safe_normalize(x, axis=-1, eps=1e-6):\n        \"\"\"NaN-safe L2 normalization\"\"\"\n        norm = ops.sqrt(ops.sum(ops.square(x), axis=axis, keepdims=True) + eps)\n        return x / ops.maximum(norm, eps)\n    \n    def update_teacher(self):\n        \"\"\"EMA teacher update\"\"\"\n        m = self.ema_momentum\n        for s_var, t_var in zip(self.student_vit.trainable_variables,\n                                 self.teacher_vit.trainable_variables):\n            t_var.assign(m * t_var + (1 - m) * s_var)\n        for s_var, t_var in zip(self.dino_head.trainable_variables, self.teacher_dino_head.trainable_variables):\n            t_var.assign(m * t_var + (1-m)*s_var)\n    \n    # ========================================\n    # Data Augmentation\n    # ========================================\n    \n    def create_crops(self, images):\n        \"\"\"Multi-resolution crops (local: no resize!)\"\"\"\n        images = images/255\n        g1 = tf.image.resize(images, [self.global_crop_size, self.global_crop_size])\n        g2 = self._augment(images, self.global_crop_size)\n        \n        local_crops = [\n            self._augment(images, np.random.choice(self.local_crop_sizes))\n            for _ in range(self.num_local_crops)\n        ]\n        \n        return [g1, g2], local_crops #all unmasked.\n    \n    def _augment(self, images, size):\n        \"\"\"Random crop + augmentation\"\"\"\n        shape = ops.shape(images)\n        ratio = tf.random.uniform([], 0.5, 0.7)\n        crop_h = tf.cast(tf.cast(shape[1], tf.float32) * ratio, tf.int32)\n        \n        images = tf.image.random_crop(images, [shape[0], crop_h, crop_h, shape[3]])\n        images = tf.image.resize(images, [size, size])\n        images = tf.image.random_flip_left_right(images)\n        images = tf.image.random_brightness(images, 0.1)\n        images = tf.image.random_contrast(images, 0.95, 1.05)\n        \n        return tf.clip_by_value(images, 0.0, 1.0)\n    \n    # ========================================\n    # Masking\n    # ========================================\n    \n    def generate_mask(self, batch_size, num_patches, ratio):\n        \"\"\"Generate mask (random or block)\"\"\"\n        if ratio <= 0:\n            return tf.zeros([batch_size, num_patches], dtype=tf.bool)\n        \n        if self.masking_strategy == 'random':\n            return tf.random.uniform([batch_size, num_patches]) < ratio\n        \n        elif self.masking_strategy == 'block':\n            grid = tf.cast(ops.sqrt(num_patches), \"int32\")\n            block = tf.cast(tf.cast(grid, \"float32\") * ops.sqrt(ratio), \"int32\")\n            \n            masks = []\n            for b in range(batch_size):\n                # Random block position\n                max_pos = grid - block\n                y_start = tf.random.uniform([], 0, max_pos + 1, dtype=tf.int32)\n                x_start = tf.random.uniform([], 0, max_pos + 1, dtype=tf.int32)\n                \n                # Create coordinate grids\n                y_coords = tf.range(grid, dtype=tf.int32)\n                x_coords = tf.range(grid, dtype=tf.int32)\n                \n                y_grid, x_grid = tf.meshgrid(y_coords, x_coords, indexing='ij')\n                \n                # Check if coordinates are in block\n                y_in_block = tf.logical_and(\n                    y_grid >= y_start,\n                    y_grid < y_start + block\n                )\n                x_in_block = tf.logical_and(\n                    x_grid >= x_start,\n                    x_grid < x_start + block\n                )\n                \n                # Both conditions must be true\n                mask_2d = tf.logical_and(y_in_block, x_in_block)\n                \n                # Flatten\n                mask_1d = tf.reshape(mask_2d, [num_patches])\n                masks.append(mask_1d)\n            \n            return tf.stack(masks)\n        \n        return tf.random.uniform([batch_size, num_patches]) < ratio\n    \n    def apply_mask(self, tokens, mask):\n        \"\"\"Apply mask to tokens\"\"\"\n        return tf.where(mask[:, :, None], tf.zeros_like(tokens), tokens)\n    \n    \n    # ========================================\n    # Forward Pass\n    # ========================================\n    \n    def forward_student(self, images, ratio):\n        \"\"\"Student forward with masking : returns cls token, encoded masked view patches, mask, original patches\"\"\"\n        cls, patches, _ = self.student_vit(images*255, training=True)\n        mask = self.generate_mask(ops.shape(patches)[0], ops.shape(patches)[1], ratio)\n        return cls, self.apply_mask(patches, mask), mask, patches #cls token, masked view patches, mask, original patches\n    \n    def forward_teacher(self, images):\n        \"\"\"Teacher forward (no masking)\"\"\"\n        cls, patches, _ = self.teacher_vit(images*255, training=False)\n        return ops.stop_gradient(cls), ops.stop_gradient(patches)\n    \n    def forward_dense(self, patches):\n        \"\"\"Dense prediction for segmentation\"\"\"\n        features = self.dense_head(patches, training = True)\n        return features\n    \n    \n    # ========================================\n    # Loss Functions\n    # ========================================\n    def cos_schedule_decrease(self, initial_weight = 1.0, end_weight = 0.0):\n        #1 to 0, cosine을 따르는\n        current_step = ops.cast(self.optimizer.iterations, \"float32\")\n        total_steps = ops.maximum(ops.cast(self.total_steps, \"float32\"), 1.0)\n        progress_rate = ops.clip(current_step / total_steps, 0.0, 1.0)\n        \n        pi = ops.convert_to_tensor(math.pi, dtype=\"float32\")\n        cos_decay = 0.5*(1+ops.cos(pi*progress_rate))\n        decayed_wt = end_weight + cos_decay*(initial_weight - end_weight)\n        return decayed_wt\n        \n    def linear_schedule_decrease(self, initial_weight = 1.0, end_weight = 0.0):\n        current_step = ops.cast(self.optimizer.iterations, \"float32\")\n        total_steps = ops.maximum(ops.cast(self.total_steps, \"float32\"), 1.0)\n        progress_rate = ops.clip(current_step / total_steps, 0.0, 1.0)\n        lin_decay_wt = end_weight + (1.0-progress_rate)*(initial_weight - end_weight)\n        return lin_decay_wt\n\n    def compute_ibot_loss(self, s_patches, t_patches, mask):\n        \"\"\"iBOT MIM loss\"\"\"\n        s = self.patch_head(s_patches, training = True)\n        t = ops.stop_gradient(self.patch_head(t_patches))\n        \n        t_probs = ops.softmax((t - self.center)/self.teacher_temp)\n        s_log_probs = ops.log_softmax((s - self.center)/self.student_temp)\n        \n        # Masked CE\n        mask_flat = tf.reshape(mask, [-1])\n        s_flat = tf.reshape(s_log_probs, [-1, self.embed_dims])\n        t_flat = tf.reshape(t_probs, [-1, self.embed_dims])\n        \n        s_masked = tf.boolean_mask(s_flat, mask_flat)\n        t_masked = tf.boolean_mask(t_flat, mask_flat)\n        \n        loss = -ops.sum(t_masked * s_masked, axis=-1)\n        \n        return ops.clip(ops.mean(loss), 0.0, 10.0)\n        \n    def compute_dino_loss(self, s_cls, t_cls): \n        t_cls = ops.stop_gradient(t_cls)\n        s_cls, t_cls = self.dino_head(s_cls), self.teacher_dino_head(t_cls) #both [batch, embed_dims]\n        t_cls = ops.softmax((t_cls - self.center_cls) / self.teacher_temp, axis = -1)\n        s_cls = ops.softmax(s_cls/self.student_temp, \n                            axis = -1)\n        t_cls = ops.clip(t_cls, 1e-5, 0.999)\n        s = ops.log_softmax(s_cls, axis = -1)\n        return ops.mean(ops.sum(-t_cls * s, axis = -1))\n        \n    def orthogonality_loss(self):\n        \"\"\"Compute orthogonality loss for cluster centers\"\"\"\n        # cluster_center: [K, D]\n        centers = tf.nn.l2_normalize(self.cluster_function.cluster_center, axis=1)\n        gram = tf.matmul(centers, centers, transpose_b=True)  # [K, K]\n        I = tf.eye(self.num_parts)\n        return tf.reduce_mean(tf.square(gram - I))\n    # ========================================\n    # Training\n    # ========================================\n    \n    def train_step(self, data):\n        \"\"\"Training step\"\"\"\n        images, labels = data #images는 0~255 uint8\n        batch_size, H, W, _ = ops.shape(images)\n        spatial_shape = (H//self.patch_size,W//self.patch_size)\n        global_crops, local_crops = self.create_crops(images) #0~1 사이 텐서로 변환 \n        student_spatial_shape = (self.global_crop_size//self.patch_size, self.global_crop_size//self.patch_size)\n        \n        loss_1_coeff = self.cos_schedule_decrease()\n        loss_2_coeff = 1-loss_1_coeff\n        \n        with tf.GradientTape() as tape:\n            \n            # Global: iBOT + Seg\n            _, h_global, w_global, _ = ops.shape(global_crops[0])\n            \n             \n            t_cls_g1, t_g1 = self.forward_teacher(global_crops[0])\n            t_cls_g2, t_g2 = self.forward_teacher(global_crops[1])\n            \n            s_cls_g1, s_g1, m_g1, student_patch_1 = self.forward_student(global_crops[0], self.mask_ratio)\n            s_cls_g2, s_g2, m_g2, student_patch_2 = self.forward_student(global_crops[1], self.mask_ratio)\n             \n            ibot_loss = self.compute_ibot_loss(s_g1, t_g2, m_g1) + \\\n                        self.compute_ibot_loss(s_g2, t_g1, m_g2)\n            center_new = ops.mean(ops.concatenate([t_g1, t_g2], axis = 0),\n                                  axis = (0,1))\n            cls_dino_loss = self.compute_dino_loss(s_cls = s_cls_g2, t_cls = t_cls_g1) + \\\n                            self.compute_dino_loss(s_cls = s_cls_g1, t_cls = t_cls_g2)\n            center_new_cls = ops.mean(ops.concatenate([t_cls_g1, t_cls_g2], axis = 0),\n                                  axis = 0)\n            # Local: iBOT only\n            for local in local_crops:\n                s_cls_local, s_local, m_local, _ = self.forward_student(local, self.mask_ratio)\n                \n                t_target = t_g1\n                n_local = ops.shape(s_local)[1]\n                n_global = ops.shape(t_target)[1]\n                idx = tf.random.shuffle(tf.range(n_global))[:n_local]\n                t_target = tf.gather(t_target, idx, axis=1)\n                \n                ibot_loss += self.compute_ibot_loss(s_local, t_target, m_local)\n                cls_dino_loss += self.compute_dino_loss(t_cls = t_cls_g1, s_cls = s_cls_local) + \\\n                                self.compute_dino_loss(t_cls = t_cls_g2, s_cls = s_cls_local)\n            ibot_loss /= (2 + len(local_crops))\n            cls_dino_loss /= (2 + len(local_crops))\n            \n            # Segmentation\n            cls_whole, encoded_patches, _ = self.student_vit(images, training = True)\n            encoded_patches = self.forward_dense(encoded_patches)\n            ### Learnable Clustering Layer!! ###\n            mi_loss, h_c, h_c_given_x, cluster_patches, patch_proba_soft = self.cluster_function(encoded_patches)\n            orthogonal_constraints = self.orthogonality_loss()\n            seg_loss = mi_loss + orthogonal_constraints\n            # Classification\n            cls_logits = self.cls_head(cls_whole)\n            cls_loss = ops.mean(keras.losses.sparse_categorical_crossentropy(\n                labels, cls_logits, from_logits=True\n            ))\n            \n            # Total\n            #Loss 1 : CLS level loss = cls_loss + cls_dino_loss\n            # Loss 2 : patch level loss = (1-s_w)iBOT loss + s_w*seg_loss\n            \n            loss_1 = 0.5*(cls_loss + cls_dino_loss)\n            loss_2 = (1-self.seg_weight) * ibot_loss + self.seg_weight*seg_loss\n            loss = loss_1_coeff*loss_1 + loss_2_coeff*loss_2\n            \n        self.center.assign(self.ema_momentum * self.center + (1 - self.ema_momentum) * center_new)\n        self.center_cls.assign(self.ema_momentum * self.center_cls + (1 - self.ema_momentum) * center_new_cls)\n        # Gradients\n        trainable_vars = self.trainable_variables\n        grads = tape.gradient(loss, trainable_vars)\n        \n        # NaN check\n        has_nan = tf.reduce_any([\n            tf.reduce_any(tf.math.is_nan(g)) for g in grads if g is not None\n        ])\n        \n        self.optimizer.apply_gradients(zip(grads, trainable_vars))\n        #EMA updates\n        self.update_teacher()\n        \n        # Metrics\n        self.ibot_tracker.update_state(ibot_loss)\n        self.dino_tracker.update_state(cls_dino_loss)\n        self.seg_tracker.update_state(seg_loss)\n        self.cls_tracker.update_state(cls_loss)\n        self.acc_tracker.update_state(labels, cls_logits)\n        \n        return {\n            'Total_loss': loss,\n            #1. Tokenwise(loss 1 component)\n            'tokenwise_cls': self.cls_tracker.result(),\n            'tokenwise_dino': self.dino_tracker.result(),\n            'tokenwise_acc': self.acc_tracker.result(),\n            \n            #2. patchwise(loss 2 component)\n            'patchwise_ibot': self.ibot_tracker.result(), \n            'patchwise_seg': self.seg_tracker.result(),\n            \"patchwise_MI\" : mi_loss,\n            \"patchwise_center_ortho\" : orthogonal_constraints,\n            \"H(Cluster)\" : h_c,\n            \"H(Cluster|Patch)_lower\" : h_c_given_x,\n\n            # 3. ETC\n            \"is_nan_error\" : keras.ops.mean(has_nan),\n        }\n    \n    # ========================================\n    # Inference\n    # ========================================\n    \n    def call(self, inputs, labels=None, training=False):\n        \"\"\"Inference\"\"\"\n        batch_size, H, W, _ = ops.shape(inputs)\n        spatial_shape = (H//patch_size, W//patch_size)\n        cls, patches, _ = self.teacher_vit(inputs, training=False)\n        \n        # Classification\n        logits = self.cls_head(cls)\n        if labels is None:\n            labels = ops.argmax(logits, axis=-1)\n        \n        # Segmentation\n        patches = self.forward_dense(patches)\n        mi_loss, h_c, h_c_given_x, part_seg, patch_proba_soft = self.cluster_function(patches)\n        # part_seg = [batch_size, n_patches]\n        batch_size, n_patches = ops.shape(part_seg)\n        assert n_patches == (H//patch_size) * (W//patch_size), \"call에서 part_seg이 잘못 배정됨\"\n        grid_size = ops.cast(ops.sqrt(ops.cast(n_patches, \"float32\")\n                                     ),\n                            \"int32\")\n        part_seg = ops.reshape(part_seg, [batch_size, grid_size, grid_size, 1])\n        return part_seg, logits\n    \n    # ========================================\n    # Visualization\n    # ========================================\n    \n    def visualize_results(self, images, labels=None, save_path=None):\n        \"\"\"Visualize segmentation results\"\"\"\n        part_seg, logits = self(images, labels, training=False)\n        preds = ops.argmax(logits, axis=-1)\n        \n        batch_size = int(tf.shape(images)[0])\n        res = int(tf.shape(images)[1])\n        \n        fig, axes = plt.subplots(batch_size, 3, figsize=(12, 4 * batch_size))\n        if batch_size == 1:\n            axes = axes[None, :]\n        \n        cmap = plt.cm.get_cmap('tab10', self.num_parts)\n        \n        for i in range(batch_size):\n            # Original\n            axes[i, 0].imshow(images[i])\n            axes[i, 0].set_title(f'Class: {labels[i].numpy() if labels != None else \"DUMMY\"}')\n            axes[i, 0].axis('off')\n            \n            # Parts\n            axes[i, 1].imshow(part_seg[i], cmap=cmap, vmin=0, vmax=self.num_parts-1)\n            axes[i, 1].set_title(f'Pred label : {int(preds[i])}')\n            axes[i, 1].axis('off')\n            \n            # Overlay\n            axes[i, 2].imshow(images[i])\n            axes[i, 2].imshow(tf.image.resize(part_seg[i], [res,res]), \n                              cmap=cmap, alpha=0.5, vmin=0, vmax=self.num_parts-1)\n            axes[i, 2].set_title('Seg result Upsample and Overlay')\n            axes[i, 2].axis('off')\n        \n        plt.tight_layout()\n        if save_path:\n            plt.savefig(save_path, dpi=150, bbox_inches='tight')\n        \n        return fig\n    \n    @property\n    def metrics(self):\n        return [self.ibot_tracker, self.seg_tracker, self.cls_tracker, self.acc_tracker]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T02:12:34.583272Z","iopub.status.idle":"2025-11-14T02:12:34.583812Z","shell.execute_reply.started":"2025-11-14T02:12:34.583590Z","shell.execute_reply":"2025-11-14T02:12:34.583611Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ========================================\n# Usage Example\n# ========================================\n# Create model\n\nif wandb_configs[\"use_hybrid\"]:\n    teacher = hybrid\n    student = hybrid\nelse:\n    teacher = vit\n    student = vit\nmodel = JointLearnerWithMI(\n        vit_backbone=student, teacher_model = teacher,\n        num_classes=165,\n        num_parts=32,\n        global_crop_size=384,\n        local_crop_sizes=[96, 128, 256],\n        num_local_crops=4,\n        mask_ratio=0.5,\n        masking_strategy='random',\n        seg_weight=0.5,\n    seg_distance = wandb_configs[\"segment_metric\"] #\"cosine\" or \"dot\"\n    )\n\n# Create Callbacks\nwandb.init(project = \"RadImageNet_2stage\",\n          config = wandb_configs)\nprint(\"=======Environmental Configurations=======\\n\")\npprint.pprint(wandb_configs)\n\nwandb_callback = WandbVisualizationCallback(\n    validation_images=imgs,\n    validation_labels=labs,\n    log_freq=100,\n    viz_freq=5000,\n    num_images_to_log=len(imgs)  # Number of images to show in the table\n)\n\nmodel.compile()\n\nfit_history = model.fit(train_radimagenet_ds, epochs = 1, steps_per_epoch = 10000000,\n                       callbacks = [wandb_callback])\n#output_figs = model.visualize_results(images = imgs, labels = labs)\n#visualize_token_attention_map(vit = model.teacher_vit, image = imgs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T02:12:34.585286Z","iopub.status.idle":"2025-11-14T02:12:34.585811Z","shell.execute_reply.started":"2025-11-14T02:12:34.585520Z","shell.execute_reply":"2025-11-14T02:12:34.585538Z"}},"outputs":[],"execution_count":null}]}