{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13532159,"sourceType":"datasetVersion","datasetId":5112865}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport math\n\nfrom typing import List, Dict, Optional, Union, Tuple\nfrom abc import ABC, abstractmethod\n\nimport os, sys\nimport random\nimport pydicom\nimport sklearn\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import AgglomerativeClustering, KMeans\nfrom sklearn.preprocessing import StandardScaler\nimport re\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nseed = 42\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport tensorflow as tf\nimport keras\n\n#keras.mixed_precision.set_global_policy(\"mixed_float16\")\n\nfrom keras import layers, Model\nfrom keras import ops, layers, models, losses, optimizers, metrics\nimport keras_hub\nimport keras_cv\nimport keras_nlp\n\nimport cv2\nfrom skimage.io import imread\nkeras.utils.set_random_seed(seed)\nimport tensorflow_io as tfio\nfrom kaggle_datasets import KaggleDatasets\nimport tensorflow_datasets as tfds\nimport tensorflow_probability as tfp\n\nprint(f\"Tensorflow version : {tf.__version__}\")\ntry:\n    print(f\"Keras version : {keras.__version__}\")\nexcept:\n    pass\n\nfrom keras import Input, Model, ops\nfrom keras.models import load_model\n\nfrom keras.layers import Conv2D, DepthwiseConv2D, Dense, Activation, BatchNormalization, LayerNormalization, MultiHeadAttention, Embedding, Subtract, Add, Multiply, GlobalAveragePooling2D, GlobalAveragePooling1D, LayerNormalization\nfrom keras.utils import load_img, img_to_array\nfrom keras.applications import *\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nfrom sklearn.model_selection import train_test_split\nfrom keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\nfrom tqdm.notebook import tqdm\nimport wandb\ndef wandb_config():\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    try:\n        secret_value_0 = user_secrets.get_secret(\"__gcloud_sdk_auth__\")\n        secret_value_1 = user_secrets.get_secret(\"huggingface_key\")\n        secret_value_2 = user_secrets.get_secret(\"wandb_key\")\n        !wandb login $secret_value_2\n    except:\n        secret_value_0 = user_secrets.get_secret(\"wandb_key\")\n        !wandb login $secret_value_0\n    \ndef auto_select_accelerator():\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n        print(\"Running on TPU:\", tpu.master())\n    except ValueError:\n        tpu = False\n        strategy = tf.distribute.MirroredStrategy() # for GPU or multi-GPU machines\n    print(f\"Running on {strategy.num_replicas_in_sync} replicas\")\n    \n    return tpu, strategy\n\ntpu, strategy = auto_select_accelerator()\n\nimport PIL\nfrom PIL import Image as PILImage\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nimport pprint\nfrom pprint import pprint as pp\nimport wandb\nwandb_config()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"execution":{"execution_failed":"2025-11-13T03:38:26.063Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Hyperparameter setting","metadata":{}},{"cell_type":"code","source":"image_size = res = 512\nbatch_size = 24\nviz = False\npatch_size = 16\nembed_dims = 384\nuse_hybrid = True\nif use_hybrid:\n    interleaved = False\n    n_fourier = 0\n    n_attention = 4\nelse:\n    interleaved = False\n    n_fourier = 0\n    n_attention = 8\nwandb_configs = config = {\n    \"res\": res,\n    \"batch_size\": batch_size,\n    \"patch_size\" : patch_size,\n    \"embed_dims\" : embed_dims,\n    'F_A_interleaved' : interleaved,\n    'n_fourier' : n_fourier,\n    'n_attention' : n_attention,\n    'use_hybrid' : use_hybrid\n}\n\npprint.pprint(wandb_configs)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"execution_failed":"2025-11-13T03:38:26.064Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def _parse_tfrecord(res = res):\n    def parse_tfrecord(tfrecord):\n        features = {'image': tf.io.FixedLenFeature([], tf.string),\n                    'label': tf.io.FixedLenFeature([], tf.int64),\n                    }\n        x = tf.io.parse_single_example(tfrecord, features)\n        image_train = tf.image.decode_jpeg(x['image'], channels=1)\n        image_train = _transform_images(res = res)(image_train)\n        label = tf.cast(x[\"label\"], tf.int32)\n        return (image_train, label)\n    \n    return parse_tfrecord\n\n\ndef _transform_images(res = res):\n    def transform_images(x_train):\n        x_train = tf.image.resize_with_pad(x_train, res, res, antialias = True)\n        x_train = tf.cast(x_train, tf.uint8)\n        return x_train\n    return transform_images\n\ndef load_tfrecord_dataset(tfrecord_name, res = res, batch_size = batch_size, shuffle=True, buffer_size=10240):\n    \"\"\"load dataset from tfrecord\"\"\"\n    raw_dataset = tf.data.TFRecordDataset(tfrecord_name, compression_type = \"GZIP\")\n    raw_dataset = raw_dataset.repeat()\n    if shuffle:\n        raw_dataset = raw_dataset.shuffle(buffer_size=buffer_size)\n    dataset = raw_dataset.map(\n        _parse_tfrecord(),\n        num_parallel_calls=tf.data.AUTOTUNE\n    )\n    dataset = dataset.batch(batch_size, drop_remainder = True)\n    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n    return dataset\n\ntrain_radimagenet_ds = load_tfrecord_dataset(\"/kaggle/input/radimagenet-and-nih-cxr-dataset-tfrecord/RagImageNet_Train_GZIP.tfrecord\")\nval_ds = load_tfrecord_dataset(\"/kaggle/input/radimagenet-and-nih-cxr-dataset-tfrecord/RagImageNet_Test_GZIP.tfrecord\")","metadata":{"trusted":true,"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"execution_failed":"2025-11-13T03:38:26.064Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train = pd.read_csv(\"/kaggle/input/radimagenet-and-nih-cxr-dataset-tfrecord/RadImgNet_train.csv\") ; total_steps = int(len(df_train)//batch_size)\nplt.hist(df_train[\"label\"], bins = range(165))\nplt.title(\"Training dataset label-wise distribution\")\n\npp(\"+=\"*50)\npp(f\"Total Training case : {len(df_train)}\")\npp(\"                                     LABELS\")\ndf_label = pd.read_csv(\"/kaggle/input/radimagenet-and-nih-cxr-dataset-tfrecord/RadImgNet_label_encoding.csv\")\npp(df_label)\npp(\"+=\"*50)\n\nfor img, lab in train_radimagenet_ds.take(1):\n    imgs = img\n    labs = lab","metadata":{"trusted":true,"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"execution_failed":"2025-11-13T03:38:26.064Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if viz:\n    for img, lab in train_radimagenet_ds.take(1):\n        imgs = img\n        labs = lab\n    print(keras.ops.max(imgs))\n    fig, axes = plt.subplots(4,4, figsize = (15,15))\n    axes = axes.flatten()\n    for idx, ax in enumerate(axes):\n        ax.imshow(imgs[idx], cmap = \"bone\")\n        lab_ = int(labs[idx])\n        name = df_label.loc[df_label.index == lab_, 'name'].values[0]\n        ax.set_title(f\"{lab_} : {name}\")\n    plt.show()\n    ","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"execution_failed":"2025-11-13T03:38:26.064Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Stepwise learning\n- Multi stage : SSL(instance level) and classification (Multi task learning) -> WSOL (patch level) -> weakly supervised, or unsupervised segmentation(pixel level)\n  \n- 추가로 구현해야 하는 Additional layer들은 최소한으로 사용하기\n- 모듈은 크게 4가지 작성: ViT module, 그 ViT를 인자로 삼아 training하는 SSL and Classification Trainer, WSOL trainer, Segmentation trainer 모듈\n    - 각자의 모듈에 optimizer 및 learning rate 미리 세팅\n    - ViT module : input as [batch_size, res, res, 1] shape tensor, output as [representation_vector, encoded_patches, attention_weights]\n        - representation vector : [batch_size, embed_dims] shape matrix\n        - encoded patches : [batch_sise, n_patches, embed_dims] shape tensor\n        - attention_weights = list of attnetion weights, attention weight of n-th MHSA layer : [batch_size, n_heads, n_patches, n_patches] shape stochastic tensor.\n    - SSL and classification trainer module : SSL as NNCLR, 두 loss를 결합하여 한 개의 single loss를 반환 (즉, total_loss, ssl_loss, classification_loss, classification_accuracy 반환)\n    - WSOL : label과 encoded_patches 등을 이용해서 진행. \n    - segmentation : pixel level learning, pixel level annotation은 없음. U-Net이나 그 유사 구조를 활용할 경우, learnable weight를 최소화할 것.","metadata":{}},{"cell_type":"markdown","source":"# Stage 0: preparing with helper functions","metadata":{}},{"cell_type":"code","source":"# 2D RoPE, Fourier block, self attention blocks - helper function for (fourier) ViT\n\nclass RoPE2D_KerasNLP(keras.layers.Layer):\n    \"\"\"\n    2D Rotary Position Embedding using keras_nlp\n    \n    keras_nlp.layers.RotaryEmbedding을 2D vision에 확장\n    \"\"\"\n    def __init__(self, \n                 embed_dims: int,\n                 max_wavelength: int = 10000,\n                 scaling_factor: float = 1.0,\n                 **kwargs):\n        super().__init__(**kwargs)\n        \n        assert embed_dims % 4 == 0, \"embed_dims must be divisible by 4\"\n        \n        self.embed_dims = embed_dims\n        self.dim_per_axis = embed_dims // 2\n        \n        # Height용 RoPE (첫 번째 절반 dimensions)\n        self.rope_height = keras_nlp.layers.RotaryEmbedding(\n            max_wavelength=max_wavelength,\n            scaling_factor=scaling_factor,\n            sequence_axis=1,  # Height axis\n            feature_axis=-1\n        )\n        \n        # Width용 RoPE (두 번째 절반 dimensions)\n        self.rope_width = keras_nlp.layers.RotaryEmbedding(\n            max_wavelength=max_wavelength,\n            scaling_factor=scaling_factor,\n            sequence_axis=2,  # Width axis\n            feature_axis=-1\n        )\n    \n    def call(self, x, spatial_shape):\n        \"\"\"\n        Args:\n            x: [batch, num_patches, embed_dims]\n            spatial_shape: (height, width) in patches\n        \n        Returns:\n            x_rope: [batch, num_patches, embed_dims] with RoPE applied\n        \"\"\"\n        batch_size = ops.shape(x)[0]\n        height, width = spatial_shape\n        \n        # Reshape to 2D spatial layout\n        x_2d = ops.reshape(x, [batch_size, height, width, self.embed_dims])\n        \n        # Split features: first half for height, second half for width\n        x_h = x_2d[..., :self.dim_per_axis]\n        x_w = x_2d[..., self.dim_per_axis:]\n        \n        # Apply RoPE separately\n        x_h_rope = self.rope_height(x_h)\n        x_w_rope = self.rope_width(x_w)\n        \n        # Concatenate\n        x_rope_2d = ops.concatenate([x_h_rope, x_w_rope], axis=-1)\n        \n        # Reshape back\n        x_rope = ops.reshape(x_rope_2d, [batch_size, height * width, self.embed_dims])\n        \n        return x_rope\nclass RoPEMultiHeadAttention(layers.MultiHeadAttention):\n    \"\"\"\n    Keras MHA를 상속받아 RoPE를 Q, K에 주입하는 레이어.\n    FlashAttention 등 Keras의 최적화를 그대로 활용.\n    \"\"\"\n    def __init__(self, rope_2d: RoPE2D_KerasNLP, **kwargs):\n        super().__init__(**kwargs)\n        self.rope_2d = rope_2d\n\n    def call(self,\n             query,\n             value,\n             key=None,\n             spatial_shape=None,  # RoPE를 위한 추가 인자\n             attention_mask=None,\n             return_attention_scores=False,\n             training=False):\n        \n        if spatial_shape is None:\n            raise ValueError(\"RoPEMultiHeadAttention을 호출할 때는 'spatial_shape' 인자가 반드시 필요합니다.\")\n\n        # Self-Attention을 위한 기본 로직\n        if key is None:\n            key = value\n\n        # --- RoPE 적용을 위한 핵심 수정 ---\n        \n        # 1. 부모 클래스(MHA)의 Dense 레이어로 Q, K, V를 프로젝션\n        q_proj = self._query_dense(query)\n        k_proj = self._key_dense(key) \n        v_proj = self._value_dense(value)\n\n        # 2. CLS 토큰과 Patch 토큰 분리\n        # (CLS 토큰은 RoPE를 적용하지 않음)\n        q_cls, q_patch = q_proj[:, :1, :], q_proj[:, 1:, :]\n        k_cls, k_patch = k_proj[:, :1, :], k_proj[:, 1:, :]\n        batch_, n_patch_, n_heads_, dims_ = ops.shape(q_patch)\n        \n        # 3. 프로젝션된 Q, K의 Patch 부분에 2D RoPE 적용\n        q_patch_rope = self.rope_2d(ops.reshape(q_patch, [batch_, n_patch_, n_heads_*dims_]), \n                                    spatial_shape=spatial_shape)\n        k_patch_rope = self.rope_2d(ops.reshape(k_patch, [batch_, n_patch_, n_heads_*dims_]), \n                                    spatial_shape=spatial_shape)\n        q_patch_rope = ops.reshape(q_patch_rope, [batch_, n_patch_, n_heads_, dims_])\n        k_patch_rope = ops.reshape(k_patch_rope, [batch_, n_patch_, n_heads_, dims_])\n        # 4. CLS 토큰과 RoPE 적용된 Patch 재결합\n        q_final = ops.concatenate([q_cls, q_patch_rope], axis=1)\n        k_final = ops.concatenate([k_cls, k_patch_rope], axis=1)\n        # V는 RoPE가 적용되지 않은 'v_proj'를 그대로 사용\n        \n        # --- Keras MHA의 나머지 로직 재사용 ---\n        \n        # 5. 최적화된 어텐션 연산 (부모 클래스의 핵심 메서드)\n        attn_output, attn_scores = self._compute_attention(\n            q_final,\n            k_final,\n            v_proj,\n            attention_mask=attention_mask,\n            training=training,\n        )\n        \n        # 6. Head 결합 및 출력 프로젝션 (부모 클래스의 내부 메서드)\n        attn_output = self._output_dense(attn_output)\n        # 8. 결과 반환\n        if return_attention_scores:\n            return attn_output, attn_scores\n        return attn_output\n\n# --- 3. Fourier Block (API 시그니처 수정) ---\nclass FourierTransformBlock(keras.layers.Layer):\n    \"\"\"\n    Fourier Transform Block for Token Mixing\n    \"\"\"\n    def __init__(self, \n                 embed_dims: int,\n                 mlp_ratio: int = 4,\n                 dropout_rate: float = 0.1,\n                 **kwargs):\n        super().__init__(**kwargs)\n        \n        self.embed_dims = embed_dims\n        self.norm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.norm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.mlp = keras.Sequential([\n            layers.Dense(embed_dims * mlp_ratio, activation='gelu'),\n            layers.Dropout(dropout_rate),\n            layers.Dense(embed_dims),\n        ])\n        self.spatial_mlp = keras.Sequential([\n            layers.Dense(embed_dims * mlp_ratio, activation='gelu'),\n            layers.Dropout(dropout_rate),\n            layers.Dense(embed_dims),\n        ])\n        \n    def call(self, inputs, spatial_shape=None, training=False):\n        \"\"\"\n        spatial_shape는 API 호환성을 위해 받지만 사용하지 않습니다.\n        \"\"\"\n        real_part = inputs\n        real_part_fft, im_part_fft = keras.ops.fft((real_part, keras.ops.zeros_like(real_part)))\n        \n        # Real Part\n        real_part_fft = self.norm1(real_part_fft)\n        real_part_fft_ffn = self.mlp(real_part_fft, training=training)\n        real_part_fft = real_part_fft + real_part_fft_ffn\n        real_part_fft = self.norm2(real_part_fft)\n        \n        # Imaginary part\n        im_part_fft = self.norm1(im_part_fft)\n        im_part_fft_ffn = self.mlp(im_part_fft, training=training)\n        im_part_fft = im_part_fft + im_part_fft_ffn\n        im_part_fft = self.norm2(im_part_fft)\n        \n        x = keras.ops.irfft((real_part_fft, im_part_fft))\n        return self.spatial_mlp(x, training=training)\n\n# --- 4. Self Attention Block (RoPEMultiHeadAttention 사용하도록 수정) ---\nclass SelfAttentionBlock(keras.layers.Layer):\n    \"\"\"\n    RoPEMultiHeadAttention을 사용하는 ViT Self-Attention Block\n    \"\"\"\n    def __init__(self,\n                 num_heads: int,\n                 embed_dims: int,\n                 rope_2d: RoPE2D_KerasNLP,  # RoPE 레이어 주입\n                 mlp_ratio: int = 4,\n                 dropout_rate: float = 0.1,\n                 **kwargs):\n        super().__init__(**kwargs)\n        \n        self.num_heads = num_heads\n        self.embed_dims = embed_dims\n        \n        # Layer Normalization\n        self.norm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.norm2 = layers.LayerNormalization(epsilon=1e-6)\n        \n        # (MODIFIED) Keras MHA 대신 RoPEMultiHeadAttention 사용\n        self.mhsa = RoPEMultiHeadAttention(\n            rope_2d=rope_2d,\n            num_heads=num_heads,\n            key_dim=embed_dims // num_heads,\n            dropout=dropout_rate\n        )\n        \n        # MLP\n        self.mlp = keras.Sequential([\n            layers.Dense(embed_dims * mlp_ratio, activation='gelu'),\n            layers.Dropout(dropout_rate),\n            layers.Dense(embed_dims),\n            layers.Dropout(dropout_rate)\n        ])\n    \n    def call(self, x, spatial_shape, training=False):\n        \"\"\"\n        Args:\n            x: [batch, num_patches+1, embed_dims] (with CLS token)\n            spatial_shape: (height, width) RoPE 적용에 필요\n        \"\"\"\n        # Multi-Head Self-Attention\n        x_norm = self.norm1(x)\n        \n        # (MODIFIED) spatial_shape를 RoPEMultiHeadAttention에 전달\n        attn_output, attn_weights = self.mhsa(\n            query=x_norm,\n            value=x_norm,\n            key=x_norm, # Self-attention\n            spatial_shape=spatial_shape,\n            return_attention_scores=True,\n            training=training\n        )\n        x = x + attn_output\n        \n        # MLP\n        x_norm = self.norm2(x)\n        mlp_output = self.mlp(x_norm, training=training)\n        x = x + mlp_output\n        \n        return x, attn_weights\n\n# --- 5. Interleaved Block (API 시그니처 수정 및 CLS 토큰 처리 수정) ---\nclass InterleavedAttentionBlock(keras.layers.Layer):\n    def __init__(self, \n                 embed_dims: int,\n                 num_heads: int = 8, \n                 mlp_ratio: int = 4,\n                 dropout_rate: float = 0.1,\n                 fourier_first = True,\n                 rope_2d = None, # RoPE 레이어 주입\n                 **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dims = embed_dims\n        self.num_heads = num_heads\n        self.mlp_ratio = mlp_ratio\n        self.dropout_rate = dropout_rate\n        self.fourier_first = fourier_first\n        \n        self.mhsa_block = SelfAttentionBlock(num_heads, embed_dims, rope_2d, mlp_ratio, dropout_rate)\n        self.fourier_block = FourierTransformBlock(embed_dims, mlp_ratio, dropout_rate)\n        \n    def call(self, x, spatial_shape, training=False):\n        \"\"\"\n        Fourier Block가 CLS 토큰을 처리하지 않도록 수정\n        \"\"\"\n        cls_token, patches = x[:, :1, :], x[:, 1:, :] # CLS와 Patch 분리\n        \n        if self.fourier_first:\n            patches = self.fourier_block(patches, spatial_shape = spatial_shape, training=training)\n            x = ops.concatenate([cls_token, patches], axis=1) # 재결합\n            x, attn_weights = self.mhsa_block(x, spatial_shape = spatial_shape, training=training)\n        else:\n            x, attn_weights = self.mhsa_block(x, spatial_shape = spatial_shape, training=training)\n            \n            cls_token, patches = x[:, :1, :], x[:, 1:, :] # MHSA 후 다시 분리\n            patches = self.fourier_block(patches, spatial_shape = spatial_shape, training=training)\n            x = ops.concatenate([cls_token, patches], axis=1) # 재결합\n            \n        return x, attn_weights","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"execution_failed":"2025-11-13T03:38:26.064Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# visualize_token_attention_map ; helper for attention weight viz\ndef visualize_token_attention_map(vit, image):\n    res = ops.shape(image)[-2]\n    cls_token, encoded_patches, att_wt_list = vit(image)\n    last_attention_weight = att_wt_list[-1]\n    batch_size, n_heads, L, _ = tf.shape(last_attention_weight) ; n_patches = L-1\n    patch_size = ops.cast(ops.sqrt(ops.cast(n_patches, \"float32\")),\n                         \"int32\")\n    head_colors = ['Blues', 'Oranges', 'Greens', 'Reds', 'Purples', 'Greys', 'YlOrBr', 'YlGnBu']\n    cls_att = last_attention_weight[:,:, 0, 1:] #batch, n_heads, n_patches\n    for b in range(batch_size):\n        fig, axes = plt.subplots(1, 3, figsize = (20,6))\n        \n        axes = axes.flatten()\n        axes[0].set_title(f\"original image {b+1}\")\n        axes[0].imshow(image[b], cmap = \"bone\")\n        axes[1].set_title(\"All head W, quantized\")\n        axes[2].set_title(f\"overlay image {b+1}\")\n        axes[2].imshow(image[b], cmap = \"bone\", alpha = 0.6)\n        #head merge\n        for h in range(n_heads):\n            att_map = cls_att[b, h]\n            att_map = ops.reshape(att_map, [patch_size, patch_size])\n            att_map = (att_map-ops.min(att_map)) / (ops.max(att_map) - ops.min(att_map) + 1e-5)\n            att_map_bin = ops.cast(att_map > ops.mean(att_map),\n                              \"int32\")\n            att_map *= 255\n            att_map_bin *= 255\n            \n            att_map = ops.cast(att_map, \"uint8\")\n            upsampled_map = tf.image.resize(att_map[None, ..., None], [res,res])[0,:,:,0]\n            upsampled_map = ops.cast(upsampled_map, \"uint8\")\n            axes[1].imshow(att_map_bin, cmap=head_colors[h % len(head_colors)], alpha=0.6)\n            axes[2].imshow(upsampled_map, cmap=head_colors[h % len(head_colors)], alpha=0.5)\n    plt.tight_layout()\n    plt.show()\n    flatten_w = ops.reshape(cls_att, [-1,])\n    plt.hist(flatten_w, density = True, histtype = \"stepfilled\", bins = 100)\n    plt.title(\"CLS Attention Weight (last layer) Distribution\")\n    plt.show()","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"execution_failed":"2025-11-13T03:38:26.064Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 6. MedicalViT (RoPE 적용 로직 수정) ---\nclass MedicalViT(keras.Model):\n    \"\"\"\n    Fourier Vision Transformer with Flexible Resolution\n    RoPE가 어텐션 블록 내부의 Q, K에만 적용되도록 수정됨.\n    \"\"\"\n    def __init__(self,\n                 patch_size,\n                 conv_base = None,\n                 interleaved = False, num_interleaved_layers = 6, fourier_first = True,\n                 num_fourier_layers: int = 8,  # N개의 Fourier blocks\n                 num_attention_layers: int = 4,\n                 num_heads: int = 6,\n                 embed_dims: int = 384,\n                 mlp_ratio: int = 2,\n                 dropout_rate: float = 0.1,\n                 **kwargs):\n        super().__init__(**kwargs)\n        \n        self.patch_size = patch_size\n        self.conv_base = conv_base\n        self.num_fourier_layers = num_fourier_layers\n        self.num_attention_layers = num_attention_layers\n        self.embed_dims = embed_dims\n        self.interleaved = interleaved\n        self.num_interleaved_layers = num_interleaved_layers\n        self.fourier_first = fourier_first\n        \n        # Patch Embedding\n        self.patch_embed = layers.Conv2D(embed_dims, \n                                         kernel_size=patch_size, \n                                         strides=patch_size, \n                                         padding='valid', activation = \"gelu\"\n                                        )\n        if self.conv_base:\n            self.conv_base.trainable = True\n            self.patch_embed = layers.Dense(units = embed_dims, activation = 'gelu', name = \"EmbedAfterConv\")\n        # CLS Token\n        self.cls_token = self.add_weight(\n            shape=(1, 1, embed_dims),\n            initializer=keras.initializers.TruncatedNormal(stddev=0.02),\n            trainable=True,\n            name='cls_token'\n        )\n        \n        # 2D RoPE (인스턴스 생성)\n        self.rope_2d = RoPE2D_KerasNLP(\n            embed_dims=embed_dims,\n            max_wavelength=10000,\n            scaling_factor=1.0\n        )\n        \n        # Fourier Transform Blocks (N layers)\n        self.fourier_blocks = []\n        for i in range(self.num_fourier_layers):\n            self.fourier_blocks.append(\n                FourierTransformBlock(\n                    embed_dims=embed_dims,\n                    mlp_ratio=mlp_ratio,\n                    dropout_rate=dropout_rate,\n                    name=f'fourier_block_{i}'\n                )\n            )\n        \n        # Self-Attention Block (RoPE 주입)\n        self.mhsa_blocks = []\n        for i in range(self.num_attention_layers):\n            self.mhsa_blocks.append(SelfAttentionBlock(\n                num_heads=num_heads,\n                embed_dims=embed_dims,\n                rope_2d=self.rope_2d,  # RoPE 레이어 전달\n                mlp_ratio=mlp_ratio,\n                dropout_rate=dropout_rate,\n                name=f'attention_block_{i}'\n            ))\n            \n        if self.interleaved : \n            self.interleaved_blocks = []\n            for i in range(self.num_interleaved_layers):\n                self.interleaved_blocks.append(InterleavedAttentionBlock(\n                    embed_dims = embed_dims,\n                    num_heads = num_heads,\n                    mlp_ratio = mlp_ratio,\n                    dropout_rate = dropout_rate,\n                    fourier_first = self.fourier_first,\n                    rope_2d=self.rope_2d  # RoPE 레이어 전달\n                  )\n                )\n        \n        # Final Normalization\n        self.norm = layers.LayerNormalization(epsilon=1e-6, name='final_norm')\n\n    def call(self, inputs, training=False):\n        \"\"\"\n        Forward pass - RoPE가 `x`에서 제거되고 블록 내부로 이동\n        \"\"\"\n        batch_size = ops.shape(inputs)[0]\n        input_height = ops.shape(inputs)[1]\n        input_width = ops.shape(inputs)[2]\n        \n        # Compute spatial dimensions\n        patch_height = input_height // self.patch_size\n        patch_width = input_width // self.patch_size\n        num_patches = patch_height * patch_width\n        spatial_shape = (patch_height, patch_width)\n        \n        # 1. Patch Embedding (순수 시맨틱)\n        if self.conv_base:\n            x = self.conv_base(inputs/255)\n            x = self.patch_embed(x)\n            batch_size, patch_height, patch_width, d_ = ops.shape(x)\n            num_patches = patch_height * patch_width\n            spatial_shape = (patch_height, patch_width)\n        else:\n            x = self.patch_embed(inputs/255)  # [batch, h//p, w//p, embed_dims]\n        x = ops.reshape(x, [batch_size, num_patches, self.embed_dims])\n        \n        # --- XX RoPE 적용 제거 (가장 큰 수정) XX ---\n        # x = self.rope_2d(x, spatial_shape=spatial_shape)\n        \n        cls_tokens = ops.repeat(self.cls_token, batch_size, axis=0)\n        attention_weights = []\n        \n        if self.interleaved:\n            x = ops.concatenate([cls_tokens, x], axis=1)\n            for interleaved_attention in self.interleaved_blocks:\n                # spatial_shape와 training 전달\n                x, att_wt = interleaved_attention(x, spatial_shape=spatial_shape, training=training)\n                attention_weights.append(att_wt)\n        else:\n            # 3. Fourier Transform Blocks (CLS 토큰 없이)\n            # spatial_shape와 training 전달\n            for fourier_block in self.fourier_blocks:\n                # (Fourier는 CLS 토큰을 처리하지 않음)\n                x = fourier_block(x, spatial_shape=spatial_shape, training=training)\n            \n            # 4. Add CLS token\n            x = ops.concatenate([cls_tokens, x], axis=1)\n            \n            # 5. Self-Attention Block\n            # spatial_shape와 training 전달\n            for mhsa_block in self.mhsa_blocks:\n                x, att_wt = mhsa_block(x, spatial_shape=spatial_shape, training=training)\n                attention_weights.append(att_wt)\n        \n        # 6. Final Normalization\n        x = self.norm(x)\n        \n        # 7. Extract outputs\n        feature_vector = x[:, 0, :]  # CLS token [batch, embed_dims]\n        encoded_patches = x[:, 1:, :]  # Patches [batch, num_patches, embed_dims]\n        \n        return feature_vector, encoded_patches, attention_weights","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-13T03:38:26.064Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"> Unsupervised image segmentation?\n\n- pretrained ViT를 vit, input medical images를 img라 하자. vit(img)는 [cls_token, encoded_patches, ]야. 목적은 encoded_patches에서 unsupervised semantic segmentation을 수행하는 것임. \n\n- encoded_patches shape을 [batch_size, n_patches, embed_dims]라 하자. encoded_patch에 cluster function C를 가해 agglomerative clustering, k-means clustering 혹은 learnable parameter를 사용하여 각 patch가 1, 2, ..., K 개의 category 중 한 개에 속하도록 만들 거야.\n즉 cluster_patches = C(encoded_patches) 는 [batch_size, n_patches] shape tensor고 각 원소는 1, 2, ..., K 중 한 개의 원소야. clustering 시 cosine similarity를 distance로 활용함.\ncluster_patches의 예시는 [[1,2,10,9,4,2,4,1,...], [2,3,7,9,10,1,12,...], ...]야. 이 discrete한 tensor를  확률분포로 변환할 건데 (변환 함수를 f_d라 하자)\ncluster_proba = f_d(cluster_patches)는 category 1, 2, 3, ..., K 각각의 확률질량값을 나타내. 즉  [P(category=1), P(category=2), ...]야. 이 다음, 가장 P 값이 높은 것부터 sort하여 결과값을 반환해 : [0.7, 0.08, 0.05, 0.01, ....]\ncluster_proba shape은 [batch_size, n_patches]야. keras.ops.sum(cluster_integer, axis = 1)은 [batch_size,] shape인 1로 이루어진 벡터야.\n\n- unsupervised segmentation loss는 cluster integer matrix에서 작동하는데, 이미지의 패치들이 1)한 카테고리에만 배정(collapse)되어서도 안 되고, 2)K개의 카테고리에 아무렇게나(즉 너무 uniform하게) 배치되어도 안 돼 (trivial solution).\n1을 위해서 KL_divergence(uniform 분포, cluster_proba)를 최소화하고, 2를 위해서 KL_divergence(target_dist, cluster_proba)를 최소화한다.\n이 때 target_dist는 a) 감마 분포, b) 표준정규분포(단, x>0로 제한) 등을 생각할 수 있다 (배경이 가장 많고, 그 다음은 category A, 그 다음은 Category B, ...)\ntarget_dist를 특정 분포에서 Sampling하여 KL divergence를 구할 수 있어야 한다","metadata":{}},{"cell_type":"code","source":"def create_target_distribution(k, method='gamma'): # 기본값을 gamma로 변경\n    \"\"\"\n    정렬된 목표 분포(target_dist)를 생성합니다.\n    \"\"\"\n    if method == 'gamma':\n        # Gamma(concentration < 1)는 0 근처에서 PDF가 가장 높음\n        lin = ops.linspace(0.1, 10.0, k) # 0을 피하기 위해 0.1에서 시작\n        dist = tfp.distributions.Gamma(concentration=0.75, rate=0.05) # concentration < 1\n        target_unnormalized = dist.prob(lin)\n        \n    elif method == \"trunc_normal\": # trunc_normal\n        # 0에서 감소하는 분포를 위해 loc=low\n        lin = ops.linspace(0.0, 3.0, k)\n        dist = tfp.distributions.TruncatedNormal(loc=0.0, scale=1.0, low=0.0, high=3.0) # high를 3.0으로 명시\n        target_unnormalized = dist.prob(lin)\n    elif method in ['zip', 'power', 'zipfian']:\n        print(method)\n        indices = ops.arange(k, dtype=\"float32\") + 1.0\n        target_unnormalized = 1.0 / ops.power(indices, 0.75)\n        return target_unnormalized / ops.sum(target_unnormalized)\n    # 요청대로 가장 P값이 높은 것부터 정렬\n    target_sorted = ops.sort(target_unnormalized)[::-1]\n    \n    # 확률분포로 정규화\n    target_dist = target_sorted / ops.sum(target_sorted)\n    return target_dist\ndist = create_target_distribution(k = 64, method = 'power')\nplt.hist(dist, bins = 64)\nplt.title(\"Target distribution(power) visualize\")\nplt.show()\n\ndist = create_target_distribution(k = 64, method = 'gamma')\nplt.hist(dist, bins = 64)\nplt.title(\"Target distribution(gamma) visualize\")\nplt.show()\n\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"execution_failed":"2025-11-13T03:38:26.064Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# How to make Encoded patches to K-categorical probability?\n\n- K-means 혹은 agglomerative clustering 진행; 이 clustering 연산을 f_d라고 하고, 코드에서는 ClusteringLayer로 구현함\n- cluster 결과 각 batch내 patch들이 K개 카테고리에 속할 확률을 알 수 있음 -> (Batch_size, n_patches, K) shape\n- 위 과정을 미분가능하게 만들어주는 것이 STEProbabilityLayer;\n> STE layer?\n \n- STEProbabilityLayer는 **순전파(Forward Pass)**와 역전파(Backward Pass) 때 서로 다른 계산을 하도록 \"속임수(trick)\"를 씁니다.\n- 이 레이어는 patch_proba_soft ([B, N, K])를 입력으로 받습니다.\n    - A. 순전파 (Forward Pass): \"이산적인 결정\":\n    - 순전파 시에는 우리가 원하는 대로 이산적인 argmax 결과를 사용합니다. cluster_patches_indices = ops.argmax(patch_proba_soft, axis=-1) 을 통해 \"Soft\" 확률([0.9, 0.1])을 \"Hard\" 결정(0)으로 바꿉니다;\n        - 출력 1: cluster_patches ([B, N]): 이산적인 인덱스 맵 (시각화 등에 사용 가능).\n          - one_hot_hard = tf.one_hot(cluster_patches_indices, ...) : 0을 [1, 0, 0, ...] 과 같은 one-hot 벡터로 만듭니다. (shape [B, N, K])\n\n    - B. 마법의 STE 공식 (순전파 + 역전파)\n\n> one_hot_ste = tf.stop_gradient(one_hot_hard - patch_proba_soft) + patch_proba_soft\n\n    순전파: tf.stop_gradient는 순전파 시 아무것도 안 합니다.\n\n    - (one_hot_hard - patch_proba_soft) + patch_proba_soft\n    - patch_proba_soft가 소거되어 결국 one_hot_hard만 남습니다.\n\n> 결론: 순전파 시에는 one_hot_hard ([B, N, K])가 사용됩니다.\n\n    - 역전파: tf.stop_gradient는 괄호 안의 모든 연산에 대한 그래디언트(기울기)를 강제로 0으로 만듭니다.\n    - 그래디언트 관점에서 tf.stop_gradient(...) 부분은 \"상수(constant)\" 취급됩니다. (기울기 0)\n    - 오직 + patch_proba_soft 부분만 그래디언트가 살아남아 뒤로 전달됩니다.\n    - 결론: 역전파 시에는 patch_proba_soft의 그래디언트가 사용됩니다.","metadata":{}},{"cell_type":"code","source":"class STEProbabilityLayer(keras.layers.Layer):\n    \"\"\"Soft-assignment([B, N, K]) -> cluster_proba([B, K]), cluster_patches([B, N])\"\"\"\n    def __init__(self, k, **kwargs):\n        super().__init__(**kwargs)\n        self.k = k\n\n    def call(self, patch_proba_soft):\n        # patch_proba_soft shape: [B, N, K]\n        \n        # 순전파용 Hard assignment\n        cluster_patches_indices = ops.argmax(patch_proba_soft, axis=-1) # [B, N]\n        one_hot_hard = tf.one_hot(cluster_patches_indices, depth=self.k, dtype=tf.float32)\n\n        # STE: 순전파(hard)와 역전파(soft) 분리\n        one_hot_ste = tf.stop_gradient(one_hot_hard - patch_proba_soft) + patch_proba_soft\n\n        # f_d: cluster_proba 계산\n        cluster_counts_ste = ops.sum(one_hot_ste, axis=1) # [B, K]\n        n_patches = ops.shape(patch_proba_soft)[1]\n        cluster_proba = cluster_counts_ste / ops.cast(n_patches, dtype=tf.float32)\n\n        return cluster_proba, cluster_patches_indices, patch_proba_soft","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"execution_failed":"2025-11-13T03:38:26.064Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ClusteringLayer(keras.layers.Layer):\n    def __init__(self, k, n_iterations=3, temperature=0.07, method='soft_kmeans',\n                 spatial_scale: float = 0.1, # 좌표의 중요도 조절\n                 distance_metric: str = 'l2', # (NEW) 거리 척도 선택 ('cosine' or 'l2')\n                 **kwargs):\n        super().__init__(**kwargs)\n        if method not in ['soft_kmeans', 'soft_agglomerative', 'kmeans', 'agglomerative']:\n            raise ValueError(\"지원되지 않는 method입니다.\")\n        if distance_metric not in ['cosine', 'cos_sim', 'cos',\n                                   'l2', 'L2', 'euclidian']: # (NEW) 거리 척도 검증\n             raise ValueError(\"distance_metric은 'cosine', 'cos_sim', 'cos', 'l2', 'L2', 또는 'euclidian'여야 합니다.\")\n            \n        self.k = k\n        self.n_iterations = n_iterations\n        self.temperature = temperature\n        self.method = method\n        self.spatial_scale = spatial_scale\n        self.distance_metric = distance_metric # (NEW)\n        self.ste_layer = STEProbabilityLayer(self.k)\n        \n    def build(self, input_shape):\n        super().build(input_shape)\n\n    # --- 좌표 관련 헬퍼 (변경 없음) ---\n    def _create_coordinate_grid(self, batch_size, spatial_shape):\n        height, width = spatial_shape\n        y_coords = ops.linspace(-1.0, 1.0, height)\n        x_coords = ops.linspace(-1.0, 1.0, width)\n        y_grid, x_grid = tf.meshgrid(y_coords, x_coords, indexing='ij')\n        coords_2d = ops.stack([y_grid, x_grid], axis=-1)\n        coords_1d = ops.reshape(coords_2d, (height * width, 2))\n        coords_1d_expanded = ops.expand_dims(coords_1d, axis=0)\n        return ops.repeat(coords_1d_expanded, batch_size, axis=0)\n\n    def _prepare_spatial_features(self, encoded_patches, spatial_shape):\n        batch_size = ops.shape(encoded_patches)[0]\n        # 시맨틱 특징 정규화 (코사인 유사도 사용 시 중요)\n        # L2 사용 시에는 정규화하지 않는 것이 벡터 크기 정보를 보존하는 데 유리할 수 있음\n        # 여기서는 우선 정규화 유지 (사용자가 필요시 수정 가능)\n        if self.distance_metric in ['cosine', 'cos_sim', 'cos']:\n            patches_norm = ops.normalize(encoded_patches, axis=-1)\n        else:\n            patches_norm = encoded_patches\n        coords = self._create_coordinate_grid(batch_size, spatial_shape = spatial_shape)\n        coords_scaled = coords * self.spatial_scale\n        features_with_coords = ops.concatenate([patches_norm, coords_scaled], axis=-1)\n        return features_with_coords\n\n    # --- 거리/유사도 계산 헬퍼 ---\n    def _cosine_similarity(self, x, y, normalize=True):\n        \"\"\" 코사인 유사도 계산 (ops.normalize 내장) \"\"\"\n        # x: [B, N, D+2], y: [K, D+2] or [B, N, D+2] or [B, K, D+2]\n        if normalize:\n            x_norm = ops.normalize(x, axis=-1)\n            y_norm = ops.normalize(y, axis=-1)\n        else: # 이미 정규화된 경우\n             x_norm = x\n             y_norm = y\n\n        # y의 형태에 따라 einsum 경로 분기\n        if len(ops.shape(y)) == 2: # [K, D+2] (K-means 센터)\n            sim = ops.einsum('bnd,kd->bnk', x_norm, y_norm)\n        elif len(ops.shape(y)) == 3 and ops.shape(x)[1] == ops.shape(y)[1]: # [B, N, D+2] (Agglo 패치-패치)\n             y_transposed = ops.transpose(y_norm, (0, 2, 1))\n             sim = ops.einsum('bnd,bdn->bnn', x_norm, y_transposed) # [B, N, N]\n        elif len(ops.shape(y)) == 3 and ops.shape(y)[1] == self.k: # [B, K, D+2] (Agglo 초기 센터)\n             y_transposed = ops.transpose(y_norm, (0, 2, 1))\n             sim = ops.einsum('bnd,bdk->bnk', x_norm, y_transposed) # [B, N, K]\n        else:\n             raise ValueError(f\"지원되지 않는 y shape: {ops.shape(y)}\")\n        return sim\n\n    def _euclidean_distance_squared(self, x, y):\n        \"\"\" 제곱 유클리드 거리 계산 \"\"\"\n        # x: [B, N, D+2], y: [K, D+2] or [B, N, D+2] or [B, K, D+2]\n        x_sq = ops.sum(ops.square(x), axis=-1, keepdims=True) # [B, N, 1]\n\n        # y의 형태에 따라 계산 분기\n        if len(ops.shape(y)) == 2: # [K, D+2] (K-means 센터)\n            y_sq = ops.sum(ops.square(y), axis=-1, keepdims=True) # [K, 1]\n            y_sq_transposed = ops.transpose(y_sq, (1, 0)) # [1, K]\n            xy = ops.einsum('bnd,kd->bnk', x, y)\n            dist_sq = x_sq - 2 * xy + y_sq_transposed # [B, N, K]\n        elif len(ops.shape(y)) == 3 and ops.shape(x)[1] == ops.shape(y)[1]: # [B, N, D+2] (Agglo 패치-패치)\n             y_sq = ops.sum(ops.square(y), axis=-1, keepdims=True) # [B, N, 1]\n             y_sq_transposed = ops.transpose(y_sq, (0, 2, 1)) # [B, 1, N]\n             xy = ops.einsum('bnd,bmd->bnm', x, y) # [B, N, N]\n             dist_sq = x_sq - 2 * xy + y_sq_transposed # [B, N, N]\n        elif len(ops.shape(y)) == 3 and ops.shape(y)[1] == self.k: # [B, K, D+2] (Agglo 초기 센터)\n             y_sq = ops.sum(ops.square(y), axis=-1, keepdims=True) # [B, K, 1]\n             y_sq_transposed = ops.transpose(y_sq, (0, 2, 1)) # [B, 1, K]\n             xy = ops.einsum('bnd,bkd->bnk', x, y) # [B, N, K]\n             dist_sq = x_sq - 2 * xy + y_sq_transposed # [B, N, K]\n        else:\n             raise ValueError(f\"지원되지 않는 y shape: {ops.shape(y)}\")\n\n        return ops.maximum(dist_sq, 0.0) # 수치적 안정성\n\n    # --- 클러스터링 로직 ---\n    def _soft_kmeans(self, spatial_features):\n        b, n, d_plus_2 = ops.shape(spatial_features)\n        patches_flat = ops.reshape(spatial_features, (b * n, d_plus_2))\n        indices = tf.random.shuffle(tf.range(b * n))[:self.k]\n        centers = tf.gather(patches_flat, indices) # [K, D+2]\n\n        patch_proba_soft = None\n        for _ in range(self.n_iterations):\n            # [E-Step] 유사도 또는 거리 계산\n            if self.distance_metric in ['cosine', 'cos', 'cos_sim']:\n                # 코사인 유사도는 클수록 좋음\n                sim = self._cosine_similarity(spatial_features, centers) # [B, N, K]\n                patch_proba_soft = ops.softmax(sim / self.temperature, axis=-1)\n            else: # 'l2'\n                # L2 거리는 작을수록 좋음 (음수 취함)\n                dist_sq = self._euclidean_distance_squared(spatial_features, centers) # [B, N, K]\n                patch_proba_soft = ops.softmax(-dist_sq / self.temperature, axis=-1)\n\n            # [M-Step] 센터 업데이트 (동일)\n            R_sum = ops.transpose(ops.sum(patch_proba_soft, axis=1, keepdims=True), (0, 2, 1))\n            R_T = ops.transpose(patch_proba_soft, (0, 2, 1))\n            weighted_sum = ops.einsum('bkn,bnd->bkd', R_T, spatial_features)\n            centers_batch = weighted_sum / (R_sum + 1e-6)\n            centers = ops.mean(centers_batch, axis=0) # [K, D+2]\n        return patch_proba_soft\n\n    def _soft_agglomerative(self, spatial_features):\n        b, n, d_plus_2 = ops.shape(spatial_features)\n        \n        # 1. 유사도/거리 그래프(W) 생성\n        if self.distance_metric  in ['cosine', 'cos', 'cos_sim']:\n            W_sim = self._cosine_similarity(spatial_features, spatial_features) # [B, N, N]\n            W = ops.softmax(W_sim / self.temperature, axis=-1)\n        else: # 'l2'\n            W_dist_sq = self._euclidean_distance_squared(spatial_features, spatial_features) # [B, N, N]\n            W = ops.softmax(-W_dist_sq / self.temperature, axis=-1)\n\n        # 2. 초기 확률(P_t) 생성\n        indices = tf.random.shuffle(tf.range(n))[:self.k]\n        centers_0 = tf.gather(spatial_features, indices, axis=1) # [B, K, D+2]\n        \n        if self.distance_metric in ['cosine', 'cos', 'cos_sim']:\n            sim_0 = self._cosine_similarity(spatial_features, centers_0) # [B, N, K]\n            P_t = ops.softmax(sim_0 / self.temperature, axis=-1)\n        else: # 'l2'\n            dist_0_sq = self._euclidean_distance_squared(spatial_features, centers_0) # [B, N, K]\n            P_t = ops.softmax(-dist_0_sq / self.temperature, axis=-1)\n\n        # 3. 확산 반복 (동일)\n        for _ in range(self.n_iterations):\n            P_logits = ops.einsum('bnj,bjk->bnk', W, P_t)\n            P_t = ops.softmax(P_logits / (self.temperature * 0.5), axis=-1)\n        return P_t\n\n    # --- call 메서드 (변경 없음) ---\n    def call(self, encoded_patches, spatial_shape):\n        spatial_features = self._prepare_spatial_features(encoded_patches, spatial_shape)\n        \n        if self.method == 'soft_kmeans' or self.method == 'kmeans':\n            patch_proba_soft = self._soft_kmeans(spatial_features)\n        elif self.method == 'soft_agglomerative' or self.method == 'agglomerative':\n            patch_proba_soft = self._soft_agglomerative(spatial_features)\n        else:\n            raise ValueError(f\"'{self.method}'는 지원되지 않는 method입니다.\")\n            \n        cluster_proba, cluster_patches, patch_proba_soft = self.ste_layer(patch_proba_soft)\n        \n        return cluster_proba, cluster_patches, patch_proba_soft","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"execution_failed":"2025-11-13T03:38:26.064Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"> Learnable clustering layer\n\n- Cluster Prototype [K, embed_dims] shape tensor, learnable, EMA update\n    - K개 클러스터의 embed space에서의 대표 벡터를 지정\n    - encoded patches와 distance 측정하여 clustering\n    - 1 step 후 EMA로 update\n- cluster layer에 loss function 넣고,\n    - clustering loss, 각 entropy 반환하여 모니터링","metadata":{}},{"cell_type":"code","source":"class LearnableClusteringLayer(keras.layers.Layer):\n    def __init__(self, k, embed_dims = embed_dims,\n                 temperature=0.07,\n                 distance_metric: str = 'l2', # (NEW) 거리 척도 선택 ('cosine' or 'l2' or 'dot')\n                 **kwargs):\n        super().__init__(**kwargs)\n        if distance_metric not in ['cosine', 'cos_sim', 'cos', #<- Cosine similarity\n                                   'l2', 'L2', 'euclidean',#<- Euclidean distance\n                                  \"dot\"]: \n             raise ValueError(\"distance_metric은 'cosine', 'cos_sim', 'cos', 'l2', 'L2', 'euclidean' 또는 'dot' 여야 합니다.\")\n            \n        self.k = k\n        self.embed_dims = embed_dims\n        self.temperature = temperature\n        self.distance_metric = distance_metric # L2 or cosine\n        self.cluster_center = self.add_weight(\n                                    shape=(k, self.embed_dims),\n                                    initializer=keras.initializers.Orthogonal(),\n                                    trainable=True,\n                                    name='cluster_center'\n                                )\n    def build(self, input_shape):\n        super().build(input_shape)\n\n    # --- 거리/유사도 계산 헬퍼 ---\n    def _dot_similarity(self, patches):\n        dim = ops.shape(self.cluster_center)[-1]\n        sim = ops.einsum(\"bnd, kd -> bnk\", patches, self.cluster_center)\n        sim /= ops.sqrt(dim) #scaled dot-product\n        return sim\n    def _cosine_similarity(self, patches, normalize=True):\n        \"\"\" 코사인 유사도 계산 (ops.normalize 내장) \"\"\"\n        # x: [B, N, D] shape tensor y: [K,D] shape tensor \n        x = patches\n        y = self.cluster_center\n        if normalize:\n            x_norm = ops.normalize(x, axis=-1)\n            y_norm = ops.normalize(y, axis=-1)\n        else: # 이미 정규화된 경우\n             x_norm = x\n             y_norm = y\n\n        sim = ops.einsum(\"bnd, kd -> bnk\", x_norm, y_norm)\n        return sim\n\n    def _euclidean_distance_squared(self, patches):\n        \"\"\" 제곱 유클리드 거리 계산 \"\"\"\n        # x: [B, N, D] shape feature tensor y: [K,D] shape cluster center tensor \n        # after broadcasting : x [B, N, 1, D] // y: [1, 1, K, D]\n        x = patches\n        y = self.cluster_center\n        batch_size, n, _ = ops.shape(x)\n        k, _ = ops.shape(y)\n        \n        x = ops.expand_dims(x, axis = 2)\n        y = ops.expand_dims(y, axis = [0,1])\n        distance = ops.sum(ops.square(x-y),\n                           axis = -1) #[B, N, K]\n        assert (batch_size, n, k) == ops.shape(distance), \"유클리드 거리 계산 내 에러\"\n        return distance\n\n    # call 시 mi loss, segment_per_patches, patch proba soft 리턴\n    # segment, soft proba는 similarity나 distance의 softmax.\n    def compute_mi_loss(self, soft_proba):\n        \n        soft_proba = ops.clip(soft_proba, 1e-6, 1.0)\n        \n        batch_size, n_patches, n_clusters = ops.shape(soft_proba)\n        p_c = ops.mean(soft_proba, axis = [0,1]) #K length vector\n        p_c = p_c/ops.sum(p_c)\n        h_c = -ops.sum(p_c*ops.log(p_c))\n\n        entropy_per_patch = -ops.sum(soft_proba * ops.log(soft_proba),\n                                     axis = -1) #batch_size, n_patches shape matrix\n        h_c_given_x = ops.mean(entropy_per_patch)\n        mi = h_c - h_c_given_x\n        return -mi, h_c, h_c_given_x\n        \n    def call(self, encoded_patches):\n        if self.distance_metric in [\"L2\", 'l2', 'euclidian']:\n            sim_matrix = self._euclidean_distance_squared(encoded_patches)\n        elif self.distance_metric == \"dot\":\n            sim_matrix = self._dot_similarity(encoded_patches) \n        else:\n            sim_matrix = self._cosine_similarity(encoded_patches)\n        sim_matrix /= self.temperature\n        #sim_matrix : [batch_size, N_patches, K]\n        patch_proba_soft = ops.softmax(sim_matrix, axis = -1) #[batch_size, N_patches, K] shape stochastic tensor\n        cluster_patches = ops.argmax(patch_proba_soft, axis = -1) #[batch_size, N_patches] shape integer matrix\n        mi_loss, h_c, h_c_given_x = self.compute_mi_loss(patch_proba_soft)\n        return mi_loss, h_c, h_c_given_x, cluster_patches, patch_proba_soft #cluster_patches :[batch_Size, N_patches]","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-13T03:38:26.064Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_unsupervised_loss(cluster_proba, target_dist, uniform_dist):\n    \"\"\" Anti-collapse 및 Anti-trivial Loss 계산 \"\"\"\n    kl_loss_fn = keras.losses.KLDivergence(reduction='none')\n    cluster_proba_safe = ops.clip(cluster_proba, 1e-6, 1.0)\n    \n    # Loss 1 (Anti-collapse): KL(uniform || cluster_proba)\n    loss_1 = kl_loss_fn(ops.expand_dims(uniform_dist, 0), cluster_proba_safe)\n    \n    # Loss 2 (Anti-trivial): KL(target_dist || sorted(cluster_proba))\n    cluster_proba_sorted = tf.sort(cluster_proba_safe, axis=-1, direction='DESCENDING')\n    loss_2 = kl_loss_fn(ops.expand_dims(target_dist, 0), cluster_proba_sorted)\n    \n    return ops.mean(loss_1), ops.mean(loss_2)","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"execution_failed":"2025-11-13T03:38:26.065Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ViT setting","metadata":{}},{"cell_type":"code","source":"conv_base = keras.applications.ConvNeXtTiny(input_shape = (None,None,1),\n                                               include_preprocessing = False,\n                                               include_top = False,\n                                               weights = None\n                                               )\nconv_input = conv_base.input\nconv_output = conv_base.get_layer(\"convnext_tiny_stage_2_block_8_identity\").output\nconv_stem = keras.Model(conv_input, conv_output,\n                           name = \"ConvNeXtTiny\")\nconv_stem.trainable = True","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-13T03:38:26.065Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"hybrid = MedicalViT(conv_base = conv_stem,\n                    patch_size = patch_size, embed_dims = embed_dims,\n                 num_fourier_layers = n_fourier, num_attention_layers = n_attention,\n                 interleaved = interleaved, num_interleaved_layers= 6, \n                     )\nhybrid.trainable = True\nvit = MedicalViT(patch_size = patch_size, embed_dims = embed_dims,\n                 num_fourier_layers = n_fourier, num_attention_layers = n_attention,\n                 interleaved = interleaved, num_interleaved_layers= 6, \n                     )","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-13T03:38:26.065Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Output Visualization helpers","metadata":{}},{"cell_type":"code","source":"#GradCAM compute\nfrom io import BytesIO\nfrom PIL import Image\n\nclass ViTGradCAM:\n    \"\"\"\n    GradCAM for Vision Transformer\n    - Handles patch-based outputs: [B, N, D]\n    - Vectorized for minimal loops\n    \"\"\"\n    def __init__(self, model):\n        self.model = model\n        self.vit = model.student_vit  # Use student for GradCAM\n        self.cls_head = model.cls_head\n    \n    def get_heatmap(self, images, class_indices=None):\n        \"\"\"\n        Computes GradCAM heatmaps for a batch of images.\n        \n        Args:\n            images: Tensor of shape [B, H, W, C].\n            class_indices: [B] tensor of target class indices. \n                           If None, uses the predicted class for each image.\n                           \n        Returns:\n            heatmaps: [B, H, W] GradCAM heatmaps.\n            preds: [B] predicted class indices.\n        \"\"\"\n        images = tf.cast(images, tf.float32)\n        \n        with tf.GradientTape() as tape:\n            tape.watch(images)\n            # Get patch embeddings (last layer before global average pooling)\n            cls_token, patch_embeddings, _ = self.vit(images, training=False)\n            \n            # Use CLS token for classification\n            logits = self.cls_head(cls_token)\n            \n            preds = tf.argmax(logits, axis=-1, output_type=tf.int32)\n            \n            if class_indices is None:\n                # If no class specified, use predicted class\n                target_indices = preds\n            else:\n                target_indices = tf.cast(class_indices, tf.int32)\n            \n            # Get the score for the target class\n            batch_size = tf.shape(images)[0]\n            gather_indices = tf.stack([tf.range(batch_size), target_indices], axis=1)\n            target_scores = tf.gather_nd(logits, gather_indices)\n        \n        # Gradients of the target score w.r.t. patch embeddings\n        grads = tape.gradient(target_scores, patch_embeddings)  # [B, N, D]\n        \n        # Global average pooling of gradients (weights for each patch)\n        weights = tf.reduce_mean(grads, axis=-1)  # [B, N]\n        \n        # Weighted sum of patch embeddings (CAM)\n        # Remove CLS token from patches and weights before multiplying\n        cam = tf.einsum('bn,bnd->bn', weights, patch_embeddings)  # [B, N]\n        cam = tf.nn.relu(cam)\n        \n        # Reshape to grid\n        num_patches_side = int(np.sqrt(cam.shape[1]))\n        cam_grid = tf.reshape(cam, [-1, num_patches_side, num_patches_side])\n        \n        # Upsample to original image size\n        H, W = tf.shape(images)[1], tf.shape(images)[2]\n        heatmap = tf.image.resize(cam_grid[..., tf.newaxis], [H, W], method='bicubic')\n        heatmap = tf.squeeze(heatmap, axis=-1)\n        \n        # Normalize per image\n        min_val = tf.reduce_min(heatmap, axis=[1, 2], keepdims=True)\n        max_val = tf.reduce_max(heatmap, axis=[1, 2], keepdims=True)\n        heatmap = (heatmap - min_val) / (max_val - min_val + 1e-8)\n        \n        return heatmap, preds\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-13T03:38:26.065Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class WandbVisualizationCallback(keras.callbacks.Callback):\n    \"\"\"\n    Logs metrics and visualizations to WandB.\n    - Logs scalars every `log_freq` steps.\n    - Logs a table of visualizations every `viz_freq` steps.\n    \"\"\"\n    def __init__(self, \n                 validation_images, \n                 validation_labels=None, \n                 log_freq=100, \n                 viz_freq=1000, \n                 num_images_to_log=4):\n        super().__init__()\n        self.val_images = validation_images\n        self.val_labels = validation_labels\n        self.log_freq = log_freq\n        self.viz_freq = viz_freq\n        self.num_images = min(num_images_to_log, validation_images.shape[0])\n        \n        # Prepare data slice for visualization\n        self.viz_images = self.val_images[:self.num_images]\n        if self.val_labels is not None:\n            self.viz_labels = self.val_labels[:self.num_images]\n        else:\n            self.viz_labels = None\n            \n        self.gradcam_computer = None\n\n    def on_train_batch_end(self, batch, logs=None):\n        # Log metrics every log_freq steps\n        if batch % self.log_freq == 0:\n            wandb.log(logs, step=batch)\n            \n        # Log visualization table every viz_freq steps\n        if (batch % self.viz_freq) == 0:# and (batch > 1):\n            self.log_visualizations(step=batch)\n\n    def log_visualizations(self, step):\n        \"\"\"Creates and logs the visualization table to WandB.\"\"\"\n        # --- 1. Get model outputs for the visualization batch ---\n        seg_maps, logits = self.model(self.viz_images, training=False)\n        preds = ops.argmax(logits, axis = -1)\n        # --- 2. Get GradCAM heatmaps ---\n        #heatmaps, preds = self.gradcam_computer.get_heatmap(\n        #    self.viz_images,\n        #    class_indices=self.viz_labels  # If None, will use predictions\n        #)\n        \n        # --- 3. Create WandB Table ---\n        table = wandb.Table(columns=[\n            \"Original Label\", \"Original Image\", \"Predicted Label\",\n            \"Segmentation Map\", \"Overlayed Image\"\n        ])\n        \n        # --- 4. Process images and add to table (vectorized where possible) ---\n        # Convert tensors to numpy for plotting\n        images_np = self.viz_images.numpy().astype(np.uint8)\n        seg_maps_np = seg_maps.numpy()\n        preds_np = preds.numpy()\n        labels_np = self.viz_labels.numpy() if self.viz_labels is not None else [None] * self.num_images\n        \n        # Upsample segmentation maps in one go\n        H, W = images_np.shape[1], images_np.shape[2]\n        seg_maps_upsampled = tf.image.resize(seg_maps_np, [H, W]).numpy()\n\n        # Plot and add to table\n        for i in range(self.num_images):\n            gt_label = int(labels_np[i]) if labels_np[i] is not None else \"N/A\"\n            pred_label = int(preds_np[i])\n            \n            # Convert images to wandb.Image\n            img_orig = wandb.Image(images_np[i])\n            img_seg = self.plot_to_wandb_image(seg_maps_np[i], cmap='tab10', vmin=0, vmax=self.model.num_parts-1)\n            img_overlay = self.plot_overlay_to_wandb_image(images_np[i], seg_maps_upsampled[i])\n            \n            table.add_data(\n                f\"GT: {gt_label}\",\n                img_orig,\n                f\"Pred: {pred_label}\",\n                img_seg,\n                img_overlay,\n                \n            )\n            \n        # Log the table\n        wandb.log({f\"Validation_Results_Step_{step}\": table}, step=step)\n        print(f\"✅ Logged visualization table to WandB at step {step}\")\n\n    def plot_to_wandb_image(self, data, cmap='viridis', vmin=None, vmax=None):\n        \"\"\"Helper to plot a numpy array and return as a wandb.Image.\"\"\"\n        fig, ax = plt.subplots()\n        ax.imshow(data, cmap=cmap, vmin=vmin, vmax=vmax)\n        ax.axis('off')\n        \n        buf = BytesIO()\n        plt.savefig(buf, format='png', bbox_inches='tight', pad_inches=0)\n        buf.seek(0)\n        plt.close(fig)\n        return wandb.Image(Image.open(buf))\n\n    def plot_overlay_to_wandb_image(self, base_image, overlay, cmap='tab10', alpha=0.5):\n        \"\"\"Helper to plot an overlay and return as a wandb.Image.\"\"\"\n        fig, ax = plt.subplots()\n        ax.imshow(base_image)\n        ax.imshow(overlay, cmap=cmap, alpha=alpha)\n        ax.axis('off')\n\n        buf = BytesIO()\n        plt.savefig(buf, format='png', bbox_inches='tight', pad_inches=0)\n        buf.seek(0)\n        plt.close(fig)\n        return wandb.Image(Image.open(buf))\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-13T03:38:26.065Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Stage 1 instance level learning\n- NNCLR + classification","metadata":{}},{"cell_type":"code","source":"class Stage1_NNCLR_Classification(keras.Model):\n    def __init__(self,\n        vit_backbone,\n        temperature = 0.07, \n        queue_size = 10240,\n        n_classes = 165,\n    ):\n        super().__init__()\n        \n        self.encoder = vit_backbone\n        self.embed_dims = self.encoder.embed_dims\n        self.projection_head = keras.Sequential(\n            [   layers.Dense(self.embed_dims, activation=\"relu\"),\n                layers.Dense(self.embed_dims),\n            ],\n            name=\"projection_head\",\n        )\n        self.t = self.temperature = temperature\n\n        self.feature_queue = keras.Variable(\n            keras.utils.normalize(\n                keras.random.normal(shape=(queue_size, self.embed_dims)),\n                axis=1,\n                order=2,\n            ),\n            trainable=False,\n        )\n        self.contrastive_augmenter = keras.Sequential([\n            layers.RandomFlip(\"horizontal\"), layers.RandomFlip(\"vertical\"), \n            #layers.RandomGaussianBlur(factor = (0.2, 0.5)),\n            layers.RandomCrop(384,384),\n            layers.RandomBrightness(factor=(0.2,0.5)),\n        ])\n        self.classification_head = keras.layers.Dense(units = n_classes)\n        self.classification_acc = keras.metrics.SparseCategoricalAccuracy()\n        self.contrastive_optimizer = keras.optimizers.AdamW(learning_rate = 5e-4, \n                                                            gradient_accumulation_steps = 8)\n        \n    def nearest_neighbour(self, projections):\n        support_similarities = ops.matmul(projections, ops.transpose(self.feature_queue))\n        nn_projections = ops.take(\n            self.feature_queue, ops.argmax(support_similarities, axis=1), axis=0\n        )\n        return projections + ops.stop_gradient(nn_projections - projections)\n\n    def contrastive_loss(self, projections_1, projections_2):\n        projections_1 = keras.utils.normalize(projections_1, axis=1, order=2)\n        projections_2 = keras.utils.normalize(projections_2, axis=1, order=2)\n\n        similarities_1_2_1 = (\n            ops.matmul(\n                self.nearest_neighbour(projections_1), ops.transpose(projections_2)\n            )\n            / self.temperature\n        )\n        similarities_1_2_2 = (\n             ops.matmul(\n                projections_2, ops.transpose(self.nearest_neighbour(projections_1))\n            )\n            / self.temperature\n        )\n\n        similarities_2_1_1 = (\n            ops.matmul(\n                self.nearest_neighbour(projections_2), ops.transpose(projections_1)\n            )\n            / self.temperature\n        )\n        similarities_2_1_2 = (\n            ops.matmul(\n                projections_1, ops.transpose(self.nearest_neighbour(projections_2))\n            )\n            / self.temperature\n        )\n\n        batch_size = ops.shape(projections_1)[0]\n        contrastive_labels = ops.arange(batch_size)\n        loss = keras.losses.sparse_categorical_crossentropy(\n            ops.concatenate(\n                [\n                    contrastive_labels,\n                    contrastive_labels,\n                    contrastive_labels,\n                    contrastive_labels,\n                ],\n                axis=0,\n            ),\n            ops.concatenate(\n                [\n                    similarities_1_2_1,\n                    similarities_1_2_2,\n                    similarities_2_1_1,\n                    similarities_2_1_2,\n                ],\n                axis=0,\n            ),\n            from_logits=True,\n        )\n        loss = ops.mean(loss)\n        self.feature_queue.assign(\n            ops.concatenate([projections_1, self.feature_queue[:-batch_size]], axis=0)\n        )\n        return loss\n\n    def train_step(self, data):\n        (images, labels) = data\n        augmented_images_1 = images\n        augmented_images_2 = self.contrastive_augmenter(images)\n\n        with tf.GradientTape() as tape:\n            features_1, encoded_patches_1, att_weights_1 = self.encoder(augmented_images_1) ; logits = self.classification_head(features_1)\n            features_2, encoded_patches_1, att_weights_1 = self.encoder(augmented_images_2)\n            projections_1 = self.projection_head(features_1)\n            projections_2 = self.projection_head(features_2)\n            contrastive_loss = self.contrastive_loss(projections_1, projections_2)\n            class_loss = keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits = True)\n            loss = 0.8*contrastive_loss + 0.2*class_loss\n            self.classification_acc.update_state(labels, logits)\n        class_acc = self.classification_acc.result()\n        gradients = tape.gradient(\n            loss,\n            self.trainable_weights\n        )\n        self.contrastive_optimizer.apply_gradients(\n            zip(\n                gradients,\n                self.trainable_weights,\n            )\n        )\n\n        return {\n            \"nnclr_loss\": contrastive_loss, 'classification_loss' : class_loss, 'total_loss' : loss,\n            'class_accuracy' : class_acc\n        }\n    def visualize_output(self, imgs):\n        if len(ops.shape(imgs)) == 3:\n            imgs = imgs[1, ...]\n            batch_size = 1\n        else:\n            batch_size = ops.shape(imgs)[0]\n        f, encoded_patches, att_wts = self.encoder(imgs)\n        last_attention_weight = att_wts[-1] #batch, heads, n_patches +1, n_patches+1\n        w = int(ops.sqrt(float(ops.shape(encoded_patches)[1])))\n        encoded_patches = ops.reshape(encoded_patches, [-1,self.embed_dims])\n        \n        pca = PCA(n_components=3)\n        encoded_rgb = pca.fit_transform(encoded_patches)\n        encoded_rgb = ops.reshape(encoded_rgb, [-1,w,w,3])\n\n        attention_weight = ops.mean(last_attention_weight, axis = 1)\n        attention_weight = attention_weight[:,0,1:]\n        attention_weight = ops.reshape(attention_weight, [batch_size, w, w])\n        \n        for i in range(batch_size):\n            fig,axes = plt.subplots(1,3, figsize = (15,5))\n            axes = axes.flatten()\n            axes[0].imshow(imgs[i], cmap = 'bone')\n            axes[1].imshow(encoded_rgb[i])\n            axes[2].imshow(attention_weight[i])\n            plt.tight_layout()\n            plt.show()\n        print(\"+-----------------------------------------------------------+\")\n        \nif False:\n    nnclr = Stage1_NNCLR_Classification(vit)\n    sample_imgs = imgs\n    nnclr.compile()\n    nnclr.fit(train_radimagenet_ds, epochs = 1, steps_per_epoch = 50000)\n    nnclr.visualize_output(sample_imgs)\n    vit = nnclr.encoder\n    visualize_token_attention_map(vit = vit, image = imgs)","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"execution_failed":"2025-11-13T03:38:26.065Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Stage 2 patch level learning\n- Unsupervised Semantic segmentation + $iBOT$","metadata":{}},{"cell_type":"code","source":"class Stage2_iBOT_PartSegmentation(keras.Model):\n    \"\"\"\n    Unified iBOT + Part Segmentation for Multi-Modal Medical Imaging\n    \n    Args:\n        vit_backbone: Pretrained ViT from Stage 1\n        num_classes: 165 (instance-level labels)\n        num_parts: 8 (parts per image)\n        global_crop_size: 224\n        local_crop_sizes: [96, 128]\n        num_local_crops: 6\n        mask_ratio: 0.4\n        masking_strategy: 'random' or 'block'\n        seg_weight: 0.5 (balance iBOT vs Seg)\n    \"\"\"\n    \n    def __init__(self,\n                 vit_backbone, teacher_model,\n                 num_classes: int = 165,\n                 num_parts: int = 32,\n                 global_crop_size: int = 224,\n                 local_crop_sizes: list = None,\n                 num_local_crops: int = 6,\n                 mask_ratio: float = 0.4,\n                 masking_strategy: str = 'random',\n                 ema_momentum = 0.99,\n                 teacher_temp: float = 0.07,\n                 student_temp: float = 0.1,\n                 seg_weight: float = 0.5,\n                 seg_target_dist_mode = \"power\",\n                 seg_method = 'agglomerative',\n                 seg_distance = \"l2\",\n                 **kwargs):\n        super().__init__(**kwargs)\n        \n        # Config\n        self.num_classes = num_classes\n        self.num_parts = num_parts\n        self.global_crop_size = global_crop_size\n        self.local_crop_sizes = local_crop_sizes or [96, 128]\n        self.num_local_crops = num_local_crops\n        self.mask_ratio = mask_ratio\n        self.masking_strategy = masking_strategy\n        self.teacher_temp = teacher_temp\n        self.student_temp = student_temp\n        self.ema_momentum = ema_momentum\n        self.seg_weight = seg_weight\n        \n        embed_dims = vit_backbone.embed_dims\n        self.embed_dims = embed_dims\n        \n        # Networks\n        self.student_vit = vit_backbone\n        self.teacher_vit = teacher_model\n        self.teacher_vit.trainable = False\n        self.patch_size = vit_backbone.patch_size\n        # Heads\n        self.patch_head = self._build_head(embed_dims, embed_dims, 'patch_head')\n        self.cls_head = self._build_head(embed_dims, num_classes, 'cls_head')\n        self.dense_head = self._build_head(embed_dims, embed_dims // 2, 'dense_head')\n        \n        self.center = self.add_weight(\n                                    shape=(1, 1, self.embed_dims),\n                                    initializer='zeros',\n                                    trainable=False,\n                                    name='center'\n                                )\n        \n        # Metrics\n        self.ibot_tracker = keras.metrics.Mean(name='ibot')\n        self.cls_tracker = keras.metrics.Mean(name='cls')\n        self.acc_tracker = keras.metrics.SparseCategoricalAccuracy(name='acc')\n        \n        #init for Unsup Seg\n        \n        self.seg_tracker = keras.metrics.Mean(name='seg')\n        self.uniform_dist = tf.fill((num_parts,), 1.0/num_parts)\n        self.target_dist = create_target_distribution(num_parts, method=seg_target_dist_mode)\n        self.cluster_function = ClusteringLayer(k = num_parts,\n                                               n_iterations = 3,\n                                               temperature = student_temp,\n                                               method = seg_method,\n                                               distance_metric = seg_distance)\n        self.kl_loss_fn_batch = keras.losses.KLDivergence(reduction='sum_over_batch_size')\n        self.kl_loss_fn = self.kl_loss_fn_per_image = keras.losses.KLDivergence(reduction='none')\n        # Optimizer\n        self.optimizer = keras.optimizers.AdamW(\n            learning_rate=1e-4,\n        )\n    \n    def build(self, input_shape):\n        super().build(input_shape)\n        self.teacher_vit.set_weights(self.student_vit.get_weights())\n    \n    # ========================================\n    # Network Components\n    # ========================================\n    \n    def _build_head(self, in_dims, out_dims, name):\n        \"\"\"Build head with LayerNorm + Dense\"\"\"\n        return keras.Sequential([\n            layers.LayerNormalization(),\n            layers.Dense(in_dims, activation='gelu'),\n            layers.LayerNormalization(),\n            layers.Dense(out_dims)\n        ], name=name)\n    \n    @staticmethod\n    def safe_normalize(x, axis=-1, eps=1e-6):\n        \"\"\"NaN-safe L2 normalization\"\"\"\n        norm = ops.sqrt(ops.sum(ops.square(x), axis=axis, keepdims=True) + eps)\n        return x / ops.maximum(norm, eps)\n    \n    def update_teacher(self):\n        \"\"\"EMA teacher update\"\"\"\n        m = self.ema_momentum\n        for s_var, t_var in zip(self.student_vit.trainable_variables,\n                                 self.teacher_vit.trainable_variables):\n            t_var.assign(m * t_var + (1 - m) * s_var)\n    \n    # ========================================\n    # Data Augmentation\n    # ========================================\n    \n    def create_crops(self, images):\n        \"\"\"Multi-resolution crops (local: no resize!)\"\"\"\n        images = images/255\n        g1 = tf.image.resize(images, [self.global_crop_size, self.global_crop_size])\n        g2 = self._augment(images, self.global_crop_size)\n        \n        local_crops = [\n            self._augment(images, np.random.choice(self.local_crop_sizes))\n            for _ in range(self.num_local_crops)\n        ]\n        \n        return [g1, g2], local_crops\n    \n    def _augment(self, images, size):\n        \"\"\"Random crop + augmentation\"\"\"\n        shape = ops.shape(images)\n        ratio = tf.random.uniform([], 0.5, 0.7)\n        crop_h = tf.cast(tf.cast(shape[1], tf.float32) * ratio, tf.int32)\n        \n        images = tf.image.random_crop(images, [shape[0], crop_h, crop_h, shape[3]])\n        images = tf.image.resize(images, [size, size])\n        images = tf.image.random_flip_left_right(images)\n        images = tf.image.random_brightness(images, 0.1)\n        images = tf.image.random_contrast(images, 0.95, 1.05)\n        \n        return tf.clip_by_value(images, 0.0, 1.0)\n    \n    # ========================================\n    # Masking\n    # ========================================\n    \n    def generate_mask(self, batch_size, num_patches, ratio):\n        \"\"\"Generate mask (random or block)\"\"\"\n        if ratio <= 0:\n            return tf.zeros([batch_size, num_patches], dtype=tf.bool)\n        \n        if self.masking_strategy == 'random':\n            return tf.random.uniform([batch_size, num_patches]) < ratio\n        \n        elif self.masking_strategy == 'block':\n            grid = tf.cast(ops.sqrt(num_patches), \"int32\")\n            block = tf.cast(tf.cast(grid, \"float32\") * ops.sqrt(ratio), \"int32\")\n            \n            masks = []\n            for b in range(batch_size):\n                # Random block position\n                max_pos = grid - block\n                y_start = tf.random.uniform([], 0, max_pos + 1, dtype=tf.int32)\n                x_start = tf.random.uniform([], 0, max_pos + 1, dtype=tf.int32)\n                \n                # Create coordinate grids\n                y_coords = tf.range(grid, dtype=tf.int32)\n                x_coords = tf.range(grid, dtype=tf.int32)\n                \n                y_grid, x_grid = tf.meshgrid(y_coords, x_coords, indexing='ij')\n                \n                # Check if coordinates are in block\n                y_in_block = tf.logical_and(\n                    y_grid >= y_start,\n                    y_grid < y_start + block\n                )\n                x_in_block = tf.logical_and(\n                    x_grid >= x_start,\n                    x_grid < x_start + block\n                )\n                \n                # Both conditions must be true\n                mask_2d = tf.logical_and(y_in_block, x_in_block)\n                \n                # Flatten\n                mask_1d = tf.reshape(mask_2d, [num_patches])\n                masks.append(mask_1d)\n            \n            return tf.stack(masks)\n        \n        return tf.random.uniform([batch_size, num_patches]) < ratio\n    \n    def apply_mask(self, tokens, mask):\n        \"\"\"Apply mask to tokens\"\"\"\n        return tf.where(mask[:, :, None], tf.zeros_like(tokens), tokens)\n    \n    def visualize_mask(self, images, save_path=None):\n        \"\"\"Visualize masking strategy\"\"\"\n        batch_size = min(tf.shape(images)[0].numpy(), 4)\n        _, patches, _ = self.student_vit(images, training=False)\n        num_patches = tf.shape(patches)[1]\n        \n        mask = self.generate_mask(batch_size, num_patches, self.mask_ratio)\n        grid_size = int(np.sqrt(num_patches))\n        \n        fig, axes = plt.subplots(batch_size, 2, figsize=(8, 4 * batch_size))\n        if batch_size == 1:\n            axes = axes[None, :]\n        \n        for i in range(batch_size):\n            axes[i, 0].imshow(images[i])\n            axes[i, 0].set_title('Original')\n            axes[i, 0].axis('off')\n            \n            mask_2d = tf.reshape(mask[i], [grid_size, grid_size])\n            mask_2d = tf.cast(mask_2d, \"int32\")\n            mask_up = tf.image.resize(\n                mask_2d[:, :, None],\n                [self.global_crop_size, self.global_crop_size],\n                method='nearest'\n            )\n            \n            masked_img = images[i] * (1 - tf.cast(mask_up, tf.float32)) + \\\n                         0.5 * tf.cast(mask_up, tf.float32)\n            \n            axes[i, 1].imshow(masked_img)\n            axes[i, 1].set_title(f'Masked ({self.masking_strategy})')\n            axes[i, 1].axis('off')\n        \n        plt.tight_layout()\n        if save_path:\n            plt.savefig(save_path, dpi=150, bbox_inches='tight')\n        \n        return fig\n    \n    # ========================================\n    # Forward Pass\n    # ========================================\n    \n    def forward_student(self, images, ratio):\n        \"\"\"Student forward with masking\"\"\"\n        cls, patches, _ = self.student_vit(images*255, training=True)\n        mask = self.generate_mask(ops.shape(patches)[0], ops.shape(patches)[1], ratio)\n        return cls, self.apply_mask(patches, mask), mask, patches\n    \n    def forward_teacher(self, images):\n        \"\"\"Teacher forward (no masking)\"\"\"\n        cls, patches, _ = self.teacher_vit(images*255, training=False)\n        return ops.stop_gradient(cls), ops.stop_gradient(patches)\n    \n    def forward_dense(self, patches):\n        \"\"\"Dense prediction for segmentation\"\"\"\n        features = self.dense_head(patches)\n        return features\n    \n    \n    # ========================================\n    # Loss Functions\n    # ========================================\n    \n    def compute_ibot_loss(self, s_patches, t_patches, mask):\n        \"\"\"iBOT MIM loss\"\"\"\n        s = self.patch_head(s_patches)\n        t = ops.stop_gradient(self.patch_head(t_patches))\n        \n        t_probs = ops.softmax((t - self.center)/self.teacher_temp)\n        s_log_probs = ops.log_softmax((s - self.center)/self.student_temp)\n        \n        # Masked CE\n        mask_flat = tf.reshape(mask, [-1])\n        s_flat = tf.reshape(s_log_probs, [-1, self.embed_dims])\n        t_flat = tf.reshape(t_probs, [-1, self.embed_dims])\n        \n        s_masked = tf.boolean_mask(s_flat, mask_flat)\n        t_masked = tf.boolean_mask(t_flat, mask_flat)\n        \n        loss = -ops.sum(t_masked * s_masked, axis=-1)\n        \n        return ops.clip(ops.mean(loss), 0.0, 10.0)\n    \n    def compute_seg_loss(self, encoded_patches, spatial_shape):\n        encoded_patches = self.forward_dense(encoded_patches)\n        cluster_proba, cluster_patches, patch_proba_soft = self.cluster_function(encoded_patches, spatial_shape = spatial_shape)\n        \n        cluster_proba_safe = ops.clip(cluster_proba, 1e-6, 1.0)\n        batchwise_cluster_proba = ops.mean(cluster_proba_safe, axis = 0)\n        anti_collapse_loss = self.kl_loss_fn_batch(ops.expand_dims(self.uniform_dist, 0),\n                                batchwise_cluster_proba)\n        cluster_proba_sorted = tf.sort(cluster_proba_safe, axis=-1, direction='DESCENDING')\n        anti_trivial_loss = self.kl_loss_fn_per_image(ops.expand_dims(self.target_dist, 0), \n                                            cluster_proba_sorted)\n        anti_collapse_loss, anti_trivial_loss = ops.mean(anti_collapse_loss), ops.mean(anti_trivial_loss)\n        loss = anti_collapse_loss + anti_trivial_loss\n        \n        return loss, anti_collapse_loss, anti_trivial_loss, cluster_patches, cluster_proba\n        \n    def compute_consistency_loss(self, proba_list):\n        \"\"\" 뷰 간의 클러스터 분포 일관성 손실 계산 \"\"\"\n        num_views = len(proba_list)\n        if num_views < 2:\n            return tf.constant(0.0, dtype=tf.float32)\n\n        total_consistency_loss = 0.0\n        num_pairs = 0\n\n        # 모든 쌍(pair)에 대해 대칭적 KL 계산\n        for i in range(num_views):\n            for j in range(i + 1, num_views):\n                p = ops.clip(proba_list[i], 1e-6, 1.0)\n                q = ops.clip(proba_list[j], 1e-6, 1.0)\n\n                # KL(p || stop_grad(q)) + KL(q || stop_grad(p))\n                loss_pq = self.kl_loss_fn_per_image(p, ops.stop_gradient(q))\n                loss_qp = self.kl_loss_fn_per_image(q, ops.stop_gradient(p))\n\n                # 배치 평균\n                consistency_pair = ops.mean(loss_pq + loss_qp) * 0.5\n                total_consistency_loss += consistency_pair\n                num_pairs += 1\n\n        return total_consistency_loss / float(num_pairs) if num_pairs > 0 else tf.constant(0.0, dtype=tf.float32)\n    \n    # ========================================\n    # Training\n    # ========================================\n    \n    def train_step(self, data):\n        \"\"\"Training step\"\"\"\n        images, labels = data #images는 0~255 uint8\n        batch_size, H, W, _ = ops.shape(images)\n        spatial_shape = (H//self.patch_size,W//self.patch_size)\n        global_crops, local_crops = self.create_crops(images) #0~1 사이 텐서로 변환 \n        student_spatial_shape = (self.global_crop_size//self.patch_size, self.global_crop_size//self.patch_size)\n        current_step = self.optimizer.iterations\n        \n        with tf.GradientTape() as tape:\n            \n            # Global: iBOT + Seg\n            _, h_global, w_global, _ = ops.shape(global_crops[0])\n            \n             \n            _, t_g1 = self.forward_teacher(global_crops[0])\n            _, t_g2 = self.forward_teacher(global_crops[1])\n            \n            s_cls_g1, s_g1, m_g1, student_patch_1 = self.forward_student(global_crops[0], self.mask_ratio)\n            s_cls_g2, s_g2, m_g2, student_patch_2 = self.forward_student(global_crops[1], self.mask_ratio)\n             \n            ibot_loss = self.compute_ibot_loss(s_g1, t_g2, m_g1) + \\\n                        self.compute_ibot_loss(s_g2, t_g1, m_g2)\n            center_new = ops.mean(ops.concatenate([t_g1, t_g2], axis = 0),\n                                  axis = (0,1))\n            # Local: iBOT only\n            for local in local_crops:\n                _, s_local, m_local, _ = self.forward_student(local, self.mask_ratio)\n                \n                t_target = t_g1\n                n_local = ops.shape(s_local)[1]\n                n_global = ops.shape(t_target)[1]\n                idx = tf.random.shuffle(tf.range(n_global))[:n_local]\n                t_target = tf.gather(t_target, idx, axis=1)\n                \n                ibot_loss += self.compute_ibot_loss(s_local, t_target, m_local)\n            \n            ibot_loss /= (2 + len(local_crops))\n            \n            # Segmentation\n            anti_collapse_coefficient = keras.ops.clip(1.0 - self.optimizer.iterations/total_steps, \n                                                       0.0, 1.0)\n            anti_trivial_coefficient = 1-anti_collapse_coefficient\n            \n            cls_whole, encoded_patches, _ = self.student_vit(images)\n            _, anti_collapse_loss, anti_trivial_loss, _, cluster_proba = self.compute_seg_loss(encoded_patches, spatial_shape = spatial_shape)\n            _, anti_collapse_loss_g1, anti_trivial_loss_g1, _, cluster_proba_g1 =self.compute_seg_loss(student_patch_1, spatial_shape = student_spatial_shape)\n            _, anti_collapse_loss_g2, anti_trivial_loss_g2, _, cluster_proba_g2 =self.compute_seg_loss(student_patch_2, spatial_shape = student_spatial_shape)\n            \n            #Semantic consistency loss\n            consistency_loss = self.compute_consistency_loss([cluster_proba,\n                                                     cluster_proba_g1,\n                                                     cluster_proba_g2])\n            seg_loss = consistency_loss/3\n            # Multi-View loss\n            seg_loss += anti_collapse_coefficient*(anti_collapse_loss + anti_collapse_loss_g1 + anti_collapse_loss_g2)/3 + anti_trivial_coefficient*(anti_trivial_loss +anti_trivial_loss_g1 + anti_trivial_loss_g2)/3\n            \n            # Classification\n            cls_logits = self.cls_head(cls_whole)\n            cls_loss = ops.mean(keras.losses.sparse_categorical_crossentropy(\n                labels, cls_logits, from_logits=True\n            ))\n            \n            # Total\n            total = (1 - self.seg_weight) * ibot_loss + \\\n                    self.seg_weight * seg_loss + 0.1*cls_loss\n        \n        self.center.assign(self.ema_momentum * self.center + (1 - self.ema_momentum) * center_new)\n        # Gradients\n        trainable = self.student_vit.trainable_variables + \\\n                    self.patch_head.trainable_variables + \\\n                    self.dense_head.trainable_variables + \\\n                    self.cls_head.trainable_variables \n        \n        grads = tape.gradient(total, trainable)\n        \n        # NaN check\n        has_nan = tf.reduce_any([\n            tf.reduce_any(tf.math.is_nan(g)) for g in grads if g is not None\n        ])\n        \n        self.optimizer.apply_gradients(zip(grads, trainable))\n        self.update_teacher()\n        \n        # Metrics\n        self.ibot_tracker.update_state(ibot_loss)\n        self.seg_tracker.update_state(seg_loss)\n        self.cls_tracker.update_state(cls_loss)\n        self.acc_tracker.update_state(labels, cls_logits)\n        \n        return {\n            'loss': total,\n            'ibot': self.ibot_tracker.result(),\n            'seg': self.seg_tracker.result(),\n            'cls': self.cls_tracker.result(),\n            'acc': self.acc_tracker.result(),\n            \"is_nan_error\" : keras.ops.mean(has_nan),\n            'seg_anti_collapse_loss' : (anti_collapse_loss + anti_collapse_loss_g1 + anti_collapse_loss_g2)/3,\n            \"seg_anti_trivial_loss\" : (anti_trivial_loss +anti_trivial_loss_g1 + anti_trivial_loss_g2)/3,\n            'seg_consistency_loss' : consistency_loss,\n        }\n    \n    # ========================================\n    # Inference\n    # ========================================\n    \n    def call(self, inputs, labels=None, training=False):\n        \"\"\"Inference\"\"\"\n        batch_size, H, W, _ = ops.shape(inputs)\n        spatial_shape = (H//patch_size, W//patch_size)\n        cls, patches, _ = self.teacher_vit(inputs, training=False)\n        \n        # Classification\n        logits = self.cls_head(cls)\n        if labels is None:\n            labels = ops.argmax(logits, axis=-1)\n        \n        # Segmentation\n        seg_loss, anti_collapse_loss, anti_trivial_loss, part_seg, _ = self.compute_seg_loss(patches, spatial_shape = spatial_shape)\n        # part_seg = [batch_size, n_patches]\n        batch_size, n_patches = ops.shape(part_seg)\n        grid_size = ops.cast(ops.sqrt(ops.cast(n_patches, \"float32\")\n                                     ),\n                            \"int32\")\n        part_seg = ops.reshape(part_seg, [batch_size, grid_size, grid_size, 1])\n        return part_seg, logits\n    \n    # ========================================\n    # Visualization\n    # ========================================\n    \n    def visualize_results(self, images, labels=None, save_path=None):\n        \"\"\"Visualize segmentation results\"\"\"\n        part_seg, logits = self(images, labels, training=False)\n        preds = ops.argmax(logits, axis=-1)\n        \n        batch_size = int(tf.shape(images)[0])\n        res = int(tf.shape(images)[1])\n        \n        fig, axes = plt.subplots(batch_size, 3, figsize=(12, 4 * batch_size))\n        if batch_size == 1:\n            axes = axes[None, :]\n        \n        cmap = plt.cm.get_cmap('tab10', self.num_parts)\n        \n        for i in range(batch_size):\n            # Original\n            axes[i, 0].imshow(images[i])\n            axes[i, 0].set_title(f'Class: {labels[i].numpy() if labels != None else \"DUMMY\"}')\n            axes[i, 0].axis('off')\n            \n            # Parts\n            axes[i, 1].imshow(part_seg[i], cmap=cmap, vmin=0, vmax=self.num_parts-1)\n            axes[i, 1].set_title(f'Pred label : {int(preds[i])}')\n            axes[i, 1].axis('off')\n            \n            # Overlay\n            axes[i, 2].imshow(images[i])\n            axes[i, 2].imshow(tf.image.resize(part_seg[i], [res,res]), \n                              cmap=cmap, alpha=0.5, vmin=0, vmax=self.num_parts-1)\n            axes[i, 2].set_title('Seg result Upsample and Overlay')\n            axes[i, 2].axis('off')\n        \n        plt.tight_layout()\n        if save_path:\n            plt.savefig(save_path, dpi=150, bbox_inches='tight')\n        \n        return fig\n    \n    @property\n    def metrics(self):\n        return [self.ibot_tracker, self.seg_tracker, self.cls_tracker, self.acc_tracker]\n\n\n# ========================================\n# Usage Example\n# ========================================\n\n# Create model\n#model = Stage2_iBOT_PartSegmentation(\n#        vit_backbone=vit, teacher_model = vit,\n#        num_classes=165,\n#        num_parts=32,\n#        global_crop_size=384,\n#        local_crop_sizes=[96, 128, 192, 256],\n#        num_local_crops=6,\n#        mask_ratio=0.7,\n#        masking_strategy='random',\n#        seg_weight=0.4,\n#    seg_target_dist_mode = \"power\",\n#    seg_method = 'agglomerative',\n#    seg_distance = \"l2\"\n#    )\n\n#model.compile()\n#model.fit(train_radimagenet_ds, epochs = 1, steps_per_epoch = 40000)\n#model.visualize_results(images = imgs, labels = labs)\n#visualize_token_attention_map(vit = model.teacher_vit, image = imgs)","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"execution_failed":"2025-11-13T03:38:26.065Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Joint learner\n- Stage 1, 2를 동시에 실행하되 Loss_1은 점점 줄어들고 (cos annealing), Loss_2 coefficient는 1-coeff_loss_1로.","metadata":{}},{"cell_type":"code","source":"class JointLearner(keras.Model):\n    \"\"\"\n    Unified [classification] + [iBOT - class loss] + [iBOT-patches loss + Part Segmentation] for Multi-Modal Medical Imaging\n    Loss_1 = classification + iBOT_CLSToken_loss -> coefficient 1 to 0\n    Loss_2 = iBOT-patches loss + Part Segmentation -> coefficient 0 to 1\n    \n    Args:\n        vit_backbone: ViT\n        num_classes: 165 (instance-level labels)\n        num_parts: 8 (parts per image)\n        global_crop_size: 224\n        local_crop_sizes: [96, 128]\n        num_local_crops: 6\n        mask_ratio: 0.4\n        masking_strategy: 'random' or 'block'\n        seg_weight: 0.5 (balance iBOT vs Seg)\n    \"\"\"\n    \n    def __init__(self,\n                 vit_backbone, teacher_model,\n                 num_classes: int = 165,\n                 num_parts: int = 32,\n                 global_crop_size: int = 224,\n                 local_crop_sizes: list = None,\n                 num_local_crops: int = 6,\n                 mask_ratio: float = 0.4,\n                 masking_strategy: str = 'random',\n                 ema_momentum = 0.99,\n                 teacher_temp: float = 0.07,\n                 student_temp: float = 0.1,\n                 seg_weight: float = 0.5,\n                 seg_target_dist_mode = \"power\",\n                 seg_method = 'agglomerative',\n                 seg_distance = \"l2\",\n                 total_steps = total_steps,\n                 **kwargs):\n        super().__init__(**kwargs)\n        \n        # Config\n        self.total_steps = total_steps\n        self.num_classes = num_classes\n        self.num_parts = num_parts\n        self.global_crop_size = global_crop_size\n        self.local_crop_sizes = local_crop_sizes or [96, 128]\n        self.num_local_crops = num_local_crops\n        self.mask_ratio = mask_ratio\n        self.masking_strategy = masking_strategy\n        self.teacher_temp = teacher_temp\n        self.student_temp = student_temp\n        self.ema_momentum = ema_momentum\n        self.seg_weight = seg_weight\n        \n        embed_dims = vit_backbone.embed_dims\n        self.embed_dims = embed_dims\n        \n        # Networks\n        self.student_vit = vit_backbone\n        self.teacher_vit = teacher_model\n        self.teacher_vit.trainable = False\n        self.patch_size = vit_backbone.patch_size\n        # Heads\n        self.dino_head = self._build_head(embed_dims, embed_dims, 'dino_head')\n        self.teacher_dino_head = self._build_head(embed_dims, embed_dims, 'teacher_dino_head') ; self.teacher_dino_head.trainable = False\n        self.patch_head = self._build_head(embed_dims, embed_dims, 'patch_head')\n        self.cls_head = self._build_head(embed_dims, num_classes, 'cls_head')\n        self.dense_head = self._build_head(embed_dims, embed_dims // 2, 'dense_head')\n        \n        self.center = self.add_weight(\n                                    shape=(1, 1, self.embed_dims),\n                                    initializer='zeros',\n                                    trainable=False,\n                                    name='center'\n                                )\n        self.center_cls = self.add_weight(\n                                    shape=(1, self.embed_dims),\n                                    initializer='zeros',\n                                    trainable=False,\n                                    name='center_cls'\n                                )\n        \n        # Metrics\n        self.ibot_tracker = keras.metrics.Mean(name='ibot')\n        self.cls_tracker = keras.metrics.Mean(name='cls')\n        self.acc_tracker = keras.metrics.SparseCategoricalAccuracy(name='acc')\n        self.dino_tracker = keras.metrics.Mean(name='dino')\n        \n        #init for Unsup Seg\n        \n        self.seg_tracker = keras.metrics.Mean(name='seg')\n        self.uniform_dist = tf.fill((num_parts,), 1.0/num_parts)\n        self.target_dist = create_target_distribution(num_parts, method=seg_target_dist_mode)\n        self.cluster_function = ClusteringLayer(k = num_parts,\n                                               n_iterations = 3,\n                                               temperature = student_temp,\n                                               method = seg_method,\n                                               distance_metric = seg_distance)\n        self.kl_loss_fn_batch = keras.losses.KLDivergence(reduction='sum_over_batch_size')\n        self.kl_loss_fn = self.kl_loss_fn_per_image = keras.losses.KLDivergence(reduction='none')\n        # Optimizer\n        self.optimizer = keras.optimizers.AdamW(\n            learning_rate=1e-4,\n        )\n    \n    def build(self, input_shape):\n        super().build(input_shape)\n        self.teacher_vit.set_weights(self.student_vit.get_weights())\n    \n    # ========================================\n    # Network Components\n    # ========================================\n    \n    def _build_head(self, in_dims, out_dims, name):\n        \"\"\"Build head with LayerNorm + Dense\"\"\"\n        return keras.Sequential([\n            layers.LayerNormalization(),\n            layers.Dense(in_dims, activation='gelu'),\n            layers.LayerNormalization(),\n            layers.Dense(out_dims)\n        ], name=name)\n    \n    @staticmethod\n    def safe_normalize(x, axis=-1, eps=1e-6):\n        \"\"\"NaN-safe L2 normalization\"\"\"\n        norm = ops.sqrt(ops.sum(ops.square(x), axis=axis, keepdims=True) + eps)\n        return x / ops.maximum(norm, eps)\n    \n    def update_teacher(self):\n        \"\"\"EMA teacher update\"\"\"\n        m = self.ema_momentum\n        for s_var, t_var in zip(self.student_vit.trainable_variables,\n                                 self.teacher_vit.trainable_variables):\n            t_var.assign(m * t_var + (1 - m) * s_var)\n        for s_var, t_var in zip(self.dino_head.trainable_variables, self.teacher_dino_head.trainable_variables):\n            t_var.assign(m * t_var + (1-m)*s_var)\n    \n    # ========================================\n    # Data Augmentation\n    # ========================================\n    \n    def create_crops(self, images):\n        \"\"\"Multi-resolution crops (local: no resize!)\"\"\"\n        images = images/255\n        g1 = tf.image.resize(images, [self.global_crop_size, self.global_crop_size])\n        g2 = self._augment(images, self.global_crop_size)\n        \n        local_crops = [\n            self._augment(images, np.random.choice(self.local_crop_sizes))\n            for _ in range(self.num_local_crops)\n        ]\n        \n        return [g1, g2], local_crops #all unmasked.\n    \n    def _augment(self, images, size):\n        \"\"\"Random crop + augmentation\"\"\"\n        shape = ops.shape(images)\n        ratio = tf.random.uniform([], 0.5, 0.7)\n        crop_h = tf.cast(tf.cast(shape[1], tf.float32) * ratio, tf.int32)\n        \n        images = tf.image.random_crop(images, [shape[0], crop_h, crop_h, shape[3]])\n        images = tf.image.resize(images, [size, size])\n        images = tf.image.random_flip_left_right(images)\n        images = tf.image.random_brightness(images, 0.1)\n        images = tf.image.random_contrast(images, 0.95, 1.05)\n        \n        return tf.clip_by_value(images, 0.0, 1.0)\n    \n    # ========================================\n    # Masking\n    # ========================================\n    \n    def generate_mask(self, batch_size, num_patches, ratio):\n        \"\"\"Generate mask (random or block)\"\"\"\n        if ratio <= 0:\n            return tf.zeros([batch_size, num_patches], dtype=tf.bool)\n        \n        if self.masking_strategy == 'random':\n            return tf.random.uniform([batch_size, num_patches]) < ratio\n        \n        elif self.masking_strategy == 'block':\n            grid = tf.cast(ops.sqrt(num_patches), \"int32\")\n            block = tf.cast(tf.cast(grid, \"float32\") * ops.sqrt(ratio), \"int32\")\n            \n            masks = []\n            for b in range(batch_size):\n                # Random block position\n                max_pos = grid - block\n                y_start = tf.random.uniform([], 0, max_pos + 1, dtype=tf.int32)\n                x_start = tf.random.uniform([], 0, max_pos + 1, dtype=tf.int32)\n                \n                # Create coordinate grids\n                y_coords = tf.range(grid, dtype=tf.int32)\n                x_coords = tf.range(grid, dtype=tf.int32)\n                \n                y_grid, x_grid = tf.meshgrid(y_coords, x_coords, indexing='ij')\n                \n                # Check if coordinates are in block\n                y_in_block = tf.logical_and(\n                    y_grid >= y_start,\n                    y_grid < y_start + block\n                )\n                x_in_block = tf.logical_and(\n                    x_grid >= x_start,\n                    x_grid < x_start + block\n                )\n                \n                # Both conditions must be true\n                mask_2d = tf.logical_and(y_in_block, x_in_block)\n                \n                # Flatten\n                mask_1d = tf.reshape(mask_2d, [num_patches])\n                masks.append(mask_1d)\n            \n            return tf.stack(masks)\n        \n        return tf.random.uniform([batch_size, num_patches]) < ratio\n    \n    def apply_mask(self, tokens, mask):\n        \"\"\"Apply mask to tokens\"\"\"\n        return tf.where(mask[:, :, None], tf.zeros_like(tokens), tokens)\n    \n    \n    # ========================================\n    # Forward Pass\n    # ========================================\n    \n    def forward_student(self, images, ratio):\n        \"\"\"Student forward with masking : returns cls token, encoded masked view patches, mask, original patches\"\"\"\n        cls, patches, _ = self.student_vit(images*255, training=True)\n        mask = self.generate_mask(ops.shape(patches)[0], ops.shape(patches)[1], ratio)\n        return cls, self.apply_mask(patches, mask), mask, patches #cls token, masked view patches, mask, original patches\n    \n    def forward_teacher(self, images):\n        \"\"\"Teacher forward (no masking)\"\"\"\n        cls, patches, _ = self.teacher_vit(images*255, training=False)\n        return ops.stop_gradient(cls), ops.stop_gradient(patches)\n    \n    def forward_dense(self, patches):\n        \"\"\"Dense prediction for segmentation\"\"\"\n        features = self.dense_head(patches)\n        return features\n    \n    \n    # ========================================\n    # Loss Functions\n    # ========================================\n    def cos_schedule_decrease(self, initial_weight = 1.0, end_weight = 0.0):\n        #1 to 0, cosine을 따르는\n        current_step = ops.cast(self.optimizer.iterations, \"float32\")\n        total_steps = ops.maximum(ops.cast(self.total_steps, \"float32\"), 1.0)\n        progress_rate = ops.clip(current_step / total_steps, 0.0, 1.0)\n        \n        pi = ops.convert_to_tensor(math.pi, dtype=\"float32\")\n        cos_decay = 0.5*(1+ops.cos(pi*progress_rate))\n        decayed_wt = end_weight + cos_decay*(initial_weight - end_weight)\n        return decayed_wt\n        \n    def linear_schedule_decrease(self, initial_weight = 1.0, end_weight = 0.0):\n        current_step = ops.cast(self.optimizer.iterations, \"float32\")\n        total_steps = ops.maximum(ops.cast(self.total_steps, \"float32\"), 1.0)\n        progress_rate = ops.clip(current_step / total_steps, 0.0, 1.0)\n        lin_decay_wt = end_weight + (1.0-progress_rate)*(initial_weight - end_weight)\n        return lin_decay_wt\n        \n    def compute_ibot_loss(self, s_patches, t_patches, mask):\n        \"\"\"iBOT MIM loss\"\"\"\n        s = self.patch_head(s_patches)\n        t = ops.stop_gradient(self.patch_head(t_patches))\n        \n        t_probs = ops.softmax((t - self.center)/self.teacher_temp)\n        s_log_probs = ops.log_softmax((s - self.center)/self.student_temp)\n        \n        # Masked CE\n        mask_flat = tf.reshape(mask, [-1])\n        s_flat = tf.reshape(s_log_probs, [-1, self.embed_dims])\n        t_flat = tf.reshape(t_probs, [-1, self.embed_dims])\n        \n        s_masked = tf.boolean_mask(s_flat, mask_flat)\n        t_masked = tf.boolean_mask(t_flat, mask_flat)\n        \n        loss = -ops.sum(t_masked * s_masked, axis=-1)\n        \n        return ops.clip(ops.mean(loss), 0.0, 10.0)\n        \n    def compute_dino_loss(self, s_cls, t_cls): \n        t_cls = ops.stop_gradient(t_cls)\n        s_cls, t_cls = self.dino_head(s_cls), self.teacher_dino_head(t_cls) #both [batch, embed_dims]\n        t_cls = ops.softmax((t_cls - self.center_cls) / self.teacher_temp, axis = -1)\n        s_cls = ops.softmax(s_cls/self.student_temp, \n                            axis = -1)\n        t_cls = ops.clip(t_cls, 1e-5, 0.999)\n        s = ops.log_softmax(s_cls, axis = -1)\n        return ops.mean(ops.sum(-t_cls * s, axis = -1))\n        \n    def compute_seg_loss(self, encoded_patches, spatial_shape):\n        encoded_patches = self.forward_dense(encoded_patches)\n        cluster_proba, cluster_patches, patch_proba_soft = self.cluster_function(encoded_patches, spatial_shape = spatial_shape)\n        \n        cluster_proba_safe = ops.clip(cluster_proba, 1e-6, 1.0)\n        batchwise_cluster_proba = ops.mean(cluster_proba_safe, axis = 0)\n        anti_collapse_loss = self.kl_loss_fn_batch(ops.expand_dims(self.uniform_dist, 0),\n                                batchwise_cluster_proba)\n        cluster_proba_sorted = tf.sort(cluster_proba_safe, axis=-1, direction='DESCENDING')\n        anti_trivial_loss = self.kl_loss_fn_per_image(ops.expand_dims(self.target_dist, 0), \n                                            cluster_proba_sorted)\n        anti_collapse_loss, anti_trivial_loss = ops.mean(anti_collapse_loss), ops.mean(anti_trivial_loss)\n        loss = anti_collapse_loss + anti_trivial_loss\n        \n        return loss, anti_collapse_loss, anti_trivial_loss, cluster_patches, cluster_proba\n        \n    def compute_consistency_loss(self, proba_list):\n        \"\"\" 뷰 간의 클러스터 분포 일관성 손실 계산 \"\"\"\n        num_views = len(proba_list)\n        if num_views < 2:\n            return tf.constant(0.0, dtype=tf.float32)\n\n        total_consistency_loss = 0.0\n        num_pairs = 0\n\n        # 모든 쌍(pair)에 대해 대칭적 KL 계산\n        for i in range(num_views):\n            for j in range(i + 1, num_views):\n                p = ops.clip(proba_list[i], 1e-6, 1.0)\n                q = ops.clip(proba_list[j], 1e-6, 1.0)\n\n                # KL(p || stop_grad(q)) + KL(q || stop_grad(p))\n                loss_pq = self.kl_loss_fn_per_image(p, ops.stop_gradient(q))\n                loss_qp = self.kl_loss_fn_per_image(q, ops.stop_gradient(p))\n\n                # 배치 평균\n                consistency_pair = ops.mean(loss_pq + loss_qp) * 0.5\n                total_consistency_loss += consistency_pair\n                num_pairs += 1\n\n        return total_consistency_loss / float(num_pairs) if num_pairs > 0 else tf.constant(0.0, dtype=tf.float32)\n    \n    # ========================================\n    # Training\n    # ========================================\n    \n    def train_step(self, data):\n        \"\"\"Training step\"\"\"\n        images, labels = data #images는 0~255 uint8\n        batch_size, H, W, _ = ops.shape(images)\n        spatial_shape = (H//self.patch_size,W//self.patch_size)\n        global_crops, local_crops = self.create_crops(images) #0~1 사이 텐서로 변환 \n        student_spatial_shape = (self.global_crop_size//self.patch_size, self.global_crop_size//self.patch_size)\n        \n        loss_1_coeff = self.cos_schedule_decrease()\n        loss_2_coeff = 1-loss_1_coeff\n\n        anti_collapse_coefficient = self.linear_schedule_decrease()\n        anti_trivial_coefficient = 1-anti_collapse_coefficient\n        \n        with tf.GradientTape() as tape:\n            \n            # Global: iBOT + Seg\n            _, h_global, w_global, _ = ops.shape(global_crops[0])\n            \n             \n            t_cls_g1, t_g1 = self.forward_teacher(global_crops[0])\n            t_cls_g2, t_g2 = self.forward_teacher(global_crops[1])\n            \n            s_cls_g1, s_g1, m_g1, student_patch_1 = self.forward_student(global_crops[0], self.mask_ratio)\n            s_cls_g2, s_g2, m_g2, student_patch_2 = self.forward_student(global_crops[1], self.mask_ratio)\n             \n            ibot_loss = self.compute_ibot_loss(s_g1, t_g2, m_g1) + \\\n                        self.compute_ibot_loss(s_g2, t_g1, m_g2)\n            center_new = ops.mean(ops.concatenate([t_g1, t_g2], axis = 0),\n                                  axis = (0,1))\n            cls_dino_loss = self.compute_dino_loss(s_cls = s_cls_g2, t_cls = t_cls_g1) + \\\n                            self.compute_dino_loss(s_cls = s_cls_g1, t_cls = t_cls_g2)\n            center_new_cls = ops.mean(ops.concatenate([t_cls_g1, t_cls_g2], axis = 0),\n                                  axis = 0)\n            # Local: iBOT only\n            for local in local_crops:\n                s_cls_local, s_local, m_local, _ = self.forward_student(local, self.mask_ratio)\n                \n                t_target = t_g1\n                n_local = ops.shape(s_local)[1]\n                n_global = ops.shape(t_target)[1]\n                idx = tf.random.shuffle(tf.range(n_global))[:n_local]\n                t_target = tf.gather(t_target, idx, axis=1)\n                \n                ibot_loss += self.compute_ibot_loss(s_local, t_target, m_local)\n                cls_dino_loss += self.compute_dino_loss(t_cls = t_cls_g1, s_cls = s_cls_local) + \\\n                                self.compute_dino_loss(t_cls = t_cls_g2, s_cls = s_cls_local)\n            ibot_loss /= (2 + len(local_crops))\n            cls_dino_loss /= (2 + len(local_crops))\n            \n            # Segmentation\n            anti_collapse_coefficient = keras.ops.clip(1.0 - self.optimizer.iterations/total_steps, \n                                                       0.0, 1.0)\n            anti_trivial_coefficient = 1-anti_collapse_coefficient\n            \n            cls_whole, encoded_patches, _ = self.student_vit(images)\n            _, anti_collapse_loss, anti_trivial_loss, _, cluster_proba = self.compute_seg_loss(encoded_patches, spatial_shape = spatial_shape)\n            _, anti_collapse_loss_g1, anti_trivial_loss_g1, _, cluster_proba_g1 =self.compute_seg_loss(student_patch_1, spatial_shape = student_spatial_shape)\n            _, anti_collapse_loss_g2, anti_trivial_loss_g2, _, cluster_proba_g2 =self.compute_seg_loss(student_patch_2, spatial_shape = student_spatial_shape)\n            \n            #Semantic consistency loss\n            consistency_loss = self.compute_consistency_loss([cluster_proba,\n                                                     cluster_proba_g1,\n                                                     cluster_proba_g2])\n            seg_loss = consistency_loss/3\n            # Multi-View loss\n            seg_loss += anti_collapse_coefficient*(anti_collapse_loss + anti_collapse_loss_g1 + anti_collapse_loss_g2)/3 + anti_trivial_coefficient*(anti_trivial_loss +anti_trivial_loss_g1 + anti_trivial_loss_g2)/3\n            \n            # Classification\n            cls_logits = self.cls_head(cls_whole)\n            cls_loss = ops.mean(keras.losses.sparse_categorical_crossentropy(\n                labels, cls_logits, from_logits=True\n            ))\n            \n            # Total\n            #Loss 1 : CLS level loss = cls_loss + cls_dino_loss\n            # Loss 2 : patch level loss = (1-s_w)iBOT loss + s_w*seg_loss\n            \n            loss_1 = cls_loss + cls_dino_loss\n            loss_2 = (1-self.seg_weight) * ibot_loss + self.seg_weight*seg_loss\n            loss = loss_1_coeff*loss_1 + loss_2_coeff*loss_2\n            \n        self.center.assign(self.ema_momentum * self.center + (1 - self.ema_momentum) * center_new)\n        self.center_cls.assign(self.ema_momentum * self.center_cls + (1 - self.ema_momentum) * center_new_cls)\n        # Gradients\n        trainable = self.student_vit.trainable_variables + \\\n                    self.patch_head.trainable_variables + \\\n                    self.dense_head.trainable_variables + \\\n                    self.cls_head.trainable_variables + self.dino_head.trainable_variables\n        \n        grads = tape.gradient(loss, trainable)\n        \n        # NaN check\n        has_nan = tf.reduce_any([\n            tf.reduce_any(tf.math.is_nan(g)) for g in grads if g is not None\n        ])\n        \n        self.optimizer.apply_gradients(zip(grads, trainable))\n        self.update_teacher()\n        \n        # Metrics\n        self.ibot_tracker.update_state(ibot_loss)\n        self.dino_tracker.update_state(cls_dino_loss)\n        self.seg_tracker.update_state(seg_loss)\n        self.cls_tracker.update_state(cls_loss)\n        self.acc_tracker.update_state(labels, cls_logits)\n        \n        return {\n            'Total_loss': loss,\n            #1. Tokenwise(loss 1 component)\n            'cls': self.cls_tracker.result(),\n            'dino_cls': self.dino_tracker.result(),\n            'acc': self.acc_tracker.result(),\n            \n            #2. patchwise(loss 2 component)\n            'ibot': self.ibot_tracker.result(), \n            'seg': self.seg_tracker.result(),\n            'seg_anti_collapse_loss' : (anti_collapse_loss + anti_collapse_loss_g1 + anti_collapse_loss_g2)/3,\n            \"seg_anti_trivial_loss\" : (anti_trivial_loss +anti_trivial_loss_g1 + anti_trivial_loss_g2)/3,\n            'seg_consistency_loss' : consistency_loss,\n            \n            # 3. ETC\n            \"is_nan_error\" : keras.ops.mean(has_nan),\n        }\n    \n    # ========================================\n    # Inference\n    # ========================================\n    \n    def call(self, inputs, labels=None, training=False):\n        \"\"\"Inference\"\"\"\n        batch_size, H, W, _ = ops.shape(inputs)\n        spatial_shape = (H//patch_size, W//patch_size)\n        cls, patches, _ = self.teacher_vit(inputs, training=False)\n        \n        # Classification\n        logits = self.cls_head(cls)\n        if labels is None:\n            labels = ops.argmax(logits, axis=-1)\n        \n        # Segmentation\n        seg_loss, anti_collapse_loss, anti_trivial_loss, part_seg, _ = self.compute_seg_loss(patches, spatial_shape = spatial_shape)\n        # part_seg = [batch_size, n_patches]\n        batch_size, n_patches = ops.shape(part_seg)\n        grid_size = ops.cast(ops.sqrt(ops.cast(n_patches, \"float32\")\n                                     ),\n                            \"int32\")\n        part_seg = ops.reshape(part_seg, [batch_size, grid_size, grid_size, 1])\n        return part_seg, logits\n    \n    # ========================================\n    # Visualization\n    # ========================================\n    \n    def visualize_results(self, images, labels=None, save_path=None):\n        \"\"\"Visualize segmentation results\"\"\"\n        part_seg, logits = self(images, labels, training=False)\n        preds = ops.argmax(logits, axis=-1)\n        \n        batch_size = int(tf.shape(images)[0])\n        res = int(tf.shape(images)[1])\n        \n        fig, axes = plt.subplots(batch_size, 3, figsize=(12, 4 * batch_size))\n        if batch_size == 1:\n            axes = axes[None, :]\n        \n        cmap = plt.cm.get_cmap('tab10', self.num_parts)\n        \n        for i in range(batch_size):\n            # Original\n            axes[i, 0].imshow(images[i])\n            axes[i, 0].set_title(f'Class: {labels[i].numpy() if labels != None else \"DUMMY\"}')\n            axes[i, 0].axis('off')\n            \n            # Parts\n            axes[i, 1].imshow(part_seg[i], cmap=cmap, vmin=0, vmax=self.num_parts-1)\n            axes[i, 1].set_title(f'Pred label : {int(preds[i])}')\n            axes[i, 1].axis('off')\n            \n            # Overlay\n            axes[i, 2].imshow(images[i])\n            axes[i, 2].imshow(tf.image.resize(part_seg[i], [res,res]), \n                              cmap=cmap, alpha=0.5, vmin=0, vmax=self.num_parts-1)\n            axes[i, 2].set_title('Seg result Upsample and Overlay')\n            axes[i, 2].axis('off')\n        \n        plt.tight_layout()\n        if save_path:\n            plt.savefig(save_path, dpi=150, bbox_inches='tight')\n        \n        return fig\n    \n    @property\n    def metrics(self):\n        return [self.ibot_tracker, self.seg_tracker, self.cls_tracker, self.acc_tracker]\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"execution_failed":"2025-11-13T03:38:26.065Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"> Unsupervised Segmentation with MI\n\n- $C$ : cluster vector [batch, K] shape tensor\n- $X$ : patch tensor [batch, N, K] shape, patch allocation info.\n- $MI(C,X)$를 최대화하는 것이 목적\n    - $MI = H(C) - H(C|X)$\n    - $H(C)$는 Category (전체 패치가 1, 2, ..., K로 배정되는 확률)의 불확실성 -> 이것이 커져야 배치, 패치가 최대한 다양한 category로 분류됨\n    - $H(C|X)$는 패치 정보가 주어졌을 때 (X=x) 카테고리의 불확실성 -> 모델이 trivial하게 모든 카테고리에 비슷한 갯수의 패치를 배정하는 것을 막음 (만일 그럴 경우 패치 정보가 주어져도 카테고리 불확실성이 큼! -> H(C|X)값이 커져 MI값이 낮아짐)\n    - H(C)가 너무 작지 않게 하되, 즉 증가하되, H(C|X)는 감소해야 함 (패치 정보를 알고 나서는 패치 allocation의 불확실성이 떨어져야 함)\n    - 즉 $MI = H(C) - H(C|X)$ 로 두면, MI 최대화 = H(C) 최대화와 동시에 H(C|X)의 최소화임.","metadata":{}},{"cell_type":"code","source":"class JointLearnerWithMI(keras.Model):\n    \"\"\"\n    Unified [classification] + [iBOT - class loss] + [iBOT-patches loss + Seg with MI] for Multi-Modal Medical Imaging\n    Loss_1 = classification + iBOT_CLSToken_loss -> coefficient 1 to 0\n    Loss_2 = iBOT-patches loss + Segmentation MI loss -> coefficient 0 to 1\n    \n    Args:\n        vit_backbone: ViT\n        num_classes: 165 (instance-level labels)\n        num_parts: 8 (parts per image)\n        global_crop_size: 224\n        local_crop_sizes: [96, 128]\n        num_local_crops: 6\n        mask_ratio: 0.4\n        masking_strategy: 'random' or 'block'\n        seg_weight: 0.5 (balance iBOT vs Seg)\n    \"\"\"\n    \n    def __init__(self,\n                 vit_backbone, teacher_model,\n                 num_classes: int = 165,\n                 num_parts: int = 32,\n                 global_crop_size: int = 224,\n                 local_crop_sizes: list = None,\n                 num_local_crops: int = 6,\n                 mask_ratio: float = 0.4,\n                 masking_strategy: str = 'random',\n                 ema_momentum = 0.99,\n                 teacher_temp: float = 0.06,\n                 student_temp: float = 0.1,\n                 seg_weight: float = 0.5,\n                 seg_distance = \"l2\",\n                 total_steps = 60000,\n                 **kwargs):\n        super().__init__(**kwargs)\n        \n        # Config\n        self.total_steps = total_steps\n        self.num_classes = num_classes\n        self.num_parts = num_parts\n        self.global_crop_size = global_crop_size\n        self.local_crop_sizes = local_crop_sizes or [96, 128]\n        self.num_local_crops = num_local_crops\n        self.mask_ratio = mask_ratio\n        self.masking_strategy = masking_strategy\n        self.teacher_temp = teacher_temp\n        self.student_temp = student_temp\n        self.initial_mi_temp = 0.5\n        self.ema_momentum = ema_momentum\n        self.seg_weight = seg_weight\n        \n        embed_dims = vit_backbone.embed_dims\n        self.embed_dims = embed_dims\n        \n        # Networks\n        self.student_vit = vit_backbone\n        self.teacher_vit = teacher_model\n        self.trainable = True\n        self.student_vit.trainable = True\n        self.teacher_vit.trainable = False\n        self.patch_size = vit_backbone.patch_size\n        # Heads\n        self.dino_head = self._build_head(embed_dims, embed_dims, 'dino_head')\n        self.teacher_dino_head = self._build_head(embed_dims, embed_dims, 'teacher_dino_head') ; self.teacher_dino_head.trainable = False\n        self.patch_head = self._build_head(embed_dims, embed_dims, 'patch_head')\n        self.cls_head = self._build_head(embed_dims, num_classes, 'cls_head')\n        self.dense_head = self._build_head(embed_dims, embed_dims, 'dense_head')\n        \n        self.center = self.add_weight(\n                                    shape=(1, 1, self.embed_dims),\n                                    initializer='zeros',\n                                    trainable=False,\n                                    name='center'\n                                )\n        self.center_cls = self.add_weight(\n                                    shape=(1, self.embed_dims),\n                                    initializer='zeros',\n                                    trainable=False,\n                                    name='center_cls'\n                                )\n        \n        # Metrics\n        self.ibot_tracker = keras.metrics.Mean(name='ibot')\n        self.cls_tracker = keras.metrics.Mean(name='cls')\n        self.acc_tracker = keras.metrics.SparseCategoricalAccuracy(name='acc')\n        self.dino_tracker = keras.metrics.Mean(name='dino')\n        \n        #init for Unsup Seg\n        self.seg_tracker = keras.metrics.Mean(name='seg')\n        self.uniform_dist = tf.fill((num_parts,), 1.0/num_parts)\n        self.cluster_function = LearnableClusteringLayer(k = num_parts, distance_metric = seg_distance)\n        # Optimizer\n        self.optimizer = keras.optimizers.AdamW(\n            learning_rate=1e-4,\n        )\n    \n    def build(self, input_shape):\n        super().build(input_shape)\n        self.teacher_vit.set_weights(self.student_vit.get_weights())\n        self.student_vit.trainable = True\n        self.teacher_vit.trainable = False\n    \n    # ========================================\n    # Network Components\n    # ========================================\n    \n    def _build_head(self, in_dims, out_dims, name):\n        \"\"\"Build head with LayerNorm + Dense\"\"\"\n        return keras.Sequential([\n            layers.LayerNormalization(),\n            layers.Dense(in_dims, activation='gelu'),\n            layers.LayerNormalization(),\n            layers.Dense(out_dims)\n        ], name=name)\n    \n    @staticmethod\n    def safe_normalize(x, axis=-1, eps=1e-6):\n        \"\"\"NaN-safe L2 normalization\"\"\"\n        norm = ops.sqrt(ops.sum(ops.square(x), axis=axis, keepdims=True) + eps)\n        return x / ops.maximum(norm, eps)\n    \n    def update_teacher(self):\n        \"\"\"EMA teacher update\"\"\"\n        m = self.ema_momentum\n        for s_var, t_var in zip(self.student_vit.trainable_variables,\n                                 self.teacher_vit.trainable_variables):\n            t_var.assign(m * t_var + (1 - m) * s_var)\n        for s_var, t_var in zip(self.dino_head.trainable_variables, self.teacher_dino_head.trainable_variables):\n            t_var.assign(m * t_var + (1-m)*s_var)\n    \n    # ========================================\n    # Data Augmentation\n    # ========================================\n    \n    def create_crops(self, images):\n        \"\"\"Multi-resolution crops (local: no resize!)\"\"\"\n        images = images/255\n        g1 = tf.image.resize(images, [self.global_crop_size, self.global_crop_size])\n        g2 = self._augment(images, self.global_crop_size)\n        \n        local_crops = [\n            self._augment(images, np.random.choice(self.local_crop_sizes))\n            for _ in range(self.num_local_crops)\n        ]\n        \n        return [g1, g2], local_crops #all unmasked.\n    \n    def _augment(self, images, size):\n        \"\"\"Random crop + augmentation\"\"\"\n        shape = ops.shape(images)\n        ratio = tf.random.uniform([], 0.5, 0.7)\n        crop_h = tf.cast(tf.cast(shape[1], tf.float32) * ratio, tf.int32)\n        \n        images = tf.image.random_crop(images, [shape[0], crop_h, crop_h, shape[3]])\n        images = tf.image.resize(images, [size, size])\n        images = tf.image.random_flip_left_right(images)\n        images = tf.image.random_brightness(images, 0.1)\n        images = tf.image.random_contrast(images, 0.95, 1.05)\n        \n        return tf.clip_by_value(images, 0.0, 1.0)\n    \n    # ========================================\n    # Masking\n    # ========================================\n    \n    def generate_mask(self, batch_size, num_patches, ratio):\n        \"\"\"Generate mask (random or block)\"\"\"\n        if ratio <= 0:\n            return tf.zeros([batch_size, num_patches], dtype=tf.bool)\n        \n        if self.masking_strategy == 'random':\n            return tf.random.uniform([batch_size, num_patches]) < ratio\n        \n        elif self.masking_strategy == 'block':\n            grid = tf.cast(ops.sqrt(num_patches), \"int32\")\n            block = tf.cast(tf.cast(grid, \"float32\") * ops.sqrt(ratio), \"int32\")\n            \n            masks = []\n            for b in range(batch_size):\n                # Random block position\n                max_pos = grid - block\n                y_start = tf.random.uniform([], 0, max_pos + 1, dtype=tf.int32)\n                x_start = tf.random.uniform([], 0, max_pos + 1, dtype=tf.int32)\n                \n                # Create coordinate grids\n                y_coords = tf.range(grid, dtype=tf.int32)\n                x_coords = tf.range(grid, dtype=tf.int32)\n                \n                y_grid, x_grid = tf.meshgrid(y_coords, x_coords, indexing='ij')\n                \n                # Check if coordinates are in block\n                y_in_block = tf.logical_and(\n                    y_grid >= y_start,\n                    y_grid < y_start + block\n                )\n                x_in_block = tf.logical_and(\n                    x_grid >= x_start,\n                    x_grid < x_start + block\n                )\n                \n                # Both conditions must be true\n                mask_2d = tf.logical_and(y_in_block, x_in_block)\n                \n                # Flatten\n                mask_1d = tf.reshape(mask_2d, [num_patches])\n                masks.append(mask_1d)\n            \n            return tf.stack(masks)\n        \n        return tf.random.uniform([batch_size, num_patches]) < ratio\n    \n    def apply_mask(self, tokens, mask):\n        \"\"\"Apply mask to tokens\"\"\"\n        return tf.where(mask[:, :, None], tf.zeros_like(tokens), tokens)\n    \n    \n    # ========================================\n    # Forward Pass\n    # ========================================\n    \n    def forward_student(self, images, ratio):\n        \"\"\"Student forward with masking : returns cls token, encoded masked view patches, mask, original patches\"\"\"\n        cls, patches, _ = self.student_vit(images*255, training=True)\n        mask = self.generate_mask(ops.shape(patches)[0], ops.shape(patches)[1], ratio)\n        return cls, self.apply_mask(patches, mask), mask, patches #cls token, masked view patches, mask, original patches\n    \n    def forward_teacher(self, images):\n        \"\"\"Teacher forward (no masking)\"\"\"\n        cls, patches, _ = self.teacher_vit(images*255, training=False)\n        return ops.stop_gradient(cls), ops.stop_gradient(patches)\n    \n    def forward_dense(self, patches):\n        \"\"\"Dense prediction for segmentation\"\"\"\n        features = self.dense_head(patches)\n        return features\n    \n    \n    # ========================================\n    # Loss Functions\n    # ========================================\n    def cos_schedule_decrease(self, initial_weight = 1.0, end_weight = 0.0):\n        #1 to 0, cosine을 따르는\n        current_step = ops.cast(self.optimizer.iterations, \"float32\")\n        total_steps = ops.maximum(ops.cast(self.total_steps, \"float32\"), 1.0)\n        progress_rate = ops.clip(current_step / total_steps, 0.0, 1.0)\n        \n        pi = ops.convert_to_tensor(math.pi, dtype=\"float32\")\n        cos_decay = 0.5*(1+ops.cos(pi*progress_rate))\n        decayed_wt = end_weight + cos_decay*(initial_weight - end_weight)\n        return decayed_wt\n        \n    def linear_schedule_decrease(self, initial_weight = 1.0, end_weight = 0.0):\n        current_step = ops.cast(self.optimizer.iterations, \"float32\")\n        total_steps = ops.maximum(ops.cast(self.total_steps, \"float32\"), 1.0)\n        progress_rate = ops.clip(current_step / total_steps, 0.0, 1.0)\n        lin_decay_wt = end_weight + (1.0-progress_rate)*(initial_weight - end_weight)\n        return lin_decay_wt\n\n    def compute_ibot_loss(self, s_patches, t_patches, mask):\n        \"\"\"iBOT MIM loss\"\"\"\n        s = self.patch_head(s_patches)\n        t = ops.stop_gradient(self.patch_head(t_patches))\n        \n        t_probs = ops.softmax((t - self.center)/self.teacher_temp)\n        s_log_probs = ops.log_softmax((s - self.center)/self.student_temp)\n        \n        # Masked CE\n        mask_flat = tf.reshape(mask, [-1])\n        s_flat = tf.reshape(s_log_probs, [-1, self.embed_dims])\n        t_flat = tf.reshape(t_probs, [-1, self.embed_dims])\n        \n        s_masked = tf.boolean_mask(s_flat, mask_flat)\n        t_masked = tf.boolean_mask(t_flat, mask_flat)\n        \n        loss = -ops.sum(t_masked * s_masked, axis=-1)\n        \n        return ops.clip(ops.mean(loss), 0.0, 10.0)\n        \n    def compute_dino_loss(self, s_cls, t_cls): \n        t_cls = ops.stop_gradient(t_cls)\n        s_cls, t_cls = self.dino_head(s_cls), self.teacher_dino_head(t_cls) #both [batch, embed_dims]\n        t_cls = ops.softmax((t_cls - self.center_cls) / self.teacher_temp, axis = -1)\n        s_cls = ops.softmax(s_cls/self.student_temp, \n                            axis = -1)\n        t_cls = ops.clip(t_cls, 1e-5, 0.999)\n        s = ops.log_softmax(s_cls, axis = -1)\n        return ops.mean(ops.sum(-t_cls * s, axis = -1))\n        \n    def orthogonality_loss(self):\n        \"\"\"Compute orthogonality loss for cluster centers\"\"\"\n        # cluster_center: [K, D]\n        centers = tf.nn.l2_normalize(self.cluster_function.cluster_center, axis=1)\n        gram = tf.matmul(centers, centers, transpose_b=True)  # [K, K]\n        I = tf.eye(self.num_parts)\n        return tf.reduce_mean(tf.square(gram - I))\n    # ========================================\n    # Training\n    # ========================================\n    \n    def train_step(self, data):\n        \"\"\"Training step\"\"\"\n        images, labels = data #images는 0~255 uint8\n        batch_size, H, W, _ = ops.shape(images)\n        spatial_shape = (H//self.patch_size,W//self.patch_size)\n        global_crops, local_crops = self.create_crops(images) #0~1 사이 텐서로 변환 \n        student_spatial_shape = (self.global_crop_size//self.patch_size, self.global_crop_size//self.patch_size)\n        \n        loss_1_coeff = self.cos_schedule_decrease()\n        loss_2_coeff = 1-loss_1_coeff\n        \n        with tf.GradientTape() as tape:\n            \n            # Global: iBOT + Seg\n            _, h_global, w_global, _ = ops.shape(global_crops[0])\n            \n             \n            t_cls_g1, t_g1 = self.forward_teacher(global_crops[0])\n            t_cls_g2, t_g2 = self.forward_teacher(global_crops[1])\n            \n            s_cls_g1, s_g1, m_g1, student_patch_1 = self.forward_student(global_crops[0], self.mask_ratio)\n            s_cls_g2, s_g2, m_g2, student_patch_2 = self.forward_student(global_crops[1], self.mask_ratio)\n             \n            ibot_loss = self.compute_ibot_loss(s_g1, t_g2, m_g1) + \\\n                        self.compute_ibot_loss(s_g2, t_g1, m_g2)\n            center_new = ops.mean(ops.concatenate([t_g1, t_g2], axis = 0),\n                                  axis = (0,1))\n            cls_dino_loss = self.compute_dino_loss(s_cls = s_cls_g2, t_cls = t_cls_g1) + \\\n                            self.compute_dino_loss(s_cls = s_cls_g1, t_cls = t_cls_g2)\n            center_new_cls = ops.mean(ops.concatenate([t_cls_g1, t_cls_g2], axis = 0),\n                                  axis = 0)\n            # Local: iBOT only\n            for local in local_crops:\n                s_cls_local, s_local, m_local, _ = self.forward_student(local, self.mask_ratio)\n                \n                t_target = t_g1\n                n_local = ops.shape(s_local)[1]\n                n_global = ops.shape(t_target)[1]\n                idx = tf.random.shuffle(tf.range(n_global))[:n_local]\n                t_target = tf.gather(t_target, idx, axis=1)\n                \n                ibot_loss += self.compute_ibot_loss(s_local, t_target, m_local)\n                cls_dino_loss += self.compute_dino_loss(t_cls = t_cls_g1, s_cls = s_cls_local) + \\\n                                self.compute_dino_loss(t_cls = t_cls_g2, s_cls = s_cls_local)\n            ibot_loss /= (2 + len(local_crops))\n            cls_dino_loss /= (2 + len(local_crops))\n            \n            # Segmentation\n            cls_whole, encoded_patches, _ = self.student_vit(images)\n            encoded_patches = self.forward_dense(encoded_patches)\n            ### Learnable Clustering Layer!! ###\n            mi_loss, h_c, h_c_given_x, cluster_patches, patch_proba_soft = self.cluster_function(encoded_patches)\n            orthogonal_constraints = self.orthogonality_loss()\n            seg_loss = mi_loss + orthogonal_constraints\n            # Classification\n            cls_logits = self.cls_head(cls_whole)\n            cls_loss = ops.mean(keras.losses.sparse_categorical_crossentropy(\n                labels, cls_logits, from_logits=True\n            ))\n            \n            # Total\n            #Loss 1 : CLS level loss = cls_loss + cls_dino_loss\n            # Loss 2 : patch level loss = (1-s_w)iBOT loss + s_w*seg_loss\n            \n            loss_1 = 0.5*(cls_loss + cls_dino_loss)\n            loss_2 = (1-self.seg_weight) * ibot_loss + self.seg_weight*seg_loss\n            loss = loss_1_coeff*loss_1 + loss_2_coeff*loss_2\n            \n        self.center.assign(self.ema_momentum * self.center + (1 - self.ema_momentum) * center_new)\n        self.center_cls.assign(self.ema_momentum * self.center_cls + (1 - self.ema_momentum) * center_new_cls)\n        # Gradients\n        trainable_vars = self.trainable_variables\n        grads = tape.gradient(loss, trainable_vars)\n        \n        # NaN check\n        has_nan = tf.reduce_any([\n            tf.reduce_any(tf.math.is_nan(g)) for g in grads if g is not None\n        ])\n        \n        self.optimizer.apply_gradients(zip(grads, trainable_vars))\n        #EMA updates\n        self.update_teacher()\n        \n        # Metrics\n        self.ibot_tracker.update_state(ibot_loss)\n        self.dino_tracker.update_state(cls_dino_loss)\n        self.seg_tracker.update_state(seg_loss)\n        self.cls_tracker.update_state(cls_loss)\n        self.acc_tracker.update_state(labels, cls_logits)\n        \n        return {\n            'Total_loss': loss,\n            #1. Tokenwise(loss 1 component)\n            'tokenwise_cls': self.cls_tracker.result(),\n            'tokenwise_dino': self.dino_tracker.result(),\n            'tokenwise_acc': self.acc_tracker.result(),\n            \n            #2. patchwise(loss 2 component)\n            'patchwise_ibot': self.ibot_tracker.result(), \n            'patchwise_seg': self.seg_tracker.result(),\n            \"patchwise_MI\" : mi_loss,\n            \"patchwise_center_ortho\" : orthogonal_constraints,\n            \"H(Cluster)\" : h_c,\n            \"H(Cluster|Patch)_lower\" : h_c_given_x,\n\n            # 3. ETC\n            \"is_nan_error\" : keras.ops.mean(has_nan),\n        }\n    \n    # ========================================\n    # Inference\n    # ========================================\n    \n    def call(self, inputs, labels=None, training=False):\n        \"\"\"Inference\"\"\"\n        batch_size, H, W, _ = ops.shape(inputs)\n        spatial_shape = (H//patch_size, W//patch_size)\n        cls, patches, _ = self.teacher_vit(inputs, training=False)\n        \n        # Classification\n        logits = self.cls_head(cls)\n        if labels is None:\n            labels = ops.argmax(logits, axis=-1)\n        \n        # Segmentation\n        patches = self.forward_dense(patches)\n        mi_loss, h_c, h_c_given_x, part_seg, patch_proba_soft = self.cluster_function(patches)\n        # part_seg = [batch_size, n_patches]\n        batch_size, n_patches = ops.shape(part_seg)\n        assert n_patches == (H//patch_size) * (W//patch_size), \"call에서 part_seg이 잘못 배정됨\"\n        grid_size = ops.cast(ops.sqrt(ops.cast(n_patches, \"float32\")\n                                     ),\n                            \"int32\")\n        part_seg = ops.reshape(part_seg, [batch_size, grid_size, grid_size, 1])\n        return part_seg, logits\n    \n    # ========================================\n    # Visualization\n    # ========================================\n    \n    def visualize_results(self, images, labels=None, save_path=None):\n        \"\"\"Visualize segmentation results\"\"\"\n        part_seg, logits = self(images, labels, training=False)\n        preds = ops.argmax(logits, axis=-1)\n        \n        batch_size = int(tf.shape(images)[0])\n        res = int(tf.shape(images)[1])\n        \n        fig, axes = plt.subplots(batch_size, 3, figsize=(12, 4 * batch_size))\n        if batch_size == 1:\n            axes = axes[None, :]\n        \n        cmap = plt.cm.get_cmap('tab10', self.num_parts)\n        \n        for i in range(batch_size):\n            # Original\n            axes[i, 0].imshow(images[i])\n            axes[i, 0].set_title(f'Class: {labels[i].numpy() if labels != None else \"DUMMY\"}')\n            axes[i, 0].axis('off')\n            \n            # Parts\n            axes[i, 1].imshow(part_seg[i], cmap=cmap, vmin=0, vmax=self.num_parts-1)\n            axes[i, 1].set_title(f'Pred label : {int(preds[i])}')\n            axes[i, 1].axis('off')\n            \n            # Overlay\n            axes[i, 2].imshow(images[i])\n            axes[i, 2].imshow(tf.image.resize(part_seg[i], [res,res]), \n                              cmap=cmap, alpha=0.5, vmin=0, vmax=self.num_parts-1)\n            axes[i, 2].set_title('Seg result Upsample and Overlay')\n            axes[i, 2].axis('off')\n        \n        plt.tight_layout()\n        if save_path:\n            plt.savefig(save_path, dpi=150, bbox_inches='tight')\n        \n        return fig\n    \n    @property\n    def metrics(self):\n        return [self.ibot_tracker, self.seg_tracker, self.cls_tracker, self.acc_tracker]\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-13T03:38:26.065Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ========================================\n# Usage Example\n# ========================================\n# Create model\nif wandb_configs[\"use_hybrid\"]:\n    teacher = hybrid\n    student = hybrid\nelse:\n    teacher = vit\n    student = vit\nmodel = JointLearnerWithMI(\n        vit_backbone=student, teacher_model = teacher,\n        num_classes=165,\n        num_parts=32,\n        global_crop_size=384,\n        local_crop_sizes=[96, 128, 256],\n        num_local_crops=4,\n        mask_ratio=0.5,\n        masking_strategy='random',\n        seg_weight=0.5,\n    seg_distance = \"dot\"\n    )\n\ndummy = tf.random.normal([1,res,res,1])\nmodel(dummy)\nprint(f\"Inside model > student model trainable : {model.student_vit.trainable}, Teacher model : {model.teacher_vit.trainable}\")\n\nwandb_configs['segment_distance_metric'] = \"dot\"\n# Create Callbacks\nwandb.init(project = \"RadImageNet_2stage\",\n          config = wandb_configs)\nprint(\"=======Environmental Configurations=======\\n\")\npprint.pprint(wandb_configs)\n\nwandb_callback = WandbVisualizationCallback(\n    validation_images=imgs,\n    validation_labels=labs,\n    log_freq=100,\n    viz_freq=5000,\n    num_images_to_log=len(imgs)  # Number of images to show in the table\n)\n\nmodel.compile()\n\nfit_history = model.fit(train_radimagenet_ds, epochs = 1, steps_per_epoch = 10,\n                       callbacks = [wandb_callback])\n#output_figs = model.visualize_results(images = imgs, labels = labs)\n#visualize_token_attention_map(vit = model.teacher_vit, image = imgs)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-13T03:38:26.065Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#before\n\nvit = hybrid\na = vit(imgs[:2])\n#after\nvit = model.student_vit ; print(vit.trainable)\nb = vit(imgs[:2])\n\np1, p2 = a[1], b[1]\ndelta = p2-p1\ndelta = ops.reshape(delta,[2, 32, 32, -1])\ndelta = ops.mean(delta, axis = -1)\nplt.imshow(delta[0])\nplt.show()\nplt.imshow(delta[1])\nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-13T03:38:26.065Z"}},"outputs":[],"execution_count":null}]}