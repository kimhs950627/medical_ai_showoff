{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13451,"databundleVersionId":1188070,"sourceType":"competition"},{"sourceId":58333,"sourceType":"datasetVersion","datasetId":38326},{"sourceId":1950595,"sourceType":"datasetVersion","datasetId":1164135},{"sourceId":4729375,"sourceType":"datasetVersion","datasetId":2683088},{"sourceId":4800870,"sourceType":"datasetVersion","datasetId":2727590},{"sourceId":7923692,"sourceType":"datasetVersion","datasetId":4616374},{"sourceId":8471595,"sourceType":"datasetVersion","datasetId":5051531},{"sourceId":169421886,"sourceType":"kernelVersion"},{"sourceId":177600510,"sourceType":"kernelVersion"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os, sys\nimport random\nimport pydicom\n\nfrom sklearn.manifold import TSNE\nimport re\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nseed = 2024\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# ML tools \nsys.path.append(\"/kaggle/input/kimm-keras-image-model-repository\"\n               )\n\nimport tensorflow as tf\nimport keras# ; keras.config.set_dtype_policy(\"mixed_float16\")\nimport kimm\nimport keras_cv\nimport keras_nlp\n\nimport cv2\nfrom skimage.io import imread\nkeras.utils.set_random_seed(seed)\nimport tensorflow_io as tfio\nfrom kaggle_datasets import KaggleDatasets\nimport tensorflow_datasets as tfds\nimport tensorflow_probability as tfp\nimport tensorflow_decision_forests as tfdf\n\nprint(f\"Tensorflow version : {tf.__version__}\")\ntry:\n    print(f\"Keras version : {keras.__version__}\")\nexcept:\n    pass\n\nfrom keras import Input, Model, ops\nfrom keras.models import load_model\n\nfrom keras.layers import Conv2D, DepthwiseConv2D, Dense, Activation, BatchNormalization, LayerNormalization, MultiHeadAttention, Embedding, Subtract, Add, Multiply, GlobalAveragePooling2D, GlobalAveragePooling1D, LayerNormalization\nfrom keras.utils import load_img, img_to_array\nfrom keras.applications import *\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nfrom sklearn.model_selection import train_test_split\nfrom keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\nfrom tqdm.notebook import tqdm\nimport wandb\ndef wandb_config():\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    try:\n        secret_value_0 = user_secrets.get_secret(\"__gcloud_sdk_auth__\")\n        secret_value_1 = user_secrets.get_secret(\"huggingface_key\")\n        secret_value_2 = user_secrets.get_secret(\"wandb_key\")\n        !wandb login $secret_value_2\n    except:\n        secret_value_0 = user_secrets.get_secret(\"huggingface_key\")\n        secret_value_1 = user_secrets.get_secret(\"wandb_key\")\n        !wandb login $secret_value_1\n    \n\nres = int(1.0*256)\nsmall_res = 64\nbatch_size = 16\nembed_dims = 768\nn_multicrop = 5\n\ndef auto_select_accelerator():\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n        print(\"Running on TPU:\", tpu.master())\n    except ValueError:\n        tpu = False\n        strategy = tf.distribute.MirroredStrategy() # for GPU or multi-GPU machines\n    print(f\"Running on {strategy.num_replicas_in_sync} replicas\")\n    \n    return tpu, strategy\n\ntpu, strategy = auto_select_accelerator()\nbatch_size = strategy.num_replicas_in_sync * batch_size\nprint('batch size', batch_size)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-05-31T01:18:00.425569Z","iopub.execute_input":"2024-05-31T01:18:00.425920Z","iopub.status.idle":"2024-05-31T01:18:24.308732Z","shell.execute_reply.started":"2024-05-31T01:18:00.425883Z","shell.execute_reply":"2024-05-31T01:18:24.307726Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-05-31 01:18:04.605951: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-31 01:18:04.606066: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-31 01:18:04.720552: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Tensorflow version : 2.15.0\nKeras version : 3.3.3\nRunning on 1 replicas\nbatch size 16\n","output_type":"stream"}]},{"cell_type":"code","source":"import ssl_module\nfrom ssl_module import get_map_fn, get_gcvit_configs, get_flops, att_visualize, get_full_model, AttentionPooling, BarlowModel, VICRegModel, Moco, SimSiam, CLIP, SigLIP\nimport nas_ftp_module\nfrom nas_ftp_module import upload_file, download_file\nssl_module.available_models()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-05-31T01:18:24.310273Z","iopub.execute_input":"2024-05-31T01:18:24.310535Z","iopub.status.idle":"2024-05-31T01:18:24.466403Z","shell.execute_reply.started":"2024-05-31T01:18:24.310512Z","shell.execute_reply":"2024-05-31T01:18:24.465455Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirements loaded, keras : v3.3.3, Tensorflow : v2.15.0\nRandAug Component in this SSL module :  ['random_color_degeneration', 'random_contrast', 'random_brightness', 'random_shear', 'random_shear_1', 'random_translation', 'random_translation_1', 'grid_mask']\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"{'models_from_kimm': ['ConvMixer1024D20',\n  'ConvMixer1536D20',\n  'ConvMixer736D32',\n  'ConvNeXtAtto',\n  'ConvNeXtBase',\n  'ConvNeXtFemto',\n  'ConvNeXtLarge',\n  'ConvNeXtNano',\n  'ConvNeXtPico',\n  'ConvNeXtSmall',\n  'ConvNeXtTiny',\n  'ConvNeXtXLarge',\n  'DenseNet121',\n  'DenseNet161',\n  'DenseNet169',\n  'DenseNet201',\n  'EfficientNetB0',\n  'EfficientNetB1',\n  'EfficientNetB2',\n  'EfficientNetB3',\n  'EfficientNetB4',\n  'EfficientNetB5',\n  'EfficientNetB6',\n  'EfficientNetB7',\n  'EfficientNetLiteB0',\n  'EfficientNetLiteB1',\n  'EfficientNetLiteB2',\n  'EfficientNetLiteB3',\n  'EfficientNetLiteB4',\n  'EfficientNetV2B0',\n  'EfficientNetV2B1',\n  'EfficientNetV2B2',\n  'EfficientNetV2B3',\n  'EfficientNetV2L',\n  'EfficientNetV2M',\n  'EfficientNetV2S',\n  'EfficientNetV2XL',\n  'GhostNet050',\n  'GhostNet100',\n  'GhostNet100V2',\n  'GhostNet130',\n  'GhostNet130V2',\n  'GhostNet160V2',\n  'HGNetBase',\n  'HGNetSmall',\n  'HGNetTiny',\n  'HGNetV2B0',\n  'HGNetV2B1',\n  'HGNetV2B2',\n  'HGNetV2B3',\n  'HGNetV2B4',\n  'HGNetV2B5',\n  'HGNetV2B6',\n  'InceptionNeXtBase',\n  'InceptionNeXtSmall',\n  'InceptionNeXtTiny',\n  'InceptionV3',\n  'LCNet035',\n  'LCNet050',\n  'LCNet075',\n  'LCNet100',\n  'LCNet150',\n  'MobileNetV2W050',\n  'MobileNetV2W100',\n  'MobileNetV2W110',\n  'MobileNetV2W120',\n  'MobileNetV2W140',\n  'MobileNetV3W050Small',\n  'MobileNetV3W075Small',\n  'MobileNetV3W100Large',\n  'MobileNetV3W100LargeMinimal',\n  'MobileNetV3W100Small',\n  'MobileNetV3W100SmallMinimal',\n  'MobileOneS0',\n  'MobileOneS1',\n  'MobileOneS2',\n  'MobileOneS3',\n  'MobileViTS',\n  'MobileViTV2W050',\n  'MobileViTV2W075',\n  'MobileViTV2W100',\n  'MobileViTV2W125',\n  'MobileViTV2W150',\n  'MobileViTV2W175',\n  'MobileViTV2W200',\n  'MobileViTXS',\n  'MobileViTXXS',\n  'RegNetX002',\n  'RegNetX004',\n  'RegNetX006',\n  'RegNetX008',\n  'RegNetX016',\n  'RegNetX032',\n  'RegNetX040',\n  'RegNetX064',\n  'RegNetX080',\n  'RegNetX120',\n  'RegNetX160',\n  'RegNetX320',\n  'RegNetY002',\n  'RegNetY004',\n  'RegNetY006',\n  'RegNetY008',\n  'RegNetY016',\n  'RegNetY032',\n  'RegNetY040',\n  'RegNetY064',\n  'RegNetY080',\n  'RegNetY120',\n  'RegNetY160',\n  'RegNetY320',\n  'RepVGGA0',\n  'RepVGGA1',\n  'RepVGGA2',\n  'RepVGGB0',\n  'RepVGGB1',\n  'RepVGGB2',\n  'RepVGGB3',\n  'ResNet101',\n  'ResNet152',\n  'ResNet18',\n  'ResNet34',\n  'ResNet50',\n  'TinyNetA',\n  'TinyNetB',\n  'TinyNetC',\n  'TinyNetD',\n  'TinyNetE',\n  'VGG11',\n  'VGG13',\n  'VGG16',\n  'VGG19',\n  'VisionTransformerBase16',\n  'VisionTransformerBase32',\n  'VisionTransformerLarge16',\n  'VisionTransformerLarge32',\n  'VisionTransformerSmall16',\n  'VisionTransformerSmall32',\n  'VisionTransformerTiny16',\n  'VisionTransformerTiny32',\n  'Xception'],\n 'models_from_keras': ['effnet',\n  'effnet_small',\n  'effnet_base',\n  'convnext',\n  'convnext_small',\n  'convnext_base',\n  'mlpmixer_patch_depth_dims',\n  'convmixer_patch_depth_dims']}"},"metadata":{}}]},{"cell_type":"markdown","source":"# Data import (with Generator)\n- 목적 : CXR의 prior knowledge를 SwAV으로 feature map generator에 주입시키기\n- Bounding box의 information을 사용하지 않음 + External data를 사용하자","metadata":{}},{"cell_type":"code","source":"metainfo_dir = \"/kaggle/input/chexdet-image-and-annotations/ChestXDet_Metainformations/ChestX-Det-Dataset-main\"\ntrain_det_dir = \"/kaggle/input/chexdet-image-and-annotations/train_data/train\"\nval_det_dir = \"/kaggle/input/chexdet-image-and-annotations/test_data/test\"\n\ndf_det_train = pd.read_json(\"/kaggle/input/chexdet-image-and-annotations/ChestXDet_Metainformations/ChestX-Det-Dataset-main/ChestX_Det_train.json\")\ndf_det_train[\"file_name\"] = [os.path.join(train_det_dir, fname) for fname in df_det_train.file_name.values]\ndf_det_train = df_det_train.loc[:, [\"file_name\"]]\n\ndf_val = pd.read_json(\"/kaggle/input/chexdet-image-and-annotations/ChestXDet_Metainformations/ChestX-Det-Dataset-main/ChestX_Det_test.json\")\ndf_val[\"file_name\"] = [os.path.join(val_det_dir, fname) for fname in df_val.file_name.values]\ndf_val_cxr = df_val.loc[:, [\"file_name\"]]\n\n#\next_dir = \"/kaggle/input/vinbigdata-chest-xray-original-png/train\"\ndict_ext = {\"file_name\" : [os.path.join(ext_dir, fname) for fname in os.listdir(ext_dir)] }\ndf_ext = pd.DataFrame(dict_ext)\ndf_train_cxr = pd.concat([df_det_train, df_ext], axis = 0)\nprint(f\"Total training cases for CXR : {len(df_train_cxr)} cases, Validation case : {len(df_val_cxr)} case\")","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-05-31T01:18:24.468456Z","iopub.execute_input":"2024-05-31T01:18:24.469061Z","iopub.status.idle":"2024-05-31T01:18:25.894050Z","shell.execute_reply.started":"2024-05-31T01:18:24.469026Z","shell.execute_reply":"2024-05-31T01:18:25.893051Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Total training cases for CXR : 18025 cases, Validation case : 553 case\n","output_type":"stream"}]},{"cell_type":"markdown","source":"> Deeplesion metainformation dataframe 생성","metadata":{}},{"cell_type":"code","source":"ct_fname = []\nbase_img_dir = '/kaggle/input/nih-deeplesion-subset/minideeplesion'\nfor dirname, _, filenames in tqdm(os.walk(base_img_dir)):\n    for filename in filenames:\n        ct_fname.append(os.path.join(dirname, filename))\n        \ndf_ct_whole = pd.DataFrame({\"file_name\" : ct_fname})\n\ndf_ct_train, df_ct_val = train_test_split(df_ct_whole, \n                                         test_size = 134,\n                                         random_state = seed)\nprint(f\"Total training cases of Chest/Abdomen CT : {len(df_ct_train)} cases, Validation case : {len(df_ct_val)} case\")","metadata":{"execution":{"iopub.status.busy":"2024-05-31T01:18:25.895923Z","iopub.execute_input":"2024-05-31T01:18:25.896204Z","iopub.status.idle":"2024-05-31T01:18:47.379180Z","shell.execute_reply.started":"2024-05-31T01:18:25.896178Z","shell.execute_reply":"2024-05-31T01:18:47.378297Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ccd7fc8c854432bb8efb2a1ce948098"}},"metadata":{}},{"name":"stdout","text":"Total training cases of Chest/Abdomen CT : 33200 cases, Validation case : 134 case\n","output_type":"stream"}]},{"cell_type":"markdown","source":"> import RSNA ICH dataset metainformation dataframe","metadata":{}},{"cell_type":"code","source":"dicom_dir = \"/kaggle/input/rsna-intracranial-hemorrhage-detection/rsna-intracranial-hemorrhage-detection/stage_2_train\"\ndf_train_brainct = pd.read_csv(\"/kaggle/input/rsna-ich-detection-metadata/df_train_split.csv\")\ndf_val_brainct = pd.read_csv(\"/kaggle/input/rsna-ich-detection-metadata/df_val_splt.csv\").head(300)\n\nfor df in [df_train_brainct, df_val_brainct]:\n    df[\"file_name\"] = [os.path.join(dicom_dir, fname + \".dcm\") for fname in df['SOPInstanceUID']]\n    \nprint(f\"Total training cases of Brain, NonCE CT : {len(df_train_brainct)} cases, Validation case : {len(df_val_brainct)} case\")","metadata":{"execution":{"iopub.status.busy":"2024-05-31T01:18:47.380543Z","iopub.execute_input":"2024-05-31T01:18:47.380938Z","iopub.status.idle":"2024-05-31T01:18:47.532837Z","shell.execute_reply.started":"2024-05-31T01:18:47.380903Z","shell.execute_reply":"2024-05-31T01:18:47.531878Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Total training cases of Brain, NonCE CT : 32000 cases, Validation case : 300 case\n","output_type":"stream"}]},{"cell_type":"code","source":"df_train = pd.concat([df_ct_train, df_train_cxr, df_train_brainct], axis = 0, join='inner')\ndf_val = pd.concat([df_val_cxr, df_ct_val, df_val_brainct], axis = 0, join='inner')\n\ndf_train.to_csv(\"df_train_ER_SSL.csv\", index = False)\ndf_val.to_csv(\"df_val_ER_SSL.csv\", index = False)\n\ndf_train.sample(10)","metadata":{"execution":{"iopub.status.busy":"2024-05-31T01:18:47.534016Z","iopub.execute_input":"2024-05-31T01:18:47.534269Z","iopub.status.idle":"2024-05-31T01:18:47.923633Z","shell.execute_reply.started":"2024-05-31T01:18:47.534247Z","shell.execute_reply":"2024-05-31T01:18:47.922759Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                                               file_name\n18983  /kaggle/input/rsna-intracranial-hemorrhage-det...\n27904  /kaggle/input/rsna-intracranial-hemorrhage-det...\n4397   /kaggle/input/rsna-intracranial-hemorrhage-det...\n33135  /kaggle/input/nih-deeplesion-subset/minideeple...\n6872   /kaggle/input/nih-deeplesion-subset/minideeple...\n2586   /kaggle/input/rsna-intracranial-hemorrhage-det...\n2259   /kaggle/input/rsna-intracranial-hemorrhage-det...\n5180   /kaggle/input/rsna-intracranial-hemorrhage-det...\n13304  /kaggle/input/nih-deeplesion-subset/minideeple...\n27737  /kaggle/input/rsna-intracranial-hemorrhage-det...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>file_name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>18983</th>\n      <td>/kaggle/input/rsna-intracranial-hemorrhage-det...</td>\n    </tr>\n    <tr>\n      <th>27904</th>\n      <td>/kaggle/input/rsna-intracranial-hemorrhage-det...</td>\n    </tr>\n    <tr>\n      <th>4397</th>\n      <td>/kaggle/input/rsna-intracranial-hemorrhage-det...</td>\n    </tr>\n    <tr>\n      <th>33135</th>\n      <td>/kaggle/input/nih-deeplesion-subset/minideeple...</td>\n    </tr>\n    <tr>\n      <th>6872</th>\n      <td>/kaggle/input/nih-deeplesion-subset/minideeple...</td>\n    </tr>\n    <tr>\n      <th>2586</th>\n      <td>/kaggle/input/rsna-intracranial-hemorrhage-det...</td>\n    </tr>\n    <tr>\n      <th>2259</th>\n      <td>/kaggle/input/rsna-intracranial-hemorrhage-det...</td>\n    </tr>\n    <tr>\n      <th>5180</th>\n      <td>/kaggle/input/rsna-intracranial-hemorrhage-det...</td>\n    </tr>\n    <tr>\n      <th>13304</th>\n      <td>/kaggle/input/nih-deeplesion-subset/minideeple...</td>\n    </tr>\n    <tr>\n      <th>27737</th>\n      <td>/kaggle/input/rsna-intracranial-hemorrhage-det...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Building Dataloader in keras-3 style\n- Merging 2 kinds of dataset : original files with pd dataframe and tfrecord\n    - using this [tf dataset method](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#sample_from_datasets)\n    - [reference code](https://www.kaggle.com/code/calebeverett/combining-dataset-examples#Sample)","metadata":{}},{"cell_type":"markdown","source":"# Original Files with dataframe\n- using keras.utils.Sequence","metadata":{}},{"cell_type":"code","source":"class ImageDataLoader(keras.utils.Sequence):\n    def __init__(self, dataframe, x_col, res, batch_size, y_col = None, shuffle = True):\n        self.df = dataframe\n        self.x_col = x_col ; self.y_col = y_col\n        self.res = res\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.on_epoch_end()\n    def dicom_to_tensor(self, dicom_path):\n        dataset = pydicom.dcmread(dicom_path)\n        tensor = np.array(dataset.pixel_array)\n        slope = dataset.RescaleSlope   # dicom header (Rescale slope)\n        intercept = dataset.RescaleIntercept   # dicom header (Rescale intercept)\n        center = dataset.WindowCenter   # dicom header (Window center)\n        width = dataset.WindowWidth   # dicom header (Window width)\n\n        if(type(dataset.WindowCenter) == pydicom.multival.MultiValue):\n                center = float(dataset.WindowCenter[0])\n                width = float(dataset.WindowWidth[0])       \n        else:    \n                center = float(dataset.WindowCenter)\n                width = float(dataset.WindowWidth)\n\n        tensor = slope*tensor + intercept\n        lbound, ubound = center - 0.5*width, center + 0.5*width\n        tensor[np.where(tensor < lbound)] = lbound\n        tensor[np.where(tensor > ubound)] = ubound\n        tensor = tf.image.resize(tensor[:,:,tf.newaxis], [self.res,self.res],\n                                antialias = True) #HU unit\n        if tf.shape(tensor)[-1] == 1 :#gray\n            tensor = tf.image.grayscale_to_rgb(tensor)\n            \n        tensor = (tensor - tf.reduce_min(tensor)) / (tf.reduce_max(tensor) - tf.reduce_min(tensor) + 1e-4) #HU unit to Uint8\n        tensor = tensor*255.0\n        try:\n            del dataset\n        except:\n            pass\n        #print(f\"Dicom tensor shape : {ops.shape(tensor)}\")\n        return tensor\n    \n    def image_to_tensor(self, path):\n        if path.split(\".\")[-1] == \"dcm\":\n            return self.dicom_to_tensor(path)\n        \n        if \"minideeplesion\" in str(path).split(\"/\"):\n            image = imread(path).astype(np.float32)-32768\n            image = image[..., tf.newaxis]\n            image = tf.image.resize(image, [self.res, self.res],\n                                   antialias = True)\n            #print(f\"deepLesion tensor shape : {ops.shape(image)}\")\n            image = tf.clip_by_value(image, -750.0, 700.0)\n            image = (image - tf.reduce_min(image))/(tf.reduce_max(image) - tf.reduce_min(image) + 1e-3)\n            image = image * 255.0\n            \n        else:           \n            image = load_img(path, target_size = [self.res, self.res])\n            image = img_to_array(image)\n            #print(f\"other tensor shape : {ops.shape(image)}\")\n        if tf.shape(image)[-1] == 1 :#gray\n            image = tf.image.grayscale_to_rgb(image)\n            #image = np.array(image)\n            \n\n        return image\n        \n    def on_epoch_end(self):\n        self.indexes = np.arange(len(self.df))\n        if self.shuffle:\n            np.random.shuffle(self.indexes)\n            \n    def __len__(self):\n        return int(np.floor(len(self.df) / self.batch_size))\n    \n    def __data_generation(self, img_name):\n        ## path를 받아 img화 및 token화 하여 실제로 Feeding할 데이터를 반환\n        X = []\n        for i, fname in enumerate(img_name):\n            img = self.image_to_tensor(fname)\n            img = tf.convert_to_tensor(img)\n            img = tf.cast(img, tf.uint8)\n            X.append(img)\n        \n        return X\n        \n                \n    def __getitem__(self, index):\n        indexes = self.indexes[index * self.batch_size : (index + 1) * self.batch_size]\n        \n        img_name = [self.df.iloc[k].loc[self.x_col] for k in indexes]\n        \n        X = self.__data_generation(img_name)\n        #X = np.array(X).reshape([-1, self.res, self.res, 3])\n        return X\n    \ndef get_train_gen():\n    return ImageDataLoader(df_train, x_col = \"file_name\",\n                         res = res, batch_size = batch_size)\n\ndef get_val_gen():\n    return ImageDataLoader(df_val, x_col = \"file_name\",\n                         res = res, batch_size = batch_size)\n\n\ntrain_ds = tf.data.Dataset.from_generator(get_train_gen, (tf.uint8), output_shapes = (batch_size, res, res,3) ).ignore_errors().prefetch(tf.data.AUTOTUNE).repeat()\nval_ds = tf.data.Dataset.from_generator(get_val_gen, (tf.uint8), output_shapes = (batch_size, res, res,3) ).ignore_errors().prefetch(tf.data.AUTOTUNE).repeat()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-05-31T01:18:47.924973Z","iopub.execute_input":"2024-05-31T01:18:47.925303Z","iopub.status.idle":"2024-05-31T01:18:48.013048Z","shell.execute_reply.started":"2024-05-31T01:18:47.925272Z","shell.execute_reply":"2024-05-31T01:18:48.012055Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Spinal X-ray dataset in TFrecords","metadata":{}},{"cell_type":"code","source":"#spinal xray dataset\n\nlabel_map = {0: 'Disc space narrowing', 1: 'Foraminal stenosis', 2: 'No finding', 3: 'Osteophytes', 4: 'Other lesions', 5: 'Spondylolysthesis', 6: 'Surgical implant', 7: 'Vertebral collapse'}\n\nlabels = list(label_map.values())\nlabels.sort()\nn_labels = len(label_map)\n\n\ndef deserialize_example(serialized_string, train = True):\n    image_feature_description = {\n            'image': tf.io.FixedLenFeature([], tf.string),\n            'label': tf.io.FixedLenFeature([], tf.string)\n        }\n    parsed_record = tf.io.parse_single_example(serialized_string, image_feature_description)\n    image = tf.io.parse_tensor(parsed_record[\"image\"], tf.float32)\n    image = (image - tf.reduce_min(image))/(tf.reduce_max(image)-tf.reduce_min(image)+1e-4)\n    image = image * 255.0\n    image = tf.cast(image, tf.uint8)\n    image = ops.reshape(image, [res, res, 3])\n    label = tf.io.decode_raw(parsed_record['label'], tf.int32)\n    label = ops.reshape(label, [n_labels,])\n    return image, label\n    \ndef load_dataset(filenames):\n    ignore_order = tf.data.Options()\n    ignore_order.experimental_deterministic = False # disable order, increase speed\n    dataset = tf.data.TFRecordDataset(filenames, compression_type = \"GZIP\", \n                                      num_parallel_reads=tf.data.AUTOTUNE) # automatically interleaves reads from multiple files\n    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n    dataset = dataset.map(deserialize_example, num_parallel_calls=tf.data.AUTOTUNE)\n    return dataset\n\noriginal_train_ds = load_dataset(\"/kaggle/input/tfrecords-vindr-spinexr-tfrecords/train_gzip_384.tfrecord\")\noriginal_val_ds = load_dataset(\"/kaggle/input/tfrecords-vindr-spinexr-tfrecords/val_gzip_384.tfrecord\")\n\nspine_train_ds = original_train_ds.batch(batch_size, drop_remainder = True).map(lambda x,y:x).ignore_errors().repeat().prefetch(tf.data.AUTOTUNE)\nspine_val_ds = original_val_ds.batch(batch_size, drop_remainder = True).map(lambda x,y:x).ignore_errors().repeat().prefetch(tf.data.AUTOTUNE)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-05-31T01:18:48.014286Z","iopub.execute_input":"2024-05-31T01:18:48.014591Z","iopub.status.idle":"2024-05-31T01:18:48.289072Z","shell.execute_reply.started":"2024-05-31T01:18:48.014565Z","shell.execute_reply":"2024-05-31T01:18:48.288303Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Merging 2 dataset","metadata":{}},{"cell_type":"code","source":"merged_train_ds = tf.data.Dataset.sample_from_datasets([train_ds.unbatch(), spine_train_ds.unbatch()], weights = [0.5, 0.5]).batch(batch_size).ignore_errors().repeat().prefetch(tf.data.AUTOTUNE)\nmerged_val_ds = tf.data.Dataset.sample_from_datasets([val_ds.unbatch(), spine_val_ds.unbatch()], weights = [0.5, 0.5]).batch(batch_size).ignore_errors().repeat().prefetch(tf.data.AUTOTUNE)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-05-31T01:18:48.290134Z","iopub.execute_input":"2024-05-31T01:18:48.290429Z","iopub.status.idle":"2024-05-31T01:18:50.076959Z","shell.execute_reply.started":"2024-05-31T01:18:48.290403Z","shell.execute_reply":"2024-05-31T01:18:50.076007Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"> calculating train and validation steps per epoch\n\n- Spinal dataset : [여기 참고](https://www.kaggle.com/code/khsmdjjys/self-supervised-learning-with-tfrecord)","metadata":{}},{"cell_type":"code","source":"train_1 = len(df_train) ; val_1 = len(df_val)\ntrain_2 = 8389 ; val_2 = 2077\n\ntrain_steps = (train_1 + train_2)//batch_size\nval_steps = (val_1 + val_2)//batch_size\n\nprint(f\"Total Train cases, Val cases : {train_1 + train_2, val_1 + val_2}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-31T01:18:50.081224Z","iopub.execute_input":"2024-05-31T01:18:50.081512Z","iopub.status.idle":"2024-05-31T01:18:50.087545Z","shell.execute_reply.started":"2024-05-31T01:18:50.081487Z","shell.execute_reply":"2024-05-31T01:18:50.086613Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Total Train cases, Val cases : (91614, 3064)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Applying SSL functions\n- A. Basic function : return 2 global views (g=2)\n- B. SwAV-like strategy : return 2 global views + additional local views (l = 4)\n- Use get_map_fn in SSL module:\n> parameters of get_map_fn:\n    - res = image resolution, \n    - input_type = \"without_label\" or \"supervised\"\n    - output_type = \"ssl\" or \"ssl_with_label\"\n    - n_view = HOW MANY VIEWS? -> n_view >= 3일 때, 첫번째 이미지와 두 번째 이미지는 비교적 global information을 담고, 나머지 이미지는 local image (가로/세로 1/2)임.","metadata":{}},{"cell_type":"code","source":"multiview_fn = get_map_fn(res = res, input_type = \"without_label\", output_type = \"ssl\",\n                         n_view = n_multicrop)\ntwo_view_fn = get_map_fn(res = res, input_type = \"without_label\", output_type = \"ssl\",\n                         n_view = 2)\n\ntrain_ds = merged_train_ds.unbatch().map(two_view_fn, num_parallel_calls=tf.data.AUTOTUNE).batch(batch_size).ignore_errors().prefetch(tf.data.AUTOTUNE)\nval_ds = merged_val_ds.unbatch().map(two_view_fn, num_parallel_calls=tf.data.AUTOTUNE).batch(batch_size).ignore_errors().prefetch(tf.data.AUTOTUNE)\ntrain_ds_multiview = merged_train_ds.unbatch().map(multiview_fn, num_parallel_calls=tf.data.AUTOTUNE).batch(batch_size).ignore_errors().prefetch(tf.data.AUTOTUNE)\nval_ds_multiview = merged_val_ds.unbatch().map(multiview_fn, num_parallel_calls=tf.data.AUTOTUNE).batch(batch_size).ignore_errors().prefetch(tf.data.AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2024-05-31T01:18:50.088886Z","iopub.execute_input":"2024-05-31T01:18:50.089318Z","iopub.status.idle":"2024-05-31T01:18:58.230721Z","shell.execute_reply.started":"2024-05-31T01:18:50.089284Z","shell.execute_reply":"2024-05-31T01:18:58.229917Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"for imgs in val_ds_multiview.take(1):\n    images = imgs\nsample_img = images[0]\ntest_set = tuple([comp[:2] for comp in images])","metadata":{"execution":{"iopub.status.busy":"2024-05-31T01:18:58.231737Z","iopub.execute_input":"2024-05-31T01:18:58.231990Z","iopub.status.idle":"2024-05-31T01:19:18.080077Z","shell.execute_reply.started":"2024-05-31T01:18:58.231968Z","shell.execute_reply":"2024-05-31T01:19:18.079086Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"> Curate the dataset","metadata":{}},{"cell_type":"code","source":"view_curation = False","metadata":{"execution":{"iopub.status.busy":"2024-05-31T01:19:18.081471Z","iopub.execute_input":"2024-05-31T01:19:18.081929Z","iopub.status.idle":"2024-05-31T01:19:18.086518Z","shell.execute_reply.started":"2024-05-31T01:19:18.081887Z","shell.execute_reply":"2024-05-31T01:19:18.085559Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"if view_curation:\n    print(\"Training dataset Curation, with Basic SSL Fn (2 global views)\")\n    for originals, augs in train_ds.take(1):\n        for origin, aug in zip(originals, augs):\n            fig, axes = plt.subplots(1,2, figsize = (16, 8))\n            axes = axes.flatten()\n            axes[0].imshow(ops.cast(origin, \"uint8\"))\n            axes[1].imshow(ops.cast(aug, \"uint8\"))\n            axes[0].set_title(\"ORIGINAL\")\n            axes[1].set_title(\"GLOBAL VIEW AUGMENTATION\")\n            plt.show()\n\n    print(\"Validation dataset Curation, with Basic SSL Fn (2 global views)\")\n    for originals, augs in val_ds.take(1):\n        for origin, aug in zip(originals, augs):\n            fig, axes = plt.subplots(1,2, figsize = (16, 8))\n            axes = axes.flatten()\n            axes[0].imshow(ops.cast(origin, \"uint8\"))\n            axes[1].imshow(ops.cast(aug, \"uint8\"))\n            axes[0].set_title(\"ORIGINAL\")\n            axes[1].set_title(\"GLOBAL VIEW AUGMENTATION\")\n            plt.show()","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-05-31T01:19:18.087872Z","iopub.execute_input":"2024-05-31T01:19:18.088257Z","iopub.status.idle":"2024-05-31T01:19:18.100727Z","shell.execute_reply.started":"2024-05-31T01:19:18.088222Z","shell.execute_reply":"2024-05-31T01:19:18.099918Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"if view_curation:\n    for multiset in train_ds_multiview.take(1):\n        global_views = multiset[:2]\n        local_views = multiset[2:]\n    for idx in tqdm(range(batch_size)):\n        print(f\"=================\\nBatch No.{idx}\\n===================\")\n        print(\"Global Views\")\n        fig, axes = plt.subplots(1,2, figsize = (20,10))\n        axes = axes.flatten()\n        g1, g2 = global_views[0][idx], global_views[1][idx] \n        axes[0].imshow(ops.cast(ops.squeeze(g1), \"uint8\"))\n        axes[1].imshow(ops.cast(ops.squeeze(g2), \"uint8\"))\n        plt.show()\n        print(\"=================\\nLocal Views\\n===================\")\n        fig, axes = plt.subplots(2,2, figsize = (16,16))\n        axes = axes.flatten()\n        local_set = [local_views[0][idx], local_views[1][idx], local_views[2][idx], local_views[3][idx]] \n\n        for k in range(4):\n            axes[k].imshow(ops.cast(ops.squeeze(local_set[k]), \"uint8\"))\n        plt.show()","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-05-31T01:19:18.101811Z","iopub.execute_input":"2024-05-31T01:19:18.102061Z","iopub.status.idle":"2024-05-31T01:19:18.115896Z","shell.execute_reply.started":"2024-05-31T01:19:18.102039Z","shell.execute_reply":"2024-05-31T01:19:18.115117Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# SSL experiment : Information-Maximization","metadata":{}},{"cell_type":"markdown","source":"> Model Save and Attention map visualize callbacks","metadata":{}},{"cell_type":"code","source":"class ModelSaveCallback(keras.callbacks.Callback):\n    def __init__(self, exp_name, **kwargs):\n        super().__init__(**kwargs)\n        self.exp_name = exp_name\n    def on_epoch_end(self, epoch, logs=None):\n        save_dir = \"/kaggle/working/\" ; target_dir = '/kaggle/working/model_save'\n        os.makedirs(target_dir, exist_ok = True)\n        if (epoch % 1 == 0):\n            try:\n                print(\"\\nModel Saving to local notebook...\")\n                file_name = f\"{self.exp_name}_{self.model.name}_keras_v3_Epoch{epoch}.keras\"\n                filepath = os.path.join(target_dir, file_name)\n                saved_dir = self.model.save(filepath, overwrite=True)\n                if (epoch+1) % 5 == 0:\n                    print(\"\\nModel Uploading to NAS...\")\n                    upload_file(file_name, filepath)\n                    print(\"\\nModel Saved to Local NAS\")\n            except Exception as e: \n                print('Model Saving Error:\\n', e)\n    def on_train_batch_end(self, batch, logs=None):\n        save_dir = \"/kaggle/working/\" ; target_dir = '/kaggle/working/model_save'\n        os.makedirs(target_dir, exist_ok = True)\n        if (batch % 5000 == 0) and (batch != 0): \n            try:\n                print(\"\\nModel Saving to local notebook...\")\n                file_name = f\"{self.exp_name}_{self.model.name}_keras_v3_Batch{batch}.keras\"\n                filepath = os.path.join(target_dir, file_name)\n                saved_dir = self.model.save(filepath, overwrite=True)\n                if (batch % 10000 == 0):\n                    print(\"\\nModel Uploading to NAS...\")\n                    upload_file(file_name, filepath)\n                    print(\"\\nModel Saved to Local NAS\")\n            except Exception as e: \n                print('Model Saving Error:\\n', e)\n                \n                \nclass TrainingViz(keras.callbacks.Callback):\n    def __init__(self, run):\n        super().__init__()\n        self.run = run\n    def on_epoch_end(self, epoch, logs=None):\n        try:\n            configs = self.model.get_config() ; method = configs[\"SSL_method\"]\n            if method in [\"CLIP\" , \"SigLIP\", \"SPARC\"]:\n                feature_extractor = self.model\n            else:\n                feature_extractor = self.model.feature_extractor\n            viz_weights = ssl_module.att_visualize(feature_extractor, sample_img, res,\n                                                  thresholding = True)\n            viz_weights = np.array(viz_weights) #batch, heads, res, res, 3\n            heads = viz_weights.shape[1]\n            origin = [\"Original Image\"]\n            col = [f\"Head{idx + 1}\" for idx in range(heads)]\n            col = origin + col\n\n            visualize_data = []\n            for idx, weights in enumerate(viz_weights):\n                origin_img = [wandb.Image(sample_img[idx])]\n                tmp = [wandb.Image(weights[idx]) for idx in range(heads)]\n                tmp = origin_img + tmp\n                visualize_data.append(tmp)\n                del tmp, origin_img\n            tbl = wandb.Table(columns = col, data = visualize_data)\n            wandb.log({f\"Epoch{epoch+1}_{method}_result\": tbl})\n            del feature_extractor, tbl\n            tf.keras.backend.clear_session()\n        except Exception as e: \n                print('Model Saving Error:\\n', e)\n        \n    def on_train_batch_end(self, batch, logs=None):\n        if (batch == (train_steps//2)): \n            try:\n                configs = self.model.get_config() ; method = configs[\"SSL_method\"]\n                if method in [\"CLIP\" , \"SigLIP\", \"SPARC\"]:\n                    feature_extractor = self.model\n                else:\n                    feature_extractor = self.model.feature_extractor\n                viz_weights = ssl_module.att_visualize(feature_extractor, sample_img, res,\n                                                      thresholding = True)\n                viz_weights = np.array(viz_weights) #batch, heads, res, res, 3\n                heads = viz_weights.shape[1]\n                origin = [\"Original Image\"]\n                col = [f\"Head{idx + 1}\" for idx in range(heads)]\n                col = origin + col\n                visualize_data = []\n                for idx, weights in enumerate(viz_weights):\n                    origin_img = [wandb.Image(sample_img[idx])]\n                    tmp = [wandb.Image(weights[idx]) for idx in range(heads)]\n                    tmp = origin_img + tmp\n                    visualize_data.append(tmp)\n                    del tmp, origin_img\n                tbl = wandb.Table(columns = col, data = visualize_data)\n                if batch == 0:\n                    wandb.log({f\"ZeroBatch_{method}_result\": tbl})\n                else:\n                    wandb.log({f\"MidEpoch_{method}_result\": tbl})\n                del feature_extractor, tbl\n                tf.keras.backend.clear_session()\n            except Exception as e:\n                print(\"Error code in callback : \", e)\n           \n        else:\n            pass","metadata":{"execution":{"iopub.status.busy":"2024-05-31T01:19:18.117236Z","iopub.execute_input":"2024-05-31T01:19:18.117571Z","iopub.status.idle":"2024-05-31T01:19:18.142952Z","shell.execute_reply.started":"2024-05-31T01:19:18.117541Z","shell.execute_reply":"2024-05-31T01:19:18.142134Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"> Training/logging helper function","metadata":{}},{"cell_type":"code","source":"def run_exp(model, train_ds = train_ds, val_ds = val_ds, epochs = 10, note= None):\n    try:\n        wandb.finish()\n    except:\n        pass\n    \n    if True :\n        wandb_config()\n        configs = model.get_config()\n        method = configs[\"SSL_method\"]\n        if method in ['CLIP', \"SigLIP\", \"SPARC\"]:\n            _ = model((example_images[:2], example_reports[:2]))\n            feature_extractor_flops = get_flops(model.image_encoder, [example_images[:1]])\n        else:\n            _ = model(test_set)\n            feature_extractor_flops = get_flops(model.feature_extractor, [sample_img[:1]])\n        env_config = {\"batch_size\" : batch_size, \"original resolution\" : res, \"local view resolution\" : small_res,\n                     \"Training steps\" : train_steps,\n                     \"Val steps\" : val_steps,\n                     \"train cases\" : (train_1 + train_2),\n                     \"val cases\" : (val_1 + val_2),\n                     \"embed_dims\" : embed_dims,\n                     \"Image resolution\" : res,\n                     \"(Image) Encoder Flops(G)\" : feature_extractor_flops,\n                     \"dtype\" : keras.mixed_precision.dtype_policy(),\n                      \"Optimizer configs\" : model.optimizer.get_config(),\n                      \"Multicrop N\" : n_multicrop,\n                     }\n        configs.update(env_config)\n        \n        wd = \"/kaggle/working/\"\n        file_name = os.path.join(wd, f\"{method}_GrandCXR_mini.keras\")\n        print(configs, \"\\n\\n\")\n        model.summary()\n        run = wandb.init(project=\"FusionFocus\", \n                         entity=\"gongbungkim\", config = configs, notes = note)\n\n        pass_error = keras.callbacks.TerminateOnNaN()\n        wb_callback = wandb.keras.WandbMetricsLogger(log_freq = 100)\n        \n        callbacks = [pass_error, wb_callback, ModelSaveCallback(f\"FF_SSL_{method}\"), \n                     TrainingViz(run)]\n        if val_ds is not None:\n            hist = model.fit(train_ds, \n                             steps_per_epoch = train_steps, \n                             epochs = epochs, \n                             validation_data = val_ds, \n                             validation_steps = val_steps, \n                             verbose = 1,\n                             callbacks = callbacks)\n        else:\n            hist = model.fit(train_ds, \n                         steps_per_epoch = train_steps, \n                         epochs = epochs, \n                         verbose = 1,\n                         callbacks = callbacks)\n    return hist","metadata":{"execution":{"iopub.status.busy":"2024-05-31T01:19:18.144146Z","iopub.execute_input":"2024-05-31T01:19:18.144538Z","iopub.status.idle":"2024-05-31T01:19:18.159353Z","shell.execute_reply.started":"2024-05-31T01:19:18.144513Z","shell.execute_reply":"2024-05-31T01:19:18.158534Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"> Feature extractor setting\n- General-Context Vision Transformer,\n- Convolution-based models:\n    - EfficientNetV2B0, Small\n    - ConvNeXtTiny, Small","metadata":{}},{"cell_type":"code","source":"gc_xxtiny_configs = get_gcvit_configs(res, 64, \"GC_ViT_xxtiny\")\ngc_xxtiny_configs[\"level_depth\"] = [1,1,2,2]\n\ngc_tiny_configs = get_gcvit_configs(res, 64, \"GC_ViT_tiny\")\ngc_tiny_configs[\"level_depth\"] = [1,1,2,4]\n\ngc_small_configs = get_gcvit_configs(res, 64, \"GC_ViT_small\")\ngc_small_configs[\"level_depth\"] = [1,2,4,6]","metadata":{"execution":{"iopub.status.busy":"2024-05-31T01:19:18.160452Z","iopub.execute_input":"2024-05-31T01:19:18.161351Z","iopub.status.idle":"2024-05-31T01:19:18.172728Z","shell.execute_reply.started":"2024-05-31T01:19:18.161318Z","shell.execute_reply":"2024-05-31T01:19:18.171929Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"- Setting Final Feature Extractors","metadata":{}},{"cell_type":"code","source":"gcvit_xxtiny = get_full_model(gc_xxtiny_configs, res = res, pe_type = None, att_depth = 1, att_heads = 16, embed_dims = embed_dims)\ngcvit_tiny = get_full_model(gc_tiny_configs, res = res, pe_type = None, att_depth = 1, att_heads = 16, embed_dims = embed_dims)\ngcvit_small = get_full_model(gc_small_configs, res = res, pe_type = None, att_depth = 1, att_heads = 16, embed_dims = embed_dims)\n###############\nmlpmixer = get_full_model(\"mlpmixer_16_4_512\",res = res, att_depth = 1, att_heads = 16, embed_dims = embed_dims)\nconvmixer = get_full_model(\"convmixer_16_4_512\",res = res, att_depth = 1, att_heads = 16, embed_dims = embed_dims)\n\npretrained = kimm.models.MobileViTS(input_shape = [res,res,3], include_top = False)\npretrained = get_full_model(pretrained, res = res, embed_dims = embed_dims, pe_type = None, att_depth = 1, att_heads = 16)","metadata":{"execution":{"iopub.status.busy":"2024-05-31T01:19:18.173794Z","iopub.execute_input":"2024-05-31T01:19:18.174050Z","iopub.status.idle":"2024-05-31T01:19:33.337493Z","shell.execute_reply.started":"2024-05-31T01:19:18.174027Z","shell.execute_reply":"2024-05-31T01:19:33.336267Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Downloading data from https://github.com/james77777778/kimm/releases/download/0.1.0/mobilevits_mobilevit_s.cvnets_in1k.keras\n\u001b[1m22944350/22944350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","output_type":"stream"}]},{"cell_type":"markdown","source":"> learning rate setting","metadata":{}},{"cell_type":"code","source":"cosine_decay = keras.optimizers.schedules.CosineDecay(\n    initial_learning_rate = 1e-6,\n    decay_steps = int(0.5*train_steps),\n    alpha=1e-5,\n    name='CosineDecay',\n    warmup_target=5e-3,\n    warmup_steps=train_steps - int(0.3*train_steps)\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-31T01:19:33.339712Z","iopub.execute_input":"2024-05-31T01:19:33.340580Z","iopub.status.idle":"2024-05-31T01:19:33.346518Z","shell.execute_reply.started":"2024-05-31T01:19:33.340534Z","shell.execute_reply":"2024-05-31T01:19:33.345439Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def ssl_train(module, feature_extractor, embed_dims = embed_dims, multiview = True, gradient_accumulation = None,\n             note = \"\"):\n    ssl_trainer = module(feature_extractor, embed_dims = embed_dims, multiview = multiview)\n    ssl_trainer.compile(optimizer = keras.optimizers.AdamW(learning_rate = 1e-4,\n                                                         clipvalue = 1.0,\n                                                         #amsgrad = True,\n                                                           gradient_accumulation_steps=gradient_accumulation,\n                                                         ),\n                        #run_eagerly = True\n                      )\n    run_exp(ssl_trainer, train_ds_multiview, None, epochs = 100,\n       note = \"Without validation d/t lack of resources\" + note)","metadata":{"execution":{"iopub.status.busy":"2024-05-31T01:19:33.348175Z","iopub.execute_input":"2024-05-31T01:19:33.348468Z","iopub.status.idle":"2024-05-31T01:19:33.360486Z","shell.execute_reply.started":"2024-05-31T01:19:33.348442Z","shell.execute_reply":"2024-05-31T01:19:33.359531Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"--------","metadata":{}},{"cell_type":"markdown","source":"# Experiment","metadata":{}},{"cell_type":"code","source":"#ssl_train(ssl_module.BarlowModel, pretrained, #gradient_accumulation = 8,\n#         note = \"+ NEW aug, Attentional pooling with register\")","metadata":{"execution":{"iopub.status.busy":"2024-05-31T01:19:33.361509Z","iopub.execute_input":"2024-05-31T01:19:33.361777Z","iopub.status.idle":"2024-05-31T01:19:33.370805Z","shell.execute_reply.started":"2024-05-31T01:19:33.361753Z","shell.execute_reply":"2024-05-31T01:19:33.369889Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"# Barlow Twins","metadata":{}},{"cell_type":"code","source":"#barlow_trainer = BarlowModel(convmixer, \n#                             embed_dims = embed_dims, multiview = True)\n#barlow_trainer.compile(optimizer = keras.optimizers.AdamW(learning_rate = cosine_decay,\n#                                                         clipvalue = 1.0,\n#                                                         #amsgrad = True\n#                                                         )\n#                      )\n#run_exp(barlow_trainer, train_ds_multiview, None, epochs = 100,\n#       note = \"Without validation d/t lack of resources\")","metadata":{"execution":{"iopub.status.busy":"2024-05-31T01:19:33.371925Z","iopub.execute_input":"2024-05-31T01:19:33.372295Z","iopub.status.idle":"2024-05-31T01:19:33.385007Z","shell.execute_reply.started":"2024-05-31T01:19:33.372270Z","shell.execute_reply":"2024-05-31T01:19:33.384009Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"# VICReg","metadata":{}},{"cell_type":"code","source":"#vic_trainer = VICRegModel(eff_tiny, \n#                             embed_dims = embed_dims, multiview = True)\n#vic_trainer.compile(optimizer = keras.optimizers.AdamW(learning_rate = cosine_decay,\n#                                                         clipvalue = 1.0,\n#                                                         #amsgrad = True\n#                                                         )\n#                      )\n#run_exp(vic_trainer, train_ds_multiview, None, epochs = 100,\n#       note = \"Without validation d/t lack of resources\")","metadata":{"execution":{"iopub.status.busy":"2024-05-31T01:19:33.386065Z","iopub.execute_input":"2024-05-31T01:19:33.386336Z","iopub.status.idle":"2024-05-31T01:19:33.394899Z","shell.execute_reply.started":"2024-05-31T01:19:33.386312Z","shell.execute_reply":"2024-05-31T01:19:33.394077Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"# SimSiam\n- instant collapse....","metadata":{}},{"cell_type":"code","source":"#ssl_train(ssl_module.SimSiam, eff_tiny)","metadata":{"execution":{"iopub.status.busy":"2024-05-31T01:19:33.396040Z","iopub.execute_input":"2024-05-31T01:19:33.396323Z","iopub.status.idle":"2024-05-31T01:19:33.405229Z","shell.execute_reply.started":"2024-05-31T01:19:33.396299Z","shell.execute_reply":"2024-05-31T01:19:33.404232Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"# SimCLR","metadata":{}},{"cell_type":"code","source":"#ssl_train(ssl_module.SimCLR, eff_small,\n#         gradient_accumulation = 8, note = \"+ Attentional pooling with register\")","metadata":{"execution":{"iopub.status.busy":"2024-05-31T01:19:33.406296Z","iopub.execute_input":"2024-05-31T01:19:33.406587Z","iopub.status.idle":"2024-05-31T01:19:33.414561Z","shell.execute_reply.started":"2024-05-31T01:19:33.406563Z","shell.execute_reply":"2024-05-31T01:19:33.413651Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"# DINO","metadata":{}},{"cell_type":"code","source":"dino_trainer = ssl_module.DINO(pretrained, \n                             embed_dims = embed_dims, multiview = True,\n                              apply_simclr = True,\n                              apply_barlow = True)\ndino_trainer.compile(optimizer = keras.optimizers.AdamW(learning_rate = 1e-4,\n                                                         clipvalue = 1.0,\n                                                         #amsgrad = True\n                                                         )\n                      )\nrun_exp(dino_trainer, train_ds_multiview, None, epochs = 100,\n       note = \"Without validation d/t lack of resources\")","metadata":{"execution":{"iopub.status.busy":"2024-05-31T01:19:33.415639Z","iopub.execute_input":"2024-05-31T01:19:33.415980Z","iopub.status.idle":"2024-05-31T01:28:22.717633Z","shell.execute_reply.started":"2024-05-31T01:19:33.415946Z","shell.execute_reply":"2024-05-31T01:28:22.715747Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n{'feature_extractor_name': 'MobileViTS_depth1_dims768_heads16', 'embed_dims': 768, 'Multiview(>2)': True, 'SSL_method': 'DINOv1', 'Linear Probe': False, 'N_Categories': 0, 'Probe Activation': 'NA', 'Apply SimCLR': True, 'Apply Barlow': True, 'batch_size': 16, 'original resolution': 256, 'local view resolution': 64, 'Training steps': 5725, 'Val steps': 191, 'train cases': 91614, 'val cases': 3064, 'Image resolution': 256, '(Image) Encoder Flops(G)': 4.461431412, 'dtype': <FloatDTypePolicy \"float32\">, 'Optimizer configs': {'name': 'adamw', 'learning_rate': 9.999999747378752e-05, 'weight_decay': 0.004, 'clipnorm': None, 'global_clipnorm': None, 'clipvalue': 1.0, 'use_ema': False, 'ema_momentum': 0.99, 'ema_overwrite_frequency': None, 'loss_scale_factor': None, 'gradient_accumulation_steps': None, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}, 'Multicrop N': 5} \n\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"dino\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"dino\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ MobileViTS_depth1_dims768_head… │ ?                      │     \u001b[38;5;34m7,801,472\u001b[0m │\n│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ Teacher_DINO_predictor          │ ?                      │     \u001b[38;5;34m1,774,080\u001b[0m │\n│ (\u001b[38;5;33mSequential\u001b[0m)                    │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ Student_DINO_predictor          │ ?                      │     \u001b[38;5;34m1,181,952\u001b[0m │\n│ (\u001b[38;5;33mSequential\u001b[0m)                    │                        │               │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ MobileViTS_depth1_dims768_head… │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">7,801,472</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ Teacher_DINO_predictor          │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,774,080</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)                    │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ Student_DINO_predictor          │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,181,952</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)                    │                        │               │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m10,757,504\u001b[0m (41.04 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,757,504</span> (41.04 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m10,745,248\u001b[0m (40.99 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,745,248</span> (40.99 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m12,256\u001b[0m (47.88 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,256</span> (47.88 KB)\n</pre>\n"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgongbungkim\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240531_011952-zd21t0jw</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/gongbungkim/FusionFocus/runs/zd21t0jw' target=\"_blank\">icy-snowball-113</a></strong> to <a href='https://wandb.ai/gongbungkim/FusionFocus' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/gongbungkim/FusionFocus' target=\"_blank\">https://wandb.ai/gongbungkim/FusionFocus</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/gongbungkim/FusionFocus/runs/zd21t0jw' target=\"_blank\">https://wandb.ai/gongbungkim/FusionFocus/runs/zd21t0jw</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/100\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1717118668.154269     107 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\nW0000 00:00:1717118668.346329     107 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1717118668.352028     107 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1717118668.354327     107 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1717118668.359867     107 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1717118668.362239     107 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1717118668.467113     107 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1717118668.498462     107 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1717118668.511083     107 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Unable to log learning rate.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m 158/5725\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:15:15\u001b[0m 1s/step - Barlow_loss: 0.4131 - DINO_feature_std: 0.1569 - DINO_loss: 0.0132 - SimCLR_loss: 6.0174 - Total_loss: 3.4350","output_type":"stream"},{"name":"stderr","text":"Exception ignored in: <function _xla_gc_callback at 0x79d104c78040>\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/jax/_src/lib/__init__.py\", line 98, in _xla_gc_callback\n    def _xla_gc_callback(*args):\nKeyboardInterrupt: \n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[27], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m dino_trainer \u001b[38;5;241m=\u001b[39m ssl_module\u001b[38;5;241m.\u001b[39mDINO(pretrained, \n\u001b[1;32m      2\u001b[0m                              embed_dims \u001b[38;5;241m=\u001b[39m embed_dims, multiview \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      3\u001b[0m                               apply_simclr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      4\u001b[0m                               apply_barlow \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m dino_trainer\u001b[38;5;241m.\u001b[39mcompile(optimizer \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdamW(learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-4\u001b[39m,\n\u001b[1;32m      6\u001b[0m                                                          clipvalue \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m,\n\u001b[1;32m      7\u001b[0m                                                          \u001b[38;5;66;03m#amsgrad = True\u001b[39;00m\n\u001b[1;32m      8\u001b[0m                                                          )\n\u001b[1;32m      9\u001b[0m                       )\n\u001b[0;32m---> 10\u001b[0m \u001b[43mrun_exp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdino_trainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_ds_multiview\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m       \u001b[49m\u001b[43mnote\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWithout validation d/t lack of resources\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[17], line 52\u001b[0m, in \u001b[0;36mrun_exp\u001b[0;34m(model, train_ds, val_ds, epochs, note)\u001b[0m\n\u001b[1;32m     44\u001b[0m         hist \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(train_ds, \n\u001b[1;32m     45\u001b[0m                          steps_per_epoch \u001b[38;5;241m=\u001b[39m train_steps, \n\u001b[1;32m     46\u001b[0m                          epochs \u001b[38;5;241m=\u001b[39m epochs, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m                          verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     50\u001b[0m                          callbacks \u001b[38;5;241m=\u001b[39m callbacks)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         hist \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                     \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hist\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py:314\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[1;32m    313\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 314\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[1;32m    316\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:877\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    875\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 877\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    881\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    882\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m     args,\n\u001b[1;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1327\u001b[0m     executing_eagerly)\n\u001b[1;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1501\u001b[0m   )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"wandb.finish()","metadata":{"execution":{"iopub.status.busy":"2024-05-31T01:28:25.046551Z","iopub.execute_input":"2024-05-31T01:28:25.047398Z","iopub.status.idle":"2024-05-31T01:28:33.008782Z","shell.execute_reply.started":"2024-05-31T01:28:25.047366Z","shell.execute_reply":"2024-05-31T01:28:33.008030Z"},"trusted":true},"execution_count":28,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.186 MB of 0.186 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch/Barlow_loss</td><td>█▁</td></tr><tr><td>batch/DINO_feature_std</td><td>▁█</td></tr><tr><td>batch/DINO_loss</td><td>█▁</td></tr><tr><td>batch/SimCLR_loss</td><td>█▁</td></tr><tr><td>batch/Total_loss</td><td>█▁</td></tr><tr><td>batch/batch_step</td><td>▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch/Barlow_loss</td><td>0.33039</td></tr><tr><td>batch/DINO_feature_std</td><td>0.16547</td></tr><tr><td>batch/DINO_loss</td><td>0.01268</td></tr><tr><td>batch/SimCLR_loss</td><td>4.68341</td></tr><tr><td>batch/Total_loss</td><td>2.68477</td></tr><tr><td>batch/batch_step</td><td>100</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">icy-snowball-113</strong> at: <a href='https://wandb.ai/gongbungkim/FusionFocus/runs/zd21t0jw' target=\"_blank\">https://wandb.ai/gongbungkim/FusionFocus/runs/zd21t0jw</a><br/> View project at: <a href='https://wandb.ai/gongbungkim/FusionFocus' target=\"_blank\">https://wandb.ai/gongbungkim/FusionFocus</a><br/>Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 1 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240531_011952-zd21t0jw/logs</code>"},"metadata":{}}]}]}