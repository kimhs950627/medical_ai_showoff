{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceType":"competition","sourceId":13451,"datasetId":654585,"databundleVersionId":1188070},{"sourceType":"datasetVersion","sourceId":7923692,"datasetId":4616374,"databundleVersionId":8031839},{"sourceType":"datasetVersion","sourceId":4800870,"datasetId":2727590,"databundleVersionId":4864291},{"sourceType":"datasetVersion","sourceId":8471595,"datasetId":5051531,"databundleVersionId":8611166},{"sourceType":"datasetVersion","sourceId":58333,"datasetId":38326,"databundleVersionId":60763},{"sourceType":"datasetVersion","sourceId":4729375,"datasetId":2683088,"databundleVersionId":4792279},{"sourceType":"datasetVersion","sourceId":1950595,"datasetId":1164135,"databundleVersionId":1989350},{"sourceType":"kernelVersion","sourceId":169421886},{"sourceType":"kernelVersion","sourceId":178935162}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os, sys\nimport random\nimport pydicom\n\nfrom sklearn.manifold import TSNE\nimport re\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nseed = 2024\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# ML tools \nsys.path.append(\"/kaggle/input/kimm-keras-image-model-repository\"\n               )\n\nimport tensorflow as tf\nimport keras# ; keras.config.set_dtype_policy(\"mixed_float16\")\nimport kimm\nimport keras_cv\nimport keras_nlp\n\nimport cv2\nfrom skimage.io import imread\nkeras.utils.set_random_seed(seed)\nimport tensorflow_io as tfio\nfrom kaggle_datasets import KaggleDatasets\nimport tensorflow_datasets as tfds\nimport tensorflow_probability as tfp\nimport tensorflow_decision_forests as tfdf\n\nprint(f\"Tensorflow version : {tf.__version__}\")\ntry:\n    print(f\"Keras version : {keras.__version__}\")\nexcept:\n    pass\n\nfrom keras import Input, Model, ops\nfrom keras.models import load_model\n\nfrom keras.layers import Conv2D, DepthwiseConv2D, Dense, Activation, BatchNormalization, LayerNormalization, MultiHeadAttention, Embedding, Subtract, Add, Multiply, GlobalAveragePooling2D, GlobalAveragePooling1D, LayerNormalization\nfrom keras.utils import load_img, img_to_array\nfrom keras.applications import *\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nfrom sklearn.model_selection import train_test_split\nfrom keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\nfrom tqdm.notebook import tqdm\nimport wandb\nfrom wandb.keras import WandbCallback, WandbModelCheckpoint, WandbMetricsLogger\ndef wandb_config():\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    try:\n        secret_value_0 = user_secrets.get_secret(\"__gcloud_sdk_auth__\")\n        secret_value_1 = user_secrets.get_secret(\"huggingface_key\")\n        secret_value_2 = user_secrets.get_secret(\"wandb_key\")\n        !wandb login $secret_value_2\n    except:\n        secret_value_0 = user_secrets.get_secret(\"huggingface_key\")\n        secret_value_1 = user_secrets.get_secret(\"wandb_key\")\n        !wandb login $secret_value_1\n    \n\nres = int(1.0*256)\nsmall_res = 64\nbatch_size = 16\nembed_dims = 512\nn_multicrop = 4\n\ndef auto_select_accelerator():\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n        print(\"Running on TPU:\", tpu.master())\n    except ValueError:\n        tpu = False\n        strategy = tf.distribute.MirroredStrategy() # for GPU or multi-GPU machines\n    print(f\"Running on {strategy.num_replicas_in_sync} replicas\")\n    \n    return tpu, strategy\n\ntpu, strategy = auto_select_accelerator()\nbatch_size = strategy.num_replicas_in_sync * batch_size\nprint('batch size', batch_size)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-05-22T02:48:17.275197Z","iopub.execute_input":"2024-05-22T02:48:17.275543Z","iopub.status.idle":"2024-05-22T02:48:40.668634Z","shell.execute_reply.started":"2024-05-22T02:48:17.275514Z","shell.execute_reply":"2024-05-22T02:48:40.667604Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-05-22 02:48:21.201604: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-22 02:48:21.201704: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-22 02:48:21.324027: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Tensorflow version : 2.15.0\nKeras version : 3.2.1\nRunning on 1 replicas\nbatch size 16\n","output_type":"stream"}]},{"cell_type":"code","source":"import ssl_module\nfrom ssl_module import get_map_fn, get_gcvit_configs, get_flops, att_visualize, get_full_model, AttentionPooling, BarlowModel, VICRegModel, Moco, SimSiam, CLIP, SigLIP\nimport nas_ftp_module\nfrom nas_ftp_module import upload_file, download_file\nssl_module.available_models()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-05-22T02:48:40.670597Z","iopub.execute_input":"2024-05-22T02:48:40.671384Z","iopub.status.idle":"2024-05-22T02:48:40.784782Z","shell.execute_reply.started":"2024-05-22T02:48:40.671346Z","shell.execute_reply":"2024-05-22T02:48:40.783890Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirements loaded, keras : v3.2.1, Tensorflow : v2.15.0\nRandAug Component in this SSL module :  ['random_color_degeneration', 'random_contrast', 'random_brightness', 'random_shear', 'random_shear_1', 'random_translation', 'random_translation_1', 'grid_mask']\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"{'models_from_kimm': ['ConvMixer1024D20',\n  'ConvMixer1536D20',\n  'ConvMixer736D32',\n  'ConvNeXtAtto',\n  'ConvNeXtBase',\n  'ConvNeXtFemto',\n  'ConvNeXtLarge',\n  'ConvNeXtNano',\n  'ConvNeXtPico',\n  'ConvNeXtSmall',\n  'ConvNeXtTiny',\n  'ConvNeXtXLarge',\n  'DenseNet121',\n  'DenseNet161',\n  'DenseNet169',\n  'DenseNet201',\n  'EfficientNetB0',\n  'EfficientNetB1',\n  'EfficientNetB2',\n  'EfficientNetB3',\n  'EfficientNetB4',\n  'EfficientNetB5',\n  'EfficientNetB6',\n  'EfficientNetB7',\n  'EfficientNetLiteB0',\n  'EfficientNetLiteB1',\n  'EfficientNetLiteB2',\n  'EfficientNetLiteB3',\n  'EfficientNetLiteB4',\n  'EfficientNetV2B0',\n  'EfficientNetV2B1',\n  'EfficientNetV2B2',\n  'EfficientNetV2B3',\n  'EfficientNetV2L',\n  'EfficientNetV2M',\n  'EfficientNetV2S',\n  'EfficientNetV2XL',\n  'GhostNet050',\n  'GhostNet100',\n  'GhostNet100V2',\n  'GhostNet130',\n  'GhostNet130V2',\n  'GhostNet160V2',\n  'HGNetBase',\n  'HGNetSmall',\n  'HGNetTiny',\n  'HGNetV2B0',\n  'HGNetV2B1',\n  'HGNetV2B2',\n  'HGNetV2B3',\n  'HGNetV2B4',\n  'HGNetV2B5',\n  'HGNetV2B6',\n  'InceptionNeXtBase',\n  'InceptionNeXtSmall',\n  'InceptionNeXtTiny',\n  'InceptionV3',\n  'LCNet035',\n  'LCNet050',\n  'LCNet075',\n  'LCNet100',\n  'LCNet150',\n  'MobileNetV2W050',\n  'MobileNetV2W100',\n  'MobileNetV2W110',\n  'MobileNetV2W120',\n  'MobileNetV2W140',\n  'MobileNetV3W050Small',\n  'MobileNetV3W075Small',\n  'MobileNetV3W100Large',\n  'MobileNetV3W100LargeMinimal',\n  'MobileNetV3W100Small',\n  'MobileNetV3W100SmallMinimal',\n  'MobileOneS0',\n  'MobileOneS1',\n  'MobileOneS2',\n  'MobileOneS3',\n  'MobileViTS',\n  'MobileViTV2W050',\n  'MobileViTV2W075',\n  'MobileViTV2W100',\n  'MobileViTV2W125',\n  'MobileViTV2W150',\n  'MobileViTV2W175',\n  'MobileViTV2W200',\n  'MobileViTXS',\n  'MobileViTXXS',\n  'RegNetX002',\n  'RegNetX004',\n  'RegNetX006',\n  'RegNetX008',\n  'RegNetX016',\n  'RegNetX032',\n  'RegNetX040',\n  'RegNetX064',\n  'RegNetX080',\n  'RegNetX120',\n  'RegNetX160',\n  'RegNetX320',\n  'RegNetY002',\n  'RegNetY004',\n  'RegNetY006',\n  'RegNetY008',\n  'RegNetY016',\n  'RegNetY032',\n  'RegNetY040',\n  'RegNetY064',\n  'RegNetY080',\n  'RegNetY120',\n  'RegNetY160',\n  'RegNetY320',\n  'RepVGGA0',\n  'RepVGGA1',\n  'RepVGGA2',\n  'RepVGGB0',\n  'RepVGGB1',\n  'RepVGGB2',\n  'RepVGGB3',\n  'ResNet101',\n  'ResNet152',\n  'ResNet18',\n  'ResNet34',\n  'ResNet50',\n  'TinyNetA',\n  'TinyNetB',\n  'TinyNetC',\n  'TinyNetD',\n  'TinyNetE',\n  'VGG11',\n  'VGG13',\n  'VGG16',\n  'VGG19',\n  'VisionTransformerBase16',\n  'VisionTransformerBase32',\n  'VisionTransformerLarge16',\n  'VisionTransformerLarge32',\n  'VisionTransformerSmall16',\n  'VisionTransformerSmall32',\n  'VisionTransformerTiny16',\n  'VisionTransformerTiny32',\n  'Xception'],\n 'models_from_keras': ['effnet',\n  'effnet_small',\n  'effnet_base',\n  'convnext',\n  'convnext_small',\n  'convnext_base',\n  'mlpmixer_patch_depth_dims',\n  'convmixer_patch_depth_dims']}"},"metadata":{}}]},{"cell_type":"markdown","source":"# Data import (with Generator)\n- 목적 : CXR의 prior knowledge를 SwAV으로 feature map generator에 주입시키기\n- Bounding box의 information을 사용하지 않음 + External data를 사용하자","metadata":{}},{"cell_type":"code","source":"metainfo_dir = \"/kaggle/input/chexdet-image-and-annotations/ChestXDet_Metainformations/ChestX-Det-Dataset-main\"\ntrain_det_dir = \"/kaggle/input/chexdet-image-and-annotations/train_data/train\"\nval_det_dir = \"/kaggle/input/chexdet-image-and-annotations/test_data/test\"\n\ndf_det_train = pd.read_json(\"/kaggle/input/chexdet-image-and-annotations/ChestXDet_Metainformations/ChestX-Det-Dataset-main/ChestX_Det_train.json\")\ndf_det_train[\"file_name\"] = [os.path.join(train_det_dir, fname) for fname in df_det_train.file_name.values]\ndf_det_train = df_det_train.loc[:, [\"file_name\"]]\n\ndf_val = pd.read_json(\"/kaggle/input/chexdet-image-and-annotations/ChestXDet_Metainformations/ChestX-Det-Dataset-main/ChestX_Det_test.json\")\ndf_val[\"file_name\"] = [os.path.join(val_det_dir, fname) for fname in df_val.file_name.values]\ndf_val_cxr = df_val.loc[:, [\"file_name\"]]\n\n#\next_dir = \"/kaggle/input/vinbigdata-chest-xray-original-png/train\"\ndict_ext = {\"file_name\" : [os.path.join(ext_dir, fname) for fname in os.listdir(ext_dir)] }\ndf_ext = pd.DataFrame(dict_ext)\ndf_train_cxr = pd.concat([df_det_train, df_ext], axis = 0)\nprint(f\"Total training cases for CXR : {len(df_train_cxr)} cases, Validation case : {len(df_val_cxr)} case\")","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-05-22T02:48:40.785844Z","iopub.execute_input":"2024-05-22T02:48:40.786108Z","iopub.status.idle":"2024-05-22T02:48:41.509142Z","shell.execute_reply.started":"2024-05-22T02:48:40.786085Z","shell.execute_reply":"2024-05-22T02:48:41.508151Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Total training cases for CXR : 18025 cases, Validation case : 553 case\n","output_type":"stream"}]},{"cell_type":"markdown","source":"> Deeplesion metainformation dataframe 생성","metadata":{}},{"cell_type":"code","source":"ct_fname = []\nbase_img_dir = '/kaggle/input/nih-deeplesion-subset/minideeplesion'\nfor dirname, _, filenames in tqdm(os.walk(base_img_dir)):\n    for filename in filenames:\n        ct_fname.append(os.path.join(dirname, filename))\n        \ndf_ct_whole = pd.DataFrame({\"file_name\" : ct_fname})\n\ndf_ct_train, df_ct_val = train_test_split(df_ct_whole, \n                                         test_size = 134,\n                                         random_state = seed)\nprint(f\"Total training cases of Chest/Abdomen CT : {len(df_ct_train)} cases, Validation case : {len(df_ct_val)} case\")","metadata":{"execution":{"iopub.status.busy":"2024-05-22T02:48:41.511201Z","iopub.execute_input":"2024-05-22T02:48:41.511505Z","iopub.status.idle":"2024-05-22T02:48:46.927736Z","shell.execute_reply.started":"2024-05-22T02:48:41.511479Z","shell.execute_reply":"2024-05-22T02:48:46.926600Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5861f2762d649bcbc94729d482a22b4"}},"metadata":{}},{"name":"stdout","text":"Total training cases of Chest/Abdomen CT : 33200 cases, Validation case : 134 case\n","output_type":"stream"}]},{"cell_type":"markdown","source":"> import RSNA ICH dataset metainformation dataframe","metadata":{}},{"cell_type":"code","source":"dicom_dir = \"/kaggle/input/rsna-intracranial-hemorrhage-detection/rsna-intracranial-hemorrhage-detection/stage_2_train\"\ndf_train_brainct = pd.read_csv(\"/kaggle/input/rsna-ich-detection-metadata/df_train_split.csv\")\ndf_val_brainct = pd.read_csv(\"/kaggle/input/rsna-ich-detection-metadata/df_val_splt.csv\").head(300)\n\nfor df in [df_train_brainct, df_val_brainct]:\n    df[\"file_name\"] = [os.path.join(dicom_dir, fname + \".dcm\") for fname in df['SOPInstanceUID']]\n    \nprint(f\"Total training cases of Brain, NonCE CT : {len(df_train_brainct)} cases, Validation case : {len(df_val_brainct)} case\")","metadata":{"execution":{"iopub.status.busy":"2024-05-22T02:48:46.929131Z","iopub.execute_input":"2024-05-22T02:48:46.929523Z","iopub.status.idle":"2024-05-22T02:48:47.080293Z","shell.execute_reply.started":"2024-05-22T02:48:46.929490Z","shell.execute_reply":"2024-05-22T02:48:47.079325Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Total training cases of Brain, NonCE CT : 32000 cases, Validation case : 300 case\n","output_type":"stream"}]},{"cell_type":"code","source":"df_train = pd.concat([df_ct_train, df_train_cxr, df_train_brainct], axis = 0, join='inner')\ndf_val = pd.concat([df_val_cxr, df_ct_val, df_val_brainct], axis = 0, join='inner')\n\ndf_train.to_csv(\"df_train_ER_SSL.csv\", index = False)\ndf_val.to_csv(\"df_val_ER_SSL.csv\", index = False)\n\ndf_train.sample(10)","metadata":{"execution":{"iopub.status.busy":"2024-05-22T02:48:47.081670Z","iopub.execute_input":"2024-05-22T02:48:47.082396Z","iopub.status.idle":"2024-05-22T02:48:47.464718Z","shell.execute_reply.started":"2024-05-22T02:48:47.082357Z","shell.execute_reply":"2024-05-22T02:48:47.463782Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                                               file_name\n27863  /kaggle/input/rsna-intracranial-hemorrhage-det...\n1222   /kaggle/input/nih-deeplesion-subset/minideeple...\n12077  /kaggle/input/nih-deeplesion-subset/minideeple...\n25883  /kaggle/input/nih-deeplesion-subset/minideeple...\n11373  /kaggle/input/vinbigdata-chest-xray-original-p...\n13095  /kaggle/input/vinbigdata-chest-xray-original-p...\n2090   /kaggle/input/nih-deeplesion-subset/minideeple...\n10721  /kaggle/input/vinbigdata-chest-xray-original-p...\n1321   /kaggle/input/chexdet-image-and-annotations/tr...\n30151  /kaggle/input/rsna-intracranial-hemorrhage-det...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>file_name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>27863</th>\n      <td>/kaggle/input/rsna-intracranial-hemorrhage-det...</td>\n    </tr>\n    <tr>\n      <th>1222</th>\n      <td>/kaggle/input/nih-deeplesion-subset/minideeple...</td>\n    </tr>\n    <tr>\n      <th>12077</th>\n      <td>/kaggle/input/nih-deeplesion-subset/minideeple...</td>\n    </tr>\n    <tr>\n      <th>25883</th>\n      <td>/kaggle/input/nih-deeplesion-subset/minideeple...</td>\n    </tr>\n    <tr>\n      <th>11373</th>\n      <td>/kaggle/input/vinbigdata-chest-xray-original-p...</td>\n    </tr>\n    <tr>\n      <th>13095</th>\n      <td>/kaggle/input/vinbigdata-chest-xray-original-p...</td>\n    </tr>\n    <tr>\n      <th>2090</th>\n      <td>/kaggle/input/nih-deeplesion-subset/minideeple...</td>\n    </tr>\n    <tr>\n      <th>10721</th>\n      <td>/kaggle/input/vinbigdata-chest-xray-original-p...</td>\n    </tr>\n    <tr>\n      <th>1321</th>\n      <td>/kaggle/input/chexdet-image-and-annotations/tr...</td>\n    </tr>\n    <tr>\n      <th>30151</th>\n      <td>/kaggle/input/rsna-intracranial-hemorrhage-det...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Building Dataloader in keras-3 style\n- Merging 2 kinds of dataset : original files with pd dataframe and tfrecord\n    - using this [tf dataset method](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#sample_from_datasets)\n    - [reference code](https://www.kaggle.com/code/calebeverett/combining-dataset-examples#Sample)","metadata":{}},{"cell_type":"markdown","source":"# Original Files with dataframe\n- using keras.utils.Sequence","metadata":{}},{"cell_type":"code","source":"class ImageDataLoader(keras.utils.Sequence):\n    def __init__(self, dataframe, x_col, res, batch_size, y_col = None, shuffle = True):\n        self.df = dataframe\n        self.x_col = x_col ; self.y_col = y_col\n        self.res = res\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.on_epoch_end()\n    def dicom_to_tensor(self, dicom_path):\n        dataset = pydicom.dcmread(dicom_path)\n        tensor = np.array(dataset.pixel_array)\n        slope = dataset.RescaleSlope   # dicom header (Rescale slope)\n        intercept = dataset.RescaleIntercept   # dicom header (Rescale intercept)\n        center = dataset.WindowCenter   # dicom header (Window center)\n        width = dataset.WindowWidth   # dicom header (Window width)\n\n        if(type(dataset.WindowCenter) == pydicom.multival.MultiValue):\n                center = float(dataset.WindowCenter[0])\n                width = float(dataset.WindowWidth[0])       \n        else:    \n                center = float(dataset.WindowCenter)\n                width = float(dataset.WindowWidth)\n\n        tensor = slope*tensor + intercept\n        lbound, ubound = center - 0.5*width, center + 0.5*width\n        tensor[np.where(tensor < lbound)] = lbound\n        tensor[np.where(tensor > ubound)] = ubound\n        tensor = tf.image.resize(tensor[:,:,tf.newaxis], [self.res,self.res],\n                                antialias = True) #HU unit\n        if tf.shape(tensor)[-1] == 1 :#gray\n            tensor = tf.image.grayscale_to_rgb(tensor)\n            \n        tensor = (tensor - tf.reduce_min(tensor)) / (tf.reduce_max(tensor) - tf.reduce_min(tensor) + 1e-4) #HU unit to Uint8\n        tensor = tensor*255.0\n        try:\n            del dataset\n        except:\n            pass\n        #print(f\"Dicom tensor shape : {ops.shape(tensor)}\")\n        return tensor\n    \n    def image_to_tensor(self, path):\n        if path.split(\".\")[-1] == \"dcm\":\n            return self.dicom_to_tensor(path)\n        \n        if \"minideeplesion\" in str(path).split(\"/\"):\n            image = imread(path).astype(np.float32)-32768\n            image = image[..., tf.newaxis]\n            image = tf.image.resize(image, [self.res, self.res],\n                                   antialias = True)\n            #print(f\"deepLesion tensor shape : {ops.shape(image)}\")\n            image = tf.clip_by_value(image, -750.0, 700.0)\n            image = (image - tf.reduce_min(image))/(tf.reduce_max(image) - tf.reduce_min(image) + 1e-3)\n            image = image * 255.0\n            \n        else:           \n            image = load_img(path, target_size = [self.res, self.res])\n            image = img_to_array(image)\n            #print(f\"other tensor shape : {ops.shape(image)}\")\n        if tf.shape(image)[-1] == 1 :#gray\n            image = tf.image.grayscale_to_rgb(image)\n            #image = np.array(image)\n            \n\n        return image\n        \n    def on_epoch_end(self):\n        self.indexes = np.arange(len(self.df))\n        if self.shuffle:\n            np.random.shuffle(self.indexes)\n            \n    def __len__(self):\n        return int(np.floor(len(self.df) / self.batch_size))\n    \n    def __data_generation(self, img_name):\n        ## path를 받아 img화 및 token화 하여 실제로 Feeding할 데이터를 반환\n        X = []\n        for i, fname in enumerate(img_name):\n            img = self.image_to_tensor(fname)\n            img = tf.convert_to_tensor(img)\n            img = tf.cast(img, tf.uint8)\n            X.append(img)\n        \n        return X\n        \n                \n    def __getitem__(self, index):\n        indexes = self.indexes[index * self.batch_size : (index + 1) * self.batch_size]\n        \n        img_name = [self.df.iloc[k].loc[self.x_col] for k in indexes]\n        \n        X = self.__data_generation(img_name)\n        #X = np.array(X).reshape([-1, self.res, self.res, 3])\n        return X\n    \ndef get_train_gen():\n    return ImageDataLoader(df_train, x_col = \"file_name\",\n                         res = res, batch_size = batch_size)\n\ndef get_val_gen():\n    return ImageDataLoader(df_val, x_col = \"file_name\",\n                         res = res, batch_size = batch_size)\n\n\ntrain_ds = tf.data.Dataset.from_generator(get_train_gen, (tf.uint8), output_shapes = (batch_size, res, res,3) ).ignore_errors().prefetch(tf.data.AUTOTUNE).repeat()\nval_ds = tf.data.Dataset.from_generator(get_val_gen, (tf.uint8), output_shapes = (batch_size, res, res,3) ).ignore_errors().prefetch(tf.data.AUTOTUNE).repeat()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-05-22T02:48:47.466051Z","iopub.execute_input":"2024-05-22T02:48:47.466379Z","iopub.status.idle":"2024-05-22T02:48:47.554963Z","shell.execute_reply.started":"2024-05-22T02:48:47.466354Z","shell.execute_reply":"2024-05-22T02:48:47.553950Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Spinal X-ray dataset in TFrecords","metadata":{}},{"cell_type":"code","source":"#spinal xray dataset\n\nlabel_map = {0: 'Disc space narrowing', 1: 'Foraminal stenosis', 2: 'No finding', 3: 'Osteophytes', 4: 'Other lesions', 5: 'Spondylolysthesis', 6: 'Surgical implant', 7: 'Vertebral collapse'}\n\nlabels = list(label_map.values())\nlabels.sort()\nn_labels = len(label_map)\n\n\ndef deserialize_example(serialized_string, train = True):\n    image_feature_description = {\n            'image': tf.io.FixedLenFeature([], tf.string),\n            'label': tf.io.FixedLenFeature([], tf.string)\n        }\n    parsed_record = tf.io.parse_single_example(serialized_string, image_feature_description)\n    image = tf.io.parse_tensor(parsed_record[\"image\"], tf.float32)\n    image = (image - tf.reduce_min(image))/(tf.reduce_max(image)-tf.reduce_min(image)+1e-4)\n    image = image * 255.0\n    image = tf.cast(image, tf.uint8)\n    image = ops.reshape(image, [res, res, 3])\n    label = tf.io.decode_raw(parsed_record['label'], tf.int32)\n    label = ops.reshape(label, [n_labels,])\n    return image, label\n    \ndef load_dataset(filenames):\n    ignore_order = tf.data.Options()\n    ignore_order.experimental_deterministic = False # disable order, increase speed\n    dataset = tf.data.TFRecordDataset(filenames, compression_type = \"GZIP\", \n                                      num_parallel_reads=tf.data.AUTOTUNE) # automatically interleaves reads from multiple files\n    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n    dataset = dataset.map(deserialize_example, num_parallel_calls=tf.data.AUTOTUNE)\n    return dataset\n\noriginal_train_ds = load_dataset(\"/kaggle/input/tfrecords-vindr-spinexr-tfrecords/train_gzip_384.tfrecord\")\noriginal_val_ds = load_dataset(\"/kaggle/input/tfrecords-vindr-spinexr-tfrecords/val_gzip_384.tfrecord\")\n\nspine_train_ds = original_train_ds.batch(batch_size, drop_remainder = True).map(lambda x,y:x).ignore_errors().repeat().prefetch(tf.data.AUTOTUNE)\nspine_val_ds = original_val_ds.batch(batch_size, drop_remainder = True).map(lambda x,y:x).ignore_errors().repeat().prefetch(tf.data.AUTOTUNE)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-05-22T02:48:47.556397Z","iopub.execute_input":"2024-05-22T02:48:47.557061Z","iopub.status.idle":"2024-05-22T02:48:47.953510Z","shell.execute_reply.started":"2024-05-22T02:48:47.557026Z","shell.execute_reply":"2024-05-22T02:48:47.952527Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Merging 2 dataset","metadata":{}},{"cell_type":"code","source":"merged_train_ds = tf.data.Dataset.sample_from_datasets([train_ds.unbatch(), spine_train_ds.unbatch()], weights = [0.5, 0.5]).batch(batch_size).ignore_errors().repeat().prefetch(tf.data.AUTOTUNE)\nmerged_val_ds = tf.data.Dataset.sample_from_datasets([val_ds.unbatch(), spine_val_ds.unbatch()], weights = [0.5, 0.5]).batch(batch_size).ignore_errors().repeat().prefetch(tf.data.AUTOTUNE)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-05-22T02:48:47.954733Z","iopub.execute_input":"2024-05-22T02:48:47.955008Z","iopub.status.idle":"2024-05-22T02:48:49.772266Z","shell.execute_reply.started":"2024-05-22T02:48:47.954986Z","shell.execute_reply":"2024-05-22T02:48:49.771439Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"> calculating train and validation steps per epoch\n\n- Spinal dataset : [여기 참고](https://www.kaggle.com/code/khsmdjjys/self-supervised-learning-with-tfrecord)","metadata":{}},{"cell_type":"code","source":"train_1 = len(df_train) ; val_1 = len(df_val)\ntrain_2 = 8389 ; val_2 = 2077\n\ntrain_steps = (train_1 + train_2)//batch_size\nval_steps = (val_1 + val_2)//batch_size\n\nprint(f\"Total Train cases, Val cases : {train_1 + train_2, val_1 + val_2}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-22T02:48:49.775797Z","iopub.execute_input":"2024-05-22T02:48:49.776091Z","iopub.status.idle":"2024-05-22T02:48:49.782072Z","shell.execute_reply.started":"2024-05-22T02:48:49.776065Z","shell.execute_reply":"2024-05-22T02:48:49.781126Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Total Train cases, Val cases : (91614, 3064)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Applying SSL functions\n- A. Basic function : return 2 global views (g=2)\n- B. SwAV-like strategy : return 2 global views + additional local views (l = 4)\n- Use get_map_fn in SSL module:\n> parameters of get_map_fn:\n    - res = image resolution, \n    - input_type = \"without_label\" or \"supervised\"\n    - output_type = \"ssl\" or \"ssl_with_label\"\n    - n_view = HOW MANY VIEWS? -> n_view >= 3일 때, 첫번째 이미지와 두 번째 이미지는 비교적 global information을 담고, 나머지 이미지는 local image (가로/세로 1/2)임.","metadata":{}},{"cell_type":"code","source":"multiview_fn = get_map_fn(res = res, input_type = \"without_label\", output_type = \"ssl\",\n                         n_view = n_multicrop)\ntwo_view_fn = get_map_fn(res = res, input_type = \"without_label\", output_type = \"ssl\",\n                         n_view = 2)\n\ntrain_ds = merged_train_ds.unbatch().map(two_view_fn, num_parallel_calls=tf.data.AUTOTUNE).batch(batch_size).ignore_errors().prefetch(tf.data.AUTOTUNE)\nval_ds = merged_val_ds.unbatch().map(two_view_fn, num_parallel_calls=tf.data.AUTOTUNE).batch(batch_size).ignore_errors().prefetch(tf.data.AUTOTUNE)\ntrain_ds_multiview = merged_train_ds.unbatch().map(multiview_fn, num_parallel_calls=tf.data.AUTOTUNE).batch(batch_size).ignore_errors().prefetch(tf.data.AUTOTUNE)\nval_ds_multiview = merged_val_ds.unbatch().map(multiview_fn, num_parallel_calls=tf.data.AUTOTUNE).batch(batch_size).ignore_errors().prefetch(tf.data.AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2024-05-22T02:48:49.783165Z","iopub.execute_input":"2024-05-22T02:48:49.783437Z","iopub.status.idle":"2024-05-22T02:48:57.614755Z","shell.execute_reply.started":"2024-05-22T02:48:49.783415Z","shell.execute_reply":"2024-05-22T02:48:57.613680Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"for imgs in val_ds_multiview.take(1):\n    images = imgs\nsample_img = images[0]\ntest_set = tuple([comp[:2] for comp in images])","metadata":{"execution":{"iopub.status.busy":"2024-05-22T02:48:57.616091Z","iopub.execute_input":"2024-05-22T02:48:57.616413Z","iopub.status.idle":"2024-05-22T02:49:15.294540Z","shell.execute_reply.started":"2024-05-22T02:48:57.616385Z","shell.execute_reply":"2024-05-22T02:49:15.293388Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"> Curate the dataset","metadata":{}},{"cell_type":"code","source":"view_curation = False","metadata":{"execution":{"iopub.status.busy":"2024-05-22T02:49:15.295822Z","iopub.execute_input":"2024-05-22T02:49:15.296121Z","iopub.status.idle":"2024-05-22T02:49:15.300823Z","shell.execute_reply.started":"2024-05-22T02:49:15.296096Z","shell.execute_reply":"2024-05-22T02:49:15.299496Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"if view_curation:\n    print(\"Training dataset Curation, with Basic SSL Fn (2 global views)\")\n    for originals, augs in train_ds.take(1):\n        for origin, aug in zip(originals, augs):\n            fig, axes = plt.subplots(1,2, figsize = (16, 8))\n            axes = axes.flatten()\n            axes[0].imshow(ops.cast(origin, \"uint8\"))\n            axes[1].imshow(ops.cast(aug, \"uint8\"))\n            axes[0].set_title(\"ORIGINAL\")\n            axes[1].set_title(\"GLOBAL VIEW AUGMENTATION\")\n            plt.show()\n\n    print(\"Validation dataset Curation, with Basic SSL Fn (2 global views)\")\n    for originals, augs in val_ds.take(1):\n        for origin, aug in zip(originals, augs):\n            fig, axes = plt.subplots(1,2, figsize = (16, 8))\n            axes = axes.flatten()\n            axes[0].imshow(ops.cast(origin, \"uint8\"))\n            axes[1].imshow(ops.cast(aug, \"uint8\"))\n            axes[0].set_title(\"ORIGINAL\")\n            axes[1].set_title(\"GLOBAL VIEW AUGMENTATION\")\n            plt.show()","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-05-22T02:49:15.301893Z","iopub.execute_input":"2024-05-22T02:49:15.302128Z","iopub.status.idle":"2024-05-22T02:49:15.316410Z","shell.execute_reply.started":"2024-05-22T02:49:15.302107Z","shell.execute_reply":"2024-05-22T02:49:15.315522Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"if view_curation:\n    for multiset in train_ds_multiview.take(1):\n        global_views = multiset[:2]\n        local_views = multiset[2:]\n    for idx in tqdm(range(batch_size)):\n        print(f\"=================\\nBatch No.{idx}\\n===================\")\n        print(\"Global Views\")\n        fig, axes = plt.subplots(1,2, figsize = (20,10))\n        axes = axes.flatten()\n        g1, g2 = global_views[0][idx], global_views[1][idx] \n        axes[0].imshow(ops.cast(ops.squeeze(g1), \"uint8\"))\n        axes[1].imshow(ops.cast(ops.squeeze(g2), \"uint8\"))\n        plt.show()\n        print(\"=================\\nLocal Views\\n===================\")\n        fig, axes = plt.subplots(2,2, figsize = (16,16))\n        axes = axes.flatten()\n        local_set = [local_views[0][idx], local_views[1][idx], local_views[2][idx], local_views[3][idx]] \n\n        for k in range(4):\n            axes[k].imshow(ops.cast(ops.squeeze(local_set[k]), \"uint8\"))\n        plt.show()","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-05-22T02:49:15.317642Z","iopub.execute_input":"2024-05-22T02:49:15.317957Z","iopub.status.idle":"2024-05-22T02:49:15.329918Z","shell.execute_reply.started":"2024-05-22T02:49:15.317932Z","shell.execute_reply":"2024-05-22T02:49:15.328846Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# SSL experiment : Information-Maximization","metadata":{}},{"cell_type":"markdown","source":"> Model Save and Attention map visualize callbacks","metadata":{}},{"cell_type":"code","source":"class ModelSaveCallback(keras.callbacks.Callback):\n    def __init__(self, exp_name, **kwargs):\n        super().__init__(**kwargs)\n        self.exp_name = exp_name\n    def on_epoch_end(self, epoch, logs=None):\n        save_dir = \"/kaggle/working/\" ; target_dir = '/kaggle/working/model_save'\n        os.makedirs(target_dir, exist_ok = True)\n        if (epoch % 1 == 0):\n            try:\n                print(\"\\nModel Saving to local notebook...\")\n                file_name = f\"{self.exp_name}_{self.model.name}_keras_v3_Epoch{epoch}.keras\"\n                filepath = os.path.join(target_dir, file_name)\n                saved_dir = self.model.save(filepath, overwrite=True)\n                if (epoch+1) % 5 == 0:\n                    print(\"\\nModel Uploading to NAS...\")\n                    upload_file(file_name, filepath)\n                    print(\"\\nModel Saved to Local NAS\")\n            except Exception as e: \n                print('Model Saving Error:\\n', e)\n    def on_train_batch_end(self, batch, logs=None):\n        save_dir = \"/kaggle/working/\" ; target_dir = '/kaggle/working/model_save'\n        os.makedirs(target_dir, exist_ok = True)\n        if (batch % 5000 == 0) and (batch != 0): \n            try:\n                print(\"\\nModel Saving to local notebook...\")\n                file_name = f\"{self.exp_name}_{self.model.name}_keras_v3_Batch{batch}.keras\"\n                filepath = os.path.join(target_dir, file_name)\n                saved_dir = self.model.save(filepath, overwrite=True)\n                if (batch % 10000 == 0):\n                    print(\"\\nModel Uploading to NAS...\")\n                    upload_file(file_name, filepath)\n                    print(\"\\nModel Saved to Local NAS\")\n            except Exception as e: \n                print('Model Saving Error:\\n', e)\n                \n                \nclass TrainingViz(keras.callbacks.Callback):\n    def __init__(self, run):\n        super().__init__()\n        self.run = run\n    def on_epoch_end(self, epoch, logs=None):\n        try:\n            configs = self.model.get_config() ; method = configs[\"SSL_method\"]\n            if method in [\"CLIP\" , \"SigLIP\", \"SPARC\"]:\n                feature_extractor = self.model\n            else:\n                feature_extractor = self.model.feature_extractor\n            viz_weights = ssl_module.att_visualize(feature_extractor, sample_img, res,\n                                                  thresholding = False)\n            viz_weights = np.array(viz_weights) #batch, heads, res, res, 3\n            heads = viz_weights.shape[1]\n            origin = [\"Original Image\"]\n            col = [f\"Head{idx + 1}\" for idx in range(heads)]\n            col = origin + col\n\n            visualize_data = []\n            for idx, weights in enumerate(viz_weights):\n                origin_img = [wandb.Image(sample_img[idx])]\n                tmp = [wandb.Image(weights[idx]) for idx in range(heads)]\n                tmp = origin_img + tmp\n                visualize_data.append(tmp)\n                del tmp, origin_img\n            tbl = wandb.Table(columns = col, data = visualize_data)\n            wandb.log({f\"Epoch{epoch+1}_{method}_result\": tbl})\n            del feature_extractor, tbl\n            tf.keras.backend.clear_session()\n        except Exception as e: \n                print('Model Saving Error:\\n', e)\n        \n    def on_train_batch_end(self, batch, logs=None):\n        if (batch == (train_steps//2)): \n            try:\n                configs = self.model.get_config() ; method = configs[\"SSL_method\"]\n                if method in [\"CLIP\" , \"SigLIP\", \"SPARC\"]:\n                    feature_extractor = self.model\n                else:\n                    feature_extractor = self.model.feature_extractor\n                viz_weights = ssl_module.att_visualize(feature_extractor, sample_img, res,\n                                                      thresholding = False)\n                viz_weights = np.array(viz_weights) #batch, heads, res, res, 3\n                heads = viz_weights.shape[1]\n                origin = [\"Original Image\"]\n                col = [f\"Head{idx + 1}\" for idx in range(heads)]\n                col = origin + col\n                visualize_data = []\n                for idx, weights in enumerate(viz_weights):\n                    origin_img = [wandb.Image(sample_img[idx])]\n                    tmp = [wandb.Image(weights[idx]) for idx in range(heads)]\n                    tmp = origin_img + tmp\n                    visualize_data.append(tmp)\n                    del tmp, origin_img\n                tbl = wandb.Table(columns = col, data = visualize_data)\n                if batch == 0:\n                    wandb.log({f\"ZeroBatch_{method}_result\": tbl})\n                else:\n                    wandb.log({f\"MidEpoch_{method}_result\": tbl})\n                del feature_extractor, tbl\n                tf.keras.backend.clear_session()\n            except Exception as e:\n                print(\"Error code in callback : \", e)\n           \n        else:\n            pass","metadata":{"execution":{"iopub.status.busy":"2024-05-22T02:49:15.331589Z","iopub.execute_input":"2024-05-22T02:49:15.331900Z","iopub.status.idle":"2024-05-22T02:49:15.357319Z","shell.execute_reply.started":"2024-05-22T02:49:15.331875Z","shell.execute_reply":"2024-05-22T02:49:15.356375Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"> Training/logging helper function","metadata":{}},{"cell_type":"code","source":"def run_exp(model, train_ds = train_ds, val_ds = val_ds, epochs = 10, note= None):\n    try:\n        wandb.finish()\n    except:\n        pass\n    \n    if True :\n        wandb_config()\n        configs = model.get_config()\n        method = configs[\"SSL_method\"]\n        if method in ['CLIP', \"SigLIP\", \"SPARC\"]:\n            _ = model((example_images[:2], example_reports[:2]))\n            feature_extractor_flops = get_flops(model.image_encoder, [example_images[:1]])\n        else:\n            _ = model(test_set)\n            feature_extractor_flops = get_flops(model.feature_extractor, [sample_img[:1]])\n        env_config = {\"batch_size\" : batch_size, \"original resolution\" : res, \"local view resolution\" : small_res,\n                     \"Training steps\" : train_steps,\n                     \"Val steps\" : val_steps,\n                     \"train cases\" : (train_1 + train_2),\n                     \"val cases\" : (val_1 + val_2),\n                     \"embed_dims\" : embed_dims,\n                     \"Image resolution\" : res,\n                     \"(Image) Encoder Flops(G)\" : feature_extractor_flops,\n                     \"dtype\" : keras.mixed_precision.dtype_policy(),\n                      \"Optimizer configs\" : model.optimizer.get_config(),\n                      \"Multicrop N\" : n_multicrop,\n                     }\n        configs.update(env_config)\n        \n        wd = \"/kaggle/working/\"\n        file_name = os.path.join(wd, f\"{method}_GrandCXR_mini.keras\")\n        print(configs, \"\\n\\n\")\n        model.summary()\n        run = wandb.init(project=\"FusionFocus\", \n                         entity=\"gongbungkim\", config = configs, notes = note)\n\n        pass_error = keras.callbacks.TerminateOnNaN()\n        wb_callback = WandbMetricsLogger(log_freq = 100)\n        \n        callbacks = [pass_error, wb_callback, ModelSaveCallback(f\"FF_SSL_{method}\"), \n                     TrainingViz(run)]\n        if val_ds is not None:\n            hist = model.fit(train_ds, \n                             steps_per_epoch = train_steps, \n                             epochs = epochs, \n                             validation_data = val_ds, \n                             validation_steps = val_steps, \n                             verbose = 1,\n                             callbacks = callbacks)\n        else:\n            hist = model.fit(train_ds, \n                         steps_per_epoch = train_steps, \n                         epochs = epochs, \n                         verbose = 1,\n                         callbacks = callbacks)\n    return hist","metadata":{"execution":{"iopub.status.busy":"2024-05-22T02:49:15.358883Z","iopub.execute_input":"2024-05-22T02:49:15.359167Z","iopub.status.idle":"2024-05-22T02:49:15.372440Z","shell.execute_reply.started":"2024-05-22T02:49:15.359133Z","shell.execute_reply":"2024-05-22T02:49:15.371450Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"> Feature extractor setting\n- General-Context Vision Transformer,\n- Convolution-based models:\n    - EfficientNetV2B0, Small\n    - ConvNeXtTiny, Small","metadata":{}},{"cell_type":"code","source":"gc_xxtiny_configs = get_gcvit_configs(res, 64, \"GC_ViT_xxtiny\")\ngc_xxtiny_configs[\"level_depth\"] = [1,1,2,2]\n\ngc_tiny_configs = get_gcvit_configs(res, 64, \"GC_ViT_tiny\")\ngc_tiny_configs[\"level_depth\"] = [1,1,2,4]\n\ngc_small_configs = get_gcvit_configs(res, 64, \"GC_ViT_small\")\ngc_small_configs[\"level_depth\"] = [1,2,4,6]","metadata":{"execution":{"iopub.status.busy":"2024-05-22T02:49:15.373640Z","iopub.execute_input":"2024-05-22T02:49:15.373953Z","iopub.status.idle":"2024-05-22T02:49:15.384747Z","shell.execute_reply.started":"2024-05-22T02:49:15.373928Z","shell.execute_reply":"2024-05-22T02:49:15.383907Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"- Setting Final Feature Extractors","metadata":{}},{"cell_type":"code","source":"gcvit_xxtiny = get_full_model(gc_xxtiny_configs, res = res, pe_type = None, att_depth = 1, att_heads = 16, embed_dims = embed_dims)\ngcvit_tiny = get_full_model(gc_tiny_configs, res = res, pe_type = None, att_depth = 1, att_heads = 16, embed_dims = embed_dims)\ngcvit_small = get_full_model(gc_small_configs, res = res, pe_type = None, att_depth = 1, att_heads = 16, embed_dims = embed_dims)\n###############\neff_tiny = get_full_model(\"effnet\", res = res, pe_type = 'learnable', att_depth = 1, att_heads = 16, embed_dims = embed_dims)\neff_small = get_full_model(\"effnet_small\", res = res, pe_type = 'learnable', att_depth = 1, att_heads = 16, embed_dims = embed_dims)\n\nconv_tiny = get_full_model(\"convnext\", res = res, pe_type = 'learnable', att_depth = 1, att_heads = 16, embed_dims = embed_dims)\nconv_small = get_full_model(\"convnext_small\", res = res, pe_type = 'learnable', att_depth = 1, att_heads = 16, embed_dims = embed_dims)\n\nmlpmixer = get_full_model(\"mlpmixer_16_4_512\",res = res, att_depth = 1, att_heads = 16, embed_dims = embed_dims)\nconvmixer = get_full_model(\"convmixer_16_4_512\",res = res, att_depth = 1, att_heads = 16, embed_dims = embed_dims)\n\nmobilevit = kimm.models.MobileViTV2W050(input_shape = [res,res,3], include_top = False)\nmobilevit = get_full_model(mobilevit, res = res, embed_dims = embed_dims, pe_type = None, att_depth = 1, att_heads = 16)","metadata":{"execution":{"iopub.status.busy":"2024-05-22T02:49:15.385890Z","iopub.execute_input":"2024-05-22T02:49:15.386177Z","iopub.status.idle":"2024-05-22T02:49:51.454307Z","shell.execute_reply.started":"2024-05-22T02:49:15.386154Z","shell.execute_reply":"2024-05-22T02:49:51.453519Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/efficientnet_v2/efficientnetv2-b1_notop.h5\n\u001b[1m28456008/28456008\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/efficientnet_v2/efficientnetv2-s_notop.h5\n\u001b[1m82420632/82420632\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/convnext/convnext_tiny_notop.h5\n\u001b[1m111650432/111650432\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/convnext/convnext_small_notop.h5\n\u001b[1m198551472/198551472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 0us/step\nDownloading data from https://github.com/james77777778/kimm/releases/download/0.1.0/mobilevits_mobilevit_s.cvnets_in1k.keras\n\u001b[1m22944350/22944350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","output_type":"stream"}]},{"cell_type":"markdown","source":"> learning rate setting","metadata":{}},{"cell_type":"code","source":"cosine_decay = keras.optimizers.schedules.CosineDecay(\n    initial_learning_rate = 1e-6,\n    decay_steps = int(0.5*train_steps),\n    alpha=1e-5,\n    name='CosineDecay',\n    warmup_target=1e-3,\n    warmup_steps=train_steps - int(0.3*train_steps)\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-22T02:49:51.455355Z","iopub.execute_input":"2024-05-22T02:49:51.455624Z","iopub.status.idle":"2024-05-22T02:49:51.460846Z","shell.execute_reply.started":"2024-05-22T02:49:51.455601Z","shell.execute_reply":"2024-05-22T02:49:51.459879Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def ssl_train(module, feature_extractor, embed_dims = embed_dims, multiview = True, gradient_accumulation = None,\n             note = \"\"):\n    ssl_trainer = module(feature_extractor, embed_dims = embed_dims, multiview = multiview)\n    ssl_trainer.compile(optimizer = keras.optimizers.AdamW(learning_rate = 1e-4,\n                                                         clipvalue = 1.0,\n                                                         #amsgrad = True,\n                                                           gradient_accumulation_steps=gradient_accumulation,\n                                                         ),\n                        #run_eagerly = True\n                      )\n    run_exp(ssl_trainer, train_ds_multiview, None, epochs = 100,\n       note = \"Without validation d/t lack of resources\" + note)","metadata":{"execution":{"iopub.status.busy":"2024-05-22T02:49:51.461910Z","iopub.execute_input":"2024-05-22T02:49:51.462163Z","iopub.status.idle":"2024-05-22T02:49:51.470168Z","shell.execute_reply.started":"2024-05-22T02:49:51.462141Z","shell.execute_reply":"2024-05-22T02:49:51.469313Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"--------","metadata":{}},{"cell_type":"markdown","source":"# Experiment","metadata":{}},{"cell_type":"code","source":"ssl_train(ssl_module.DINO, mobilevit, gradient_accumulation = 8,\n         note = \"+ NEW aug, Attentional pooling with register\")","metadata":{"execution":{"iopub.status.busy":"2024-05-22T02:49:51.471308Z","iopub.execute_input":"2024-05-22T02:49:51.471922Z","iopub.status.idle":"2024-05-22T03:31:15.603261Z","shell.execute_reply.started":"2024-05-22T02:49:51.471890Z","shell.execute_reply":"2024-05-22T03:31:15.601321Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n{'feature_extractor_name': 'MobileViTS_depth1_dims512_heads16', 'embed_dims': 512, 'Multiview(>2)': True, 'SSL_method': 'DINOv1', 'Linear Probe': False, 'N_Categories': 0, 'Probe Activation': 'NA', 'batch_size': 16, 'original resolution': 256, 'local view resolution': 64, 'Training steps': 5725, 'Val steps': 191, 'train cases': 91614, 'val cases': 3064, 'Image resolution': 256, '(Image) Encoder Flops(G)': 4.264707956, 'dtype': <FloatDTypePolicy \"float32\">, 'Optimizer configs': {'name': 'adamw', 'learning_rate': 9.999999747378752e-05, 'weight_decay': 0.004, 'clipnorm': None, 'global_clipnorm': None, 'clipvalue': 1.0, 'use_ema': False, 'ema_momentum': 0.99, 'ema_overwrite_frequency': None, 'loss_scale_factor': None, 'gradient_accumulation_steps': 8, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}, 'Multicrop N': 4} \n\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"dino\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"dino\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ MobileViTS_depth1_dims512_head… │ ?                      │     \u001b[38;5;34m6,326,656\u001b[0m │\n│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ Teacher_DINO_predictor          │ ?                      │       \u001b[38;5;34m526,848\u001b[0m │\n│ (\u001b[38;5;33mSequential\u001b[0m)                    │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ Student_DINO_predictor          │ ?                      │       \u001b[38;5;34m526,848\u001b[0m │\n│ (\u001b[38;5;33mSequential\u001b[0m)                    │                        │               │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ MobileViTS_depth1_dims512_head… │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">6,326,656</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ Teacher_DINO_predictor          │ ?                      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">526,848</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)                    │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ Student_DINO_predictor          │ ?                      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">526,848</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)                    │                        │               │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m7,380,352\u001b[0m (28.15 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,380,352</span> (28.15 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m7,366,048\u001b[0m (28.10 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,366,048</span> (28.10 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m14,304\u001b[0m (55.88 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,304</span> (55.88 KB)\n</pre>\n"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgongbungkim\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240522_025008-h7bck1p4</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/gongbungkim/FusionFocus/runs/h7bck1p4' target=\"_blank\">flowing-feather-103</a></strong> to <a href='https://wandb.ai/gongbungkim/FusionFocus' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/gongbungkim/FusionFocus' target=\"_blank\">https://wandb.ai/gongbungkim/FusionFocus</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/gongbungkim/FusionFocus/runs/h7bck1p4' target=\"_blank\">https://wandb.ai/gongbungkim/FusionFocus/runs/h7bck1p4</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/100\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1716346436.190976      86 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\nW0000 00:00:1716346436.427551      86 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1716346436.431181      86 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1716346436.437051      86 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1716346436.443129      86 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Unable to log learning rate.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1908/5725\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:13:42\u001b[0m 1s/step - DINO_feature_std: 0.0317 - DINO_loss: 0.0122Using Raw model with Att pooling \n Possible error: \n 'Functional' object has no attribute 'get_full_model'\n\u001b[1m1909/5725\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:13:55\u001b[0m 1s/step - DINO_feature_std: 0.0317 - DINO_loss: 0.0122","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)","Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mssl_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mssl_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDINO\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmobilevit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient_accumulation\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m         \u001b[49m\u001b[43mnote\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m+ NEW aug, Attentional pooling with register\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[21], line 11\u001b[0m, in \u001b[0;36mssl_train\u001b[0;34m(module, feature_extractor, embed_dims, multiview, gradient_accumulation, note)\u001b[0m\n\u001b[1;32m      3\u001b[0m ssl_trainer \u001b[38;5;241m=\u001b[39m module(feature_extractor, embed_dims \u001b[38;5;241m=\u001b[39m embed_dims, multiview \u001b[38;5;241m=\u001b[39m multiview)\n\u001b[1;32m      4\u001b[0m ssl_trainer\u001b[38;5;241m.\u001b[39mcompile(optimizer \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdamW(learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-4\u001b[39m,\n\u001b[1;32m      5\u001b[0m                                                      clipvalue \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m,\n\u001b[1;32m      6\u001b[0m                                                      \u001b[38;5;66;03m#amsgrad = True,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m                     \u001b[38;5;66;03m#run_eagerly = True\u001b[39;00m\n\u001b[1;32m     10\u001b[0m                   )\n\u001b[0;32m---> 11\u001b[0m \u001b[43mrun_exp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mssl_trainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_ds_multiview\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m   \u001b[49m\u001b[43mnote\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWithout validation d/t lack of resources\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnote\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[17], line 52\u001b[0m, in \u001b[0;36mrun_exp\u001b[0;34m(model, train_ds, val_ds, epochs, note)\u001b[0m\n\u001b[1;32m     44\u001b[0m         hist \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(train_ds, \n\u001b[1;32m     45\u001b[0m                          steps_per_epoch \u001b[38;5;241m=\u001b[39m train_steps, \n\u001b[1;32m     46\u001b[0m                          epochs \u001b[38;5;241m=\u001b[39m epochs, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m                          verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     50\u001b[0m                          callbacks \u001b[38;5;241m=\u001b[39m callbacks)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         hist \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                     \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hist\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","\u001b[0;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"/opt/conda/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/opt/conda/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n\n  File \"/opt/conda/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 701, in start\n\n  File \"/opt/conda/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/opt/conda/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in dispatch_queue\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 523, in process_one\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 429, in dispatch_shell\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 429, in do_execute\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3051, in run_cell\n\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3106, in _run_cell\n\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3311, in run_cell_async\n\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3493, in run_ast_nodes\n\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"/tmp/ipykernel_34/1560657912.py\", line 1, in <module>\n\n  File \"/tmp/ipykernel_34/2293257310.py\", line 11, in ssl_train\n\n  File \"/tmp/ipykernel_34/2373934021.py\", line 52, in run_exp\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 329, in fit\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 122, in one_step_on_iterator\n\nOut of memory while trying to allocate 12588795488 bytes.\nBufferAssignment OOM Debugging.\nBufferAssignment stats:\n             parameter allocation:  145.44MiB\n              constant allocation:       280B\n        maybe_live_out allocation:  106.44MiB\n     preallocated temp allocation:   11.72GiB\n  preallocated temp fragmentation:    2.53MiB (0.02%)\n                 total allocation:   11.87GiB\n              total fragmentation:    7.06MiB (0.06%)\nPeak buffers:\n\tBuffer 1:\n\t\tSize: 512.00MiB\n\t\tOperator: op_type=\"Mul\" op_name=\"MobileViTS_depth1_dims512_heads16_3/MobileViTS_1/stages_1_0_conv_pw_1/mul_1\" source_file=\"/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1160\n\t\tXLA Label: fusion\n\t\tShape: f32[64,128,128,128]\n\t\t==========================\n\n\tBuffer 2:\n\t\tSize: 512.00MiB\n\t\tOperator: op_type=\"Mul\" op_name=\"MobileViTS_depth1_dims512_heads16_3/MobileViTS_1/stages_1_0_conv_pw_1/mul_1\" source_file=\"/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1160\n\t\tXLA Label: fusion\n\t\tShape: f32[64,128,128,128]\n\t\t==========================\n\n\tBuffer 3:\n\t\tSize: 512.00MiB\n\t\tOperator: op_type=\"Conv2D\" op_name=\"MobileViTS_depth1_dims512_heads16_3/MobileViTS_1/stages_1_0_conv_pw_conv2d_1/convolution\" source_file=\"/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1160\n\t\tXLA Label: custom-call\n\t\tShape: f32[64,128,128,128]\n\t\t==========================\n\n\tBuffer 4:\n\t\tSize: 256.00MiB\n\t\tOperator: op_type=\"Mul\" op_name=\"MobileViTS_depth1_dims512_heads16_3/MobileViTS_1/stages_1_1_conv_dw_1/mul_1\" source_file=\"/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1160 deduplicated_name=\"fusion.3163\"\n\t\tXLA Label: fusion\n\t\tShape: f32[64,256,64,64]\n\t\t==========================\n\n\tBuffer 5:\n\t\tSize: 256.00MiB\n\t\tOperator: op_type=\"Mul\" op_name=\"MobileViTS_depth1_dims512_heads16_3/MobileViTS_1/stages_1_1_conv_dw_1/mul_1\" source_file=\"/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1160 deduplicated_name=\"fusion.3163\"\n\t\tXLA Label: fusion\n\t\tShape: f32[64,256,64,64]\n\t\t==========================\n\n\tBuffer 6:\n\t\tSize: 256.00MiB\n\t\tOperator: op_type=\"DepthwiseConv2dNative\" op_name=\"MobileViTS_depth1_dims512_heads16_3/MobileViTS_1/stages_1_1_conv_dw_dwconv2d_1/depthwise\" source_file=\"/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1160\n\t\tXLA Label: custom-call\n\t\tShape: f32[64,256,64,64]\n\t\t==========================\n\n\tBuffer 7:\n\t\tSize: 256.00MiB\n\t\tOperator: op_type=\"Mul\" op_name=\"MobileViTS_depth1_dims512_heads16_3/MobileViTS_1/stages_1_1_conv_pw_1/mul_1\" source_file=\"/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1160 deduplicated_name=\"fusion.3163\"\n\t\tXLA Label: fusion\n\t\tShape: f32[64,256,64,64]\n\t\t==========================\n\n\tBuffer 8:\n\t\tSize: 256.00MiB\n\t\tOperator: op_type=\"Mul\" op_name=\"MobileViTS_depth1_dims512_heads16_3/MobileViTS_1/stages_1_1_conv_pw_1/mul_1\" source_file=\"/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1160 deduplicated_name=\"fusion.3163\"\n\t\tXLA Label: fusion\n\t\tShape: f32[64,256,64,64]\n\t\t==========================\n\n\tBuffer 9:\n\t\tSize: 256.00MiB\n\t\tOperator: op_type=\"Conv2D\" op_name=\"MobileViTS_depth1_dims512_heads16_3/MobileViTS_1/stages_1_1_conv_pw_conv2d_1/convolution\" source_file=\"/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1160\n\t\tXLA Label: custom-call\n\t\tShape: f32[64,256,64,64]\n\t\t==========================\n\n\tBuffer 10:\n\t\tSize: 256.00MiB\n\t\tOperator: op_type=\"Mul\" op_name=\"MobileViTS_depth1_dims512_heads16_3/MobileViTS_1/stages_0_0_conv_dw_1/mul_1\" source_file=\"/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1160 deduplicated_name=\"fusion.3159\"\n\t\tXLA Label: fusion\n\t\tShape: f32[64,64,128,128]\n\t\t==========================\n\n\tBuffer 11:\n\t\tSize: 256.00MiB\n\t\tOperator: op_type=\"Mul\" op_name=\"MobileViTS_depth1_dims512_heads16_3/MobileViTS_1/stages_0_0_conv_dw_1/mul_1\" source_file=\"/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1160 deduplicated_name=\"fusion.3159\"\n\t\tXLA Label: fusion\n\t\tShape: f32[64,64,128,128]\n\t\t==========================\n\n\tBuffer 12:\n\t\tSize: 256.00MiB\n\t\tOperator: op_type=\"DepthwiseConv2dNative\" op_name=\"MobileViTS_depth1_dims512_heads16_3/MobileViTS_1/stages_0_0_conv_dw_dwconv2d_1/depthwise\" source_file=\"/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1160\n\t\tXLA Label: custom-call\n\t\tShape: f32[64,64,128,128]\n\t\t==========================\n\n\tBuffer 13:\n\t\tSize: 256.00MiB\n\t\tOperator: op_type=\"Mul\" op_name=\"MobileViTS_depth1_dims512_heads16_3/MobileViTS_1/stages_0_0_conv_pw_1/mul_1\" source_file=\"/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1160 deduplicated_name=\"fusion.3159\"\n\t\tXLA Label: fusion\n\t\tShape: f32[64,64,128,128]\n\t\t==========================\n\n\tBuffer 14:\n\t\tSize: 256.00MiB\n\t\tOperator: op_type=\"Mul\" op_name=\"MobileViTS_depth1_dims512_heads16_3/MobileViTS_1/stages_0_0_conv_pw_1/mul_1\" source_file=\"/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1160 deduplicated_name=\"fusion.3159\"\n\t\tXLA Label: fusion\n\t\tShape: f32[64,64,128,128]\n\t\t==========================\n\n\tBuffer 15:\n\t\tSize: 256.00MiB\n\t\tOperator: op_type=\"Conv2D\" op_name=\"MobileViTS_depth1_dims512_heads16_3/MobileViTS_1/stages_0_0_conv_pw_conv2d_1/convolution\" source_file=\"/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1160\n\t\tXLA Label: custom-call\n\t\tShape: f32[64,64,128,128]\n\t\t==========================\n\n\n\t [[{{node StatefulPartitionedCall}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_one_step_on_iterator_142289]"],"ename":"ResourceExhaustedError","evalue":"Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"/opt/conda/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/opt/conda/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n\n  File \"/opt/conda/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 701, in start\n\n  File \"/opt/conda/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/opt/conda/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in dispatch_queue\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 523, in process_one\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 429, in dispatch_shell\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 429, in do_execute\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3051, in run_cell\n\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3106, in _run_cell\n\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3311, in run_cell_async\n\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3493, in run_ast_nodes\n\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"/tmp/ipykernel_34/1560657912.py\", line 1, in <module>\n\n  File \"/tmp/ipykernel_34/2293257310.py\", line 11, in ssl_train\n\n  File \"/tmp/ipykernel_34/2373934021.py\", line 52, in run_exp\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 329, in fit\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 122, in one_step_on_iterator\n\nOut of memory while trying to allocate 12588795488 bytes.\nBufferAssignment OOM Debugging.\nBufferAssignment stats:\n             parameter allocation:  145.44MiB\n              constant allocation:       280B\n        maybe_live_out allocation:  106.44MiB\n     preallocated temp allocation:   11.72GiB\n  preallocated temp fragmentation:    2.53MiB (0.02%)\n                 total allocation:   11.87GiB\n              total fragmentation:    7.06MiB (0.06%)\nPeak buffers:\n\tBuffer 1:\n\t\tSize: 512.00MiB\n\t\tOperator: op_type=\"Mul\" op_name=\"MobileViTS_depth1_dims512_heads16_3/MobileViTS_1/stages_1_0_conv_pw_1/mul_1\" source_file=\"/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1160\n\t\tXLA Label: fusion\n\t\tShape: f32[64,128,128,128]\n\t\t==========================\n\n\tBuffer 2:\n\t\tSize: 512.00MiB\n\t\tOperator: op_type=\"Mul\" op_name=\"MobileViTS_depth1_dims512_heads16_3/MobileViTS_1/stages_1_0_conv_pw_1/mul_1\" source_file=\"/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1160\n\t\tXLA Label: fusion\n\t\tShape: f32[64,128,128,128]\n\t\t==========================\n\n\tBuffer 3:\n\t\tSize: 512.00MiB\n\t\tOperator: op_type=\"Conv2D\" op_name=\"MobileViTS_depth1_dims512_heads16_3/MobileViTS_1/stages_1_0_conv_pw_conv2d_1/convolution\" source_file=\"/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1160\n\t\tXLA Label: custom-call\n\t\tShape: f32[64,128,128,128]\n\t\t==========================\n\n\tBuffer 4:\n\t\tSize: 256.00MiB\n\t\tOperator: op_type=\"Mul\" op_name=\"MobileViTS_depth1_dims512_heads16_3/MobileViTS_1/stages_1_1_conv_dw_1/mul_1\" source_file=\"/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1160 deduplicated_name=\"fusion.3163\"\n\t\tXLA Label: fusion\n\t\tShape: f32[64,256,64,64]\n\t\t==========================\n\n\tBuffer 5:\n\t\tSize: 256.00MiB\n\t\tOperator: op_type=\"Mul\" op_name=\"MobileViTS_depth1_dims512_heads16_3/MobileViTS_1/stages_1_1_conv_dw_1/mul_1\" source_file=\"/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1160 deduplicated_name=\"fusion.3163\"\n\t\tXLA Label: fusion\n\t\tShape: f32[64,256,64,64]\n\t\t==========================\n\n\tBuffer 6:\n\t\tSize: 256.00MiB\n\t\tOperator: op_type=\"DepthwiseConv2dNative\" op_name=\"MobileViTS_depth1_dims512_heads16_3/MobileViTS_1/stages_1_1_conv_dw_dwconv2d_1/depthwise\" source_file=\"/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1160\n\t\tXLA Label: custom-call\n\t\tShape: f32[64,256,64,64]\n\t\t==========================\n\n\tBuffer 7:\n\t\tSize: 256.00MiB\n\t\tOperator: op_type=\"Mul\" op_name=\"MobileViTS_depth1_dims512_heads16_3/MobileViTS_1/stages_1_1_conv_pw_1/mul_1\" source_file=\"/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1160 deduplicated_name=\"fusion.3163\"\n\t\tXLA Label: fusion\n\t\tShape: f32[64,256,64,64]\n\t\t==========================\n\n\tBuffer 8:\n\t\tSize: 256.00MiB\n\t\tOperator: op_type=\"Mul\" op_name=\"MobileViTS_depth1_dims512_heads16_3/MobileViTS_1/stages_1_1_conv_pw_1/mul_1\" source_file=\"/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1160 deduplicated_name=\"fusion.3163\"\n\t\tXLA Label: fusion\n\t\tShape: f32[64,256,64,64]\n\t\t==========================\n\n\tBuffer 9:\n\t\tSize: 256.00MiB\n\t\tOperator: op_type=\"Conv2D\" op_name=\"MobileViTS_depth1_dims512_heads16_3/MobileViTS_1/stages_1_1_conv_pw_conv2d_1/convolution\" source_file=\"/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1160\n\t\tXLA Label: custom-call\n\t\tShape: f32[64,256,64,64]\n\t\t==========================\n\n\tBuffer 10:\n\t\tSize: 256.00MiB\n\t\tOperator: op_type=\"Mul\" op_name=\"MobileViTS_depth1_dims512_heads16_3/MobileViTS_1/stages_0_0_conv_dw_1/mul_1\" source_file=\"/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1160 deduplicated_name=\"fusion.3159\"\n\t\tXLA Label: fusion\n\t\tShape: f32[64,64,128,128]\n\t\t==========================\n\n\tBuffer 11:\n\t\tSize: 256.00MiB\n\t\tOperator: op_type=\"Mul\" op_name=\"MobileViTS_depth1_dims512_heads16_3/MobileViTS_1/stages_0_0_conv_dw_1/mul_1\" source_file=\"/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1160 deduplicated_name=\"fusion.3159\"\n\t\tXLA Label: fusion\n\t\tShape: f32[64,64,128,128]\n\t\t==========================\n\n\tBuffer 12:\n\t\tSize: 256.00MiB\n\t\tOperator: op_type=\"DepthwiseConv2dNative\" op_name=\"MobileViTS_depth1_dims512_heads16_3/MobileViTS_1/stages_0_0_conv_dw_dwconv2d_1/depthwise\" source_file=\"/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1160\n\t\tXLA Label: custom-call\n\t\tShape: f32[64,64,128,128]\n\t\t==========================\n\n\tBuffer 13:\n\t\tSize: 256.00MiB\n\t\tOperator: op_type=\"Mul\" op_name=\"MobileViTS_depth1_dims512_heads16_3/MobileViTS_1/stages_0_0_conv_pw_1/mul_1\" source_file=\"/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1160 deduplicated_name=\"fusion.3159\"\n\t\tXLA Label: fusion\n\t\tShape: f32[64,64,128,128]\n\t\t==========================\n\n\tBuffer 14:\n\t\tSize: 256.00MiB\n\t\tOperator: op_type=\"Mul\" op_name=\"MobileViTS_depth1_dims512_heads16_3/MobileViTS_1/stages_0_0_conv_pw_1/mul_1\" source_file=\"/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1160 deduplicated_name=\"fusion.3159\"\n\t\tXLA Label: fusion\n\t\tShape: f32[64,64,128,128]\n\t\t==========================\n\n\tBuffer 15:\n\t\tSize: 256.00MiB\n\t\tOperator: op_type=\"Conv2D\" op_name=\"MobileViTS_depth1_dims512_heads16_3/MobileViTS_1/stages_0_0_conv_pw_conv2d_1/convolution\" source_file=\"/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1160\n\t\tXLA Label: custom-call\n\t\tShape: f32[64,64,128,128]\n\t\t==========================\n\n\n\t [[{{node StatefulPartitionedCall}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_one_step_on_iterator_142289]","output_type":"error"}]},{"cell_type":"markdown","source":"# Barlow Twins","metadata":{}},{"cell_type":"code","source":"#barlow_trainer = BarlowModel(convmixer, \n#                             embed_dims = embed_dims, multiview = True)\n#barlow_trainer.compile(optimizer = keras.optimizers.AdamW(learning_rate = cosine_decay,\n#                                                         clipvalue = 1.0,\n#                                                         #amsgrad = True\n#                                                         )\n#                      )\n#run_exp(barlow_trainer, train_ds_multiview, None, epochs = 100,\n#       note = \"Without validation d/t lack of resources\")","metadata":{"execution":{"iopub.status.busy":"2024-05-22T03:31:15.604332Z","iopub.status.idle":"2024-05-22T03:31:15.604755Z","shell.execute_reply.started":"2024-05-22T03:31:15.604578Z","shell.execute_reply":"2024-05-22T03:31:15.604596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# VICReg","metadata":{}},{"cell_type":"code","source":"#vic_trainer = VICRegModel(eff_tiny, \n#                             embed_dims = embed_dims, multiview = True)\n#vic_trainer.compile(optimizer = keras.optimizers.AdamW(learning_rate = cosine_decay,\n#                                                         clipvalue = 1.0,\n#                                                         #amsgrad = True\n#                                                         )\n#                      )\n#run_exp(vic_trainer, train_ds_multiview, None, epochs = 100,\n#       note = \"Without validation d/t lack of resources\")","metadata":{"execution":{"iopub.status.busy":"2024-05-22T03:31:15.607173Z","iopub.status.idle":"2024-05-22T03:31:15.608485Z","shell.execute_reply.started":"2024-05-22T03:31:15.608173Z","shell.execute_reply":"2024-05-22T03:31:15.608196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SimSiam\n- instant collapse....","metadata":{}},{"cell_type":"code","source":"#ssl_train(ssl_module.SimSiam, eff_tiny)","metadata":{"execution":{"iopub.status.busy":"2024-05-22T03:31:15.609743Z","iopub.status.idle":"2024-05-22T03:31:15.610237Z","shell.execute_reply.started":"2024-05-22T03:31:15.609978Z","shell.execute_reply":"2024-05-22T03:31:15.610001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SimCLR","metadata":{}},{"cell_type":"code","source":"#ssl_train(ssl_module.SimCLR, eff_small,\n#         gradient_accumulation = 8, note = \"+ Attentional pooling with register\")","metadata":{"execution":{"iopub.status.busy":"2024-05-22T03:31:15.612096Z","iopub.status.idle":"2024-05-22T03:31:15.612451Z","shell.execute_reply.started":"2024-05-22T03:31:15.612277Z","shell.execute_reply":"2024-05-22T03:31:15.612291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DINO","metadata":{}},{"cell_type":"code","source":"#ssl_train(ssl_module.DINO, regnet, gradient_accumulation = 8,\n#         note = \"+ Attentional pooling with register\")","metadata":{"execution":{"iopub.status.busy":"2024-05-22T03:31:15.614085Z","iopub.status.idle":"2024-05-22T03:31:15.614427Z","shell.execute_reply.started":"2024-05-22T03:31:15.614259Z","shell.execute_reply":"2024-05-22T03:31:15.614273Z"},"trusted":true},"execution_count":null,"outputs":[]}]}