{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport random\nimport pydicom\n\nfrom sklearn.manifold import TSNE\nimport re\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nseed = 2024\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# ML tools \n\nimport tensorflow as tf\nimport keras\nimport keras_cv\nimport keras_nlp\n#tf.keras.mixed_precision.set_global_policy('mixed_float16')\n\nimport cv2\nkeras.utils.set_random_seed(seed)\nimport tensorflow_io as tfio\nfrom kaggle_datasets import KaggleDatasets\nimport tensorflow_datasets as tfds\nimport tensorflow_probability as tfp\nimport tensorflow_decision_forests as tfdf\n\nprint(f\"Tensorflow version : {tf.__version__}\")\ntry:\n    print(f\"Keras version : {keras.__version__}\")\nexcept:\n    pass\n\nfrom keras import Input, Model, ops\nfrom keras.models import load_model\n\nfrom keras.layers import Conv2D, DepthwiseConv2D, Dense, Activation, BatchNormalization, LayerNormalization, MultiHeadAttention, Embedding, Subtract, Add, Multiply, GlobalAveragePooling2D, GlobalAveragePooling1D, LayerNormalization\nfrom keras.utils import load_img, img_to_array\nfrom keras.applications import *\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nfrom sklearn.model_selection import train_test_split\nfrom keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\nfrom tqdm.notebook import tqdm\nimport wandb\nfrom wandb.keras import WandbCallback, WandbModelCheckpoint, WandbMetricsLogger\ndef wandb_config():\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    try:\n        secret_value_0 = user_secrets.get_secret(\"__gcloud_sdk_auth__\")\n        secret_value_1 = user_secrets.get_secret(\"huggingface_key\")\n        secret_value_2 = user_secrets.get_secret(\"wandb_key\")\n        !wandb login $secret_value_2\n    except:\n        secret_value_0 = user_secrets.get_secret(\"huggingface_key\")\n        secret_value_1 = user_secrets.get_secret(\"wandb_key\")\n        !wandb login $secret_value_1\n    \n\nres = int(1.5*256)\nbatch_size = 32\nembed_dims = 768\n\ndef auto_select_accelerator():\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n        print(\"Running on TPU:\", tpu.master())\n    except ValueError:\n        tpu = False\n        strategy = tf.distribute.MirroredStrategy() # for GPU or multi-GPU machines\n    print(f\"Running on {strategy.num_replicas_in_sync} replicas\")\n    \n    return tpu, strategy\n\ntpu, strategy = auto_select_accelerator()\nbatch_size = strategy.num_replicas_in_sync * batch_size\nprint('batch size', batch_size)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-02-02T06:15:43.886637Z","iopub.execute_input":"2023-02-02T06:15:43.887052Z","iopub.status.idle":"2023-02-02T06:15:58.621758Z","shell.execute_reply.started":"2023-02-02T06:15:43.886971Z","shell.execute_reply":"2023-02-02T06:15:58.620765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import ssl_module\nfrom ssl_module import get_gcvit_configs, get_flops, att_visualize, get_full_model, AttentionPooling, BarlowModel, VICRegModel, Moco, SimSiam, CLIP, SigLIP\nimport nas_ftp_module\nfrom nas_ftp_module import upload_file, download_file","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data import (with Generator)\n- 목적 : CXR의 prior knowledge를 SwAV으로 feature map generator에 주입시키기\n- Bounding box의 information을 사용하지 않음 + External data를 사용하자","metadata":{}},{"cell_type":"code","source":"metainfo_dir = \"/kaggle/input/chexdet-image-and-annotations/ChestXDet_Metainformations/ChestX-Det-Dataset-main\"\ntrain_det_dir = \"/kaggle/input/chexdet-image-and-annotations/train_data/train\"\nval_det_dir = \"/kaggle/input/chexdet-image-and-annotations/test_data/test\"\n\ndf_det_train = pd.read_json(\"/kaggle/input/chexdet-image-and-annotations/ChestXDet_Metainformations/ChestX-Det-Dataset-main/ChestX_Det_train.json\")\ndf_det_train[\"file_name\"] = [os.path.join(train_det_dir, fname) for fname in df_det_train.file_name.values]\ndf_det_train = df_det_train.loc[:, [\"file_name\"]]\n\ndf_val = pd.read_json(\"/kaggle/input/chexdet-image-and-annotations/ChestXDet_Metainformations/ChestX-Det-Dataset-main/ChestX_Det_test.json\")\ndf_val[\"file_name\"] = [os.path.join(val_det_dir, fname) for fname in df_val.file_name.values]\ndf_val_cxr = df_val.loc[:, [\"file_name\"]]\n\n#\next_dir = \"/kaggle/input/vinbigdata-chest-xray-original-png/train\"\ndict_ext = {\"file_name\" : [os.path.join(ext_dir, fname) for fname in os.listdir(ext_dir)] }\ndf_ext = pd.DataFrame(dict_ext)\ndf_train_cxr = pd.concat([df_det_train, df_ext], axis = 0)\nprint(f\"Total training cases for CXR : {len(df_train_cxr)} cases, Validation case : {len(df_val_cxr)} case\")","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-02-02T06:16:01.739233Z","iopub.execute_input":"2023-02-02T06:16:01.739598Z","iopub.status.idle":"2023-02-02T06:16:02.727185Z","shell.execute_reply.started":"2023-02-02T06:16:01.739547Z","shell.execute_reply":"2023-02-02T06:16:02.726090Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Deeplesion metainformation dataframe 생성","metadata":{}},{"cell_type":"code","source":"ct_fname = []\nbase_img_dir = '/kaggle/input/nih-deeplesion-subset/minideeplesion'\nfor dirname, _, filenames in tqdm(os.walk(base_img_dir)):\n    for filename in filenames:\n        ct_fname.append(os.path.join(dirname, filename))\n        \ndf_ct_whole = pd.DataFrame({\"file_name\" : ct_fname})\n\ndf_ct_train, df_ct_val = train_test_split(df_ct_whole, \n                                         test_size = 134,\n                                         random_state = 1)\nprint(f\"Total training cases of Chest/Abdomen CT : {len(df_ct_train)} cases, Validation case : {len(df_ct_val)} case\")","metadata":{"execution":{"iopub.status.busy":"2023-02-02T06:16:02.730557Z","iopub.execute_input":"2023-02-02T06:16:02.730866Z","iopub.status.idle":"2023-02-02T06:16:09.247406Z","shell.execute_reply.started":"2023-02-02T06:16:02.730840Z","shell.execute_reply":"2023-02-02T06:16:09.246505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> import RSNA ICH dataset metainformation dataframe","metadata":{}},{"cell_type":"code","source":"dicom_dir = \"/kaggle/input/rsna-intracranial-hemorrhage-detection/rsna-intracranial-hemorrhage-detection/stage_2_train\"\ndf_train_brainct = pd.read_csv(\"/kaggle/input/rsna-ich-detection-metadata/df_train_split.csv\")\ndf_val_brainct = pd.read_csv(\"/kaggle/input/rsna-ich-detection-metadata/df_val_splt.csv\").head(300)\n\nfor df in [df_train_brainct, df_val_brainct]:\n    df[\"file_name\"] = [os.path.join(dicom_dir, fname + \".dcm\") for fname in df['SOPInstanceUID']]\n    \nprint(f\"Total training cases of Brain, NonCE CT : {len(df_train_brainct)} cases, Validation case : {len(df_val_brainct)} case\")","metadata":{"execution":{"iopub.status.busy":"2023-02-02T06:16:09.248757Z","iopub.execute_input":"2023-02-02T06:16:09.249221Z","iopub.status.idle":"2023-02-02T06:16:09.374254Z","shell.execute_reply.started":"2023-02-02T06:16:09.249183Z","shell.execute_reply":"2023-02-02T06:16:09.373243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.concat([df_ct_train, df_train_cxr, df_train_brainct], axis = 0, join='inner')\ndf_val = pd.concat([df_val_cxr, df_ct_val, df_val_brainct], axis = 0, join='inner')\n\ndf_train.to_csv(\"df_train_ER_SSL.csv\", index = False)\ndf_val.to_csv(\"df_val_ER_SSL.csv\", index = False)\n\nprint(f\"Total train cases : {len(df_train)}, val cases : {len(df_val)} cases\")\ndf_train.sample(10)","metadata":{"execution":{"iopub.status.busy":"2023-02-02T06:16:09.375822Z","iopub.execute_input":"2023-02-02T06:16:09.376202Z","iopub.status.idle":"2023-02-02T06:16:09.565076Z","shell.execute_reply.started":"2023-02-02T06:16:09.376166Z","shell.execute_reply":"2023-02-02T06:16:09.564049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building Dataloader in keras-3 style\n- For SwAV training, consider multi-crop with high-res and low-res","metadata":{}},{"cell_type":"code","source":"class ImageDataLoader(keras.utils.Sequence):\n    def __init__(self, dataframe, x_col, res, batch_size, y_col = None, shuffle = True):\n        self.df = dataframe\n        self.x_col = x_col ; self.y_col = y_col\n        self.res = res\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.on_epoch_end()\n    def dicom_to_tensor(self, dicom_path):\n        dataset = pydicom.dcmread(dicom_path)\n        tensor = np.array(dataset.pixel_array)\n        slope = dataset.RescaleSlope   # dicom header (Rescale slope)\n        intercept = dataset.RescaleIntercept   # dicom header (Rescale intercept)\n        center = dataset.WindowCenter   # dicom header (Window center)\n        width = dataset.WindowWidth   # dicom header (Window width)\n\n        if(type(dataset.WindowCenter) == pydicom.multival.MultiValue):\n                center = float(dataset.WindowCenter[0])\n                width = float(dataset.WindowWidth[0])       \n        else:    \n                center = float(dataset.WindowCenter)\n                width = float(dataset.WindowWidth)\n\n        tensor = slope*tensor + intercept\n        lbound, ubound = center - 0.5*width, center + 0.5*width\n        tensor[np.where(tensor < lbound)] = lbound\n        tensor[np.where(tensor > ubound)] = ubound\n        tensor = tf.image.resize(tensor[:,:,tf.newaxis], [self.res,self.res]) #HU unit\n        if tf.shape(tensor)[-1] == 1 :#gray\n            tensor = tf.image.grayscale_to_rgb(tensor)\n            \n        tensor = (tensor - tf.reduce_min(tensor)) / (tf.reduce_max(tensor) - tf.reduce_min(tensor)) #HU unit to Uint8\n        tensor = tensor*255.0\n        tensor = np.array(tensor).astype(np.uint8)\n        try:\n            del dataset\n        except:\n            pass\n        return tensor\n    \n    def image_to_tensor(self, path):\n        if path.split(\".\")[-1] == \"dcm\":\n            return self.dicom_to_tensor(path)\n        \n        if \"minideeplesion\" in str(path).split(\"/\"):\n            image = imread(path).astype(np.float32)-32768\n            image = image[..., tf.newaxis]\n            image = tf.image.resize(image, [self.res, self.res])\n            image = tf.clip_by_value(image, -750.0, 700.0)\n            image = (image - tf.reduce_min(image))/(tf.reduce_max(image) - tf.reduce_min(image) + 1e-3)\n            image = image * 255.0\n        else:           \n            image = load_img(path, target_size = [self.res, self.res])\n            image = img_to_array(image)\n            \n        if tf.shape(image)[-1] == 1 :#gray\n            image = tf.image.grayscale_to_rgb(image)\n            image = np.array(image)\n            \n        image = np.array(image).astype(\"uint8\")\n        return image\n        \n    def on_epoch_end(self):\n        self.indexes = np.arange(len(self.df))\n        if self.shuffle:\n            np.random.shuffle(self.indexes)\n            \n    def __len__(self):\n        return int(np.floor(len(self.df) / self.batch_size))\n    \n    def __data_generation(self, img_name):\n        ## path를 받아 img화 및 token화 하여 실제로 Feeding할 데이터를 반환\n        X = []\n        for i, fname in enumerate(img_name):\n            img = self.image_to_tensor(fname)\n            X.append(img)\n        X = np.array(X)\n        return X\n        \n                \n    def __getitem__(self, index):\n        indexes = self.indexes[index * self.batch_size : (index + 1) * self.batch_size]\n        \n        img_name = [self.df.iloc[k].loc[self.x_col] for k in indexes]\n        \n        X = self.__data_generation(img_name)\n        #X = np.array(X).reshape([-1, self.res, self.res, 3])\n        return X\n    \n\ndef get_train_gen():\n    return ImageDataLoader(df_train, x_col = \"file_name\",\n                         res = original_res, batch_size = batch_size)\n\ndef get_val_gen():\n    return ImageDataLoader(df_val, x_col = \"file_name\",\n                         res = original_res, batch_size = batch_size)\ntest_ds = tf.data.Dataset.from_generator(get_val_gen, (tf.uint8), output_shapes = (batch_size, original_res, original_res,3) ).repeat()","metadata":{"execution":{"iopub.status.busy":"2023-02-02T06:16:09.566764Z","iopub.execute_input":"2023-02-02T06:16:09.567141Z","iopub.status.idle":"2023-02-02T06:16:09.632011Z","shell.execute_reply.started":"2023-02-02T06:16:09.567103Z","shell.execute_reply":"2023-02-02T06:16:09.631105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds = tf.data.Dataset.from_generator(get_train_gen, (tf.uint8), output_shapes = (batch_size, original_res, original_res,3) ).repeat()\nval_ds = tf.data.Dataset.from_generator(get_val_gen, (tf.uint8), output_shapes = (batch_size, original_res, original_res,3) ).repeat()\nprint(\"Original\")\nfor img in train_ds.take(1):\n    for i in img:\n        plt.imshow(i)\n        plt.colorbar()\n        plt.show()\n# Multi-Crop dataset 제작\n# 한 개의 이미지로부터 총 5개의 이미지를 만드는데, 2개는 원본 크기와 같은 high-resolution, \n# 나머지 3개는 작은 resolution \n# 전자는 Sinkhorn 알고리즘으로 Matrix C를 학습시키는 데 사용 (즉, Code Q를 만드는 데 사용)\n# 그렇게 계산한 Matrix C (Code Q)를 후자인 3개의 이미지로부터 encoder가 생성한 feature vector Z로 예측함\n\naug = album.Compose([ \n                     album.CoarseDropout(), album.RandomResizedCrop(res, res,\n                                                                   scale = (0.75, 1),\n                                                                   p=1.0),\n                    album.OneOf([album.HorizontalFlip(p=1),\n                                album.Rotate(limit = 60, p = 1),\n                                album.VerticalFlip(p=1)            \n                                ], p=1),\n                    album.OneOf([album.ColorJitter(), \n                                 album.GaussNoise()],\n                                p = 1.0),\n                    album.CLAHE(p = 1.0),\n                    ]\n                   )\n \naug_low_res = album.Compose([ \n                     album.CoarseDropout(),\n                    album.RandomResizedCrop(low_res, low_res, scale = (0.1, 0.5), p=1.0),\n                    album.OneOf([album.HorizontalFlip(p=1),\n                                album.Rotate(limit = 60, p = 1),\n                                album.VerticalFlip(p=1)            \n                                ], p=1),\n                    album.OneOf([album.ColorJitter(), \n                                 album.GaussNoise()],\n                                p = 1.0),\n                    album.CLAHE(p = 1.0),\n                    ]\n                   )    \n\ndef aug_fn(image):\n    high_resolution_set = []\n    low_resolution_set = []\n    for idx in highres_idx:\n        aug_data = aug(image = image)\n        aug_img = aug_data['image']\n        aug_img = np.array(aug_img).astype(np.uint8)\n        high_resolution_set.append(aug_img)\n        del aug_img\n    for idx in lowres_idx:\n        aug_data = aug_low_res(image = image)\n        aug_img = aug_data['image']\n        aug_img = np.array(aug_img).astype(np.uint8)\n        high_resolution_set.append(aug_img)\n        del aug_img\n    result = high_resolution_set + low_resolution_set\n    return result\n\n\ndef swav_preprocess_data(image):\n    data = tf.numpy_function(func = aug_fn, inp = [image], \n                                Tout = ([tf.uint8 for _ in range(n_tot)])\n                               )\n    \n    return data\n\ntrain_ds = train_ds.unbatch().map(swav_preprocess_data, num_parallel_calls = tf.data.AUTOTUNE).batch(batch_size = batch_size).prefetch(tf.data.AUTOTUNE)\ntrain_ds = train_ds.repeat()\nval_ds = val_ds.unbatch().map(swav_preprocess_data, num_parallel_calls = tf.data.AUTOTUNE).batch(batch_size = batch_size).prefetch(tf.data.AUTOTUNE)\nval_ds = val_ds.repeat()\n\nfor imgset in train_ds.take(1):\n    data = imgset\n    for sub_img in data:\n        print(sub_img.shape)\n        \nfor show_n in range(batch_size):\n    fig, axes = plt.subplots(1, len(highres_idx), figsize = (10, 20))\n    axes = axes.flatten()\n    for idx, ax in zip(highres_idx, axes):\n        ax.imshow(data[idx][show_n])\n        ax.set_title(f\"Image index : {idx}\")\n\n    plt.show()\n\n    fig, axes = plt.subplots(2, len(lowres_idx)//2, figsize = (12,8))\n    axes = axes.flatten()\n    for idx, ax in zip(lowres_idx, axes):\n        ax.imshow(data[idx][show_n])\n        ax.set_title(f\"Image index : {idx}\")\n\n    plt.show()","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-02-02T06:16:09.633714Z","iopub.execute_input":"2023-02-02T06:16:09.634081Z","iopub.status.idle":"2023-02-02T06:16:19.033402Z","shell.execute_reply.started":"2023-02-02T06:16:09.634045Z","shell.execute_reply":"2023-02-02T06:16:19.032594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Teacher and Student network modeling\n- SwAV이나 다른 네트워크와는 다르게, DINO는 Student와 teacher 모두 정확히 동일한 network 구조를 가짐\n- image input을 받아 K-dimension feature vector를 생성하는 모델","metadata":{}},{"cell_type":"code","source":"# transformer base\ndef get_vision_transformer(patch_size,\n                           att_depth,\n                          att_dims, att_heads, K_dims = K_dims,\n                          use_local_attention = False):\n    # Get Adjusted Vision Transformer base\n    \n    model_name = f\"ViT{patch_size}_depth{att_depth}_with_{att_heads}heads\"\n    inputs = Input([None,None,3], name = \"ImageInput\")\n    scaled_inputs = inputs/255\n    patches = Conv2D(filters = att_dims, kernel_size = patch_size, strides = patch_size, padding = \"SAME\", name = f\"Patching_Convolution_PatchSize{patch_size}\")(scaled_inputs)\n    patches = LayerNormalization()(patches)\n    patches = Activation(\"gelu\", name = \"PatchAct\")(patches)\n    \n    batch_size = tf.shape(inputs)[0]\n    patch_dims = patches.shape[-1]\n    patches = tf.reshape(patches, [batch_size, -1, patch_dims]) #batch_size, seq_len, patch_dims\n    seq_len = tf.shape(patches)[1]\n    \n    class_token = tf.random.normal([batch_size, 1, patch_dims])\n    patches = tf.concat([class_token, patches],\n                       axis = 1) #batch_size, seq_len+1, patch_dims\n    \n    #positional encoding, learnable\n    pos_info = tf.random.normal([1, seq_len+1, patch_dims])/(tf.math.sqrt(tf.cast(patch_dims, tf.float32)))\n    encoded_patches = patches + pos_info\n    #encoded_patches = patches\n    \n    # Optional) local attention : patch간 attention시 자기 자신에 관한 attention은 수행하지 않도록\n    attn_mask = 1-tf.eye(seq_len + 1)\n    attn_mask = tf.reshape(attn_mask, [1, seq_len + 1, seq_len + 1])\n    attn_mask = tf.tile(attn_mask, [batch_size, 1, 1])\n    attn_mask = tf.cast(attn_mask, tf.int32 )\n    if use_local_attention:\n        mask = attn_mask\n    else:\n        mask = None\n    #Transformer Encoder block\n    for idx in range(att_depth):\n        \n        x0, attention_score = MultiHeadAttention(att_heads, att_dims)(encoded_patches, \n                                                     encoded_patches, \n                                                    attention_mask = mask,\n                                                     return_attention_scores = True\n                                                    )\n        x1 = x0 + encoded_patches\n        x2 = LayerNormalization()(x1)\n        \n        x3 = Dense(units = 2*att_dims)(x2)\n        x4 = LayerNormalization()(x3)\n        x5 = Activation(\"gelu\")(x4)\n        x6 = Dense(units = att_dims)(x5)\n        x7 = LayerNormalization()(x6)\n        x8 = Activation('gelu')(x7)\n        \n        x9 = x2 + x8\n        encoded_patches = LayerNormalization()(x9)\n    \n    encoded_token = encoded_patches[:, 0, :]\n    encoded_token = tf.reshape(encoded_token, [batch_size, att_dims])\n    proj = get_proj(encoded_token)\n    feature_vector = Dense(units = K_dims, name = \"FinalLinear\", kernel_regularizer=tf.keras.regularizers.L1L2(l1=1e-4, l2=1e-4))(proj)\n    \n    model = Model(inputs = inputs,\n                 outputs = [feature_vector, attention_score],\n                 name = model_name)\n    return model","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-02-02T06:16:19.034893Z","iopub.execute_input":"2023-02-02T06:16:19.035391Z","iopub.status.idle":"2023-02-02T06:16:19.053483Z","shell.execute_reply.started":"2023-02-02T06:16:19.035359Z","shell.execute_reply":"2023-02-02T06:16:19.052588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Visualize the attention scores\n- Attention block을 다 거친 뒤에,\n- 모델이 어디를 보고 있는지를 시각화함 (Code below)","metadata":{}},{"cell_type":"code","source":"batch_n = 1\nhead_n = 3\n\ndef visualize_att_score(score, batch_n, head_n):\n    #input score : batch, heads, (seqlen+1), (seqlen+1) shape tensor\n    example_weight = score[batch_n, head_n, ...]\n    parsed_weight = example_weight[0, 1:] #class token의, (class token을 제외한) 다른 patches(tokens) 간의 attention weight\n    print(f\"Mean of Attention weight between each patches ifself: {tf.reduce_mean(tf.linalg.tensor_diag_part(example_weight))}\")\n    vis_res = tf.math.sqrt(tf.cast(tf.shape(parsed_weight)[-1], tf.float32))\n    \n    parsed_weight = tf.reshape(parsed_weight, [vis_res, vis_res])\n    plt.figure(figsize = (6,6))\n    plt.imshow(parsed_weight)\n    plt.colorbar()\n    plt.title(f\"Attention Weight, img idx{batch_n}, att head idx{head_n}\")\n    plt.show()\n    return parsed_weight","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-02-02T06:16:19.058132Z","iopub.execute_input":"2023-02-02T06:16:19.058461Z","iopub.status.idle":"2023-02-02T06:16:19.069994Z","shell.execute_reply.started":"2023-02-02T06:16:19.058431Z","shell.execute_reply":"2023-02-02T06:16:19.068882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> 중간 점검\n- 각 Head마다 주로 보는 곳이 다름\n    - 어떤 head는 radiopaque한 곳 위주로 (bone, soft tissue)\n    - 어떤 head는 lung tissue 위주로 (radiolucent한 곳 - lung, 배경 등)\n- 거의 같은 크기의 weight -> Dino를 거쳐 정말 중요한 부분에 weight를 극대화하여야 함","metadata":{}},{"cell_type":"code","source":"def sinkhorn(c, temp = 0.05, repeat = 3): #q_example = [batch, K]\n    c = tf.cast(c, tf.float32)\n    q = tf.transpose(tf.exp(c/temp)) #transpose 후 -> [K, batch]\n    # q도 확률 분포여야 함 -> sum of q == 1이 되게 Normalize\n    q /= tf.reduce_sum(q)\n    k, batch = tf.shape(c)[0], tf.shape(c)[1]\n    \n    r = tf.ones_like(k, dtype = tf.float32)/tf.cast(k, dtype = tf.float32) #생산하는 측은 k개의 cluster에 uniform하게 배당될 수 있음\n    c = tf.ones_like(batch, tf.float32)/tf.cast(batch, tf.float32) #수용하는 측은 b개의 생산자로부터 동일한 확률로 (prior가 없음) 받을 수 있음\n    for idx in range(repeat):\n        # 이 때의 목적은, q가 여전히 stochastic하면서\n        # 열 방향의 합과 행 방향의 합 벡터가 각각 (Transpose 이후의 기준으로) 1/batch, 1/k로 채워진 vector가 되게 하는 것\n        u = tf.reduce_sum(q, axis=1) #Q matrix의 행방향 합 -> u는 합이 1인(stochastic한) K-length vector가 된다.\n        u_other = tf.reduce_sum(q, axis = 0)\n        \n        q *= tf.expand_dims((r/u), axis = 1) # Code reference) https://github.com/ayulockin/SwAV-TF/blob/master/Train_SwAV_10_epochs.ipynb\n        q *= tf.expand_dims(c/u_other, axis = 0) #이 과정에서 n번째 \n        \n    # 다시 원래 모양으로 Transpose해 주고, ---(1)\n    # 나아가 Stochastic하게 만듬 -> 출발점에 상관없이,임의의 출발점(Mine, Producer)에서 모든 도착지로 갈 확률을 구한 뒤 합하면 1이 되어야 함\n    # 즉, (transpose한 뒤) 열방향 합으로 나눠, N번째 행의 모든 열 원소의 합이 1이 되게 Normalize. --- (2)\n    final_q = tf.transpose(q) #(1)\n    final_q /= tf.reduce_sum(final_q, axis = -1, keepdims = True) #(2)\n    return final_q","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-02-02T06:16:19.071917Z","iopub.execute_input":"2023-02-02T06:16:19.072354Z","iopub.status.idle":"2023-02-02T06:16:19.083607Z","shell.execute_reply.started":"2023-02-02T06:16:19.072319Z","shell.execute_reply":"2023-02-02T06:16:19.082556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> 생각해 볼 점\n- DINO에서 teacher centering을 담당하는 C는 epoch이나 step마다 초기화되는 것이 아님\n- 한 step 즉 dataset에서 한 batch를 받고 loss를 계산한 뒤에 update됨.\n    - fit method를 그대로 쓰지 않고 for loop으로 custom fit cycle을 만들기는 keras 기본 UI나 callback 등이 좋아서 별로 내키지 않음\n    - 그러므로 DINO module을 만들기 전에, 혹은 initialize하면서, random한 one batch에 대해 Center를 계산하고 이것을 update하는 식으로 진행\n    \n- 한 SSL pretrainer module은 다음과 같은 SSL method를 포함:\n    - DINO\n    - SwAV\n    - Barlow twins","metadata":{}},{"cell_type":"markdown","source":"# SwAV implementation","metadata":{}},{"cell_type":"code","source":"class SwAV_module(keras.Model):\n    def __init__(self, \n                 patch_size = patch_size, att_depth = 3, att_dims = 256, att_heads = 8, prototype_c_dim = K_dims, #<-transformer 관련 hyperparameter\n                 conv_base = None, \n                 use_local_attention = False,\n                 t_teacher = t_teacher,\n                 highres_idx = highres_idx, lowres_idx = lowres_idx, **kwargs\n                ):\n        super().__init__(**kwargs)\n        if conv_base is None:\n        #feature extractors\n            self.patch_size = patch_size\n            self.att_depth = att_depth\n            self.att_dims = att_dims\n            self.att_heads = att_heads\n            self.K_dims = prototype_c_dim\n            self.use_local_attention = use_local_attention\n\n            self.teacher = self.get_vision_transformer()\n        else:\n            self.conv = conv_base\n            self.patch_size = 0\n            self.att_depth = 0\n            self.att_dims = 0\n            self.att_heads = 0\n            self.K_dims = prototype_c_dim\n            self.use_local_attention = use_local_attention\n            \n            self.teacher = self.build_conv_base()\n            \n        #hyperparameters\n        self.temp = t_teacher\n        self.highres_idx = highres_idx\n        self.lowres_idx = lowres_idx\n        #datasets\n        self.n_highres = len(highres_idx)\n        self.n_lowres = len(lowres_idx)\n        self.n_tot_view = self.n_highres + self.n_lowres\n        \n        self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n        self.collapse_tracker = tf.keras.metrics.Mean(name=\"Entropy\")\n        self.std_tracker = tf.keras.metrics.Mean(name=\"StDev\")\n        \n        self.ce_loss = tf.keras.losses.CategoricalCrossentropy(reduction=\"none\")\n        self.collapse_tracker = tf.keras.metrics.Mean(name=\"Entropy\")\n        self.std_tracker = tf.keras.metrics.Mean(name=\"STDEV\")\n        \n    def build_conv_base(self):\n        conv = self.conv\n        K_dims = self.K_dims\n        try:\n            name = conv.name\n        except:\n            name = \"ConvolutionBaseModel\"\n        inputs = conv.inputs\n        conv_out = conv.output\n        dims = conv_out.shape[-1]\n        #dims = tf.shape(conv_out)[-1]\n        encoded_patches = keras.layers.Reshape([-1, dims], name = \"EncodedPatches\")(conv_out)\n        pool = tf.keras.layers.GlobalAveragePooling1D(name = \"PoolingFromEncodedPatches\")(encoded_patches)\n        proj = get_proj(pool)\n        feature_vector = Dense(units = K_dims, name = \"FinalLinear\", kernel_regularizer=tf.keras.regularizers.L1L2(l1=1e-4, l2=1e-4))(proj)\n\n        return Model(inputs, feature_vector,\n                        name = name)\n\n    def get_vision_transformer(self):\n        # Get Adjusted Vision Transformer base\n        \n        patch_size = self.patch_size\n        att_depth = self.att_depth\n        att_dims = self.att_dims\n        att_heads = self.att_heads \n        K_dims = self.K_dims\n        use_local_attention = self.use_local_attention\n        \n        model_name = f\"ViT{patch_size}_depth{att_depth}_with_{att_heads}heads\"\n        inputs = Input([None,None,3], name = \"ImageInput\")\n        scaled_inputs = inputs/255\n        patches = Conv2D(filters = att_dims, kernel_size = patch_size, strides = patch_size, padding = \"SAME\", name = f\"Patching_Convolution_PatchSize{patch_size}\")(scaled_inputs)\n        patches = LayerNormalization()(patches)\n        patches = Activation(\"gelu\", name = \"PatchAct\")(patches)\n\n        batch_size = tf.shape(inputs)[0]\n        patch_dims = patches.shape[-1]\n        patches = tf.reshape(patches, [batch_size, -1, patch_dims]) #batch_size, seq_len, patch_dims\n        seq_len = tf.shape(patches)[1]\n\n        class_token = tf.random.normal([batch_size, 1, patch_dims])\n        patches = tf.concat([class_token, patches],\n                           axis = 1) #batch_size, seq_len+1, patch_dims\n\n        #positional encoding, learnable\n        pos_info = tf.random.normal([1, seq_len+1, patch_dims])\n        encoded_patches = patches + pos_info\n        #encoded_patches = patches\n\n        # Optional) local attention : patch간 attention시 자기 자신에 관한 attention은 수행하지 않도록\n        attn_mask = 1-tf.eye(seq_len + 1)\n        attn_mask = tf.reshape(attn_mask, [1, seq_len + 1, seq_len + 1])\n        attn_mask = tf.tile(attn_mask, [batch_size, 1, 1])\n        attn_mask = tf.cast(attn_mask, tf.int32 )\n        if use_local_attention:\n            mask = attn_mask\n        else:\n            mask = None\n        #Transformer Encoder block\n        for idx in range(att_depth):\n\n            x0, attention_score = MultiHeadAttention(att_heads, att_dims)(encoded_patches, \n                                                         encoded_patches, \n                                                        attention_mask = mask,\n                                                         return_attention_scores = True\n                                                        )\n            x1 = x0 + encoded_patches\n            x2 = LayerNormalization()(x1)\n\n            x3 = Dense(units = att_dims)(x2)\n            x4 = LayerNormalization()(x3)\n            x5 = Activation(\"gelu\")(x4)\n            \n            x6 = x2 + x5\n            if idx == att_depth - 1:\n                layer_name = \"EncodedPatches\"\n            else:\n                layer_name = f\"Feature_Sequence_DepthIdx{idx}\"\n            encoded_patches = LayerNormalization(name = layer_name)(x6)\n\n        encoded_token = encoded_patches[:, 0, :]\n        encoded_token = tf.reshape(encoded_token, [batch_size, att_dims])\n        proj = get_proj(encoded_token)\n        feature_vector = Dense(units = K_dims, name = \"FinalLinear\", kernel_regularizer=tf.keras.regularizers.L1L2(l1=1e-4, l2=1e-4))(proj)\n\n        model = Model(inputs = inputs,\n                     outputs = [feature_vector, attention_score],\n                     name = model_name)\n        return model\n    \n    \n    def get_config(self):\n        return {\"TrainMethod\" : \"SwAV\",\n                \"feature_extractor_name\" : self.teacher.name,\n                \"N_params\" : self.teacher.count_params(),\n                \"patch_size\" : self.patch_size,\n                \"att_depth\" : self.att_depth,\n                \"att_dims\" : self.att_dims,\n                \"att_heads\" : self.att_heads,\n                \"K_dims\" : self.K_dims,\n                \"use_local_attention\" : self.use_local_attention,\n                \"feature_extractor_name\" : self.teacher.name,\n                \"softmax_temperature\" : self.temp,\n                \"Global_view_index\" : self.highres_idx,\n                \"Local_view_index\" : self.lowres_idx\n               }\n    \n    def sinkhorn(self, q_example):\n        q = tf.transpose(tf.exp(q_example/self.temp)) #transpose 후 -> [K, batch]\n        q /= tf.reduce_sum(q)\n        k, batch = tf.shape(q_example)[0], tf.shape(q_example)[1]\n\n        r = tf.ones_like(k, dtype = tf.float32)/tf.cast(k, dtype = tf.float32) #생산하는 측은 k개의 cluster에 uniform하게 배당될 수 있음\n        c = tf.ones_like(batch, tf.float32)/tf.cast(batch, tf.float32) #수용하는 측은 b개의 생산자로부터 동일한 확률로 (prior가 없음) 받을 수 있음\n        for idx in range(3):\n            # 이 때의 목적은, q가 여전히 stochastic하면서\n            # 열 방향의 합과 행 방향의 합 벡터가 각각 (Transpose 이후의 기준으로) 1/batch, 1/k로 채워진 vector가 되게 하는 것\n            u = tf.reduce_sum(q, axis=1) #Q matrix의 행방향 합 -> u는 합이 1인(stochastic한) K-length vector가 된다.\n            u_other = tf.reduce_sum(q, axis = 0)\n\n            q *= tf.expand_dims((r/u), axis = 1) # Code reference) https://github.com/ayulockin/SwAV-TF/blob/master/Train_SwAV_10_epochs.ipynb\n            q *= tf.expand_dims(c/u_other, axis = 0) #이 과정에서 n번째 \n\n        # 다시 원래 모양으로 Transpose해 주고, ---(1)\n        # 나아가 Stochastic하게 만듬 -> 출발점에 상관없이,임의의 출발점(Mine, Producer)에서 모든 도착지로 갈 확률을 구한 뒤 합하면 1이 되어야 함\n        # 즉, (transpose한 뒤) 열방향 합으로 나눠, N번째 행의 모든 열 원소의 합이 1이 되게 Normalize. --- (2)\n        final_q = tf.transpose(q) #(1)\n        final_q /= tf.reduce_sum(final_q, axis = -1, keepdims = True) #(2)\n        return final_q\n    \n    def compute_loss(self, dataset, training = True):\n        \n        feature_output = []\n        \n        for idx in self.highres_idx:\n            if len(self.teacher.outputs) >= 2:\n                f = self.teacher(dataset[idx], training = training)[0]                \n            else:\n                f = self.teacher(dataset[idx], training = training)\n            feature_output.append(f)\n\n        for idx in self.lowres_idx:\n            if len(self.teacher.outputs) >= 2:\n                f = self.teacher(dataset[idx], training = training)[0]\n            else:\n                f = self.teacher(dataset[idx], training = training)\n            feature_output.append(f)\n        \n        # SwAV에서는 feature output이 Prototype C로 작동 (L2 normalize를 거친 K-dimensional vector)\n        prototype = feature_output\n        \n        loss = [] #initialize\n        for idx_global in self.highres_idx:\n            global_prototype = prototype[idx_global]\n            q_global = self.sinkhorn(global_prototype)\n            tf.stop_gradient(q_global) #true 역할을 하는 q는 stop gradient\n            for idx, feature in enumerate(prototype):\n                if idx == idx_global:\n                    continue\n                else:\n                    p = tf.nn.softmax(feature / self.temp, axis = -1)\n                    _loss = self.ce_loss(y_true = q_global, y_pred = p)\n                    _loss = tf.reduce_mean(_loss)\n                    loss.append(_loss)\n        loss = tf.reduce_mean(loss)\n        return loss, tf.concat(feature_output, axis = 0)\n    \n    #2. training and validation loop\n    def train_step(self, dataset, testing_stage = False, training = True):\n        if testing_stage:\n            print(\"Loss Computation\")\n        with tf.GradientTape() as tape:\n            loss, features = self.compute_loss(dataset, training = training)\n        \n        if testing_stage:\n            print(\"Teacher Backpropagation\")\n        training_vars = self.teacher.trainable_variables\n        gradients = tape.gradient(loss, training_vars) #teacher update without momentum encoding\n        self.optimizer.apply_gradients(zip(gradients, training_vars))\n        \n        \n        global_entropy = self.ce_loss(y_true = features, y_pred = features)\n        global_entropy = tf.reduce_mean(global_entropy)\n        std = tf.math.reduce_std(features) \n        self.loss_tracker.update_state(loss)\n        self.collapse_tracker.update_state(global_entropy)\n        self.std_tracker.update_state(std)\n        if testing_stage:\n            print(\"Single training stage over!\")\n            \n        return {\"loss\": self.loss_tracker.result(),\n               \"H_glob_glob\" : self.collapse_tracker.result(),\n               \"std\" : self.std_tracker.result()}\n    \n    def call(self, dataset, training = False): \n        return self.teacher(dataset, training = training)\n","metadata":{"execution":{"iopub.status.busy":"2023-02-02T06:16:19.085346Z","iopub.execute_input":"2023-02-02T06:16:19.086850Z","iopub.status.idle":"2023-02-02T06:16:19.128728Z","shell.execute_reply.started":"2023-02-02T06:16:19.086819Z","shell.execute_reply":"2023-02-02T06:16:19.127809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DINO implementation","metadata":{}},{"cell_type":"code","source":"class DINO_module(keras.Model):\n    def __init__(self, conv_base = None, \n                 patch_size = patch_size, att_depth = 3, att_heads = 8, att_dims = 512, use_local_attention = False,\n                 K_dims = K_dims,\n                 t_stu = t_stu, t_teacher = t_teacher,\n                 lamb = lamb, m = m,\n                 highres_idx = highres_idx, lowres_idx = lowres_idx, **kwargs\n                ):\n        super().__init__(**kwargs)\n        #feature extractors\n        if conv_base is None:\n            self.patch_size = patch_size\n            self.att_depth = att_depth\n            self.att_dims = att_dims\n            self.att_heads = att_heads\n            self.K_dims = K_dims\n            self.use_local_attention = use_local_attention\n\n            self.teacher = self.get_vision_transformer()\n            self.student = self.get_vision_transformer()\n        else:\n            self.conv = conv_base\n            self.patch_size = 0\n            self.att_depth = 0\n            self.att_dims = 0\n            self.att_heads = 0\n            self.K_dims = K_dims\n            self.use_local_attention = use_local_attention\n            \n            self.teacher = self.build_conv_base()\n            self.student = self.build_conv_base()\n            \n        \n        #hyperparameters\n        self.k = K_dims\n        self.tps = t_stu\n        self.tpt = t_teacher\n        self.lamb = float(lamb)\n        self.m = m\n        self.highres_idx = highres_idx\n        self.lowres_idx = lowres_idx\n        #datasets\n        self.n_highres = len(highres_idx)\n        self.n_lowres = len(lowres_idx)\n        self.n_tot_view = self.n_highres + self.n_lowres\n        \n        #Basic initializing\n        self.initialize_weight_and_center()\n        \n        self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n        self.collapse_tracker = tf.keras.metrics.Mean(name=\"Entropy\")\n        self.std_tracker = tf.keras.metrics.Mean(name=\"Entropy\")\n        \n        self.ce_loss = tf.keras.losses.CategoricalCrossentropy(reduction=\"none\")\n    \n    \n    # Get feature extractors\n    def build_conv_base(self):\n        conv = self.conv\n        K_dims = self.K_dims\n        try:\n            name = conv.name\n        except:\n            name = \"ConvolutionBaseModel\"\n        inputs = conv.inputs\n        conv_out = conv.output\n        dims = conv_out.shape[-1]\n        #dims = tf.shape(conv_out)[-1]\n        encoded_patches = keras.layers.Reshape([-1, dims], name = \"EncodedPatches\")(conv_out)\n        pool = tf.keras.layers.GlobalAveragePooling1D(name = \"PoolingFromEncodedPatches\")(encoded_patches)\n        proj = get_proj(pool)\n        feature_vector = Dense(units = K_dims, name = \"FinalLinear\", kernel_regularizer=tf.keras.regularizers.L1L2(l1=1e-4, l2=1e-4))(proj)\n\n        return Model(inputs, feature_vector,\n                        name = name)\n\n    def get_vision_transformer(self):\n        # Get Adjusted Vision Transformer base\n        \n        patch_size = self.patch_size\n        att_depth = self.att_depth\n        att_dims = self.att_dims\n        att_heads = self.att_heads \n        K_dims = self.K_dims\n        use_local_attention = self.use_local_attention\n        \n        model_name = f\"ViT{patch_size}_depth{att_depth}_with_{att_heads}heads\"\n        inputs = Input([None,None,3], name = \"ImageInput\")\n        scaled_inputs = inputs/255\n        patches = Conv2D(filters = att_dims, kernel_size = patch_size, strides = patch_size, padding = \"SAME\", name = f\"Patching_Convolution_PatchSize{patch_size}\")(scaled_inputs)\n        patches = LayerNormalization()(patches)\n        patches = Activation(\"gelu\", name = \"PatchAct\")(patches)\n\n        batch_size = tf.shape(inputs)[0]\n        patch_dims = patches.shape[-1]\n        patches = tf.reshape(patches, [batch_size, -1, patch_dims]) #batch_size, seq_len, patch_dims\n        seq_len = tf.shape(patches)[1]\n\n        class_token = tf.random.normal([batch_size, 1, patch_dims])\n        patches = tf.concat([class_token, patches],\n                           axis = 1) #batch_size, seq_len+1, patch_dims\n\n        #positional encoding, learnable\n        pos_info = tf.random.normal([1, seq_len+1, patch_dims])\n        encoded_patches = patches + pos_info\n        #encoded_patches = patches\n\n        # Optional) local attention : patch간 attention시 자기 자신에 관한 attention은 수행하지 않도록\n        attn_mask = 1-tf.eye(seq_len + 1)\n        attn_mask = tf.reshape(attn_mask, [1, seq_len + 1, seq_len + 1])\n        attn_mask = tf.tile(attn_mask, [batch_size, 1, 1])\n        attn_mask = tf.cast(attn_mask, tf.int32 )\n        if use_local_attention:\n            mask = attn_mask\n        else:\n            mask = None\n        #Transformer Encoder block\n        for idx in range(att_depth):\n\n            x0, attention_score = MultiHeadAttention(att_heads, att_dims)(encoded_patches, \n                                                         encoded_patches, \n                                                        attention_mask = mask,\n                                                         return_attention_scores = True\n                                                        )\n            x1 = x0 + encoded_patches\n            x2 = LayerNormalization()(x1)\n\n            x3 = Dense(units = att_dims)(x2)\n            x4 = LayerNormalization()(x3)\n            x5 = Activation(\"gelu\")(x4)\n            \n            x6 = x2 + x5\n            if idx == att_depth - 1:\n                layer_name = \"EncodedPatches\"\n            else:\n                layer_name = f\"Feature_Sequence_DepthIdx{idx}\"\n            encoded_patches = LayerNormalization(name = layer_name)(x6)\n\n        encoded_token = encoded_patches[:, 0, :]\n        encoded_token = tf.reshape(encoded_token, [batch_size, att_dims])\n        proj = get_proj(encoded_token)\n        feature_vector = Dense(units = K_dims, name = \"FinalLinear\", kernel_regularizer=tf.keras.regularizers.L1L2(l1=1e-4, l2=1e-4))(proj)\n\n        model = Model(inputs = inputs,\n                     outputs = [feature_vector, attention_score],\n                     name = model_name)\n        return model\n    \n    def initialize_weight_and_center(self):\n        # Initially, teacher weight == student weight\n        self.teacher.set_weights(self.student.get_weights())\n        self.C = 0.0    \n    def get_config(self):\n        return {\"TrainMethod\" : \"DINO\",\n                \"feature_extractor_name\" : self.teacher.name,\n                \"N_params\" : self.teacher.count_params(),\n               'patch_size' : self.patch_size, \n                \"K_dims\" : self.k,\n                \"softmax_temperature_student\" : self.tps,\n                \"softmax_temperature_teacher\" : self.tpt,\n               'teacher_weight_lambda': self.lamb,\n                \"centering_rate_parameter_m\" : self.m,\n                \"Global_view_index\" : self.highres_idx,\n                \"Local_view_index\" : self.lowres_idx\n               }\n    \n    \n    def compute_loss(self, dataset, training):\n        #dataset 중 high resolution images는 highres_idx, low resolution images는 lowres_idx에 있음\n        teacher_features = [] #<- for teacher net\n        student_features = [] #<- for student net\n        \n        # Stage 1. teacher and student features 생성\n        # teacher's feature set에는 global feature\n        # student's feature set에는 both global and local feature\n        \n        for idx in self.highres_idx:\n            if len(self.teacher.outputs) >= 2:\n                f = self.teacher(dataset[idx], training = training)[0]\n                f_s = self.student(dataset[idx], training = training)[0]\n                tf.stop_gradient(f)\n            else:\n                f = self.teacher(dataset[idx], training = training)\n                f_s = self.student(dataset[idx], training = training)\n                tf.stop_gradient(f)\n            teacher_features.append(f)\n            student_features.append(f_s)\n            \n        for idx in self.lowres_idx:\n            if len(self.student.outputs) >= 2:\n                f = self.student(dataset[idx], training = training)[0]\n            else:\n                f = self.student(dataset[idx], training = training)\n            student_features.append(f)\n            \n        # Stage 2. loss computation\n        # n_highres = 2라 가정할 때\n        # teacher's features set을 T1, T2 (<- 둘 다 global features)\n        # student's feature set을 S1, S2, ..., Sk (<- S1, S2만 global features)\n        # ce loss를 구하는데, T1과는 S2, S3, S4,..., Sk 간 loss를,\n        # T2와는 S1, S3, S4, ..., Sk 간 loss를 구한 뒤 평균\n        \n        # Global view(teacher)는 stop gradient 및 centering(C with self.m) + sharpening(self.tpt)\n        # student view는 sharpening만 (self.tps)\n        loss = []\n\n        for idx, teacher_f in enumerate(teacher_features):\n            tf.stop_gradient(teacher_f) #batch, K shape tensor\n            #teacher softmax with sharpening and centering\n            teacher_f = tf.nn.softmax((teacher_f - self.C)/self.tpt, axis = -1)\n            for j, student_f in enumerate(student_features):\n                if j == idx:\n                    # if, teacher_features의 첫 번째 feature vector에 대해 loss를 계산할 때,\n                    # student features의 첫 번째 feature vector는,\n                    # teacher features의 첫 번째 feature vector와 같은 view이므로 건너뜀(Loss 계산 X)\n                    continue\n                else:\n                    #print(f\"Troubleshooting, Teacher feature idx {idx} (out of {len(teacher_features)} features) and student feature idx {j} (out of {len(student_features)} features) loss comput.\")\n                    #student softmax with scaling(sharpening, self.tps)\n                    student_f = tf.nn.softmax(student_f/self.tps, axis = -1)\n                    # 각 pair별로 Loss 계산 후 reduce mean\n                    _loss = tf.reduce_mean(self.ce_loss(y_true = teacher_f, #<--이부분 hand-craft해보기\n                                                        y_pred = student_f)\n                                          )\n                    loss.append(_loss)\n\n        loss = tf.reduce_mean(loss)\n        \n        return loss, tf.concat(teacher_features, axis = 0)\n    \n    #2. training and validation loop\n    def train_step(self, dataset, testing_stage = False, training = True):\n        #A. Loss computation\n        if testing_stage:\n            print(\"Loss Computation\")\n        with tf.GradientTape() as tape:\n            loss, teacher_features = self.compute_loss(dataset, training = training)\n        \n        # B. Backpropa to student network\n        if testing_stage:\n            print(\"Backpropagation to student network\")\n        student_vars = self.student.trainable_variables\n        gradients = tape.gradient(loss, student_vars)\n        self.optimizer.apply_gradients(zip(gradients, student_vars))\n        \n        #C. EMA update the teacher variables\n        if testing_stage:\n            print(\"Teacher network update\")\n        \n        w_student_total = self.student.get_weights()\n        w_teacher_total = self.teacher.get_weights()\n        w_teacher_total_new = []\n        for w_t, w_s in zip(w_teacher_total, w_student_total):\n            w_teacher_new = self.lamb * w_t + (1-self.lamb) * w_s\n            w_teacher_total_new.append(w_teacher_new)\n        \n        self.teacher.set_weights(w_teacher_total_new)\n\n        # D. self.C의 EMA식 Update\n        if testing_stage:\n            print(\"Centering value update\")\n        \n        self.C = self.m*self.C + (1-self.m) * (tf.reduce_mean(teacher_features,\n                                                              axis = 0, keepdims = True))\n        if testing_stage:\n            print(\"Single Training step is over!\")\n        \n        global_entropy = self.ce_loss(y_true = teacher_features, y_pred = teacher_features)\n        global_entropy = tf.reduce_mean(global_entropy)\n        std = tf.math.reduce_std(teacher_features) \n        \n        self.loss_tracker.update_state(loss)\n        self.collapse_tracker.update_state(global_entropy)\n        self.std_tracker.update_state(std)\n        \n        return {\"loss\": self.loss_tracker.result(),\n               \"H_glob_glob\" : self.collapse_tracker.result(),\n               \"std\" : self.std_tracker.result()}\n\n    \n    def call(self, image, training = False): #각각 float tensor, string, string\n        teacher_output = self.teacher(image)\n        student_output = self.student(image)\n        return {\"teacher_output\" : teacher_output,\n               \"student_output\" : student_output}","metadata":{"execution":{"iopub.status.busy":"2023-02-02T06:16:19.130419Z","iopub.execute_input":"2023-02-02T06:16:19.130792Z","iopub.status.idle":"2023-02-02T06:16:19.173174Z","shell.execute_reply.started":"2023-02-02T06:16:19.130759Z","shell.execute_reply":"2023-02-02T06:16:19.172121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Barlow Twins (adjusted)\n- Cosine Similarity matrix에:\n    - diagonal하게는 1\n    - 반대는 0\n    - 과 MSE or huber loss 적용","metadata":{}},{"cell_type":"code","source":"class Barlow_module(keras.Model):\n    def __init__(self, \n                 patch_size = patch_size, att_depth = 3, att_dims = 256, att_heads = 8, prototype_c_dim = K_dims, #<-transformer 관련 hyperparameter\n                 conv_base = None, \n                 use_local_attention = False,\n                 t_teacher = t_teacher,\n                 highres_idx = highres_idx, lowres_idx = lowres_idx, **kwargs\n                ):\n        super().__init__(**kwargs)\n        if conv_base is None:\n        #feature extractors\n            self.patch_size = patch_size\n            self.att_depth = att_depth\n            self.att_dims = att_dims\n            self.att_heads = att_heads\n            self.K_dims = prototype_c_dim\n            self.use_local_attention = use_local_attention\n\n            self.teacher = self.get_vision_transformer()\n        else:\n            self.conv = conv_base\n            self.patch_size = 0\n            self.att_depth = 0\n            self.att_dims = 0\n            self.att_heads = 0\n            self.K_dims = prototype_c_dim\n            self.use_local_attention = use_local_attention\n            \n            self.teacher = self.build_conv_base()\n            \n        #hyperparameters\n        self.temp = t_teacher\n        self.highres_idx = highres_idx\n        self.lowres_idx = lowres_idx\n        #datasets\n        self.n_highres = len(highres_idx)\n        self.n_lowres = len(lowres_idx)\n        self.n_tot_view = self.n_highres + self.n_lowres\n        \n        self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n        self.collapse_tracker = tf.keras.metrics.Mean(name=\"Entropy\")\n        self.std_tracker = tf.keras.metrics.Mean(name=\"StDev\")\n        \n        self.reg_loss = tf.keras.losses.Huber(reduction=\"none\")\n        self.ce_loss = tf.keras.losses.CategoricalCrossentropy(reduction=\"none\")\n        self.collapse_tracker = tf.keras.metrics.Mean(name=\"Entropy\")\n        self.std_tracker = tf.keras.metrics.Mean(name=\"STDEV\")\n        \n    def build_conv_base(self):\n        conv = self.conv\n        K_dims = self.K_dims\n        try:\n            name = conv.name\n        except:\n            name = \"ConvolutionBaseModel\"\n        inputs = conv.inputs\n        conv_out = conv.output\n        dims = conv_out.shape[-1]\n        #dims = tf.shape(conv_out)[-1]\n        encoded_patches = keras.layers.Reshape([-1, dims], name = \"EncodedPatches\")(conv_out)\n        pool = tf.keras.layers.GlobalAveragePooling1D(name = \"PoolingFromEncodedPatches\")(encoded_patches)\n        proj = get_proj(pool)\n        feature_vector = Dense(units = K_dims, name = \"FinalLinear\", kernel_regularizer=tf.keras.regularizers.L1L2(l1=1e-4, l2=1e-4))(proj)\n\n        return Model(inputs, feature_vector,\n                        name = name)\n\n    def get_vision_transformer(self):\n        # Get Adjusted Vision Transformer base\n        \n        patch_size = self.patch_size\n        att_depth = self.att_depth\n        att_dims = self.att_dims\n        att_heads = self.att_heads \n        K_dims = self.K_dims\n        use_local_attention = self.use_local_attention\n        \n        model_name = f\"ViT{patch_size}_depth{att_depth}_with_{att_heads}heads\"\n        inputs = Input([None,None,3], name = \"ImageInput\")\n        scaled_inputs = inputs/255\n        patches = Conv2D(filters = att_dims, kernel_size = patch_size, strides = patch_size, padding = \"SAME\", name = f\"Patching_Convolution_PatchSize{patch_size}\")(scaled_inputs)\n        patches = LayerNormalization()(patches)\n        patches = Activation(\"gelu\", name = \"PatchAct\")(patches)\n\n        batch_size = tf.shape(inputs)[0]\n        patch_dims = patches.shape[-1]\n        patches = tf.reshape(patches, [batch_size, -1, patch_dims]) #batch_size, seq_len, patch_dims\n        seq_len = tf.shape(patches)[1]\n\n        class_token = tf.random.normal([batch_size, 1, patch_dims])\n        patches = tf.concat([class_token, patches],\n                           axis = 1) #batch_size, seq_len+1, patch_dims\n\n        #positional encoding, learnable\n        pos_info = tf.random.normal([1, seq_len+1, patch_dims])\n        encoded_patches = patches + pos_info\n        #encoded_patches = patches\n\n        # Optional) local attention : patch간 attention시 자기 자신에 관한 attention은 수행하지 않도록\n        attn_mask = 1-tf.eye(seq_len + 1)\n        attn_mask = tf.reshape(attn_mask, [1, seq_len + 1, seq_len + 1])\n        attn_mask = tf.tile(attn_mask, [batch_size, 1, 1])\n        attn_mask = tf.cast(attn_mask, tf.int32 )\n        if use_local_attention:\n            mask = attn_mask\n        else:\n            mask = None\n        #Transformer Encoder block\n        for idx in range(att_depth):\n\n            x0, attention_score = MultiHeadAttention(att_heads, att_dims)(encoded_patches, \n                                                         encoded_patches, \n                                                        attention_mask = mask,\n                                                         return_attention_scores = True\n                                                        )\n            x1 = x0 + encoded_patches\n            x2 = LayerNormalization()(x1)\n\n            x3 = Dense(units = att_dims)(x2)\n            x4 = LayerNormalization()(x3)\n            x5 = Activation(\"gelu\")(x4)\n            \n            x6 = x2 + x5\n            \n            if idx == att_depth - 1:\n                layer_name = \"EncodedPatches\"\n            else:\n                layer_name = f\"Feature_Sequence_DepthIdx{idx}\"\n            encoded_patches = LayerNormalization(name = layer_name)(x6)\n\n        encoded_token = encoded_patches[:, 0, :]\n        encoded_token = tf.reshape(encoded_token, [batch_size, att_dims])\n        proj = get_proj(encoded_token)\n        feature_vector = Dense(units = K_dims, name = \"FinalLinear\", kernel_regularizer=tf.keras.regularizers.L1L2(l1=1e-4, l2=1e-4))(proj)\n\n        model = Model(inputs = inputs,\n                     outputs = [feature_vector, attention_score],\n                     name = model_name)\n        return model\n    \n    \n    def get_config(self):\n        return {\"TrainMethod\" : \"Barlow\",\n                \"feature_extractor_name\" : self.teacher.name,\n                \"N_params\" : self.teacher.count_params(),\n                \"patch_size\" : self.patch_size,\n                \"att_depth\" : self.att_depth,\n                \"att_dims\" : self.att_dims,\n                \"att_heads\" : self.att_heads,\n                \"K_dims\" : self.K_dims,\n                \"use_local_attention\" : self.use_local_attention,\n                \"feature_extractor_name\" : self.teacher.name,\n                \"softmax_temperature\" : self.temp,\n                \"Global_view_index\" : self.highres_idx,\n                \"Local_view_index\" : self.lowres_idx\n               }\n    \n    def get_cos_sim_matrix(self, feature_a, feature_b):\n        # input = [batch, K_dims] shape feature vectors (feature a, feature b) 모두\n        \n        feature_a = tf.nn.softmax(feature_a/self.temp,\n                                      axis = -1)\n        feature_a = tf.math.l2_normalize(feature_a, axis = -1)\n        \n        feature_b = tf.nn.softmax(feature_b/self.temp,\n                                      axis = -1)\n        feature_b = tf.math.l2_normalize(feature_b, axis = -1)\n        \n        cos_matrix = tf.matmul(feature_a, feature_b, transpose_b = True)\n        \n        return cos_matrix\n    \n    def compute_loss(self, dataset, training = True):\n        \n        feature_output = []\n        \n        for idx in self.highres_idx:\n            if len(self.teacher.outputs) >= 2:\n                f = self.teacher(dataset[idx], training = training)[0]                \n            else:\n                f = self.teacher(dataset[idx], training = training)\n            feature_output.append(f)\n\n        for idx in self.lowres_idx:\n            if len(self.teacher.outputs) >= 2:\n                f = self.teacher(dataset[idx], training = training)[0]\n            else:\n                f = self.teacher(dataset[idx], training = training)\n            feature_output.append(f)\n        \n        loss = [] #initialize\n        for i in self.highres_idx:\n            view_i = feature_output[i]\n            for j, feature in enumerate(feature_output):\n                if i == j:\n                    continue\n                else:\n                    view_j = feature\n                    cos_matrix = self.get_cos_sim_matrix(view_i, view_j)\n                    _loss = self.reg_loss(y_true = tf.eye(tf.shape(cos_matrix)[0]),\n                                          y_pred = cos_matrix)\n                    _loss = tf.reduce_mean(_loss)\n                    loss.append(_loss)\n        loss = tf.reduce_mean(loss)\n        \n        return loss, tf.concat(feature_output, axis = 0)\n    \n    #2. training and validation loop\n    def train_step(self, dataset, testing_stage = False, training = True):\n        if testing_stage:\n            print(\"Loss Computation\")\n        with tf.GradientTape() as tape:\n            loss, features = self.compute_loss(dataset, training = training)\n        \n        if testing_stage:\n            print(\"Teacher Backpropagation\")\n        training_vars = self.teacher.trainable_variables\n        gradients = tape.gradient(loss, training_vars) #teacher update without momentum encoding\n        self.optimizer.apply_gradients(zip(gradients, training_vars))\n        \n        \n        global_entropy = self.ce_loss(y_true = features, y_pred = features)\n        global_entropy = tf.reduce_mean(global_entropy)\n        std = tf.math.reduce_std(features) \n        self.loss_tracker.update_state(loss)\n        self.collapse_tracker.update_state(global_entropy)\n        self.std_tracker.update_state(std)\n        if testing_stage:\n            print(\"Single training step over!\")\n            \n        return {\"loss\": self.loss_tracker.result(),\n               \"H_glob_glob\" : self.collapse_tracker.result(),\n               \"std\" : self.std_tracker.result()}\n    \n    def call(self, dataset, training = False): \n        return self.teacher(dataset, training = training)\n","metadata":{"execution":{"iopub.status.busy":"2023-02-02T06:16:19.174930Z","iopub.execute_input":"2023-02-02T06:16:19.175320Z","iopub.status.idle":"2023-02-02T06:16:19.214252Z","shell.execute_reply.started":"2023-02-02T06:16:19.175281Z","shell.execute_reply":"2023-02-02T06:16:19.213288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Optimizing Technique\n- AdamW optimizer with Warmup + Cosine decay learning rate\n- Learning rate schedule : code below, customized lr schedule!\n     - Lr setting은 original paper를 따름\n     - code reference) https://stackabuse.com/learning-rate-warmup-with-cosine-decay-in-keras-and-tensorflow/","metadata":{}},{"cell_type":"code","source":"target_lr = 0.0001\nsteps_per_epoch = len(df_train)//batch_size\ntotal_steps = 50000\nwarmup_steps = 1000 # due to restricted resources, warmsteps is lower than original paper\n\nfrom keras import backend as K\ndef lr_warmup_cosine_decay(global_step,\n                           warmup_steps,\n                           hold = 0,\n                           total_steps=0,\n                           start_lr=0.0,\n                           target_lr=1e-3):\n    # Cosine decay\n    # There is no tf.pi so we wrap np.pi as a TF constant\n    learning_rate = 0.5 * target_lr * (1 + tf.cos(tf.constant(np.pi) * (global_step - warmup_steps - hold) / float(total_steps - warmup_steps - hold)))\n\n    # Target LR * progress of warmup (=1 at the final warmup step)\n    warmup_lr = target_lr * (global_step / warmup_steps)\n\n    # Choose between `warmup_lr`, `target_lr` and `learning_rate` based on whether `global_step < warmup_steps` and we're still holding.\n    # i.e. warm up if we're still warming up and use cosine decayed lr otherwise\n    if hold > 0:\n        learning_rate = tf.where(global_step > warmup_steps + hold,\n                                 learning_rate, target_lr)\n    \n    learning_rate = tf.where(global_step < warmup_steps, warmup_lr, learning_rate)\n    return learning_rate\n\n# Visualization\nsteps = np.arange(0, total_steps//100, 1)\nlrs = []\nfor step in steps:\n    lrs.append(lr_warmup_cosine_decay(step, \n                                      total_steps=len(steps), \n                                      warmup_steps=warmup_steps//100, \n                                      hold=0, target_lr = target_lr))\nplt.figure(figsize = (10, 7))\nplt.plot(lrs)\nplt.title(\"Naive Learning Rate Schedule visualization\")\nplt.grid()\nplt.show()\n\nclass WarmUpCosineDecay(keras.optimizers.schedules.LearningRateSchedule):\n    def __init__(self, start_lr, target_lr, warmup_steps, total_steps, hold):\n        super().__init__()\n        self.start_lr = start_lr\n        self.target_lr = target_lr\n        self.warmup_steps = warmup_steps\n        self.total_steps = total_steps\n        self.hold = hold\n\n    def __call__(self, step):\n        lr = lr_warmup_cosine_decay(global_step=step,\n                                    total_steps=self.total_steps,\n                                    warmup_steps=self.warmup_steps,\n                                    start_lr=self.start_lr,\n                                    target_lr=self.target_lr,\n                                    hold=self.hold)\n\n        return tf.where(\n            step > self.total_steps, 1e-7, lr, name=\"learning_rate\"\n        )","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-02-02T06:16:19.216153Z","iopub.execute_input":"2023-02-02T06:16:19.216507Z","iopub.status.idle":"2023-02-02T06:16:19.924511Z","shell.execute_reply.started":"2023-02-02T06:16:19.216470Z","shell.execute_reply":"2023-02-02T06:16:19.923506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"adamw = tfa.optimizers.AdamW(weight_decay = 0.08, #<-original paper와는 다르게 그냥 Constant로..\n                                learning_rate = WarmUpCosineDecay(start_lr = 2e-5,\n                                                                 target_lr = target_lr,\n                                                                  warmup_steps = 500,\n                                                                  total_steps = total_steps,\n                                                                  hold = 0\n                                                                 )\n                                )\nadam = tf.keras.optimizers.Adam(learning_rate = WarmUpCosineDecay(start_lr = 2e-5,\n                                                                 target_lr = target_lr,\n                                                                  warmup_steps = 500,\n                                                                  total_steps = total_steps,\n                                                                  hold = 0\n                                                                 ))","metadata":{"execution":{"iopub.status.busy":"2023-02-02T06:16:19.927550Z","iopub.execute_input":"2023-02-02T06:16:19.927859Z","iopub.status.idle":"2023-02-02T06:16:19.936794Z","shell.execute_reply.started":"2023-02-02T06:16:19.927832Z","shell.execute_reply":"2023-02-02T06:16:19.935920Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Training!\n- with training loop customizing.\n- keras의 fit 메소드를 사용하면 중간에 에러가 자주 난다","metadata":{}},{"cell_type":"code","source":"def normalize_and_int(img_tensor):\n    img_tensor = np.array(img_tensor)\n    if np.max(img_tensor) > np.min(img_tensor):\n        img_tensor = (img_tensor - np.min(img_tensor)) / (np.max(img_tensor) - np.min(img_tensor))\n    else:\n        img_tensor = img_tensor\n    img_tensor = img_tensor * 255\n    img_tensor = img_tensor.astype(np.uint8)\n    return img_tensor\n\ndef visualize_model(feature_extractor, val_ds, title = \"dummy\", log = False, n_heads = 5, batch_n = 0):\n    output = []\n    for imgs in test_ds.take(1):\n        input_image = imgs[:1]\n    input_image = tf.image.resize(input_image, \n                                  [res,res]\n                                 )\n    if len(feature_extractor.outputs) == 2:\n        example_vector, score = feature_extractor(input_image) \n        for head_n in range(n_heads):\n            #input score : batch, heads, (seqlen+1), (seqlen+1) shape tensor\n            example_weight = score[batch_n, head_n, ...]\n            parsed_weight = example_weight[0, 1:] #class token의, (class token을 제외한) 다른 patches(tokens) 간의 attention weight\n            print(f\"Mean of Attention weight between each patches ifself: {tf.reduce_mean(tf.linalg.tensor_diag_part(example_weight))}\")\n            \n            vis_res = tf.math.sqrt(tf.cast(tf.shape(parsed_weight)[-1], tf.float32))\n            parsed_weight = tf.cast(tf.reshape(parsed_weight, [vis_res, vis_res]), tf.float32)\n            plt.figure(figsize = (6,6))\n            plt.imshow(parsed_weight)\n            plt.colorbar()\n            plt.title(f\"Attention Weight, img idx{batch_n}, att head idx{head_n}\")\n            plt.show()\n            print(f\"Mean of Attention weight, pixelwise: {tf.reduce_mean(parsed_weight)}, shape : {tf.shape(parsed_weight)}, max/min/std : {tf.reduce_max(parsed_weight)}, {tf.reduce_min(parsed_weight)}, {tf.math.reduce_std(parsed_weight)}\")\n            output.append(parsed_weight)\n    else:\n        inputs_ = feature_extractor.input\n        feature_map_ = feature_extractor.get_layer(\"EncodedPatches\").input\n        model_ = Model(inputs_, feature_map_)\n        feature_map = model_(input_image)\n        _, w, h, dims = feature_map.shape\n        var = tf.math.reduce_std(feature_map, axis = (0, 1, 2))\n        high_var_indices = tf.math.top_k(var, k = n_heads).indices.numpy()\n        feature_map = feature_map[0]\n        for idx in high_var_indices:\n            output.append(feature_map[:,:,idx])\n            plt.figure(figsize = (6,6))\n            plt.imshow(feature_map[:,:,idx])\n            plt.colorbar()\n            plt.title(f\"Feature map, img idx{batch_n}, high std idx{idx}\")\n            plt.show()\n    if log : \n        data = [[wandb.Image(input_image)] + [wandb.Image(normalize_and_int(tensor)) for tensor in output]]\n        tbl = wandb.Table(data = data,\n                          columns = [\"Original img\"] + [f\"Vis_map{idx}\" for idx in range(n_heads)])\n        wandb.log({f\"{title}_table\" : tbl})\n    else:\n        pass\n    return output","metadata":{"execution":{"iopub.status.busy":"2023-02-02T06:16:19.940158Z","iopub.execute_input":"2023-02-02T06:16:19.940427Z","iopub.status.idle":"2023-02-02T06:16:19.956939Z","shell.execute_reply.started":"2023-02-02T06:16:19.940402Z","shell.execute_reply":"2023-02-02T06:16:19.956093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def custom_train_loop(model, train_ds, val_ds, epochs = epochs,\n                      exp_name = None, configs = None, show_head_n = 3,\n                      log = False, troubleshoot = False):\n    if configs is None:\n        teacher_name = \"teacher\"\n        student_name = \"student\"\n    else:\n        ext_name = configs[\"feature_extractor_name\"]\n        teacher_name = f\"teacher_{ext_name}\"\n        student_name = f\"student_{ext_name}\"\n        \n    if troubleshoot:\n        epochs = 2\n        show_interval = 3\n        train_steps_per_epoch = 6\n        exp_name = exp_name + \"_test_delete\"\n    else:\n        show_interval = 1000\n        train_steps_per_epoch = 50000\n        \n    if log:\n        run = wandb.init(project=\"[Train]ScaleUp_SSL_ScreeningRadiology_AdamCosAneal\", \n                         entity=\"gongbungkim\",\n              name = exp_name, config = configs)\n    else:\n        pass\n    #============================================#\n    \n    for epoch in tqdm(range(epochs)):\n        #============================================#\n        for step, data in tqdm(enumerate(train_ds), total = train_steps_per_epoch):\n            if step > train_steps_per_epoch:\n                break\n            hist = model.train_step(data, \n                                    testing_stage = troubleshoot, \n                                    training = True)\n            loss = float(hist[\"loss\"])\n            h = float(hist[\"H_glob_glob\"])\n            std = float(hist[\"std\"])\n            \n            \n            if (step + 1) % show_interval == 0 or step == 0:\n                print(f\"{step + 1} step, loss : {loss}, entropy of global view : {h}, std. dev of global view vector : {std}\")\n                \n                visualize_model(model.teacher, val_ds = val_ds, title = f\"{step+1}step_visualize\", \n                                    log = log, n_heads = show_head_n, \n                                    batch_n = 0)  \n                print(\"Save the feature extractor\")\n                model.teacher.save(teacher_name)\n                if log:\n                    trained_teacher_artifact = wandb.Artifact(f\"{exp_name}_teacher\", type = \"model\",\n                                                 description = f\"SSL-trained feature extractor (teacher) at the end of step {step+1}, epoch {epoch}\")\n                    wd = \"/kaggle/working/\"\n                    teacher_dir = os.path.join(wd, teacher_name)\n                    trained_teacher_artifact.add_dir(teacher_dir)\n                    run.log_artifact(trained_teacher_artifact)\n                \n            if log:\n                wandb.log({\"step_loss\": loss, \"H(global_view)\" : h, \"Stdev(global_view)\" : std,\n                          \"step\" : step})\n                \n                \n        if log:\n            wandb.log({\"epoch_loss\": loss, \"epoch_H(global_view)\" : h, \"epoch_Stdev(global_view)\" : std,\n                     \"epoch\" : epoch})\n                        \n        print(f\"Epoch {epoch} Training loss : {loss}, Global View Entropy : {h}, std. dev of global view vector : {std}\")\n        \n        #----------------------------------------------#\n        \n        print(\"Save the feature extractor\")\n        model.teacher.save(teacher_name)\n        if log:\n            trained_teacher_artifact = wandb.Artifact(f\"{exp_name}_teacher\", type = \"model\",\n                                                 description = f\"SSL-trained feature extractor (teacher) at the end of epoch {epoch}\")\n            wd = \"/kaggle/working/\"\n            teacher_dir = os.path.join(wd, teacher_name)\n            trained_teacher_artifact.add_dir(teacher_dir)\n            run.log_artifact(trained_teacher_artifact)\n            \n    return model","metadata":{"execution":{"iopub.status.busy":"2023-02-02T06:16:19.960391Z","iopub.execute_input":"2023-02-02T06:16:19.960752Z","iopub.status.idle":"2023-02-02T06:16:19.976921Z","shell.execute_reply.started":"2023-02-02T06:16:19.960717Z","shell.execute_reply":"2023-02-02T06:16:19.976006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"class TrainingModule:\n    def __init__(self, train_type = \"swav\",\n                 patch_size = patch_size, att_depth = 3, att_dims = 512, att_heads = 8, use_local_attention = False,\n                conv_base = None):\n        if train_type == \"swav\":\n            self.ssl_module = SwAV_module(conv_base = conv_base,\n                                     patch_size = patch_size, att_depth = att_depth, att_dims = att_dims, att_heads = att_heads, \n                                     use_local_attention = use_local_attention)\n        elif train_type == \"dino\":\n            self.ssl_module = DINO_module(conv_base = conv_base,\n                                     patch_size = patch_size, att_depth = att_depth, att_dims = att_dims, att_heads = att_heads, \n                                     use_local_attention = use_local_attention)\n        elif train_type == \"barlow\":\n            self.ssl_module = Barlow_module(conv_base = conv_base,\n                                     patch_size = patch_size, att_depth = att_depth, att_dims = att_dims, att_heads = att_heads, \n                                     use_local_attention = use_local_attention)\n        self.ssl_module.compile(adam)\n        self.configs = self.ssl_module.get_config()\n        self.exp_name = train_type + \"_\" + self.configs[\"feature_extractor_name\"]\n        print(\"SSL module Configuration : \", self.configs, \"\\n\", \"--------------\", \"\\n\", \"SSL experiment name : \", self.exp_name, \"\\n\")\n        \n    def init_train(self, log, troubleshoot, show_head_n = 6,\n                  ):\n        model = custom_train_loop(self.ssl_module, \n                          train_ds = train_ds, val_ds = val_ds, \n                          exp_name = self.exp_name, configs = self.configs, show_head_n = min(show_head_n, 20),\n                          log = log, troubleshoot = troubleshoot)\n        return model","metadata":{"execution":{"iopub.status.busy":"2023-02-02T06:16:19.980023Z","iopub.execute_input":"2023-02-02T06:16:19.980295Z","iopub.status.idle":"2023-02-02T06:16:19.990594Z","shell.execute_reply.started":"2023-02-02T06:16:19.980270Z","shell.execute_reply":"2023-02-02T06:16:19.989448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Deep Transformer training","metadata":{}},{"cell_type":"code","source":"DeepTF_swav = TrainingModule(train_type = \"swav\",\n                     patch_size = patch_size, att_depth = 4, att_dims = 768, att_heads = 6, use_local_attention = False,\n                    conv_base = None)\nDeepTF_barlow = TrainingModule(train_type = \"barlow\",\n                     patch_size = patch_size, att_depth = 4, att_dims = 768, att_heads = 6, use_local_attention = False,\n                    conv_base = None)\nDeepTF_dino = TrainingModule(train_type = \"dino\",\n                     patch_size = patch_size, att_depth = 4, att_dims = 768, att_heads = 6, use_local_attention = False,\n                    conv_base = None)\n\n#DeepTF_swav.init_train(log = True, troubleshoot = False, show_head_n = 6)\nDeepTF_barlow.init_train(log = True, troubleshoot = True, show_head_n = 6)\n#DeepTF_dino.init_train(log = True, troubleshoot = False, show_head_n = 8)","metadata":{"execution":{"iopub.status.busy":"2023-02-02T06:16:19.991757Z","iopub.execute_input":"2023-02-02T06:16:19.992290Z","iopub.status.idle":"2023-02-02T06:17:36.780281Z","shell.execute_reply.started":"2023-02-02T06:16:19.992255Z","shell.execute_reply":"2023-02-02T06:17:36.773631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Wide Transformer training","metadata":{}},{"cell_type":"code","source":"WideTF_swav = TrainingModule(train_type = \"swav\",\n                 patch_size = patch_size, att_depth = 1, att_dims = 256, att_heads = 24, use_local_attention = False,\n                conv_base = None)\nWideTF_barlow = TrainingModule(train_type = \"barlow\",\n                 patch_size = patch_size, att_depth = 1, att_dims = 256, att_heads = 24, use_local_attention = False,\n                conv_base = None)\nWideTF_dino = TrainingModule(train_type = \"dino\",\n                 patch_size = patch_size, att_depth = 2, att_dims = 256, att_heads = 12, use_local_attention = False,\n                conv_base = None)\n\n#WideTF_swav.init_train(log = True, troubleshoot = False, show_head_n = 12)\n#WideTF_barlow.init_train(log = True, troubleshoot = False, show_head_n = 12)\n#WideTF_dino.init_train(log = True, troubleshoot = False, show_head_n = 12)","metadata":{"execution":{"iopub.status.busy":"2023-02-02T06:17:36.781544Z","iopub.status.idle":"2023-02-02T06:17:36.782737Z","shell.execute_reply.started":"2023-02-02T06:17:36.782461Z","shell.execute_reply":"2023-02-02T06:17:36.782485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> EfficientNet lineage training","metadata":{}},{"cell_type":"code","source":"eff0_swav = TrainingModule(train_type = \"swav\",\n                 patch_size = patch_size, att_depth = 1, att_dims = 256, att_heads = 24, use_local_attention = False,\n                conv_base = eff0)\neff0_barlow = TrainingModule(train_type = \"barlow\",\n                 patch_size = patch_size, att_depth = 1, att_dims = 256, att_heads = 24, use_local_attention = False,\n                conv_base = eff0)\neff0_dino = TrainingModule(train_type = \"dino\",\n                 patch_size = patch_size, att_depth = 1, att_dims = 256, att_heads = 24, use_local_attention = False,\n                conv_base = eff0)\n\n\n#eff0_swav.init_train(log = True, troubleshoot = False,show_head_n = 6)\n#eff0_barlow.init_train(log = True, troubleshoot = False,show_head_n = 6)\n#eff0_dino.init_train(log = True, troubleshoot = False,show_head_n = 6)","metadata":{"execution":{"iopub.status.busy":"2023-02-02T06:17:36.784295Z","iopub.status.idle":"2023-02-02T06:17:36.785112Z","shell.execute_reply.started":"2023-02-02T06:17:36.784843Z","shell.execute_reply":"2023-02-02T06:17:36.784868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eff1_swav = TrainingModule(train_type = \"swav\",\n                 patch_size = patch_size, att_depth = 1, att_dims = 256, att_heads = 24, use_local_attention = False,\n                conv_base = eff1)\neff1_barlow = TrainingModule(train_type = \"barlow\",\n                 patch_size = patch_size, att_depth = 1, att_dims = 256, att_heads = 24, use_local_attention = False,\n                conv_base = eff1)\neff1_dino = TrainingModule(train_type = \"dino\",\n                 patch_size = patch_size, att_depth = 1, att_dims = 256, att_heads = 24, use_local_attention = False,\n                conv_base = eff1)\n\n\n#eff1_swav.init_train(log = True, troubleshoot = False,show_head_n = 6)\n#eff1_barlow.init_train(log = True, troubleshoot = False,show_head_n = 6)\n#eff1_dino.init_train(log = True, troubleshoot = False,show_head_n = 6)","metadata":{"execution":{"iopub.status.busy":"2023-02-02T06:17:36.786599Z","iopub.status.idle":"2023-02-02T06:17:36.787362Z","shell.execute_reply.started":"2023-02-02T06:17:36.787100Z","shell.execute_reply":"2023-02-02T06:17:36.787136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eff2_swav = TrainingModule(train_type = \"swav\",\n                 patch_size = patch_size, att_depth = 1, att_dims = 256, att_heads = 24, use_local_attention = False,\n                conv_base = eff2)\neff2_barlow = TrainingModule(train_type = \"barlow\",\n                 patch_size = patch_size, att_depth = 1, att_dims = 256, att_heads = 24, use_local_attention = False,\n                conv_base = eff2)\neff2_dino = TrainingModule(train_type = \"dino\",\n                 patch_size = patch_size, att_depth = 1, att_dims = 256, att_heads = 24, use_local_attention = False,\n                conv_base = eff2)\n\n\n#eff2_swav.init_train(log = True, troubleshoot = False,show_head_n = 6)\n#eff2_barlow.init_train(log = True, troubleshoot = False,show_head_n = 6)\n#eff2_dino.init_train(log = True, troubleshoot = False,show_head_n = 6)","metadata":{"execution":{"iopub.status.busy":"2023-02-02T06:17:36.788765Z","iopub.status.idle":"2023-02-02T06:17:36.789529Z","shell.execute_reply.started":"2023-02-02T06:17:36.789280Z","shell.execute_reply":"2023-02-02T06:17:36.789305Z"},"trusted":true},"execution_count":null,"outputs":[]}]}