{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad254021",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-07-19T00:47:13.235828Z",
     "iopub.status.busy": "2024-07-19T00:47:13.235321Z",
     "iopub.status.idle": "2024-07-19T00:47:43.346793Z",
     "shell.execute_reply": "2024-07-19T00:47:43.345224Z"
    },
    "papermill": {
     "duration": 30.126812,
     "end_time": "2024-07-19T00:47:43.353894",
     "exception": false,
     "start_time": "2024-07-19T00:47:13.227082",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-19 00:47:16.978637: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-19 00:47:16.978798: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-19 00:47:17.169297: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirements loaded, keras : v3.4.1, Tensorflow : v2.15.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "seed = 1\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ML tools \n",
    "\n",
    "import tensorflow as tf\n",
    "import keras #; keras.config.set_dtype_policy(\"mixed_float16\")\n",
    "import keras_nlp\n",
    "import keras_cv\n",
    "from keras import ops\n",
    "\n",
    "keras.utils.set_random_seed(seed)\n",
    "\n",
    "import cv2\n",
    "import tensorflow_io as tfio\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from keras import Input, Model, layers\n",
    "from keras.models import load_model\n",
    "from keras.layers import Layer\n",
    "from keras.layers import Conv2D, DepthwiseConv2D, Dense, Activation, BatchNormalization, LayerNormalization, MultiHeadAttention, Embedding, Subtract, Add, Multiply, GlobalAveragePooling2D, GlobalAveragePooling1D, LayerNormalization\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.applications import *\n",
    "import os, sys\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "from tqdm.notebook import tqdm\n",
    "import wandb\n",
    "print(f\"Requirements loaded, keras : v{keras.__version__}, Tensorflow : v{tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a1099e",
   "metadata": {
    "papermill": {
     "duration": 0.00578,
     "end_time": "2024-07-19T00:47:43.365841",
     "exception": false,
     "start_time": "2024-07-19T00:47:43.360061",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Setting hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8833c226",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-07-19T00:47:43.380334Z",
     "iopub.status.busy": "2024-07-19T00:47:43.379822Z",
     "iopub.status.idle": "2024-07-19T00:47:43.387447Z",
     "shell.execute_reply": "2024-07-19T00:47:43.386170Z"
    },
    "papermill": {
     "duration": 0.018395,
     "end_time": "2024-07-19T00:47:43.390413",
     "exception": false,
     "start_time": "2024-07-19T00:47:43.372018",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hyperparameters about images\n",
    "res = 384\n",
    "batch_size = 16\n",
    "# hyperparameters about text\n",
    "seq_len = 64\n",
    "# hyperparameters about cross attentive decoder\n",
    "att_depth = 1\n",
    "att_heads = 16\n",
    "att_dims = att_heads * 32\n",
    "use_bias = False\n",
    "\n",
    "train_cases = 70108\n",
    "val_cases = 9972\n",
    "train_steps = train_cases//batch_size\n",
    "val_steps = val_cases//batch_size\n",
    "\n",
    "train_dataset_dir = '/kaggle/input/roco-v2-tfrecord-dataset/Tot70108cases_RoCoV2_radiology_train_GZIP.tfrecord'\n",
    "val_dataset_dir = '/kaggle/input/roco-v2-tfrecord-dataset/Tot9972cases_RoCoV2_radiology_test_GZIP.tfrecord'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0e689d",
   "metadata": {
    "papermill": {
     "duration": 0.00576,
     "end_time": "2024-07-19T00:47:43.402415",
     "exception": false,
     "start_time": "2024-07-19T00:47:43.396655",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Parsing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b11a1c5c",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-07-19T00:47:43.416382Z",
     "iopub.status.busy": "2024-07-19T00:47:43.415948Z",
     "iopub.status.idle": "2024-07-19T00:47:58.659965Z",
     "shell.execute_reply": "2024-07-19T00:47:58.658556Z"
    },
    "papermill": {
     "duration": 15.25482,
     "end_time": "2024-07-19T00:47:58.663281",
     "exception": false,
     "start_time": "2024-07-19T00:47:43.408461",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _parse_tfrecord(c, res = res):\n",
    "    def parse_tfrecord(tfrecord):\n",
    "        features = {'image': tf.io.FixedLenFeature([], tf.string),\n",
    "                    'report': tf.io.FixedLenFeature([], tf.string),\n",
    "                    }\n",
    "        x = tf.io.parse_single_example(tfrecord, features)\n",
    "        image_train = tf.image.decode_jpeg(x['image'], channels=c)\n",
    "        image_train = _transform_images(res = res)(image_train)\n",
    "        report = tf.cast(x[\"report\"], tf.string)\n",
    "        return image_train, report\n",
    "    \n",
    "    return parse_tfrecord\n",
    "\n",
    "\n",
    "def _transform_images(res = res):\n",
    "    def transform_images(x_train):\n",
    "        x_train = tf.image.resize_with_pad(x_train, res, res, antialias = True)\n",
    "        x_train = tf.cast(x_train, tf.uint8)\n",
    "        return x_train\n",
    "    return transform_images\n",
    "\n",
    "def load_tfrecord_dataset(tfrecord_name, res = res, batch_size = batch_size, shuffle=True, buffer_size=10240, grayscale = False):\n",
    "    \"\"\"load dataset from tfrecord\"\"\"\n",
    "    c = 1 if grayscale else 3\n",
    "    raw_dataset = tf.data.TFRecordDataset(tfrecord_name, compression_type = \"GZIP\")\n",
    "    raw_dataset = raw_dataset.repeat()\n",
    "    if shuffle:\n",
    "        raw_dataset = raw_dataset.shuffle(buffer_size=buffer_size)\n",
    "    dataset = raw_dataset.map(\n",
    "        _parse_tfrecord(c = c, res = res),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "train_ds = load_tfrecord_dataset(train_dataset_dir)\n",
    "val_ds = load_tfrecord_dataset(val_dataset_dir) #image, report 2 outputs\n",
    "\n",
    "for a, b in val_ds.take(1):\n",
    "    val_images = a\n",
    "    val_texts = b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ad1dae",
   "metadata": {
    "papermill": {
     "duration": 0.006215,
     "end_time": "2024-07-19T00:47:58.676337",
     "exception": false,
     "start_time": "2024-07-19T00:47:58.670122",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Preparing Natural language decoder (pretrained on plain texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81f65bd3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-19T00:47:58.690784Z",
     "iopub.status.busy": "2024-07-19T00:47:58.690299Z",
     "iopub.status.idle": "2024-07-19T00:48:26.559176Z",
     "shell.execute_reply": "2024-07-19T00:48:26.557497Z"
    },
    "papermill": {
     "duration": 27.880374,
     "end_time": "2024-07-19T00:48:26.562889",
     "exception": false,
     "start_time": "2024-07-19T00:47:58.682515",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attaching 'model.safetensors' from model 'keras/gpt2/keras/gpt2_base_en/2' to your Kaggle notebook...\n",
      "Attaching 'model.safetensors.index.json' from model 'keras/gpt2/keras/gpt2_base_en/2' to your Kaggle notebook...\n",
      "Attaching 'metadata.json' from model 'keras/gpt2/keras/gpt2_base_en/2' to your Kaggle notebook...\n",
      "Attaching 'metadata.json' from model 'keras/gpt2/keras/gpt2_base_en/2' to your Kaggle notebook...\n",
      "Attaching 'config.json' from model 'keras/gpt2/keras/gpt2_base_en/2' to your Kaggle notebook...\n",
      "Attaching 'config.json' from model 'keras/gpt2/keras/gpt2_base_en/2' to your Kaggle notebook...\n",
      "Attaching 'model.weights.h5' from model 'keras/gpt2/keras/gpt2_base_en/2' to your Kaggle notebook...\n",
      "Attaching 'model.safetensors' from model 'keras/gpt2/keras/gpt2_base_en/2' to your Kaggle notebook...\n",
      "Attaching 'model.safetensors.index.json' from model 'keras/gpt2/keras/gpt2_base_en/2' to your Kaggle notebook...\n",
      "Attaching 'metadata.json' from model 'keras/gpt2/keras/gpt2_base_en/2' to your Kaggle notebook...\n",
      "Attaching 'metadata.json' from model 'keras/gpt2/keras/gpt2_base_en/2' to your Kaggle notebook...\n",
      "Attaching 'preprocessor.json' from model 'keras/gpt2/keras/gpt2_base_en/2' to your Kaggle notebook...\n",
      "Attaching 'tokenizer.json' from model 'keras/gpt2/keras/gpt2_base_en/2' to your Kaggle notebook...\n",
      "Attaching 'tokenizer.json' from model 'keras/gpt2/keras/gpt2_base_en/2' to your Kaggle notebook...\n",
      "Attaching 'assets/tokenizer/vocabulary.json' from model 'keras/gpt2/keras/gpt2_base_en/2' to your Kaggle notebook...\n",
      "Attaching 'assets/tokenizer/merges.txt' from model 'keras/gpt2/keras/gpt2_base_en/2' to your Kaggle notebook...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gpt2_backbone\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gpt2_backbone\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ token_ids           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ token_embedding     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │ <span style=\"color: #00af00; text-decoration-color: #00af00\">38,597,376</span> │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbeddi…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ position_embedding  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">786,432</span> │ token_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionEmbedding</span>) │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embeddings_add      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ token_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │                   │            │ position_embeddi… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embeddings_dropout  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embeddings_add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ padding_mask        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_layer_0 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">7,087,872</span> │ embeddings_dropo… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecode…</span> │                   │            │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_layer_1 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">7,087,872</span> │ transformer_laye… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecode…</span> │                   │            │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_layer_2 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">7,087,872</span> │ transformer_laye… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecode…</span> │                   │            │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_layer_3 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">7,087,872</span> │ transformer_laye… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecode…</span> │                   │            │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_layer_4 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">7,087,872</span> │ transformer_laye… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecode…</span> │                   │            │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_layer_5 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">7,087,872</span> │ transformer_laye… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecode…</span> │                   │            │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_layer_6 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">7,087,872</span> │ transformer_laye… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecode…</span> │                   │            │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_layer_7 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">7,087,872</span> │ transformer_laye… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecode…</span> │                   │            │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_layer_8 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">7,087,872</span> │ transformer_laye… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecode…</span> │                   │            │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_layer_9 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">7,087,872</span> │ transformer_laye… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecode…</span> │                   │            │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_layer_… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">7,087,872</span> │ transformer_laye… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecode…</span> │                   │            │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_layer_… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">7,087,872</span> │ transformer_laye… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecode…</span> │                   │            │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_norm          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ transformer_laye… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ token_ids           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ token_embedding     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │ \u001b[38;5;34m38,597,376\u001b[0m │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mReversibleEmbeddi…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ position_embedding  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │    \u001b[38;5;34m786,432\u001b[0m │ token_embedding[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mPositionEmbedding\u001b[0m) │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embeddings_add      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ token_embedding[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mAdd\u001b[0m)               │                   │            │ position_embeddi… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embeddings_dropout  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ embeddings_add[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ padding_mask        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_layer_0 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │  \u001b[38;5;34m7,087,872\u001b[0m │ embeddings_dropo… │\n",
       "│ (\u001b[38;5;33mTransformerDecode…\u001b[0m │                   │            │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_layer_1 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │  \u001b[38;5;34m7,087,872\u001b[0m │ transformer_laye… │\n",
       "│ (\u001b[38;5;33mTransformerDecode…\u001b[0m │                   │            │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_layer_2 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │  \u001b[38;5;34m7,087,872\u001b[0m │ transformer_laye… │\n",
       "│ (\u001b[38;5;33mTransformerDecode…\u001b[0m │                   │            │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_layer_3 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │  \u001b[38;5;34m7,087,872\u001b[0m │ transformer_laye… │\n",
       "│ (\u001b[38;5;33mTransformerDecode…\u001b[0m │                   │            │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_layer_4 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │  \u001b[38;5;34m7,087,872\u001b[0m │ transformer_laye… │\n",
       "│ (\u001b[38;5;33mTransformerDecode…\u001b[0m │                   │            │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_layer_5 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │  \u001b[38;5;34m7,087,872\u001b[0m │ transformer_laye… │\n",
       "│ (\u001b[38;5;33mTransformerDecode…\u001b[0m │                   │            │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_layer_6 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │  \u001b[38;5;34m7,087,872\u001b[0m │ transformer_laye… │\n",
       "│ (\u001b[38;5;33mTransformerDecode…\u001b[0m │                   │            │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_layer_7 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │  \u001b[38;5;34m7,087,872\u001b[0m │ transformer_laye… │\n",
       "│ (\u001b[38;5;33mTransformerDecode…\u001b[0m │                   │            │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_layer_8 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │  \u001b[38;5;34m7,087,872\u001b[0m │ transformer_laye… │\n",
       "│ (\u001b[38;5;33mTransformerDecode…\u001b[0m │                   │            │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_layer_9 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │  \u001b[38;5;34m7,087,872\u001b[0m │ transformer_laye… │\n",
       "│ (\u001b[38;5;33mTransformerDecode…\u001b[0m │                   │            │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_layer_… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │  \u001b[38;5;34m7,087,872\u001b[0m │ transformer_laye… │\n",
       "│ (\u001b[38;5;33mTransformerDecode…\u001b[0m │                   │            │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_layer_… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │  \u001b[38;5;34m7,087,872\u001b[0m │ transformer_laye… │\n",
       "│ (\u001b[38;5;33mTransformerDecode…\u001b[0m │                   │            │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_norm          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │      \u001b[38;5;34m1,536\u001b[0m │ transformer_laye… │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">124,439,808</span> (474.70 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m124,439,808\u001b[0m (474.70 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">124,439,808</span> (474.70 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m124,439,808\u001b[0m (474.70 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gpt_freeset = \"gpt2_base_en\"\n",
    "text_only_decoder = keras_nlp.models.Backbone.from_preset(\n",
    "    gpt_freeset,\n",
    "    trainable=False,\n",
    ")\n",
    "preprocessor = keras_nlp.models.GPT2Preprocessor.from_preset(\n",
    "    gpt_freeset,\n",
    "    sequence_length=seq_len,\n",
    "    add_start_token = False\n",
    ")\n",
    "preprocessor.trainable = False\n",
    "text_only_decoder.trainable = False\n",
    "text_only_decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20d4522c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-19T00:48:26.582764Z",
     "iopub.status.busy": "2024-07-19T00:48:26.582295Z",
     "iopub.status.idle": "2024-07-19T00:48:28.934422Z",
     "shell.execute_reply": "2024-07-19T00:48:28.932785Z"
    },
    "papermill": {
     "duration": 2.365876,
     "end_time": "2024-07-19T00:48:28.937760",
     "exception": false,
     "start_time": "2024-07-19T00:48:26.571884",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#according to summarized GPT2 model structure:\n",
    "text_embed_dims = 768\n",
    "n_vocab = preprocessor.tokenizer.vocabulary_size()\n",
    "pad_token = preprocessor.tokenizer.pad_token_id\n",
    "start_packer = keras_nlp.layers.StartEndPacker(\n",
    "    sequence_length=seq_len,\n",
    "    start_value=None,\n",
    ")\n",
    "prompt_words_ = preprocessor.tokenizer([\"This image shows\"]) ; start_word_idx = len(prompt_words_[0]) \n",
    "prompt = start_packer(prompt_words_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1904a890",
   "metadata": {
    "papermill": {
     "duration": 0.01072,
     "end_time": "2024-07-19T00:48:28.958479",
     "exception": false,
     "start_time": "2024-07-19T00:48:28.947759",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Modelling\n",
    "- building a single model that functions as a testbed;\n",
    "    - image encoder, text encoder(GPT2), cross-attentive attention layer로 구성\n",
    "    - 모델은 image와 raw text를 input으로 받고\n",
    "        - image -> encoder -> [CLS_token, encoded_patches, attention_weight] 3개의 output return\n",
    "        - raw text -> GPT2 -> encoded_text & get mask\n",
    "    - cross_attended_text = TransformerDecoder(query = raw_text, key = encoded_patches, value = encoded_patches, mask = mask) -> Dense -> Perplexity, accuracy 측정\n",
    "    - 위 과정과 cls_token 및 pooled encoded_text의 batchwise cross-correlation 구해서 contrastive accuracy 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "465a122a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-19T00:48:28.986421Z",
     "iopub.status.busy": "2024-07-19T00:48:28.985961Z",
     "iopub.status.idle": "2024-07-19T00:48:29.046073Z",
     "shell.execute_reply": "2024-07-19T00:48:29.043789Z"
    },
    "papermill": {
     "duration": 0.078311,
     "end_time": "2024-07-19T00:48:29.049304",
     "exception": false,
     "start_time": "2024-07-19T00:48:28.970993",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MedicalCaptioner(keras.Model): \n",
    "    def __init__(self, image_encoder, \n",
    "                 preprocessor, text_encoder,\n",
    "                 att_heads = att_heads, att_dims = att_dims, att_depth = att_depth,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.image_encoder = image_encoder\n",
    "        self.text_encoder = text_encoder ; \n",
    "        self.text_preprocessor = preprocessor ; self.text_preprocessor.trainable = False\n",
    "        \n",
    "        self.mlp_text = keras.layers.Dense(units = n_vocab, activation = \"softmax\", name = \"VocabClassifier\", \n",
    "                                           kernel_regularizer=keras.regularizers.L1L2(l1=1e-5, l2=1e-4))\n",
    "        self.mlp_image = keras.layers.Dense(units = text_embed_dims)\n",
    "        self.att_heads = att_heads\n",
    "        self.att_dims = att_dims\n",
    "        self.att_depth = att_depth\n",
    "        self.cross_attention_layers = [keras_nlp.layers.TransformerDecoder(att_dims, att_heads, name = f\"CrossMHADecoder{i+1}\",\n",
    "                                                                          dropout = 0.2, activation = \"gelu\", normalize_first = True) for i in range(att_depth)]\n",
    "        self.compute_perplexity = keras_nlp.metrics.Perplexity(mask_token_id=pad_token)\n",
    "        self.compute_accuracy = keras.metrics.SparseCategoricalAccuracy()\n",
    "        self.compute_softmax_loss = keras.losses.SparseCategoricalCrossentropy(ignore_class = pad_token, reduction = None)\n",
    "    def get_config(self):\n",
    "        return {\"Image encoder name\": self.image_encoder.name,\n",
    "               \"Text encoder name\" : self.text_encoder.name,\n",
    "               \"cross attention heads\" : att_heads, \"intermediate dims\" : self.att_dims, \"cross attention depth\" : self.att_depth\n",
    "               }\n",
    "    def call(self, image, text):\n",
    "        if len(self.image_encoder.outputs) == 2 :\n",
    "            image_token, encoded_patches = self.image_encoder(image)\n",
    "        elif len(self.image_encoder.outputs) == 3 :\n",
    "            image_token, encoded_patches, image_attention_weights = self.image_encoder(image)\n",
    "        elif len(self.image_encoder.outputs) == 1:\n",
    "            encoded_patches = self.image_encoder(image)\n",
    "        \n",
    "        if len(ops.shape(encoded_patches)) == 4:\n",
    "            _, w, h, dims = ops.shape(encoded_patches)\n",
    "            encoded_patches = ops.reshape(encoded_patches, [-1, w*h, dims])\n",
    "        n_patches = ops.shape(encoded_patches)[1]\n",
    "        batch_size = ops.shape(encoded_patches)[0]\n",
    "        \n",
    "        preprocessed_text = self.text_preprocessor(text) ; text_mask = preprocessed_text[\"padding_mask\"]\n",
    "        \n",
    "        ar_input = {\"token_ids\" : preprocessed_text[\"token_ids\"][:, :-1],\n",
    "                   \"padding_mask\" : preprocessed_text[\"padding_mask\"][:, :-1]}\n",
    "        ar_output = {\"token_ids\" : preprocessed_text[\"token_ids\"][:, 1:],\n",
    "                   \"padding_mask\" : preprocessed_text[\"padding_mask\"][:, 1:]}\n",
    "        \n",
    "        encoded_text = self.text_encoder(ar_input)\n",
    "        \n",
    "        encoded_patches = self.mlp_image(encoded_patches)\n",
    "        concat_ = ops.concatenate([encoded_patches, encoded_text], axis = 1) #batch, n_patch + seqlen, dims\n",
    "        imgimg_mask = ops.ones(shape = [batch_size, n_patches, n_patches])\n",
    "        imgtext_mask = ops.zeros(shape = [batch_size, n_patches, seq_len-1])\n",
    "        textimg_mask = ops.ones(shape = [batch_size, seq_len-1, n_patches])\n",
    "        texttext_mask = tf.linalg.band_part(ops.ones(shape = [batch_size, seq_len-1, seq_len-1]), -1, 0)\n",
    "        texttext_mask = ops.minimum(texttext_mask, ar_input['padding_mask'][..., tf.newaxis])\n",
    "        \n",
    "        mask1 = ops.concatenate([imgimg_mask, imgtext_mask], axis = 2)\n",
    "        mask2 = ops.concatenate([textimg_mask, texttext_mask], axis =2)\n",
    "        mask = ops.concatenate([mask1, mask2], axis = 1)\n",
    "        # Cross-Attention\n",
    "        for idx, decoder in enumerate(self.cross_attention_layers):\n",
    "            concat_ = decoder(decoder_sequence = concat_, decoder_attention_mask = mask)\n",
    "        encoded_text = concat_[:, -seq_len:, :]\n",
    "        vocab_p = self.mlp_text(encoded_text)\n",
    "        \n",
    "        return vocab_p, ar_output[\"token_ids\"], ar_output[\"padding_mask\"]\n",
    "    \n",
    "    def get_next_proba_fn(self, image, prompt):\n",
    "        \n",
    "        def next(prompt, cache, index):\n",
    "            prompt = self.text_preprocessor.tokenizer.detokenize(prompt)\n",
    "            vocab_proba = self(image, prompt)[0]\n",
    "            logits = vocab_proba[:, index - 1, :]\n",
    "            # Ignore hidden states for now; only needed for contrastive search.\n",
    "            hidden_states = None\n",
    "            return logits, hidden_states, cache\n",
    "        return next\n",
    "    def infer(self, image):\n",
    "        if len(ops.shape(image)) == 3:\n",
    "            image = image[tf.newaxis, ...]\n",
    "        \n",
    "        next_fn = self.get_next_proba_fn(image = image, prompt = prompt)\n",
    "        \n",
    "        greedy_sampler = keras_nlp.samplers.GreedySampler()\n",
    "        nuc_sampler = keras_nlp.samplers.TopPSampler(p=0.5, k = 10)\n",
    "        \n",
    "        greedy_tokens = greedy_sampler(next=next_fn,\n",
    "                                prompt=prompt,\n",
    "                                index=start_word_idx)\n",
    "        \n",
    "        nuc_tokens = nuc_sampler(next=next_fn,\n",
    "                                prompt=prompt,\n",
    "                                index=start_word_idx)\n",
    "        \n",
    "        greedy_words, nuc_words = self.text_preprocessor.tokenizer.detokenize(greedy_tokens), self.text_preprocessor.tokenizer.detokenize(nuc_tokens)\n",
    "        greedy_words, nuc_words = greedy_words.numpy(), nuc_words.numpy()\n",
    "        try:\n",
    "            greedy_words = [w.decode() for w in greedy_words]\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            nuc_words = [w.decode() for w in nuc_words]\n",
    "        except:\n",
    "            nuc_words = [\"\".join(str(nuc_words[0]))]\n",
    "        return {\"greedy_sampling\" : greedy_words,\n",
    "               \"nucleus_sampling\" : nuc_words}\n",
    "    \n",
    "    def train_step(self, dataset): \n",
    "        image, text = dataset\n",
    "        with tf.GradientTape() as tape: \n",
    "            vocab_p, original_tokens, text_mask = self(image, text)\n",
    "            \n",
    "            loss = self.compute_softmax_loss(y_true = original_tokens, y_pred = vocab_p)\n",
    "            perplexity = self.compute_perplexity(y_true = original_tokens, y_pred = vocab_p)\n",
    "            accuracy = self.compute_accuracy(y_true = original_tokens, y_pred = vocab_p, sample_weight = text_mask)\n",
    "            loss = ops.mean(loss)\n",
    "            \n",
    "        grads = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "        \n",
    "        return {\"VocabSoftmaxLoss\" : loss,\n",
    "               \"Perplexity\" : perplexity,\n",
    "               \"TokenAccuracy\" : accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95ef93bd",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-07-19T00:48:29.072778Z",
     "iopub.status.busy": "2024-07-19T00:48:29.071953Z",
     "iopub.status.idle": "2024-07-19T00:48:29.084553Z",
     "shell.execute_reply": "2024-07-19T00:48:29.083023Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.027607,
     "end_time": "2024-07-19T00:48:29.087693",
     "exception": false,
     "start_time": "2024-07-19T00:48:29.060086",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def wandb_config():\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    user_secrets = UserSecretsClient()\n",
    "    try:\n",
    "        secret_value_0 = user_secrets.get_secret(\"__gcloud_sdk_auth__\")\n",
    "        secret_value_1 = user_secrets.get_secret(\"huggingface_key\")\n",
    "        secret_value_2 = user_secrets.get_secret(\"wandb_key\")\n",
    "        !wandb login $secret_value_2\n",
    "    except:\n",
    "        secret_value_0 = user_secrets.get_secret(\"huggingface_key\")\n",
    "        secret_value_1 = user_secrets.get_secret(\"wandb_key\")\n",
    "        !wandb login $secret_value_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "837e96e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-19T00:48:29.122147Z",
     "iopub.status.busy": "2024-07-19T00:48:29.121678Z",
     "iopub.status.idle": "2024-07-19T00:48:29.150467Z",
     "shell.execute_reply": "2024-07-19T00:48:29.147366Z"
    },
    "papermill": {
     "duration": 0.049455,
     "end_time": "2024-07-19T00:48:29.154778",
     "exception": false,
     "start_time": "2024-07-19T00:48:29.105323",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TrainingViz(keras.callbacks.Callback):\n",
    "    def __init__(self, run):\n",
    "        super().__init__()\n",
    "        self.run = run\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        try:\n",
    "            origin = [\"Original Image\"]\n",
    "            col = [\"Original image\", \"Ground truth caption\", \"Greedy caption\", \"Nucleus caption\"]\n",
    "            visualize_data = []\n",
    "            for idx, original_image in tqdm(enumerate(val_images)):\n",
    "                origin_img = [wandb.Image(original_image)]\n",
    "                gt_caption = [val_texts[idx].numpy()]\n",
    "                outputs = self.model.infer(val_images[idx])\n",
    "                greedy = [outputs[\"greedy_sampling\"]]\n",
    "                nuc = [outputs[\"nucleus_sampling\"]]\n",
    "                tmp = origin_img + gt_caption + greedy + nuc\n",
    "                visualize_data.append(tmp)\n",
    "                del tmp, origin_img\n",
    "            tbl = wandb.Table(columns = col, data = visualize_data)\n",
    "            wandb.log({f\"Epoch{epoch+1}_result\": tbl})\n",
    "            del tbl\n",
    "            tf.keras.backend.clear_session()\n",
    "        except Exception as e: \n",
    "            print('visualization error', e)\n",
    "        \n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        if (batch % 1500 == 0) and (batch != 0):\n",
    "            if True:\n",
    "                origin = [\"Original Image\"]\n",
    "                col = [\"Original image\", \"Ground truth caption\", \"Greedy caption\", \"Nucleus caption\"]\n",
    "                visualize_data = []\n",
    "                len_ = 3 if batch == 0 else batch_size\n",
    "                for idx, original_image in tqdm(enumerate(val_images[:len_])):\n",
    "                    origin_img = [wandb.Image(original_image)]\n",
    "                    gt_caption = [val_texts[idx].numpy()]\n",
    "                    outputs = self.model.infer(val_images[idx])\n",
    "                    greedy = [outputs[\"greedy_sampling\"]]\n",
    "                    nuc = [outputs[\"nucleus_sampling\"]]\n",
    "                    tmp = origin_img + gt_caption + greedy + nuc\n",
    "                    visualize_data.append(tmp)\n",
    "                    del tmp, origin_img\n",
    "                tbl = wandb.Table(columns = col, data = visualize_data)\n",
    "                wandb.log({f\"batch{batch+1}_result\": tbl})\n",
    "                del tbl\n",
    "                tf.keras.backend.clear_session()\n",
    "            #except Exception as e: \n",
    "            #    print('visualization error', e)\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e8dc577",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-19T00:48:29.184855Z",
     "iopub.status.busy": "2024-07-19T00:48:29.183906Z",
     "iopub.status.idle": "2024-07-19T00:48:29.202110Z",
     "shell.execute_reply": "2024-07-19T00:48:29.200760Z"
    },
    "papermill": {
     "duration": 0.0374,
     "end_time": "2024-07-19T00:48:29.205371",
     "exception": false,
     "start_time": "2024-07-19T00:48:29.167971",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_exp(image_encoder, notes = None, exp_name = None, epochs = 1,\n",
    "           image_encoder_trainable = True):\n",
    "    image_encoder.trainable = image_encoder_trainable\n",
    "    model = MedicalCaptioner(image_encoder, preprocessor, text_only_decoder)\n",
    "    model.compile(optimizer = keras.optimizers.Adam(learning_rate = 1e-4),\n",
    "             jit_compile = False)\n",
    "    configs = model.get_config()\n",
    "    configs[\"finetune\"] = \"full fine tuning\" if image_encoder_trainable else 'frozen, zeroshot'\n",
    "    configs[\"batch size\"] = batch_size\n",
    "    configs[\"caption sequence length\"] = seq_len\n",
    "    try:\n",
    "        wandb.finish()\n",
    "    except:\n",
    "        pass\n",
    "    print(configs)\n",
    "    \n",
    "    if exp_name is None:\n",
    "        encname = configs[\"Image encoder name\"]\n",
    "        tune_ = configs[\"finetune\"]\n",
    "        exp_name = f\"{encname}_captioning_{tune_}\"\n",
    "    pass_error = keras.callbacks.TerminateOnNaN()\n",
    "    \n",
    "    \n",
    "    wandb_config()\n",
    "    run = wandb.init(project=\"Eval_RadImageNet_caption\", \n",
    "                         entity=\"gongbungkim\", config = configs, notes = notes,\n",
    "                        name = exp_name)\n",
    "    wb_callback = wandb.keras.WandbMetricsLogger(log_freq = 100)\n",
    "    vizcallback = TrainingViz(run)\n",
    "    callbacks = [pass_error, wb_callback, vizcallback]\n",
    "    hist = model.fit(train_ds, steps_per_epoch = train_steps, epochs = epochs, verbose = 1, callbacks = callbacks)\n",
    "    return hist, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c36e802d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-19T00:48:29.227036Z",
     "iopub.status.busy": "2024-07-19T00:48:29.226556Z",
     "iopub.status.idle": "2024-07-19T00:48:29.231932Z",
     "shell.execute_reply": "2024-07-19T00:48:29.230744Z"
    },
    "papermill": {
     "duration": 0.019888,
     "end_time": "2024-07-19T00:48:29.234777",
     "exception": false,
     "start_time": "2024-07-19T00:48:29.214889",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#run_exp(keras.applications.ResNet50V2(include_top = False, input_shape = [res,res,3]),\n",
    "#       epochs = 5,\n",
    "#       notes = \"GIT\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5073985,
     "sourceId": 8501861,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 2823,
     "modelInstanceId": 4694,
     "sourceId": 6074,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30746,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 82.290901,
   "end_time": "2024-07-19T00:48:32.142153",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-07-19T00:47:09.851252",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
