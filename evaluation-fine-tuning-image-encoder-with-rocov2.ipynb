{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8501861,"sourceType":"datasetVersion","datasetId":5073985},{"sourceId":6074,"sourceType":"modelInstanceVersion","modelInstanceId":4694}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport random\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nseed = 1\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# ML tools \n\nimport tensorflow as tf\nimport keras #; keras.config.set_dtype_policy(\"mixed_float16\")\nimport keras_nlp\nimport keras_cv\nfrom keras import ops\n\nkeras.utils.set_random_seed(seed)\n\nimport cv2\nimport tensorflow_io as tfio\nfrom kaggle_datasets import KaggleDatasets\nimport tensorflow_datasets as tfds\n\nfrom keras import Input, Model, layers\nfrom keras.models import load_model\nfrom keras.layers import Layer\nfrom keras.layers import Conv2D, DepthwiseConv2D, Dense, Activation, BatchNormalization, LayerNormalization, MultiHeadAttention, Embedding, Subtract, Add, Multiply, GlobalAveragePooling2D, GlobalAveragePooling1D, LayerNormalization\nfrom keras.preprocessing.image import load_img, img_to_array\nfrom keras.applications import *\nimport os, sys\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nfrom sklearn.model_selection import train_test_split\nfrom keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\nfrom tqdm.notebook import tqdm\nimport wandb\nprint(f\"Requirements loaded, keras : v{keras.__version__}, Tensorflow : v{tf.__version__}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-07-17T09:10:11.015066Z","iopub.execute_input":"2024-07-17T09:10:11.015841Z","iopub.status.idle":"2024-07-17T09:10:32.055213Z","shell.execute_reply.started":"2024-07-17T09:10:11.015809Z","shell.execute_reply":"2024-07-17T09:10:32.054314Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-07-17 09:10:13.858397: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-17 09:10:13.858526: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-17 09:10:13.992901: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Requirements loaded, keras : v3.4.1, Tensorflow : v2.15.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Setting hyperparameters","metadata":{}},{"cell_type":"code","source":"# hyperparameters about images\nres = 384\nbatch_size = 16\n# hyperparameters about text\nseq_len = 64\n# hyperparameters about cross attentive decoder\natt_depth = 8\natt_heads = 8\natt_dims = att_heads * 24\nuse_bias = False\n\ntrain_cases = 70108\nval_cases = 9972\ntrain_steps = train_cases//batch_size\nval_steps = val_cases//batch_size\n\ntrain_dataset_dir = '/kaggle/input/roco-v2-tfrecord-dataset/Tot70108cases_RoCoV2_radiology_train_GZIP.tfrecord'\nval_dataset_dir = '/kaggle/input/roco-v2-tfrecord-dataset/Tot9972cases_RoCoV2_radiology_test_GZIP.tfrecord'","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-07-17T09:10:32.057213Z","iopub.execute_input":"2024-07-17T09:10:32.057630Z","iopub.status.idle":"2024-07-17T09:10:32.063215Z","shell.execute_reply.started":"2024-07-17T09:10:32.057598Z","shell.execute_reply":"2024-07-17T09:10:32.062229Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Parsing Dataset","metadata":{}},{"cell_type":"code","source":"def _parse_tfrecord(c, res = res):\n    def parse_tfrecord(tfrecord):\n        features = {'image': tf.io.FixedLenFeature([], tf.string),\n                    'report': tf.io.FixedLenFeature([], tf.string),\n                    }\n        x = tf.io.parse_single_example(tfrecord, features)\n        image_train = tf.image.decode_jpeg(x['image'], channels=c)\n        image_train = _transform_images(res = res)(image_train)\n        report = tf.cast(x[\"report\"], tf.string)\n        return image_train, report\n    \n    return parse_tfrecord\n\n\ndef _transform_images(res = res):\n    def transform_images(x_train):\n        x_train = tf.image.resize_with_pad(x_train, res, res, antialias = True)\n        x_train = tf.cast(x_train, tf.uint8)\n        return x_train\n    return transform_images\n\ndef load_tfrecord_dataset(tfrecord_name, res = res, batch_size = batch_size, shuffle=True, buffer_size=10240, grayscale = False):\n    \"\"\"load dataset from tfrecord\"\"\"\n    c = 1 if grayscale else 3\n    raw_dataset = tf.data.TFRecordDataset(tfrecord_name, compression_type = \"GZIP\")\n    raw_dataset = raw_dataset.repeat()\n    if shuffle:\n        raw_dataset = raw_dataset.shuffle(buffer_size=buffer_size)\n    dataset = raw_dataset.map(\n        _parse_tfrecord(c = c, res = res),\n        num_parallel_calls=tf.data.AUTOTUNE\n    )\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n    return dataset\n\ntrain_ds = load_tfrecord_dataset(train_dataset_dir)\nval_ds = load_tfrecord_dataset(val_dataset_dir) #image, report 2 outputs\n\nfor a, b in val_ds.take(1):\n    val_images = a\n    val_texts = b","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-07-17T09:10:32.064492Z","iopub.execute_input":"2024-07-17T09:10:32.064865Z","iopub.status.idle":"2024-07-17T09:10:47.279586Z","shell.execute_reply.started":"2024-07-17T09:10:32.064832Z","shell.execute_reply":"2024-07-17T09:10:47.278536Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Preparing Natural language decoder (pretrained on plain texts)","metadata":{}},{"cell_type":"code","source":"gpt_freeset = \"gpt2_base_en\"\ntext_only_decoder = keras_nlp.models.Backbone.from_preset(\n    gpt_freeset,\n    trainable=False,\n)\npreprocessor = keras_nlp.models.GPT2Preprocessor.from_preset(\n    gpt_freeset,\n    sequence_length=seq_len,\n    add_start_token = False\n)\npreprocessor.trainable = False\ntext_only_decoder.enable_lora(8)\ntext_only_decoder.summary()","metadata":{"execution":{"iopub.status.busy":"2024-07-17T09:10:47.282596Z","iopub.execute_input":"2024-07-17T09:10:47.282970Z","iopub.status.idle":"2024-07-17T09:11:03.631317Z","shell.execute_reply.started":"2024-07-17T09:10:47.282938Z","shell.execute_reply":"2024-07-17T09:11:03.630467Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Attaching 'model.safetensors' from model 'keras/gpt2/keras/gpt2_base_en/2' to your Kaggle notebook...\nAttaching 'model.safetensors.index.json' from model 'keras/gpt2/keras/gpt2_base_en/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/gpt2/keras/gpt2_base_en/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/gpt2/keras/gpt2_base_en/2' to your Kaggle notebook...\nAttaching 'config.json' from model 'keras/gpt2/keras/gpt2_base_en/2' to your Kaggle notebook...\nAttaching 'config.json' from model 'keras/gpt2/keras/gpt2_base_en/2' to your Kaggle notebook...\nAttaching 'model.weights.h5' from model 'keras/gpt2/keras/gpt2_base_en/2' to your Kaggle notebook...\nAttaching 'model.safetensors' from model 'keras/gpt2/keras/gpt2_base_en/2' to your Kaggle notebook...\nAttaching 'model.safetensors.index.json' from model 'keras/gpt2/keras/gpt2_base_en/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/gpt2/keras/gpt2_base_en/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/gpt2/keras/gpt2_base_en/2' to your Kaggle notebook...\nAttaching 'preprocessor.json' from model 'keras/gpt2/keras/gpt2_base_en/2' to your Kaggle notebook...\nAttaching 'tokenizer.json' from model 'keras/gpt2/keras/gpt2_base_en/2' to your Kaggle notebook...\nAttaching 'tokenizer.json' from model 'keras/gpt2/keras/gpt2_base_en/2' to your Kaggle notebook...\nAttaching 'assets/tokenizer/vocabulary.json' from model 'keras/gpt2/keras/gpt2_base_en/2' to your Kaggle notebook...\nAttaching 'assets/tokenizer/merges.txt' from model 'keras/gpt2/keras/gpt2_base_en/2' to your Kaggle notebook...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"gpt2_backbone\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gpt2_backbone\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ token_ids           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ token_embedding     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │ \u001b[38;5;34m38,597,376\u001b[0m │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mReversibleEmbeddi…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ position_embedding  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │    \u001b[38;5;34m786,432\u001b[0m │ token_embedding[\u001b[38;5;34m…\u001b[0m │\n│ (\u001b[38;5;33mPositionEmbedding\u001b[0m) │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ embeddings_add      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ token_embedding[\u001b[38;5;34m…\u001b[0m │\n│ (\u001b[38;5;33mAdd\u001b[0m)               │                   │            │ position_embeddi… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ embeddings_dropout  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ embeddings_add[\u001b[38;5;34m0\u001b[0m… │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ padding_mask        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ transformer_layer_0 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │  \u001b[38;5;34m7,236,352\u001b[0m │ embeddings_dropo… │\n│ (\u001b[38;5;33mTransformerDecode…\u001b[0m │                   │            │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ transformer_layer_1 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │  \u001b[38;5;34m7,236,352\u001b[0m │ transformer_laye… │\n│ (\u001b[38;5;33mTransformerDecode…\u001b[0m │                   │            │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ transformer_layer_2 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │  \u001b[38;5;34m7,236,352\u001b[0m │ transformer_laye… │\n│ (\u001b[38;5;33mTransformerDecode…\u001b[0m │                   │            │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ transformer_layer_3 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │  \u001b[38;5;34m7,236,352\u001b[0m │ transformer_laye… │\n│ (\u001b[38;5;33mTransformerDecode…\u001b[0m │                   │            │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ transformer_layer_4 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │  \u001b[38;5;34m7,236,352\u001b[0m │ transformer_laye… │\n│ (\u001b[38;5;33mTransformerDecode…\u001b[0m │                   │            │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ transformer_layer_5 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │  \u001b[38;5;34m7,236,352\u001b[0m │ transformer_laye… │\n│ (\u001b[38;5;33mTransformerDecode…\u001b[0m │                   │            │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ transformer_layer_6 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │  \u001b[38;5;34m7,236,352\u001b[0m │ transformer_laye… │\n│ (\u001b[38;5;33mTransformerDecode…\u001b[0m │                   │            │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ transformer_layer_7 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │  \u001b[38;5;34m7,236,352\u001b[0m │ transformer_laye… │\n│ (\u001b[38;5;33mTransformerDecode…\u001b[0m │                   │            │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ transformer_layer_8 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │  \u001b[38;5;34m7,236,352\u001b[0m │ transformer_laye… │\n│ (\u001b[38;5;33mTransformerDecode…\u001b[0m │                   │            │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ transformer_layer_9 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │  \u001b[38;5;34m7,236,352\u001b[0m │ transformer_laye… │\n│ (\u001b[38;5;33mTransformerDecode…\u001b[0m │                   │            │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ transformer_layer_… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │  \u001b[38;5;34m7,236,352\u001b[0m │ transformer_laye… │\n│ (\u001b[38;5;33mTransformerDecode…\u001b[0m │                   │            │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ transformer_layer_… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │  \u001b[38;5;34m7,236,352\u001b[0m │ transformer_laye… │\n│ (\u001b[38;5;33mTransformerDecode…\u001b[0m │                   │            │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_norm          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │      \u001b[38;5;34m1,536\u001b[0m │ transformer_laye… │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ token_ids           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ token_embedding     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │ <span style=\"color: #00af00; text-decoration-color: #00af00\">38,597,376</span> │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbeddi…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ position_embedding  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">786,432</span> │ token_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionEmbedding</span>) │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ embeddings_add      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ token_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │                   │            │ position_embeddi… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ embeddings_dropout  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embeddings_add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ padding_mask        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ transformer_layer_0 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">7,236,352</span> │ embeddings_dropo… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecode…</span> │                   │            │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ transformer_layer_1 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">7,236,352</span> │ transformer_laye… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecode…</span> │                   │            │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ transformer_layer_2 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">7,236,352</span> │ transformer_laye… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecode…</span> │                   │            │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ transformer_layer_3 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">7,236,352</span> │ transformer_laye… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecode…</span> │                   │            │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ transformer_layer_4 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">7,236,352</span> │ transformer_laye… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecode…</span> │                   │            │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ transformer_layer_5 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">7,236,352</span> │ transformer_laye… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecode…</span> │                   │            │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ transformer_layer_6 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">7,236,352</span> │ transformer_laye… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecode…</span> │                   │            │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ transformer_layer_7 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">7,236,352</span> │ transformer_laye… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecode…</span> │                   │            │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ transformer_layer_8 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">7,236,352</span> │ transformer_laye… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecode…</span> │                   │            │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ transformer_layer_9 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">7,236,352</span> │ transformer_laye… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecode…</span> │                   │            │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ transformer_layer_… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">7,236,352</span> │ transformer_laye… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecode…</span> │                   │            │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ transformer_layer_… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">7,236,352</span> │ transformer_laye… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecode…</span> │                   │            │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_norm          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ transformer_laye… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m126,221,568\u001b[0m (481.50 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">126,221,568</span> (481.50 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,800,192\u001b[0m (6.87 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,800,192</span> (6.87 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m124,421,376\u001b[0m (474.63 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">124,421,376</span> (474.63 MB)\n</pre>\n"},"metadata":{}}]},{"cell_type":"code","source":"#according to summarized GPT2 model structure:\ntext_embed_dims = 768\nn_vocab = preprocessor.tokenizer.vocabulary_size()\npad_token = preprocessor.tokenizer.pad_token_id\nstart_packer = keras_nlp.layers.StartEndPacker(\n    sequence_length=seq_len,\n    start_value=None,\n)\nprompt_words_ = preprocessor.tokenizer([\"This image shows\"]) ; start_word_idx = len(prompt_words_[0]) \nprompt = start_packer(prompt_words_)","metadata":{"execution":{"iopub.status.busy":"2024-07-17T09:11:03.632376Z","iopub.execute_input":"2024-07-17T09:11:03.632635Z","iopub.status.idle":"2024-07-17T09:11:05.104085Z","shell.execute_reply.started":"2024-07-17T09:11:03.632611Z","shell.execute_reply":"2024-07-17T09:11:05.103276Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Modelling\n- building a single model that functions as a testbed;\n    - image encoder, text encoder(GPT2), cross-attentive attention layer로 구성\n    - 모델은 image와 raw text를 input으로 받고\n        - image -> encoder -> [CLS_token, encoded_patches, attention_weight] 3개의 output return\n        - raw text -> GPT2 -> encoded_text & get mask\n    - cross_attended_text = TransformerDecoder(query = raw_text, key = encoded_patches, value = encoded_patches, mask = mask) -> Dense -> Perplexity, accuracy 측정\n    - 위 과정과 cls_token 및 pooled encoded_text의 batchwise cross-correlation 구해서 contrastive accuracy 구하기","metadata":{}},{"cell_type":"code","source":"class MedicalCaptioner(keras.Model): \n    def __init__(self, image_encoder, \n                 preprocessor, text_encoder,\n                 att_heads = att_heads, att_dims = att_dims, att_depth = att_depth,\n                 **kwargs):\n        super().__init__(**kwargs)\n        self.image_encoder = image_encoder\n        self.text_encoder = text_encoder ; \n        self.text_preprocessor = preprocessor ; self.text_preprocessor.trainable = False\n        \n        self.mlp_text = keras.layers.Dense(units = n_vocab, activation = \"softmax\", name = \"VocabClassifier\")\n        self.att_heads = att_heads\n        self.att_dims = att_dims\n        self.att_depth = att_depth\n        self.cross_attention_layers = [keras_nlp.layers.TransformerDecoder(att_dims, att_heads, name = f\"CrossMHADecoder{i+1}\",\n                                                                          dropout = 0.2, activation = \"relu\") for i in range(att_depth)]\n        self.compute_perplexity = keras_nlp.metrics.Perplexity(mask_token_id=pad_token)\n        self.compute_accuracy = keras.metrics.SparseCategoricalAccuracy()\n        self.compute_softmax_loss = keras.losses.SparseCategoricalCrossentropy(ignore_class = pad_token, reduction = None)\n    def get_config(self):\n        return {\"Image encoder name\": self.image_encoder.name,\n               \"Text encoder name\" : self.text_encoder.name,\n               \"cross attention heads\" : att_heads, \"intermediate dims\" : self.att_dims, \"cross attention depth\" : self.att_depth\n               }\n    def call(self, image, text):\n        if len(self.image_encoder.outputs) == 2 :\n            image_token, encoded_patches = self.image_encoder(image)\n        elif len(self.image_encoder.outputs) == 3 :\n            image_token, encoded_patches, image_attention_weights = self.image_encoder(image)\n        elif len(self.image_encoder.outputs) == 1:\n            encoded_patches = self.image_encoder(image)\n        \n        if len(ops.shape(encoded_patches)) == 4:\n            _, w, h, dims = ops.shape(encoded_patches)\n            encoded_patches = ops.reshape(encoded_patches, [-1, w*h, dims])\n\n        preprocessed_text = self.text_preprocessor(text) ; text_mask = preprocessed_text[\"padding_mask\"]\n        original_token = preprocessed_text[\"token_ids\"]\n        encoded_text = self.text_encoder(preprocessed_text)\n        \n        # Cross-Attention\n        for idx, decoder in enumerate(self.cross_attention_layers):\n            encoded_text = decoder(decoder_sequence = encoded_text,\n                                  encoder_sequence = encoded_patches,\n                                  decoder_padding_mask = text_mask)\n        vocab_p = self.mlp_text(encoded_text)\n        \n        return vocab_p, original_token, text_mask\n    \n    def get_next_proba_fn(self, image, prompt):\n        \n        def next(prompt, cache, index):\n            prompt = self.text_preprocessor.tokenizer.detokenize(prompt)\n            vocab_proba = self(image, prompt)[0]\n            logits = vocab_proba[:, index - 1, :]\n            # Ignore hidden states for now; only needed for contrastive search.\n            hidden_states = None\n            return logits, hidden_states, cache\n        return next\n    def infer(self, image):\n        if len(ops.shape(image)) == 3:\n            image = image[tf.newaxis, ...]\n        \n        next_fn = self.get_next_proba_fn(image = image, prompt = prompt)\n        \n        greedy_sampler = keras_nlp.samplers.GreedySampler()\n        nuc_sampler = keras_nlp.samplers.TopPSampler(p=0.5, k = 10)\n        \n        greedy_tokens = greedy_sampler(next=next_fn,\n                                prompt=prompt,\n                                index=start_word_idx)\n        \n        nuc_tokens = nuc_sampler(next=next_fn,\n                                prompt=prompt,\n                                index=start_word_idx)\n        \n        greedy_words, nuc_words = self.text_preprocessor.tokenizer.detokenize(greedy_tokens), self.text_preprocessor.tokenizer.detokenize(nuc_tokens)\n        greedy_words, nuc_words = greedy_words.numpy(), nuc_words.numpy()\n        try:\n            greedy_words = [w.decode() for w in greedy_words]\n        except:\n            pass\n        try:\n            nuc_words = [w.decode() for w in nuc_words]\n        except:\n            nuc_words = \"\".join(str(nuc_words[0]))\n        return {\"greedy_sampling\" : greedy_words,\n               \"nucleus_sampling\" : nuc_words}\n    \n    def train_step(self, dataset): \n        image, text = dataset\n        with tf.GradientTape() as tape: \n            vocab_p, original_tokens, text_mask = self(image, text)\n            \n            loss = self.compute_softmax_loss(y_true = original_tokens, y_pred = vocab_p)\n            perplexity = self.compute_perplexity(y_true = original_tokens, y_pred = vocab_p)\n            accuracy = self.compute_accuracy(y_true = original_tokens, y_pred = vocab_p, sample_weight = text_mask)\n            loss = ops.mean(loss)\n            \n        grads = tape.gradient(loss, self.trainable_variables)\n        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n        \n        return {\"VocabSoftmaxLoss\" : loss,\n               \"Perplexity\" : perplexity,\n               \"TokenAccuracy\" : accuracy}","metadata":{"execution":{"iopub.status.busy":"2024-07-17T09:11:05.105188Z","iopub.execute_input":"2024-07-17T09:11:05.105476Z","iopub.status.idle":"2024-07-17T09:11:05.129425Z","shell.execute_reply.started":"2024-07-17T09:11:05.105452Z","shell.execute_reply":"2024-07-17T09:11:05.128385Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def wandb_config():\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    try:\n        secret_value_0 = user_secrets.get_secret(\"__gcloud_sdk_auth__\")\n        secret_value_1 = user_secrets.get_secret(\"huggingface_key\")\n        secret_value_2 = user_secrets.get_secret(\"wandb_key\")\n        !wandb login $secret_value_2\n    except:\n        secret_value_0 = user_secrets.get_secret(\"huggingface_key\")\n        secret_value_1 = user_secrets.get_secret(\"wandb_key\")\n        !wandb login $secret_value_1","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-07-17T09:11:05.130630Z","iopub.execute_input":"2024-07-17T09:11:05.131246Z","iopub.status.idle":"2024-07-17T09:11:05.145448Z","shell.execute_reply.started":"2024-07-17T09:11:05.131212Z","shell.execute_reply":"2024-07-17T09:11:05.144658Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class TrainingViz(keras.callbacks.Callback):\n    def __init__(self, run):\n        super().__init__()\n        self.run = run\n    def on_epoch_end(self, epoch, logs=None):\n        try:\n            origin = [\"Original Image\"]\n            col = [\"Original image\", \"Ground truth caption\", \"Greedy caption\", \"Nucleus caption\"]\n            visualize_data = []\n            for idx, original_image in tqdm(enumerate(val_images)):\n                origin_img = [wandb.Image(original_image)]\n                gt_caption = [val_texts[idx].numpy()]\n                outputs = self.model.infer(val_images[idx])\n                greedy = [outputs[\"greedy_sampling\"]]\n                nuc = [outputs[\"nucleus_sampling\"]]\n                tmp = origin_img + gt_caption + greedy + nuc\n                visualize_data.append(tmp)\n                del tmp, origin_img\n            tbl = wandb.Table(columns = col, data = visualize_data)\n            wandb.log({f\"Epoch{epoch+1}_result\": tbl})\n            del tbl\n            tf.keras.backend.clear_session()\n        except Exception as e: \n            print('visualization error', e)\n        \n    def on_train_batch_end(self, batch, logs=None):\n        if (batch % 1500 == 0) and (batch != 0):\n            if True:\n                origin = [\"Original Image\"]\n                col = [\"Original image\", \"Ground truth caption\", \"Greedy caption\", \"Nucleus caption\"]\n                visualize_data = []\n                len_ = 3 if batch == 0 else batch_size\n                for idx, original_image in tqdm(enumerate(val_images[:len_])):\n                    origin_img = [wandb.Image(original_image)]\n                    gt_caption = [val_texts[idx].numpy()]\n                    outputs = self.model.infer(val_images[idx])\n                    greedy = [outputs[\"greedy_sampling\"]]\n                    nuc = [outputs[\"nucleus_sampling\"]]\n                    tmp = origin_img + gt_caption + greedy + nuc\n                    visualize_data.append(tmp)\n                    del tmp, origin_img\n                tbl = wandb.Table(columns = col, data = visualize_data)\n                wandb.log({f\"batch{batch+1}_result\": tbl})\n                del tbl\n                tf.keras.backend.clear_session()\n            #except Exception as e: \n            #    print('visualization error', e)\n        else:\n            pass","metadata":{"execution":{"iopub.status.busy":"2024-07-17T09:11:05.146520Z","iopub.execute_input":"2024-07-17T09:11:05.146790Z","iopub.status.idle":"2024-07-17T09:11:05.162572Z","shell.execute_reply.started":"2024-07-17T09:11:05.146768Z","shell.execute_reply":"2024-07-17T09:11:05.161632Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def run_exp(image_encoder, notes = None, exp_name = None, epochs = 1,\n           image_encoder_trainable = True):\n    image_encoder.trainable = image_encoder_trainable\n    model = MedicalCaptioner(image_encoder, preprocessor, text_only_decoder)\n    model.compile(optimizer = keras.optimizers.Adam(learning_rate = 1e-4),\n             jit_compile = False)\n    configs = model.get_config()\n    configs[\"finetune\"] = \"full fine tuning\" if image_encoder_trainable else 'frozen, zeroshot'\n    configs[\"batch size\"] = batch_size\n    configs[\"caption sequence length\"] = seq_len\n    try:\n        wandb.finish()\n    except:\n        pass\n    print(configs)\n    \n    if exp_name is None:\n        encname = configs[\"Image encoder name\"]\n        tune_ = configs[\"finetune\"]\n        exp_name = f\"{encname}_captioning_{tune_}\"\n    pass_error = keras.callbacks.TerminateOnNaN()\n    \n    \n    wandb_config()\n    run = wandb.init(project=\"Eval_RadImageNet_caption\", \n                         entity=\"gongbungkim\", config = configs, notes = notes,\n                        name = exp_name)\n    wb_callback = wandb.keras.WandbMetricsLogger(log_freq = 100)\n    vizcallback = TrainingViz(run)\n    callbacks = [pass_error, wb_callback, vizcallback]\n    hist = model.fit(train_ds, steps_per_epoch = train_steps, epochs = epochs, verbose = 1, callbacks = callbacks)\n    return hist, model","metadata":{"execution":{"iopub.status.busy":"2024-07-17T09:11:05.163777Z","iopub.execute_input":"2024-07-17T09:11:05.164090Z","iopub.status.idle":"2024-07-17T09:11:05.177766Z","shell.execute_reply.started":"2024-07-17T09:11:05.164061Z","shell.execute_reply":"2024-07-17T09:11:05.176879Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"run_exp(keras.applications.EfficientNetV2B1(include_top = False, input_shape = [res,res,3]),\n       epochs = 1)","metadata":{"execution":{"iopub.status.busy":"2024-07-17T09:11:05.180308Z","iopub.execute_input":"2024-07-17T09:11:05.180951Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/efficientnet_v2/efficientnetv2-b1_notop.h5\n\u001b[1m28456008/28456008\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n{'Image encoder name': 'efficientnetv2-b1', 'Text encoder name': 'gpt2_backbone', 'cross attention heads': 8, 'intermediate dims': 192, 'cross attention depth': 8, 'finetune': 'full fine tuning', 'batch size': 16, 'caption sequence length': 64}\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgongbungkim\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.4"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240717_091112-4lfsov2q</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/gongbungkim/Eval_RadImageNet_caption/runs/4lfsov2q' target=\"_blank\">efficientnetv2-b1_captioning_full fine tuning</a></strong> to <a href='https://wandb.ai/gongbungkim/Eval_RadImageNet_caption' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/gongbungkim/Eval_RadImageNet_caption' target=\"_blank\">https://wandb.ai/gongbungkim/Eval_RadImageNet_caption</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/gongbungkim/Eval_RadImageNet_caption/runs/4lfsov2q' target=\"_blank\">https://wandb.ai/gongbungkim/Eval_RadImageNet_caption/runs/4lfsov2q</a>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Unable to log learning rate.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1500/4381\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m32:23\u001b[0m 675ms/step - Perplexity: 1028.3167 - TokenAccuracy: 0.4838 - VocabSoftmaxLoss: 1.0675","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5b36f2911294c46bae133a2e641365e"}},"metadata":{}},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1721208638.461915      34 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m3000/4381\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m29:27\u001b[0m 1s/step - Perplexity: 516.6215 - TokenAccuracy: 0.6469 - VocabSoftmaxLoss: 0.6004","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4567870d4795466396ea9c2407a28c95"}},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Network error (HTTPError), entering retry loop.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m4381/4381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - Perplexity: 354.6317 - TokenAccuracy: 0.7203 - VocabSoftmaxLoss: 0.4318","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"116c5c7f4c794290a4e51123fc270a54"}},"metadata":{}}]}]}