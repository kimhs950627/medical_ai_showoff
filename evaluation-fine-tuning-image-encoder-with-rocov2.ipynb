{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8501861,"sourceType":"datasetVersion","datasetId":5073985},{"sourceId":6074,"sourceType":"modelInstanceVersion","modelInstanceId":4694}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport random\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nseed = 1\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# ML tools \n\nimport tensorflow as tf\nimport keras #; keras.config.set_dtype_policy(\"mixed_float16\")\nimport keras_nlp\nimport keras_cv\nfrom keras import ops\n\nkeras.utils.set_random_seed(seed)\n\nimport cv2\nimport tensorflow_io as tfio\nfrom kaggle_datasets import KaggleDatasets\nimport tensorflow_datasets as tfds\n\nfrom keras import Input, Model, layers\nfrom keras.models import load_model\nfrom keras.layers import Layer\nfrom keras.layers import Conv2D, DepthwiseConv2D, Dense, Activation, BatchNormalization, LayerNormalization, MultiHeadAttention, Embedding, Subtract, Add, Multiply, GlobalAveragePooling2D, GlobalAveragePooling1D, LayerNormalization\nfrom keras.preprocessing.image import load_img, img_to_array\nfrom keras.applications import *\nimport os, sys\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nfrom sklearn.model_selection import train_test_split\nfrom keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\nfrom tqdm.notebook import tqdm\nimport wandb\nprint(f\"Requirements loaded, keras : v{keras.__version__}, Tensorflow : v{tf.__version__}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Setting hyperparameters","metadata":{}},{"cell_type":"code","source":"# hyperparameters about images\nres = 384\nbatch_size = 16\n# hyperparameters about text\nseq_len = 64\n# hyperparameters about cross attentive decoder\natt_depth = 1\natt_heads = 16\natt_dims = att_heads * 24\nuse_bias = False\n\ntrain_cases = 70108\nval_cases = 9972\ntrain_steps = train_cases//batch_size\nval_steps = val_cases//batch_size\n\ntrain_dataset_dir = '/kaggle/input/roco-v2-tfrecord-dataset/Tot70108cases_RoCoV2_radiology_train_GZIP.tfrecord'\nval_dataset_dir = '/kaggle/input/roco-v2-tfrecord-dataset/Tot9972cases_RoCoV2_radiology_test_GZIP.tfrecord'","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Parsing Dataset","metadata":{}},{"cell_type":"code","source":"def _parse_tfrecord(c, res = res):\n    def parse_tfrecord(tfrecord):\n        features = {'image': tf.io.FixedLenFeature([], tf.string),\n                    'report': tf.io.FixedLenFeature([], tf.string),\n                    }\n        x = tf.io.parse_single_example(tfrecord, features)\n        image_train = tf.image.decode_jpeg(x['image'], channels=c)\n        image_train = _transform_images(res = res)(image_train)\n        report = tf.cast(x[\"report\"], tf.string)\n        return image_train, report\n    \n    return parse_tfrecord\n\n\ndef _transform_images(res = res):\n    def transform_images(x_train):\n        x_train = tf.image.resize_with_pad(x_train, res, res, antialias = True)\n        x_train = tf.cast(x_train, tf.uint8)\n        return x_train\n    return transform_images\n\ndef load_tfrecord_dataset(tfrecord_name, res = res, batch_size = batch_size, shuffle=True, buffer_size=10240, grayscale = False):\n    \"\"\"load dataset from tfrecord\"\"\"\n    c = 1 if grayscale else 3\n    raw_dataset = tf.data.TFRecordDataset(tfrecord_name, compression_type = \"GZIP\")\n    raw_dataset = raw_dataset.repeat()\n    if shuffle:\n        raw_dataset = raw_dataset.shuffle(buffer_size=buffer_size)\n    dataset = raw_dataset.map(\n        _parse_tfrecord(c = c, res = res),\n        num_parallel_calls=tf.data.AUTOTUNE\n    )\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n    return dataset\n\ntrain_ds = load_tfrecord_dataset(train_dataset_dir)\nval_ds = load_tfrecord_dataset(val_dataset_dir) #image, report 2 outputs\n\nfor a, b in val_ds.take(1):\n    val_images = a\n    val_texts = b","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preparing Natural language decoder (pretrained on plain texts)","metadata":{}},{"cell_type":"code","source":"gpt_freeset = \"gpt2_base_en\"\ntext_only_decoder = keras_nlp.models.Backbone.from_preset(\n    gpt_freeset,\n    trainable=False,\n)\npreprocessor = keras_nlp.models.GPT2Preprocessor.from_preset(\n    gpt_freeset,\n    sequence_length=seq_len,\n    add_start_token = False\n)\npreprocessor.trainable = False\ntext_only_decoder.enable_lora(8)\ntext_only_decoder.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#according to summarized GPT2 model structure:\ntext_embed_dims = 768\nn_vocab = preprocessor.tokenizer.vocabulary_size()\npad_token = preprocessor.tokenizer.pad_token_id\nstart_packer = keras_nlp.layers.StartEndPacker(\n    sequence_length=seq_len,\n    start_value=None,\n)\nprompt_words_ = preprocessor.tokenizer([\"This image shows\"]) ; start_word_idx = len(prompt_words_[0]) \nprompt = start_packer(prompt_words_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modelling\n- building a single model that functions as a testbed;\n    - image encoder, text encoder(GPT2), cross-attentive attention layer로 구성\n    - 모델은 image와 raw text를 input으로 받고\n        - image -> encoder -> [CLS_token, encoded_patches, attention_weight] 3개의 output return\n        - raw text -> GPT2 -> encoded_text & get mask\n    - cross_attended_text = TransformerDecoder(query = raw_text, key = encoded_patches, value = encoded_patches, mask = mask) -> Dense -> Perplexity, accuracy 측정\n    - 위 과정과 cls_token 및 pooled encoded_text의 batchwise cross-correlation 구해서 contrastive accuracy 구하기","metadata":{}},{"cell_type":"code","source":"class MedicalCaptioner(keras.Model): \n    def __init__(self, image_encoder, \n                 preprocessor, text_encoder,\n                 att_heads = att_heads, att_dims = att_dims, att_depth = att_depth,\n                 **kwargs):\n        super().__init__(**kwargs)\n        self.image_encoder = image_encoder\n        self.text_encoder = text_encoder ; \n        self.text_preprocessor = preprocessor ; self.text_preprocessor.trainable = False\n        \n        self.mlp_text = keras.layers.Dense(units = n_vocab, activation = \"softmax\", name = \"VocabClassifier\", \n                                           kernel_regularizer=keras.regularizers.L1L2(l1=1e-5, l2=1e-4))\n        self.att_heads = att_heads\n        self.att_dims = att_dims\n        self.att_depth = att_depth\n        self.cross_attention_layers = [keras_nlp.layers.TransformerDecoder(att_dims, att_heads, name = f\"CrossMHADecoder{i+1}\",\n                                                                          dropout = 0.2, activation = \"gelu\", normalize_first = True) for i in range(att_depth)]\n        self.compute_perplexity = keras_nlp.metrics.Perplexity(mask_token_id=pad_token)\n        self.compute_accuracy = keras.metrics.SparseCategoricalAccuracy()\n        self.compute_softmax_loss = keras.losses.SparseCategoricalCrossentropy(ignore_class = pad_token, reduction = None)\n    def get_config(self):\n        return {\"Image encoder name\": self.image_encoder.name,\n               \"Text encoder name\" : self.text_encoder.name,\n               \"cross attention heads\" : att_heads, \"intermediate dims\" : self.att_dims, \"cross attention depth\" : self.att_depth\n               }\n    def call(self, image, text):\n        if len(self.image_encoder.outputs) == 2 :\n            image_token, encoded_patches = self.image_encoder(image)\n        elif len(self.image_encoder.outputs) == 3 :\n            image_token, encoded_patches, image_attention_weights = self.image_encoder(image)\n        elif len(self.image_encoder.outputs) == 1:\n            encoded_patches = self.image_encoder(image)\n        \n        if len(ops.shape(encoded_patches)) == 4:\n            _, w, h, dims = ops.shape(encoded_patches)\n            encoded_patches = ops.reshape(encoded_patches, [-1, w*h, dims])\n\n        preprocessed_text = self.text_preprocessor(text) ; text_mask = preprocessed_text[\"padding_mask\"]\n        original_token = preprocessed_text[\"token_ids\"]\n        encoded_text = self.text_encoder(preprocessed_text)\n        \n        # Cross-Attention\n        for idx, decoder in enumerate(self.cross_attention_layers):\n            encoded_text = decoder(decoder_sequence = encoded_text,\n                                  encoder_sequence = encoded_patches,\n                                  decoder_padding_mask = text_mask)\n        vocab_p = self.mlp_text(encoded_text)\n        \n        return vocab_p, original_token, text_mask\n    \n    def get_next_proba_fn(self, image, prompt):\n        \n        def next(prompt, cache, index):\n            prompt = self.text_preprocessor.tokenizer.detokenize(prompt)\n            vocab_proba = self(image, prompt)[0]\n            logits = vocab_proba[:, index - 1, :]\n            # Ignore hidden states for now; only needed for contrastive search.\n            hidden_states = None\n            return logits, hidden_states, cache\n        return next\n    def infer(self, image):\n        if len(ops.shape(image)) == 3:\n            image = image[tf.newaxis, ...]\n        \n        next_fn = self.get_next_proba_fn(image = image, prompt = prompt)\n        \n        greedy_sampler = keras_nlp.samplers.GreedySampler()\n        nuc_sampler = keras_nlp.samplers.TopPSampler(p=0.5, k = 10)\n        \n        greedy_tokens = greedy_sampler(next=next_fn,\n                                prompt=prompt,\n                                index=start_word_idx)\n        \n        nuc_tokens = nuc_sampler(next=next_fn,\n                                prompt=prompt,\n                                index=start_word_idx)\n        \n        greedy_words, nuc_words = self.text_preprocessor.tokenizer.detokenize(greedy_tokens), self.text_preprocessor.tokenizer.detokenize(nuc_tokens)\n        greedy_words, nuc_words = greedy_words.numpy(), nuc_words.numpy()\n        try:\n            greedy_words = [w.decode() for w in greedy_words]\n        except:\n            pass\n        try:\n            nuc_words = [w.decode() for w in nuc_words]\n        except:\n            nuc_words = \"\".join(str(nuc_words[0]))\n        return {\"greedy_sampling\" : greedy_words,\n               \"nucleus_sampling\" : nuc_words}\n    \n    def train_step(self, dataset): \n        image, text = dataset\n        with tf.GradientTape() as tape: \n            vocab_p, original_tokens, text_mask = self(image, text)\n            \n            loss = self.compute_softmax_loss(y_true = original_tokens, y_pred = vocab_p)\n            perplexity = self.compute_perplexity(y_true = original_tokens, y_pred = vocab_p)\n            accuracy = self.compute_accuracy(y_true = original_tokens, y_pred = vocab_p, sample_weight = text_mask)\n            loss = ops.mean(loss)\n            \n        grads = tape.gradient(loss, self.trainable_variables)\n        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n        \n        return {\"VocabSoftmaxLoss\" : loss,\n               \"Perplexity\" : perplexity,\n               \"TokenAccuracy\" : accuracy}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def wandb_config():\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    try:\n        secret_value_0 = user_secrets.get_secret(\"__gcloud_sdk_auth__\")\n        secret_value_1 = user_secrets.get_secret(\"huggingface_key\")\n        secret_value_2 = user_secrets.get_secret(\"wandb_key\")\n        !wandb login $secret_value_2\n    except:\n        secret_value_0 = user_secrets.get_secret(\"huggingface_key\")\n        secret_value_1 = user_secrets.get_secret(\"wandb_key\")\n        !wandb login $secret_value_1","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TrainingViz(keras.callbacks.Callback):\n    def __init__(self, run):\n        super().__init__()\n        self.run = run\n    def on_epoch_end(self, epoch, logs=None):\n        try:\n            origin = [\"Original Image\"]\n            col = [\"Original image\", \"Ground truth caption\", \"Greedy caption\", \"Nucleus caption\"]\n            visualize_data = []\n            for idx, original_image in tqdm(enumerate(val_images)):\n                origin_img = [wandb.Image(original_image)]\n                gt_caption = [val_texts[idx].numpy()]\n                outputs = self.model.infer(val_images[idx])\n                greedy = [outputs[\"greedy_sampling\"]]\n                nuc = [outputs[\"nucleus_sampling\"]]\n                tmp = origin_img + gt_caption + greedy + nuc\n                visualize_data.append(tmp)\n                del tmp, origin_img\n            tbl = wandb.Table(columns = col, data = visualize_data)\n            wandb.log({f\"Epoch{epoch+1}_result\": tbl})\n            del tbl\n            tf.keras.backend.clear_session()\n        except Exception as e: \n            print('visualization error', e)\n        \n    def on_train_batch_end(self, batch, logs=None):\n        if (batch % 1500 == 0) and (batch != 0):\n            if True:\n                origin = [\"Original Image\"]\n                col = [\"Original image\", \"Ground truth caption\", \"Greedy caption\", \"Nucleus caption\"]\n                visualize_data = []\n                len_ = 3 if batch == 0 else batch_size\n                for idx, original_image in tqdm(enumerate(val_images[:len_])):\n                    origin_img = [wandb.Image(original_image)]\n                    gt_caption = [val_texts[idx].numpy()]\n                    outputs = self.model.infer(val_images[idx])\n                    greedy = [outputs[\"greedy_sampling\"]]\n                    nuc = [outputs[\"nucleus_sampling\"]]\n                    tmp = origin_img + gt_caption + greedy + nuc\n                    visualize_data.append(tmp)\n                    del tmp, origin_img\n                tbl = wandb.Table(columns = col, data = visualize_data)\n                wandb.log({f\"batch{batch+1}_result\": tbl})\n                del tbl\n                tf.keras.backend.clear_session()\n            #except Exception as e: \n            #    print('visualization error', e)\n        else:\n            pass","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_exp(image_encoder, notes = None, exp_name = None, epochs = 1,\n           image_encoder_trainable = True):\n    image_encoder.trainable = image_encoder_trainable\n    model = MedicalCaptioner(image_encoder, preprocessor, text_only_decoder)\n    model.compile(optimizer = keras.optimizers.Adam(learning_rate = 1e-4),\n             jit_compile = False)\n    configs = model.get_config()\n    configs[\"finetune\"] = \"full fine tuning\" if image_encoder_trainable else 'frozen, zeroshot'\n    configs[\"batch size\"] = batch_size\n    configs[\"caption sequence length\"] = seq_len\n    try:\n        wandb.finish()\n    except:\n        pass\n    print(configs)\n    \n    if exp_name is None:\n        encname = configs[\"Image encoder name\"]\n        tune_ = configs[\"finetune\"]\n        exp_name = f\"{encname}_captioning_{tune_}\"\n    pass_error = keras.callbacks.TerminateOnNaN()\n    \n    \n    wandb_config()\n    run = wandb.init(project=\"Eval_RadImageNet_caption\", \n                         entity=\"gongbungkim\", config = configs, notes = notes,\n                        name = exp_name)\n    wb_callback = wandb.keras.WandbMetricsLogger(log_freq = 100)\n    vizcallback = TrainingViz(run)\n    callbacks = [pass_error, wb_callback, vizcallback]\n    hist = model.fit(train_ds, steps_per_epoch = train_steps, epochs = epochs, verbose = 1, callbacks = callbacks)\n    return hist, model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"run_exp(keras.applications.EfficientNetV2B1(include_top = False, input_shape = [res,res,3]),\n       epochs = 5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}