{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8501861,"sourceType":"datasetVersion","datasetId":5073985},{"sourceId":6074,"sourceType":"modelInstanceVersion","modelInstanceId":4694}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport random\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nseed = 2024\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# ML tools \n\nimport tensorflow as tf\nimport keras #; keras.config.set_dtype_policy(\"mixed_float16\")\nimport keras_nlp\nimport keras_cv\nfrom keras import ops\n\nkeras.utils.set_random_seed(seed)\n\nimport cv2\nimport tensorflow_io as tfio\nfrom kaggle_datasets import KaggleDatasets\nimport tensorflow_datasets as tfds\n\nfrom keras import Input, Model, layers\nfrom keras.models import load_model\nfrom keras.layers import Layer\nfrom keras.layers import Conv2D, DepthwiseConv2D, Dense, Activation, BatchNormalization, LayerNormalization, MultiHeadAttention, Embedding, Subtract, Add, Multiply, GlobalAveragePooling2D, GlobalAveragePooling1D, LayerNormalization\nfrom keras.preprocessing.image import load_img, img_to_array\nfrom keras.applications import *\nimport os, sys\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nfrom sklearn.model_selection import train_test_split\nfrom keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\nfrom tqdm.notebook import tqdm\nprint(f\"Requirements loaded, keras : v{keras.__version__}, Tensorflow : v{tf.__version__}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-07-15T07:56:36.962835Z","iopub.execute_input":"2024-07-15T07:56:36.963532Z","iopub.status.idle":"2024-07-15T07:56:56.903112Z","shell.execute_reply.started":"2024-07-15T07:56:36.963491Z","shell.execute_reply":"2024-07-15T07:56:56.902143Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-07-15 07:56:39.179151: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-15 07:56:39.179277: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-15 07:56:39.326230: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Requirements loaded, keras : v3.4.1, Tensorflow : v2.15.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Setting hyperparameters","metadata":{}},{"cell_type":"code","source":"# hyperparameters about images\nres = 384\nbatch_size = 16\n# hyperparameters about text\nseq_len = 128\n# hyperparameters about cross attentive decoder\natt_depth = 4\natt_heads = 8\natt_dims = att_heads * 32\nuse_bias = False\n\ntrain_cases = 70108\nval_cases = 9972\ntrain_steps = train_cases//batch_size\nval_steps = val_cases//batch_size\n\ntrain_dataset_dir = '/kaggle/input/roco-v2-tfrecord-dataset/Tot70108cases_RoCoV2_radiology_train_GZIP.tfrecord'\nval_dataset_dir = '/kaggle/input/roco-v2-tfrecord-dataset/Tot9972cases_RoCoV2_radiology_test_GZIP.tfrecord'","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-07-15T07:56:56.904657Z","iopub.execute_input":"2024-07-15T07:56:56.905198Z","iopub.status.idle":"2024-07-15T07:56:56.910855Z","shell.execute_reply.started":"2024-07-15T07:56:56.905172Z","shell.execute_reply":"2024-07-15T07:56:56.909998Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Parsing Dataset","metadata":{}},{"cell_type":"code","source":"def _parse_tfrecord(c, res = res):\n    def parse_tfrecord(tfrecord):\n        features = {'image': tf.io.FixedLenFeature([], tf.string),\n                    'report': tf.io.FixedLenFeature([], tf.string),\n                    }\n        x = tf.io.parse_single_example(tfrecord, features)\n        image_train = tf.image.decode_jpeg(x['image'], channels=c)\n        image_train = _transform_images(res = res)(image_train)\n        report = tf.cast(x[\"report\"], tf.string)\n        return image_train, report\n    \n    return parse_tfrecord\n\n\ndef _transform_images(res = res):\n    def transform_images(x_train):\n        x_train = tf.image.resize_with_pad(x_train, res, res, antialias = True)\n        x_train = tf.cast(x_train, tf.uint8)\n        return x_train\n    return transform_images\n\ndef load_tfrecord_dataset(tfrecord_name, res = res, batch_size = batch_size, shuffle=True, buffer_size=10240, grayscale = False):\n    \"\"\"load dataset from tfrecord\"\"\"\n    c = 1 if grayscale else 3\n    raw_dataset = tf.data.TFRecordDataset(tfrecord_name, compression_type = \"GZIP\")\n    raw_dataset = raw_dataset.repeat()\n    if shuffle:\n        raw_dataset = raw_dataset.shuffle(buffer_size=buffer_size)\n    dataset = raw_dataset.map(\n        _parse_tfrecord(c = c, res = res),\n        num_parallel_calls=tf.data.AUTOTUNE\n    )\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n    return dataset\n\ntrain_ds = load_tfrecord_dataset(train_dataset_dir)\nval_ds = load_tfrecord_dataset(val_dataset_dir) #image, report 2 outputs\n\nfor a, b in val_ds.take(1):\n    val_images = a\n    val_texts = b","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-07-15T07:56:56.912029Z","iopub.execute_input":"2024-07-15T07:56:56.912293Z","iopub.status.idle":"2024-07-15T07:57:10.139594Z","shell.execute_reply.started":"2024-07-15T07:56:56.912270Z","shell.execute_reply":"2024-07-15T07:57:10.138737Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Preparing Natural language decoder (pretrained on plain texts)","metadata":{}},{"cell_type":"code","source":"gpt_freeset = \"gpt2_base_en\"\ntext_only_decoder = keras_nlp.models.Backbone.from_preset(\n    gpt_freeset,\n    trainable=False,\n)\npreprocessor = keras_nlp.models.GPT2Preprocessor.from_preset(\n    gpt_freeset,\n    sequence_length=seq_len,\n    add_start_token = False\n)\npreprocessor.trainable = False\ntext_only_decoder.enable_lora(8)\ntext_only_decoder.summary()","metadata":{"execution":{"iopub.status.busy":"2024-07-15T07:57:10.141851Z","iopub.execute_input":"2024-07-15T07:57:10.142155Z","iopub.status.idle":"2024-07-15T07:57:26.081706Z","shell.execute_reply.started":"2024-07-15T07:57:10.142130Z","shell.execute_reply":"2024-07-15T07:57:26.080660Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Attaching 'model.safetensors' from model 'keras/gpt2/keras/gpt2_base_en/2' to your Kaggle notebook...\nAttaching 'model.safetensors.index.json' from model 'keras/gpt2/keras/gpt2_base_en/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/gpt2/keras/gpt2_base_en/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/gpt2/keras/gpt2_base_en/2' to your Kaggle notebook...\nAttaching 'config.json' from model 'keras/gpt2/keras/gpt2_base_en/2' to your Kaggle notebook...\nAttaching 'config.json' from model 'keras/gpt2/keras/gpt2_base_en/2' to your Kaggle notebook...\nAttaching 'model.weights.h5' from model 'keras/gpt2/keras/gpt2_base_en/2' to your Kaggle notebook...\nAttaching 'model.safetensors' from model 'keras/gpt2/keras/gpt2_base_en/2' to your Kaggle notebook...\nAttaching 'model.safetensors.index.json' from model 'keras/gpt2/keras/gpt2_base_en/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/gpt2/keras/gpt2_base_en/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/gpt2/keras/gpt2_base_en/2' to your Kaggle notebook...\nAttaching 'preprocessor.json' from model 'keras/gpt2/keras/gpt2_base_en/2' to your Kaggle notebook...\nAttaching 'tokenizer.json' from model 'keras/gpt2/keras/gpt2_base_en/2' to your Kaggle notebook...\nAttaching 'tokenizer.json' from model 'keras/gpt2/keras/gpt2_base_en/2' to your Kaggle notebook...\nAttaching 'assets/tokenizer/vocabulary.json' from model 'keras/gpt2/keras/gpt2_base_en/2' to your Kaggle notebook...\nAttaching 'assets/tokenizer/merges.txt' from model 'keras/gpt2/keras/gpt2_base_en/2' to your Kaggle notebook...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"gpt2_backbone\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gpt2_backbone\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ token_ids           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ token_embedding     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │ \u001b[38;5;34m38,597,376\u001b[0m │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mReversibleEmbeddi…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ position_embedding  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │    \u001b[38;5;34m786,432\u001b[0m │ token_embedding[\u001b[38;5;34m…\u001b[0m │\n│ (\u001b[38;5;33mPositionEmbedding\u001b[0m) │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ embeddings_add      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ token_embedding[\u001b[38;5;34m…\u001b[0m │\n│ (\u001b[38;5;33mAdd\u001b[0m)               │                   │            │ position_embeddi… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ embeddings_dropout  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ embeddings_add[\u001b[38;5;34m0\u001b[0m… │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ padding_mask        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ transformer_layer_0 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │  \u001b[38;5;34m7,236,352\u001b[0m │ embeddings_dropo… │\n│ (\u001b[38;5;33mTransformerDecode…\u001b[0m │                   │            │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ transformer_layer_1 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │  \u001b[38;5;34m7,236,352\u001b[0m │ transformer_laye… │\n│ (\u001b[38;5;33mTransformerDecode…\u001b[0m │                   │            │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ transformer_layer_2 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │  \u001b[38;5;34m7,236,352\u001b[0m │ transformer_laye… │\n│ (\u001b[38;5;33mTransformerDecode…\u001b[0m │                   │            │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ transformer_layer_3 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │  \u001b[38;5;34m7,236,352\u001b[0m │ transformer_laye… │\n│ (\u001b[38;5;33mTransformerDecode…\u001b[0m │                   │            │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ transformer_layer_4 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │  \u001b[38;5;34m7,236,352\u001b[0m │ transformer_laye… │\n│ (\u001b[38;5;33mTransformerDecode…\u001b[0m │                   │            │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ transformer_layer_5 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │  \u001b[38;5;34m7,236,352\u001b[0m │ transformer_laye… │\n│ (\u001b[38;5;33mTransformerDecode…\u001b[0m │                   │            │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ transformer_layer_6 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │  \u001b[38;5;34m7,236,352\u001b[0m │ transformer_laye… │\n│ (\u001b[38;5;33mTransformerDecode…\u001b[0m │                   │            │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ transformer_layer_7 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │  \u001b[38;5;34m7,236,352\u001b[0m │ transformer_laye… │\n│ (\u001b[38;5;33mTransformerDecode…\u001b[0m │                   │            │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ transformer_layer_8 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │  \u001b[38;5;34m7,236,352\u001b[0m │ transformer_laye… │\n│ (\u001b[38;5;33mTransformerDecode…\u001b[0m │                   │            │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ transformer_layer_9 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │  \u001b[38;5;34m7,236,352\u001b[0m │ transformer_laye… │\n│ (\u001b[38;5;33mTransformerDecode…\u001b[0m │                   │            │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ transformer_layer_… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │  \u001b[38;5;34m7,236,352\u001b[0m │ transformer_laye… │\n│ (\u001b[38;5;33mTransformerDecode…\u001b[0m │                   │            │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ transformer_layer_… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │  \u001b[38;5;34m7,236,352\u001b[0m │ transformer_laye… │\n│ (\u001b[38;5;33mTransformerDecode…\u001b[0m │                   │            │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_norm          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │      \u001b[38;5;34m1,536\u001b[0m │ transformer_laye… │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ token_ids           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ token_embedding     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │ <span style=\"color: #00af00; text-decoration-color: #00af00\">38,597,376</span> │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbeddi…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ position_embedding  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">786,432</span> │ token_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionEmbedding</span>) │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ embeddings_add      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ token_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │                   │            │ position_embeddi… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ embeddings_dropout  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embeddings_add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ padding_mask        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ transformer_layer_0 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">7,236,352</span> │ embeddings_dropo… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecode…</span> │                   │            │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ transformer_layer_1 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">7,236,352</span> │ transformer_laye… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecode…</span> │                   │            │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ transformer_layer_2 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">7,236,352</span> │ transformer_laye… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecode…</span> │                   │            │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ transformer_layer_3 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">7,236,352</span> │ transformer_laye… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecode…</span> │                   │            │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ transformer_layer_4 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">7,236,352</span> │ transformer_laye… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecode…</span> │                   │            │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ transformer_layer_5 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">7,236,352</span> │ transformer_laye… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecode…</span> │                   │            │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ transformer_layer_6 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">7,236,352</span> │ transformer_laye… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecode…</span> │                   │            │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ transformer_layer_7 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">7,236,352</span> │ transformer_laye… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecode…</span> │                   │            │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ transformer_layer_8 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">7,236,352</span> │ transformer_laye… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecode…</span> │                   │            │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ transformer_layer_9 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">7,236,352</span> │ transformer_laye… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecode…</span> │                   │            │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ transformer_layer_… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">7,236,352</span> │ transformer_laye… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecode…</span> │                   │            │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ transformer_layer_… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">7,236,352</span> │ transformer_laye… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecode…</span> │                   │            │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_norm          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ transformer_laye… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m126,221,568\u001b[0m (481.50 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">126,221,568</span> (481.50 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,800,192\u001b[0m (6.87 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,800,192</span> (6.87 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m124,421,376\u001b[0m (474.63 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">124,421,376</span> (474.63 MB)\n</pre>\n"},"metadata":{}}]},{"cell_type":"code","source":"#according to summarized GPT2 model structure:\ntext_embed_dims = 768\nn_vocab = preprocessor.tokenizer.vocabulary_size()\npad_token = preprocessor.tokenizer.pad_token_id\nstart_packer = keras_nlp.layers.StartEndPacker(\n    sequence_length=seq_len,\n    start_value=None,\n)\nprompt_words_ = preprocessor.tokenizer([\"This image shows\"]) ; start_word_idx = len(prompt_words_[0]) \nprompt = start_packer(prompt_words_)","metadata":{"execution":{"iopub.status.busy":"2024-07-15T07:57:26.082953Z","iopub.execute_input":"2024-07-15T07:57:26.083308Z","iopub.status.idle":"2024-07-15T07:57:27.823200Z","shell.execute_reply.started":"2024-07-15T07:57:26.083274Z","shell.execute_reply":"2024-07-15T07:57:27.822416Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Modelling\n- building a single model that functions as a testbed;\n    - image encoder, text encoder(GPT2), cross-attentive attention layer로 구성\n    - 모델은 image와 raw text를 input으로 받고\n        - image -> encoder -> [CLS_token, encoded_patches, attention_weight] 3개의 output return\n        - raw text -> GPT2 -> encoded_text & get mask\n    - cross_attended_text = TransformerDecoder(query = raw_text, key = encoded_patches, value = encoded_patches, mask = mask) -> Dense -> Perplexity, accuracy 측정\n    - 위 과정과 cls_token 및 pooled encoded_text의 batchwise cross-correlation 구해서 contrastive accuracy 구하기","metadata":{}},{"cell_type":"code","source":"class MedicalCaptioner(keras.Model): \n    def __init__(self, image_encoder, \n                 preprocessor, text_encoder,\n                 att_heads = att_heads, att_dims = att_dims, att_depth = att_depth,\n                 **kwargs):\n        super().__init__(**kwargs)\n        self.image_encoder = image_encoder\n        self.text_encoder = text_encoder ; \n        self.text_preprocessor = preprocessor ; self.text_preprocessor.trainable = False\n        \n        self.mlp_text = keras.layers.Dense(units = n_vocab, activation = \"softmax\", name = \"VocabClassifier\")\n        self.att_heads = att_heads\n        self.att_dims = att_dims\n        self.att_depth = att_depth\n        self.cross_attention_layers = [keras_nlp.layers.TransformerDecoder(att_dims, att_heads, name = f\"CrossMHADecoder{i+1}\",\n                                                                          dropout = 0.2, activation = \"gelu\") for i in range(att_depth)]\n        self.compute_perplexity = keras_nlp.metrics.Perplexity(mask_token_id=pad_token)\n        self.compute_accuracy = keras.metrics.SparseCategoricalAccuracy()\n        self.compute_softmax_loss = keras.losses.SparseCategoricalCrossentropy(ignore_class = pad_token, reduction = None)\n    def get_config(self):\n        return {\"Image encoder name\": self.image_encoder.name,\n               \"Text encoder name\" : self.text_encoder.name,\n               \"cross attention heads\" : att_heads, \"intermediate dims\" : self.att_dims, \"cross attention depth\" : self.att_depth\n               }\n    def call(self, image, text):\n        if len(self.image_encoder.outputs) == 2 :\n            image_token, encoded_patches = self.image_encoder(image)\n        elif len(self.image_encoder.outputs) == 3 :\n            image_token, encoded_patches, image_attention_weights = self.image_encoder(image)\n        elif len(self.image_encoder.outputs) == 1:\n            encoded_patches = self.image_encoder(image)\n        \n        if len(ops.shape(encoded_patches)) == 4:\n            _, w, h, dims = ops.shape(encoded_patches)\n            encoded_patches = ops.reshape(encoded_patches, [-1, w*h, dims])\n\n        preprocessed_text = self.text_preprocessor(text) ; text_mask = preprocessed_text[\"padding_mask\"]\n        original_token = preprocessed_text[\"token_ids\"]\n        encoded_text = self.text_encoder(preprocessed_text)\n        \n        # Cross-Attention\n        for idx, decoder in enumerate(self.cross_attention_layers):\n            encoded_text = decoder(decoder_sequence = encoded_text,\n                                  encoder_sequence = encoded_patches,\n                                  decoder_padding_mask = text_mask)\n        vocab_p = self.mlp_text(encoded_text)\n        \n        return vocab_p, original_token, text_mask\n    \n    def get_next_proba_fn(self, image, prompt):\n        \n        def next(prompt, cache, index):\n            prompt = self.text_preprocessor.tokenizer.detokenize(prompt)\n            vocab_proba = self(image, prompt)[0]\n            logits = vocab_proba[:, index - 1, :]\n            # Ignore hidden states for now; only needed for contrastive search.\n            hidden_states = None\n            return logits, hidden_states, cache\n        return next\n    def infer(self, image):\n        if len(ops.shape(image)) == 3:\n            image = image[tf.newaxis, ...]\n        next_fn = self.get_next_proba_fn(image = image, prompt = prompt)\n        \n        greedy_sampler = keras_nlp.samplers.GreedySampler()\n        nuc_sampler = keras_nlp.samplers.TopPSampler(p=0.5, k = 10)\n        \n        greedy_tokens = greedy_sampler(next=next_fn,\n                                prompt=prompt,\n                                index=start_word_idx)\n        \n        nuc_tokens = nuc_sampler(next=next_fn,\n                                prompt=prompt,\n                                index=start_word_idx)\n        \n        greedy_words, nuc_words = self.text_preprocessor.tokenizer.detokenize(greedy_tokens), self.text_preprocessor.tokenizer.detokenize(nuc_tokens)\n        greedy_words, nuc_words = greedy_words.numpy(), nuc_words.numpy()\n        greedy_words = [w.decode() for w in greedy_words]\n        nuc_words = [w.decode() for w in nuc_words]\n        return {\"greedy_sampling\" : greedy_words,\n               \"nucleus_sampling\" : nuc_words}\n    \n    def train_step(self, dataset): \n        image, text = dataset\n        with tf.GradientTape() as tape: \n            vocab_p, original_tokens, text_mask = self(image, text)\n            \n            loss = self.compute_softmax_loss(y_true = original_tokens, y_pred = vocab_p)\n            perplexity = self.compute_perplexity(y_true = original_tokens, y_pred = vocab_p)\n            accuracy = self.compute_accuracy(y_true = original_tokens, y_pred = vocab_p, sample_weight = text_mask)\n            loss = ops.mean(loss)\n            \n        grads = tape.gradient(loss, self.trainable_variables)\n        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n        \n        return {\"VocabSoftmaxLoss\" : loss,\n               \"Perplexity\" : perplexity,\n               \"TokenAccuracy\" : accuracy}","metadata":{"execution":{"iopub.status.busy":"2024-07-15T08:08:38.574505Z","iopub.execute_input":"2024-07-15T08:08:38.574902Z","iopub.status.idle":"2024-07-15T08:08:38.597542Z","shell.execute_reply.started":"2024-07-15T08:08:38.574871Z","shell.execute_reply":"2024-07-15T08:08:38.596567Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"#effnet = keras.applications.EfficientNetV2B0(include_top = False,\n#                                            input_shape = [res,res,3])\n#model = MedicalCaptioner(effnet, preprocessor, text_only_decoder)\n#model.compile(optimizer = keras.optimizers.AdamW(learning_rate = 1e-4),\n#             jit_compile = False)\n#model.fit(train_ds, epochs = 1, steps_per_epoch = 100)\n#model.infer(val_images[0]), val_texts[0]","metadata":{"execution":{"iopub.status.busy":"2024-07-15T08:08:41.320675Z","iopub.execute_input":"2024-07-15T08:08:41.321029Z","iopub.status.idle":"2024-07-15T08:14:42.952705Z","shell.execute_reply.started":"2024-07-15T08:08:41.321000Z","shell.execute_reply":"2024-07-15T08:14:42.951717Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 728ms/step - Perplexity: 13411.0898 - TokenAccuracy: 0.0585 - VocabSoftmaxLoss: 2.1031\n","output_type":"stream"},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"({'greedy_sampling': ['This image shows of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of..........<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|> (<|endoftext|><|endoftext|> ( ( ( ( ( ( ( ( (.. (.. ( (... (.. ( (.. ( ( (.. ( (.. ( ( ( ( ( ( ( ( ( ('],\n  'nucleus_sampling': ['This image shows in the) a,, (<|endoftext|>,arrow., a of (<|endoftext|> of.<|endoftext|>)arrow., (arrowarrow witharrow,,,- with. in<|endoftext|>arrow<|endoftext|> of of a<|endoftext|> of with<|endoftext|><|endoftext|>. in witharrow of--arrow (i. of in ( of (,arrow<|endoftext|>,arrow of<|endoftext|> with with) with in)<|endoftext|> of<|endoftext|>-)<|endoftext|> the) and<|endoftext|>- (.<|endoftext|> the of the., the- ofarrow,-),,.- aarrow,. of-<|endoftext|><|endoftext|>-,. a of in. in,,<|endoftext|> a']},\n <tf.Tensor: shape=(), dtype=string, numpy=b'Initial CT angiography chest demonstrating multifocal lung infiltrates.'>)"},"metadata":{}}]}]}